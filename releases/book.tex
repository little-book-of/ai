% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={The Little Book of Artificial Intelligence},
  pdfauthor={Duc-Tam Nguyen},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{The Little Book of Artificial Intelligence}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Version 0.1.2}
\author{Duc-Tam Nguyen}
\date{2025-09-18}
\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter{Contents}\label{contents}

\subsubsection{Volume 1 --- First Principles of
AI}\label{volume-1-first-principles-of-ai}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Defining Intelligence, Agents, and Environments
\item
  Objectives, Utility, and Reward
\item
  Information, Uncertainty, and Entropy
\item
  Computation, Complexity, and Limits
\item
  Representation and Abstraction
\item
  Learning vs.~Reasoning: Two Paths to Intelligence
\item
  Search, Optimization, and Decision-Making
\item
  Data, Signals, and Measurement
\item
  Evaluation: Ground Truth, Metrics, and Benchmarks
\item
  Reproducibility, Tooling, and the Scientific Method
\end{enumerate}

\subsubsection{Volume 2 --- Mathematical
Foundations}\label{volume-2-mathematical-foundations}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\tightlist
\item
  Linear Algebra for Representations
\item
  Differential and Integral Calculus
\item
  Probability Theory Fundamentals
\item
  Statistics and Estimation
\item
  Optimization and Convex Analysis
\item
  Numerical Methods and Stability
\item
  Information Theory
\item
  Graphs, Matrices, and Spectral Methods
\item
  Logic, Sets, and Proof Techniques
\item
  Stochastic Processes and Markov Chains
\end{enumerate}

\subsubsection{Volume 3 --- Data \&
Representation}\label{volume-3-data-representation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{20}
\tightlist
\item
  Data Lifecycle and Governance
\item
  Data Models: Tensors, Tables, Graphs
\item
  Feature Engineering and Encodings
\item
  Labeling, Annotation, and Weak Supervision
\item
  Sampling, Splits, and Experimental Design
\item
  Augmentation, Synthesis, and Simulation
\item
  Data Quality, Integrity, and Bias
\item
  Privacy, Security, and Anonymization
\item
  Datasets, Benchmarks, and Data Cards
\item
  Data Versioning and Lineage
\end{enumerate}

\subsubsection{Volume 4 --- Search \&
Planning}\label{volume-4-search-planning}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{30}
\tightlist
\item
  State Spaces and Problem Formulation
\item
  Uninformed Search (BFS, DFS, Iterative Deepening)
\item
  Informed Search (Heuristics, A*)
\item
  Constraint Satisfaction Problems
\item
  Local Search and Metaheuristics
\item
  Game Search and Adversarial Planning
\item
  Planning in Deterministic Domains
\item
  Probabilistic Planning and POMDPs
\item
  Scheduling and Resource Allocation
\item
  Meta-Reasoning and Anytime Algorithms
\end{enumerate}

\subsubsection{Volume 5 --- Logic \&
Knowledge}\label{volume-5-logic-knowledge}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{40}
\tightlist
\item
  Propositional and First-Order Logic
\item
  Knowledge Representation Schemes
\item
  Inference Engines and Theorem Proving
\item
  Ontologies and Knowledge Graphs
\item
  Description Logics and the Semantic Web
\item
  Default, Non-Monotonic, and Probabilistic Logic
\item
  Temporal, Modal, and Spatial Reasoning
\item
  Commonsense and Qualitative Reasoning
\item
  Neuro-Symbolic AI: Bridging Learning and Logic
\item
  Knowledge Acquisition and Maintenance
\end{enumerate}

\subsubsection{Volume 6 --- Probabilistic Modeling \&
Inference}\label{volume-6-probabilistic-modeling-inference}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{50}
\tightlist
\item
  Bayesian Inference Basics
\item
  Directed Graphical Models (Bayesian Networks)
\item
  Undirected Graphical Models (MRFs/CRFs)
\item
  Exact Inference (Variable Elimination, Junction Tree)
\item
  Approximate Inference (Sampling, Variational)
\item
  Latent Variable Models and EM
\item
  Sequential Models (HMMs, Kalman, Particle Filters)
\item
  Decision Theory and Influence Diagrams
\item
  Probabilistic Programming Languages
\item
  Calibration, Uncertainty Quantification, Reliability
\end{enumerate}

\subsubsection{Volume 7 --- Machine Learning Theory \&
Practice}\label{volume-7-machine-learning-theory-practice}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{60}
\tightlist
\item
  Hypothesis Spaces, Bias, and Capacity
\item
  Generalization, VC, Rademacher, PAC
\item
  Losses, Regularization, and Optimization
\item
  Model Selection, Cross-Validation, Bootstrapping
\item
  Linear and Generalized Linear Models
\item
  Kernel Methods and SVMs
\item
  Trees, Random Forests, Gradient Boosting
\item
  Feature Selection and Dimensionality Reduction
\item
  Imbalanced Data and Cost-Sensitive Learning
\item
  Evaluation, Error Analysis, and Debugging
\end{enumerate}

\subsubsection{Volume 8 --- Supervised Learning
Systems}\label{volume-8-supervised-learning-systems}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{70}
\tightlist
\item
  Regression: From Linear to Nonlinear
\item
  Classification: Binary, Multiclass, Multilabel
\item
  Structured Prediction (CRFs, Seq2Seq Basics)
\item
  Time Series and Forecasting
\item
  Tabular Modeling and Feature Stores
\item
  Hyperparameter Optimization and AutoML
\item
  Interpretability and Explainability (XAI)
\item
  Robustness, Adversarial Examples, Hardening
\item
  Deployment Patterns for Supervised Models
\item
  Monitoring, Drift, and Lifecycle Management
\end{enumerate}

\subsubsection{Volume 9 --- Unsupervised, Self-Supervised \&
Representation}\label{volume-9-unsupervised-self-supervised-representation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{80}
\tightlist
\item
  Clustering (k-Means, Hierarchical, DBSCAN)
\item
  Density Estimation and Mixture Models
\item
  Matrix Factorization and NMF
\item
  Dimensionality Reduction (PCA, t-SNE, UMAP)
\item
  Manifold Learning and Topological Methods
\item
  Topic Models and Latent Dirichlet Allocation
\item
  Autoencoders and Representation Learning
\item
  Contrastive and Self-Supervised Learning
\item
  Anomaly and Novelty Detection
\item
  Graph Representation Learning
\end{enumerate}

\subsubsection{Volume 10 --- Deep Learning
Core}\label{volume-10-deep-learning-core}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{90}
\tightlist
\item
  Computational Graphs and Autodiff
\item
  Backpropagation and Initialization
\item
  Optimizers (SGD, Momentum, Adam, etc.)
\item
  Regularization (Dropout, Norms, Batch/Layer Norm)
\item
  Convolutional Networks and Inductive Biases
\item
  Recurrent Networks and Sequence Models
\item
  Attention Mechanisms and Transformers
\item
  Architecture Patterns and Design Spaces
\item
  Training at Scale (Parallelism, Mixed Precision)
\item
  Failure Modes, Debugging, Evaluation
\end{enumerate}

\subsubsection{Volume 11 --- Large Language
Models}\label{volume-11-large-language-models}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{100}
\tightlist
\item
  Tokenization, Subwords, and Embeddings
\item
  Transformer Architecture Deep Dive
\item
  Pretraining Objectives (MLM, CLM, SFT)
\item
  Scaling Laws and Data/Compute Tradeoffs
\item
  Instruction Tuning, RLHF, and RLAIF
\item
  Parameter-Efficient Tuning (Adapters, LoRA)
\item
  Retrieval-Augmented Generation (RAG) and Memory
\item
  Tool Use, Function Calling, and Agents
\item
  Evaluation, Safety, and Prompting Strategies
\item
  Production LLM Systems and Cost Optimization
\end{enumerate}

\subsubsection{Volume 12 --- Computer
Vision}\label{volume-12-computer-vision}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{110}
\tightlist
\item
  Image Formation and Preprocessing
\item
  ConvNets for Recognition
\item
  Object Detection and Tracking
\item
  Segmentation and Scene Understanding
\item
  3D Vision and Geometry
\item
  Self-Supervised and Foundation Models for Vision
\item
  Vision Transformers and Hybrid Models
\item
  Multimodal Vision-Language (VL) Models
\item
  Datasets, Metrics, and Benchmarks
\item
  Real-World Vision Systems and Edge Deployment
\end{enumerate}

\subsubsection{Volume 13 --- Natural Language
Processing}\label{volume-13-natural-language-processing}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{120}
\tightlist
\item
  Linguistic Foundations (Morphology, Syntax, Semantics)
\item
  Classical NLP (n-Grams, HMMs, CRFs)
\item
  Word and Sentence Embeddings
\item
  Sequence-to-Sequence and Attention
\item
  Machine Translation and Multilingual NLP
\item
  Question Answering and Information Retrieval
\item
  Summarization and Text Generation
\item
  Prompting, In-Context Learning, Program Induction
\item
  Evaluation, Bias, and Toxicity in NLP
\item
  Low-Resource, Code, and Domain-Specific NLP
\end{enumerate}

\subsubsection{Volume 14 --- Speech \& Audio
Intelligence}\label{volume-14-speech-audio-intelligence}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{130}
\tightlist
\item
  Signal Processing and Feature Extraction
\item
  Automatic Speech Recognition (CTC, Transducers)
\item
  Text-to-Speech and Voice Conversion
\item
  Speaker Identification and Diarization
\item
  Music Information Retrieval
\item
  Audio Event Detection and Scene Analysis
\item
  Prosody, Emotion, and Paralinguistics
\item
  Multimodal Audio-Visual Learning
\item
  Robustness to Noise, Accents, Reverberation
\item
  Real-Time and On-Device Audio AI
\end{enumerate}

\subsubsection{Volume 15 --- Reinforcement
Learning}\label{volume-15-reinforcement-learning}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{140}
\tightlist
\item
  Markov Decision Processes and Bellman Equations
\item
  Dynamic Programming and Planning
\item
  Monte Carlo and Temporal-Difference Learning
\item
  Value-Based Methods (DQN and Variants)
\item
  Policy Gradients and Actor-Critic
\item
  Exploration, Intrinsic Motivation, Bandits
\item
  Model-Based RL and World Models
\item
  Multi-Agent RL and Games
\item
  Offline RL, Safety, and Constraints
\item
  RL in the Wild: Sim2Real and Applications
\end{enumerate}

\subsubsection{Volume 16 --- Robotics \& Embodied
AI}\label{volume-16-robotics-embodied-ai}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{150}
\tightlist
\item
  Kinematics, Dynamics, and Control
\item
  Perception for Robotics
\item
  SLAM and Mapping
\item
  Motion Planning and Trajectory Optimization
\item
  Grasping and Manipulation
\item
  Locomotion and Balance
\item
  Human-Robot Interaction and Collaboration
\item
  Simulation, Digital Twins, Domain Randomization
\item
  Learning for Manipulation and Navigation
\item
  System Integration and Real-World Deployment
\end{enumerate}

\subsubsection{Volume 17 --- Causality, Reasoning \&
Science}\label{volume-17-causality-reasoning-science}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{160}
\tightlist
\item
  Causal Graphs, SCMs, and Do-Calculus
\item
  Identification, Estimation, and Transportability
\item
  Counterfactuals and Mediation
\item
  Causal Discovery from Observational Data
\item
  Experiment Design, A/B/n Testing, Uplift
\item
  Time Series Causality and Granger
\item
  Scientific ML and Differentiable Physics
\item
  Symbolic Regression and Program Synthesis
\item
  Automated Theorem Proving and Formal Methods
\item
  Limits, Fallacies, and Robust Scientific Practice
\end{enumerate}

\subsubsection{Volume 18 --- AI Systems, MLOps \&
Infrastructure}\label{volume-18-ai-systems-mlops-infrastructure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{170}
\tightlist
\item
  Data Engineering and Feature Stores
\item
  Experiment Tracking and Reproducibility
\item
  Training Orchestration and Scheduling
\item
  Distributed Training and Parallelism
\item
  Model Packaging, Serving, and APIs
\item
  Monitoring, Telemetry, and Observability
\item
  Drift, Feedback Loops, Continuous Learning
\item
  Privacy, Security, and Model Governance
\item
  Cost, Efficiency, and Green AI
\item
  Platform Architecture and Team Practices
\end{enumerate}

\subsubsection{Volume 19 --- Multimodality, Tools \&
Agents}\label{volume-19-multimodality-tools-agents}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{180}
\tightlist
\item
  Multimodal Pretraining and Alignment
\item
  Cross-Modal Retrieval and Fusion
\item
  Vision-Language-Action Models
\item
  Memory, Datastores, and RAG Systems
\item
  Tool Use, Function APIs, and Plugins
\item
  Planning, Decomposition, Toolformer-Style Agents
\item
  Multi-Agent Simulation and Coordination
\item
  Evaluation of Agents and Emergent Behavior
\item
  Human-in-the-Loop and Interactive Systems
\item
  Case Studies: Assistants, Copilots, Autonomy
\end{enumerate}

\subsubsection{Volume 20 --- Ethics, Safety, Governance \&
Futures}\label{volume-20-ethics-safety-governance-futures}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{190}
\tightlist
\item
  Ethical Frameworks and Principles
\item
  Fairness, Bias, and Inclusion
\item
  Privacy, Surveillance, and Consent
\item
  Robustness, Reliability, and Safety Engineering
\item
  Alignment, Preference Learning, and Control
\item
  Misuse, Abuse, and Red-Teaming
\item
  Law, Regulation, and International Policy
\item
  Economic Impacts, Labor, and Society
\item
  Education, Healthcare, and Public Goods
\item
  Roadmaps, Open Problems, and Future Scenarios
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Volume 1. First principles of Artificial
Intelligence}\label{volume-1.-first-principles-of-artificial-intelligence}

\section{Chapter 1. Defining Ingelligence, Agents, and
Environments}\label{chapter-1.-defining-ingelligence-agents-and-environments}

\subsection{1. What do we mean by
``intelligence''?}\label{what-do-we-mean-by-intelligence}

Intelligence is the capacity to achieve goals across a wide variety of
environments. In AI, it means designing systems that can perceive,
reason, and act effectively, even under uncertainty. Unlike narrow
programs built for one fixed task, intelligence implies adaptability and
generalization.

\subsubsection{Picture in Your Head}\label{picture-in-your-head}

Think of a skilled traveler arriving in a new city. They don't just
follow one rigid script---they observe the signs, ask questions, and
adjust plans when the bus is late or the route is blocked. An
intelligent system works the same way: it navigates new situations by
combining perception, reasoning, and action.

\subsubsection{Deep Dive}\label{deep-dive}

Researchers debate whether intelligence should be defined by behavior,
internal mechanisms, or measurable outcomes.

\begin{itemize}
\tightlist
\item
  Behavioral definitions focus on observable success in tasks (e.g.,
  solving puzzles, playing games).
\item
  Cognitive definitions emphasize processes like reasoning, planning,
  and learning.
\item
  Formal definitions often turn to frameworks like rational agents:
  entities that choose actions to maximize expected utility.
\end{itemize}

A challenge is that intelligence is multi-dimensional---logical
reasoning, creativity, social interaction, and physical dexterity are
all aspects. No single metric fully captures it, but unifying themes
include adaptability, generalization, and goal-directed behavior.

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1712}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2703}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2613}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2973}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Perspective
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Emphasis
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Behavioral & Task performance & Chess-playing programs & May not
generalize beyond task \\
Cognitive & Reasoning, planning, learning & Cognitive architectures &
Hard to measure directly \\
Formal (agent view) & Maximizing expected utility & Reinforcement
learning agents & Depends heavily on utility design \\
Human analogy & Mimicking human-like abilities & Conversational
assistants & Anthropomorphism can mislead \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# A toy "intelligent agent" choosing actions}
\ImportTok{import}\NormalTok{ random}

\NormalTok{goals }\OperatorTok{=}\NormalTok{ [}\StringTok{"find food"}\NormalTok{, }\StringTok{"avoid danger"}\NormalTok{, }\StringTok{"explore"}\NormalTok{]}
\NormalTok{environment }\OperatorTok{=}\NormalTok{ [}\StringTok{"food nearby"}\NormalTok{, }\StringTok{"predator spotted"}\NormalTok{, }\StringTok{"unknown terrain"}\NormalTok{]}

\KeywordTok{def}\NormalTok{ choose\_action(env):}
    \ControlFlowTok{if} \StringTok{"food"} \KeywordTok{in}\NormalTok{ env:}
        \ControlFlowTok{return} \StringTok{"eat"}
    \ControlFlowTok{elif} \StringTok{"predator"} \KeywordTok{in}\NormalTok{ env:}
        \ControlFlowTok{return} \StringTok{"hide"}
    \ControlFlowTok{else}\NormalTok{:}
        \ControlFlowTok{return}\NormalTok{ random.choice([}\StringTok{"move forward"}\NormalTok{, }\StringTok{"observe"}\NormalTok{, }\StringTok{"rest"}\NormalTok{])}

\ControlFlowTok{for}\NormalTok{ situation }\KeywordTok{in}\NormalTok{ environment:}
\NormalTok{    action }\OperatorTok{=}\NormalTok{ choose\_action(situation)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Environment: }\SpecialCharTok{\{}\NormalTok{situation}\SpecialCharTok{\}}\SpecialStringTok{ {-}\textgreater{} Action: }\SpecialCharTok{\{}\NormalTok{action}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add new environments (e.g., ``ally detected'') and define how the
  agent should act.
\item
  Introduce conflicting goals (e.g., explore vs.~avoid danger) and
  create simple rules for trade-offs.
\item
  Reflect: does this toy model capture intelligence, or only a narrow
  slice of it?
\end{enumerate}

\subsection{2. Agents as entities that perceive and
act}\label{agents-as-entities-that-perceive-and-act}

An agent is anything that can perceive its environment through sensors
and act upon that environment through actuators. In AI, the agent
framework provides a clean abstraction: inputs come from the world,
outputs affect the world, and the cycle continues. This framing allows
us to model everything from a thermostat to a robot to a trading
algorithm as an agent.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-1}

Imagine a robot with eyes (cameras), ears (microphones), and wheels. The
robot sees an obstacle, hears a sound, and decides to turn left. It
takes in signals, processes them, and sends commands back out. That
perception--action loop defines what it means to be an agent.

\subsubsection{Deep Dive}\label{deep-dive-1}

Agents can be categorized by their complexity and decision-making
ability:

\begin{itemize}
\tightlist
\item
  Simple reflex agents act directly on current perceptions (if obstacle
  → turn).
\item
  Model-based agents maintain an internal representation of the world.
\item
  Goal-based agents plan actions to achieve objectives.
\item
  Utility-based agents optimize outcomes according to preferences.
\end{itemize}

This hierarchy illustrates increasing sophistication: from reactive
behaviors to deliberate reasoning and optimization. Modern AI systems
often combine multiple levels---deep learning for perception, symbolic
models for planning, and reinforcement learning for utility
maximization.

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1226}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2642}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2736}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3396}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type of Agent
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
How It Works
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Reflex & Condition → Action rules & Vacuum that turns at walls & Cannot
handle unseen situations \\
Model-based & Maintains internal state & Self-driving car localization &
Needs accurate, updated model \\
Goal-based & Chooses actions for outcomes & Path planning in robotics &
Requires explicit goal specification \\
Utility-based & Maximizes preferences & Trading algorithm & Success
depends on utility design \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-1}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simple reflex agent: if obstacle detected, turn}
\KeywordTok{def}\NormalTok{ reflex\_agent(percept):}
    \ControlFlowTok{if}\NormalTok{ percept }\OperatorTok{==} \StringTok{"obstacle"}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{"turn left"}
    \ControlFlowTok{else}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{"move forward"}

\NormalTok{percepts }\OperatorTok{=}\NormalTok{ [}\StringTok{"clear"}\NormalTok{, }\StringTok{"obstacle"}\NormalTok{, }\StringTok{"clear"}\NormalTok{]}
\ControlFlowTok{for}\NormalTok{ p }\KeywordTok{in}\NormalTok{ percepts:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Percept: }\SpecialCharTok{\{}\NormalTok{p}\SpecialCharTok{\}}\SpecialStringTok{ {-}\textgreater{} Action: }\SpecialCharTok{\{}\NormalTok{reflex\_agent(p)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Extend the agent to include a goal, such as ``reach destination,'' and
  modify the rules.
\item
  Add state: track whether the agent has already turned left, and
  prevent repeated turns.
\item
  Reflect on how increasing complexity (state, goals, utilities)
  improves generality but adds design challenges.
\end{enumerate}

\subsection{3. The role of environments in shaping
behavior}\label{the-role-of-environments-in-shaping-behavior}

An environment defines the context in which an agent operates. It
supplies the inputs the agent perceives, the consequences of the agent's
actions, and the rules of interaction. AI systems cannot be understood
in isolation---their intelligence is always relative to the environment
they inhabit.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-2}

Think of a fish in a tank. The fish swims, but the glass walls, water,
plants, and currents determine what is possible and how hard certain
movements are. Likewise, an agent's ``tank'' is its environment, shaping
its behavior and success.

\subsubsection{Deep Dive}\label{deep-dive-2}

Environments can be characterized along several dimensions:

\begin{itemize}
\tightlist
\item
  Observable vs.~partially observable: whether the agent sees the full
  state or just partial glimpses.
\item
  Deterministic vs.~stochastic: whether actions lead to predictable
  outcomes or probabilistic ones.
\item
  Static vs.~dynamic: whether the environment changes on its own or only
  when the agent acts.
\item
  Discrete vs.~continuous: whether states and actions are finite steps
  or smooth ranges.
\item
  Single-agent vs.~multi-agent: whether others also influence outcomes.
\end{itemize}

These properties determine the difficulty of building agents. A chess
game is deterministic and fully observable, while real-world driving is
stochastic, dynamic, continuous, and multi-agent. Designing intelligent
behavior means tailoring methods to the environment's structure.

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2039}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1553}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3010}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3398}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Environment Dimension
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example (Simple)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example (Complex)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Implication for AI
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Observable & Chess board & Poker game & Hidden info requires
inference \\
Deterministic & Tic-tac-toe & Weather forecasting & Uncertainty needs
probabilities \\
Static & Crossword puzzle & Stock market & Must adapt to constant
change \\
Discrete & Board games & Robotics control & Continuous control needs
calculus \\
Single-agent & Maze navigation & Autonomous driving with traffic &
Coordination and competition matter \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-2}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Environment: simple grid world}
\KeywordTok{class}\NormalTok{ GridWorld:}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, size}\OperatorTok{=}\DecValTok{3}\NormalTok{):}
        \VariableTok{self}\NormalTok{.size }\OperatorTok{=}\NormalTok{ size}
        \VariableTok{self}\NormalTok{.agent\_pos }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{]}
    
    \KeywordTok{def}\NormalTok{ step(}\VariableTok{self}\NormalTok{, action):}
        \ControlFlowTok{if}\NormalTok{ action }\OperatorTok{==} \StringTok{"right"} \KeywordTok{and} \VariableTok{self}\NormalTok{.agent\_pos[}\DecValTok{0}\NormalTok{] }\OperatorTok{\textless{}} \VariableTok{self}\NormalTok{.size }\OperatorTok{{-}} \DecValTok{1}\NormalTok{:}
            \VariableTok{self}\NormalTok{.agent\_pos[}\DecValTok{0}\NormalTok{] }\OperatorTok{+=} \DecValTok{1}
        \ControlFlowTok{elif}\NormalTok{ action }\OperatorTok{==} \StringTok{"down"} \KeywordTok{and} \VariableTok{self}\NormalTok{.agent\_pos[}\DecValTok{1}\NormalTok{] }\OperatorTok{\textless{}} \VariableTok{self}\NormalTok{.size }\OperatorTok{{-}} \DecValTok{1}\NormalTok{:}
            \VariableTok{self}\NormalTok{.agent\_pos[}\DecValTok{1}\NormalTok{] }\OperatorTok{+=} \DecValTok{1}
        \ControlFlowTok{return} \BuiltInTok{tuple}\NormalTok{(}\VariableTok{self}\NormalTok{.agent\_pos)}

\NormalTok{env }\OperatorTok{=}\NormalTok{ GridWorld()}
\NormalTok{actions }\OperatorTok{=}\NormalTok{ [}\StringTok{"right"}\NormalTok{, }\StringTok{"down"}\NormalTok{, }\StringTok{"right"}\NormalTok{]}
\ControlFlowTok{for}\NormalTok{ a }\KeywordTok{in}\NormalTok{ actions:}
\NormalTok{    pos }\OperatorTok{=}\NormalTok{ env.step(a)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Action: }\SpecialCharTok{\{}\NormalTok{a}\SpecialCharTok{\}}\SpecialStringTok{ {-}\textgreater{} Position: }\SpecialCharTok{\{}\NormalTok{pos}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Change the grid to include obstacles---how does that alter the agent's
  path?
\item
  Add randomness to actions (e.g., a 10\% chance of slipping). Does the
  agent still reach its goal reliably?
\item
  Compare this toy world to real environments---what complexities are
  missing, and why do they matter?
\end{enumerate}

\subsection{4. Inputs, outputs, and feedback
loops}\label{inputs-outputs-and-feedback-loops}

An agent exists in a constant exchange with its environment: it receives
inputs, produces outputs, and adjusts based on the results. This cycle
is known as a feedback loop. Intelligence emerges not from isolated
decisions but from continuous interaction---perception, action, and
adaptation.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-3}

Picture a thermostat in a house. It senses the temperature (input),
decides whether to switch on heating or cooling (processing), and
changes the temperature (output). The altered temperature is then sensed
again, completing the loop. The same principle scales from thermostats
to autonomous robots and learning systems.

\subsubsection{Deep Dive}\label{deep-dive-3}

Feedback loops are fundamental to control theory, cybernetics, and AI.
Key ideas include:

\begin{itemize}
\tightlist
\item
  Open-loop systems: act without monitoring results (e.g., a microwave
  runs for a fixed time).
\item
  Closed-loop systems: adjust based on feedback (e.g., cruise control in
  cars).
\item
  Positive feedback: amplifies changes (e.g., recommendation engines
  reinforcing popularity).
\item
  Negative feedback: stabilizes systems (e.g., homeostasis in biology).
\end{itemize}

For AI, well-designed feedback loops enable adaptation and stability.
Poorly designed ones can cause runaway effects, bias reinforcement, or
instability.

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1300}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3200}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Feedback Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
How It Works
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Risk or Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Open-loop & No correction from output & Batch script that ignores errors
& Fails if environment changes \\
Closed-loop & Adjusts using feedback & Robot navigation with sensors &
Slower if feedback is delayed \\
Positive & Amplifies signal & Viral content recommendation & Can lead to
echo chambers \\
Negative & Stabilizes system & PID controller in robotics & May suppress
useful variations \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-3}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Closed{-}loop temperature controller}
\NormalTok{desired\_temp }\OperatorTok{=} \DecValTok{22}
\NormalTok{current\_temp }\OperatorTok{=} \DecValTok{18}

\KeywordTok{def}\NormalTok{ thermostat(current):}
    \ControlFlowTok{if}\NormalTok{ current }\OperatorTok{\textless{}}\NormalTok{ desired\_temp:}
        \ControlFlowTok{return} \StringTok{"heat on"}
    \ControlFlowTok{elif}\NormalTok{ current }\OperatorTok{\textgreater{}}\NormalTok{ desired\_temp:}
        \ControlFlowTok{return} \StringTok{"cool on"}
    \ControlFlowTok{else}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{"idle"}

\ControlFlowTok{for}\NormalTok{ t }\KeywordTok{in}\NormalTok{ [}\DecValTok{18}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{24}\NormalTok{]:}
\NormalTok{    action }\OperatorTok{=}\NormalTok{ thermostat(t)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Temperature: }\SpecialCharTok{\{}\NormalTok{t}\SpecialCharTok{\}}\SpecialStringTok{°C {-}\textgreater{} Action: }\SpecialCharTok{\{}\NormalTok{action}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add noise to the temperature readings and see if the controller still
  stabilizes.
\item
  Modify the code to overshoot intentionally---what happens if heating
  continues after the target is reached?
\item
  Reflect on large-scale AI: where do feedback loops appear in social
  media, finance, or autonomous driving?
\end{enumerate}

\subsection{5. Rationality, bounded rationality, and
satisficing}\label{rationality-bounded-rationality-and-satisficing}

Rationality in AI means selecting the action that maximizes expected
performance given the available knowledge. However, real agents face
limits---computational power, time, and incomplete information. This
leads to bounded rationality: making good-enough decisions under
constraints. Often, agents satisfice (pick the first acceptable
solution) instead of optimizing perfectly.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-4}

Imagine grocery shopping with only ten minutes before the store closes.
You could, in theory, calculate the optimal shopping route through every
aisle. But in practice, you grab what you need in a reasonable order and
head to checkout. That's bounded rationality and satisficing at work.

\subsubsection{Deep Dive}\label{deep-dive-4}

\begin{itemize}
\tightlist
\item
  Perfect rationality assumes unlimited information, time, and
  computation---rarely possible in reality.
\item
  Bounded rationality (Herbert Simon's idea) acknowledges constraints
  and focuses on feasible choices.
\item
  Satisficing means picking an option that meets minimum criteria, not
  necessarily the absolute best.
\item
  In AI, heuristics, approximations, and greedy algorithms embody these
  ideas, enabling systems to act effectively in complex or
  time-sensitive domains.
\end{itemize}

This balance between ideal and practical rationality is central to AI
design. Systems must achieve acceptable performance within real-world
limits.

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1681}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2832}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2389}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3097}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Perfect rationality & Always chooses optimal action & Dynamic
programming solvers & Computationally infeasible at scale \\
Bounded rationality & Chooses under time/info limits & Heuristic search
(A*) & May miss optimal solutions \\
Satisficing & Picks first ``good enough'' option & Greedy algorithms &
Quality depends on threshold chosen \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-4}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Satisficing: pick the first option above a threshold}
\NormalTok{options }\OperatorTok{=}\NormalTok{ \{}\StringTok{"A"}\NormalTok{: }\FloatTok{0.6}\NormalTok{, }\StringTok{"B"}\NormalTok{: }\FloatTok{0.9}\NormalTok{, }\StringTok{"C"}\NormalTok{: }\FloatTok{0.7}\NormalTok{\}  }\CommentTok{\# scores for actions}
\NormalTok{threshold }\OperatorTok{=} \FloatTok{0.75}

\KeywordTok{def}\NormalTok{ satisficing(choices, threshold):}
    \ControlFlowTok{for}\NormalTok{ action, score }\KeywordTok{in}\NormalTok{ choices.items():}
        \ControlFlowTok{if}\NormalTok{ score }\OperatorTok{\textgreater{}=}\NormalTok{ threshold:}
            \ControlFlowTok{return}\NormalTok{ action}
    \ControlFlowTok{return} \StringTok{"no good option"}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Chosen action:"}\NormalTok{, satisficing(options, threshold))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-4}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Lower or raise the threshold---does the agent choose differently?
\item
  Shuffle the order of options---how does satisficing depend on
  ordering?
\item
  Compare results to an ``optimal'' strategy that always picks the
  highest score.
\end{enumerate}

\subsection{6. Goals, objectives, and adaptive
behavior}\label{goals-objectives-and-adaptive-behavior}

Goals give direction to an agent's behavior. Without goals, actions are
random or reflexive; with goals, behavior becomes purposeful. Objectives
translate goals into measurable targets, while adaptive behavior ensures
that agents can adjust their strategies when environments or goals
change.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-5}

Think of a GPS navigator. The goal is to reach a destination. The
objective is to minimize travel time. If a road is closed, the system
adapts by rerouting. This cycle---setting goals, pursuing objectives,
and adapting along the way---is central to intelligence.

\subsubsection{Deep Dive}\label{deep-dive-5}

\begin{itemize}
\tightlist
\item
  Goals: broad desired outcomes (e.g., ``deliver package'').
\item
  Objectives: quantifiable or operationalized targets (e.g., ``arrive in
  under 30 minutes'').
\item
  Adaptive behavior: the ability to change plans when obstacles arise.
\item
  Goal hierarchies: higher-level goals (stay safe) may constrain
  lower-level ones (move fast).
\item
  Multi-objective trade-offs: agents often balance efficiency, safety,
  cost, and fairness simultaneously.
\end{itemize}

Effective AI requires encoding not just static goals but also
flexibility---anticipating uncertainty and adjusting course as
conditions change.

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1478}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2522}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2696}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3304}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Element
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Challenge
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Goal & Desired outcome & Reach target location & May be vague or
high-level \\
Objective & Concrete, measurable target & Minimize travel time &
Requires careful specification \\
Adaptive behavior & Adjusting actions dynamically & Rerouting in
autonomous driving & Complexity grows with uncertainty \\
Goal hierarchy & Layered priorities & Safety \textgreater{} speed in
robotics & Conflicting priorities hard to resolve \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-5}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Adaptive goal pursuit}
\ImportTok{import}\NormalTok{ random}

\NormalTok{goal }\OperatorTok{=} \StringTok{"reach destination"}
\NormalTok{path }\OperatorTok{=}\NormalTok{ [}\StringTok{"road1"}\NormalTok{, }\StringTok{"road2"}\NormalTok{, }\StringTok{"road3"}\NormalTok{]}

\KeywordTok{def}\NormalTok{ travel(path):}
    \ControlFlowTok{for}\NormalTok{ road }\KeywordTok{in}\NormalTok{ path:}
        \ControlFlowTok{if}\NormalTok{ random.random() }\OperatorTok{\textless{}} \FloatTok{0.3}\NormalTok{:  }\CommentTok{\# simulate blockage}
            \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{road}\SpecialCharTok{\}}\SpecialStringTok{ blocked {-}\textgreater{} adapting route"}\NormalTok{)}
            \ControlFlowTok{continue}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Taking }\SpecialCharTok{\{}\NormalTok{road}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
        \ControlFlowTok{return} \StringTok{"destination reached"}
    \ControlFlowTok{return} \StringTok{"failed"}

\BuiltInTok{print}\NormalTok{(travel(path))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-5}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Change the blockage probability and observe how often the agent adapts
  successfully.
\item
  Add multiple goals (e.g., reach fast vs.~stay safe) and design rules
  to prioritize them.
\item
  Reflect: how do human goals shift when resources, risks, or
  preferences change?
\end{enumerate}

\subsection{7. Reactive vs.~deliberative
agents}\label{reactive-vs.-deliberative-agents}

Reactive agents respond immediately to stimuli without explicit
planning, while deliberative agents reason about the future before
acting. This distinction highlights two modes of intelligence: reflexive
speed versus thoughtful foresight. Most practical AI systems blend both
approaches.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-6}

Imagine driving a car. When a ball suddenly rolls into the street, you
react instantly by braking---this is reactive behavior. But planning a
road trip across the country, considering fuel stops and hotels,
requires deliberation. Intelligent systems must know when to be quick
and when to be thoughtful.

\subsubsection{Deep Dive}\label{deep-dive-6}

\begin{itemize}
\tightlist
\item
  Reactive agents: simple, fast, and robust in well-structured
  environments. They follow condition--action rules and excel in
  time-critical situations.
\item
  Deliberative agents: maintain models of the world, reason about
  possible futures, and plan sequences of actions. They handle complex,
  novel problems but require more computation.
\item
  Hybrid approaches: most real-world AI (e.g., robotics) combines
  reactive layers (for safety and reflexes) with deliberative layers
  (for planning and optimization).
\item
  Trade-offs: reactivity gives speed but little foresight; deliberation
  gives foresight but can stall in real time.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1165}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2621}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2816}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3398}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Agent Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Characteristics
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Reactive & Fast, rule-based, reflexive & Collision-avoidance in drones &
Shortsighted, no long-term planning \\
Deliberative & Model-based, plans ahead & Path planning in robotics &
Computationally expensive \\
Hybrid & Combines both layers & Self-driving cars & Integration
complexity \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-6}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Reactive vs. deliberative decision}
\ImportTok{import}\NormalTok{ random}

\KeywordTok{def}\NormalTok{ reactive\_agent(percept):}
    \ControlFlowTok{if}\NormalTok{ percept }\OperatorTok{==} \StringTok{"obstacle"}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{"turn"}
    \ControlFlowTok{return} \StringTok{"forward"}

\KeywordTok{def}\NormalTok{ deliberative\_agent(goal, options):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Planning for goal: }\SpecialCharTok{\{}\NormalTok{goal}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{return} \BuiltInTok{min}\NormalTok{(options, key}\OperatorTok{=}\KeywordTok{lambda}\NormalTok{ x: x[}\StringTok{"cost"}\NormalTok{])[}\StringTok{"action"}\NormalTok{]}

\CommentTok{\# Demo}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Reactive:"}\NormalTok{, reactive\_agent(}\StringTok{"obstacle"}\NormalTok{))}
\NormalTok{options }\OperatorTok{=}\NormalTok{ [\{}\StringTok{"action"}\NormalTok{: }\StringTok{"path1"}\NormalTok{, }\StringTok{"cost"}\NormalTok{: }\DecValTok{5}\NormalTok{\}, \{}\StringTok{"action"}\NormalTok{: }\StringTok{"path2"}\NormalTok{, }\StringTok{"cost"}\NormalTok{: }\DecValTok{2}\NormalTok{\}]}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Deliberative:"}\NormalTok{, deliberative\_agent(}\StringTok{"reach target"}\NormalTok{, options))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-6}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add more options to the deliberative agent and see how planning
  scales.
\item
  Simulate time pressure: what happens if the agent must decide in one
  step?
\item
  Design a hybrid agent: use reactive behavior for emergencies,
  deliberative planning for long-term goals.
\end{enumerate}

\subsection{8. Embodied, situated, and distributed
intelligence}\label{embodied-situated-and-distributed-intelligence}

Intelligence is not just about abstract computation---it is shaped by
the body it resides in (embodiment), the context it operates within
(situatedness), and how it interacts with others (distribution). These
perspectives highlight that intelligence emerges from the interaction
between mind, body, and world.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-7}

Picture a colony of ants. Each ant has limited abilities, but together
they forage, build, and defend. Their intelligence is distributed across
the colony. Now imagine a robot with wheels instead of legs---it solves
problems differently than a robot with arms. The shape of the body and
the environment it acts in fundamentally shape the form of intelligence.

\subsubsection{Deep Dive}\label{deep-dive-7}

\begin{itemize}
\tightlist
\item
  Embodied intelligence: The physical form influences cognition. A
  flying drone and a ground rover require different strategies for
  navigation.
\item
  Situated intelligence: Knowledge is tied to specific contexts. A
  chatbot trained for customer service behaves differently from one in
  medical triage.
\item
  Distributed intelligence: Multiple agents collaborate or compete,
  producing collective outcomes greater than individuals alone. Swarm
  robotics, sensor networks, and human-AI teams illustrate this
  principle.
\item
  These dimensions remind us that intelligence is not universal---it is
  adapted to bodies, places, and social structures.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1009}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2477}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3119}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3394}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Dimension
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Focus
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Key Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Embodied & Physical form shapes action & Humanoid robots vs.~drones &
Constrained by hardware design \\
Situated & Context-specific behavior & Chatbot for finance
vs.~healthcare & May fail when moved to new domain \\
Distributed & Collective problem-solving & Swarm robotics, multi-agent
games & Coordination overhead, emergent risks \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-7}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Distributed decision: majority voting among agents}
\NormalTok{agents }\OperatorTok{=}\NormalTok{ [}
    \KeywordTok{lambda}\NormalTok{: }\StringTok{"left"}\NormalTok{,}
    \KeywordTok{lambda}\NormalTok{: }\StringTok{"right"}\NormalTok{,}
    \KeywordTok{lambda}\NormalTok{: }\StringTok{"left"}
\NormalTok{]}

\NormalTok{votes }\OperatorTok{=}\NormalTok{ [agent() }\ControlFlowTok{for}\NormalTok{ agent }\KeywordTok{in}\NormalTok{ agents]}
\NormalTok{decision }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(}\BuiltInTok{set}\NormalTok{(votes), key}\OperatorTok{=}\NormalTok{votes.count)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Agents voted:"}\NormalTok{, votes)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Final decision:"}\NormalTok{, decision)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-7}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add more agents with different preferences---how stable is the final
  decision?
\item
  Replace majority voting with weighted votes---does it change outcomes?
\item
  Reflect on how embodiment, situatedness, and distribution might affect
  AI safety and robustness.
\end{enumerate}

\subsection{9. Comparing human, animal, and machine
intelligence}\label{comparing-human-animal-and-machine-intelligence}

Human intelligence, animal intelligence, and machine intelligence share
similarities but differ in mechanisms and scope. Humans excel in
abstract reasoning and language, animals demonstrate remarkable
adaptation and instinctive behaviors, while machines process vast data
and computations at scale. Studying these comparisons reveals both
inspirations for AI and its limitations.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-8}

Imagine three problem-solvers faced with the same task: finding food. A
human might draw a map and plan a route. A squirrel remembers where it
buried nuts last season and uses its senses to locate them. A search
engine crawls databases and retrieves relevant entries in milliseconds.
Each is intelligent, but in different ways.

\subsubsection{Deep Dive}\label{deep-dive-8}

\begin{itemize}
\item
  Human intelligence: characterized by symbolic reasoning, creativity,
  theory of mind, and cultural learning.
\item
  Animal intelligence: often domain-specific, optimized for survival
  tasks like navigation, hunting, or communication. Crows use tools,
  dolphins cooperate, bees dance to share information.
\item
  Machine intelligence: excels at pattern recognition, optimization, and
  brute-force computation, but lacks embodied experience, emotions, and
  intrinsic motivation.
\item
  Comparative insights:

  \begin{itemize}
  \tightlist
  \item
    Machines often mimic narrow aspects of human or animal cognition.
  \item
    Biological intelligence evolved under resource constraints, while
    machines rely on energy and data availability.
  \item
    Hybrid systems may combine strengths---machine speed with human
    judgment.
  \end{itemize}
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1217}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2783}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2783}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3217}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Dimension
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Human Intelligence
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Animal Intelligence
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Machine Intelligence
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Strength & Abstract reasoning, language & Instinct, adaptation,
perception & Scale, speed, data processing \\
Limitation & Cognitive biases, limited memory & Narrow survival domains
& Lacks common sense, embodiment \\
Learning Style & Culture, education, symbols & Evolution, imitation,
instinct & Data-driven algorithms \\
Example & Solving math proofs & Birds using tools & Neural networks for
image recognition \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-8}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Toy comparison: three "agents" solving a food search}
\ImportTok{import}\NormalTok{ random}

\KeywordTok{def}\NormalTok{ human\_agent():}
    \ControlFlowTok{return} \StringTok{"plans route to food"}

\KeywordTok{def}\NormalTok{ animal\_agent():}
    \ControlFlowTok{return}\NormalTok{ random.choice([}\StringTok{"sniffs trail"}\NormalTok{, }\StringTok{"remembers cache"}\NormalTok{])}

\KeywordTok{def}\NormalTok{ machine\_agent():}
    \ControlFlowTok{return} \StringTok{"queries database for food location"}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Human:"}\NormalTok{, human\_agent())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Animal:"}\NormalTok{, animal\_agent())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Machine:"}\NormalTok{, machine\_agent())}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-8}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Expand the code with success/failure rates---who finds food fastest or
  most reliably?
\item
  Add constraints (e.g., limited memory for humans, noisy signals for
  animals, incomplete data for machines).
\item
  Reflect: can machines ever achieve the flexibility of humans or the
  embodied instincts of animals?
\end{enumerate}

\subsection{10. Open challenges in defining AI
precisely}\label{open-challenges-in-defining-ai-precisely}

Despite decades of progress, there is still no single, universally
accepted definition of artificial intelligence. Definitions range from
engineering goals (``machines that act intelligently'') to philosophical
ambitions (``machines that think like humans''). The lack of consensus
reflects the diversity of approaches, applications, and expectations in
the field.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-9}

Imagine trying to define ``life.'' Biologists debate whether viruses
count, and new discoveries constantly stretch boundaries. AI is similar:
chess programs, chatbots, self-driving cars, and generative models all
qualify to some, but not to others. The borders of AI shift with each
breakthrough.

\subsubsection{Deep Dive}\label{deep-dive-9}

\begin{itemize}
\item
  Shifting goalposts: Once a task is automated, it is often no longer
  considered AI (``AI is whatever hasn't been done yet'').
\item
  Multiple perspectives:

  \begin{itemize}
  \tightlist
  \item
    Human-like: AI as machines imitating human thought or behavior.
  \item
    Rational agent: AI as systems that maximize expected performance.
  \item
    Tool-based: AI as advanced statistical and optimization methods.
  \end{itemize}
\item
  Cultural differences: Western AI emphasizes autonomy and competition,
  while Eastern perspectives often highlight harmony and augmentation.
\item
  Practical consequence: Without a precise definition, policy, safety,
  and evaluation frameworks must be flexible yet principled.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1416}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2832}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2743}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3009}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Perspective
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition of AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Human-like & Machines that think/act like us & Turing Test, chatbots &
Anthropomorphic and vague \\
Rational agent & Systems maximizing performance & Reinforcement learning
agents & Overly formal, utility design hard \\
Tool-based & Advanced computation techniques & Neural networks,
optimization & Reduces AI to ``just math'' \\
Cultural framing & Varies by society and philosophy & Augmenting
vs.~replacing humans & Hard to unify globally \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-9}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Toy illustration: classify "is this AI?"}
\NormalTok{systems }\OperatorTok{=}\NormalTok{ [}\StringTok{"calculator"}\NormalTok{, }\StringTok{"chess engine"}\NormalTok{, }\StringTok{"chatbot"}\NormalTok{, }\StringTok{"robot vacuum"}\NormalTok{]}

\KeywordTok{def}\NormalTok{ is\_ai(system):}
    \ControlFlowTok{if}\NormalTok{ system }\KeywordTok{in}\NormalTok{ [}\StringTok{"chatbot"}\NormalTok{, }\StringTok{"robot vacuum"}\NormalTok{, }\StringTok{"chess engine"}\NormalTok{]:}
        \ControlFlowTok{return} \VariableTok{True}
    \ControlFlowTok{return} \VariableTok{False}  \CommentTok{\# debatable, depends on definition}

\ControlFlowTok{for}\NormalTok{ s }\KeywordTok{in}\NormalTok{ systems:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{s}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\StringTok{\textquotesingle{}AI\textquotesingle{}} \ControlFlowTok{if}\NormalTok{ is\_ai(s) }\ControlFlowTok{else} \StringTok{\textquotesingle{}not AI?\textquotesingle{}}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-9}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Change the definition in the code (e.g., ``anything that adapts''
  vs.~``anything that learns'').
\item
  Add new systems like ``search engine'' or ``autopilot''---do they
  count?
\item
  Reflect: does the act of redefining AI highlight why consensus is so
  elusive?
\end{enumerate}

\section{Chapter 2. Objective, Utility, and
Reward}\label{chapter-2.-objective-utility-and-reward}

\subsection{11. Objectives as drivers of intelligent
behavior}\label{objectives-as-drivers-of-intelligent-behavior}

Objectives give an agent a sense of purpose. They specify what outcomes
are desirable and shape how the agent evaluates choices. Without
objectives, an agent has no basis for preferring one action over
another; with objectives, every decision can be judged as better or
worse.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-10}

Think of playing chess without trying to win---it would just be random
moves. But once you set the objective ``checkmate the opponent,'' every
action gains meaning. The same principle holds for AI: objectives
transform arbitrary behaviors into purposeful ones.

\subsubsection{Deep Dive}\label{deep-dive-10}

\begin{itemize}
\tightlist
\item
  Explicit objectives: encoded directly (e.g., maximize score, minimize
  error).
\item
  Implicit objectives: emerge from training data (e.g., language models
  learning next-word prediction).
\item
  Single vs.~multiple objectives: agents may have one clear goal or need
  to balance many (e.g., safety, efficiency, fairness).
\item
  Objective specification problem: poorly defined objectives can lead to
  unintended behaviors, like reward hacking.
\item
  Research frontier: designing objectives aligned with human values
  while remaining computationally tractable.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1570}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2975}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2893}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2562}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Benefit
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Risk / Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Explicit objective & Minimize classification error & Transparent, easy
to measure & Narrow, may ignore side effects \\
Implicit objective & Predict next token in language model & Emerges
naturally from data & Hard to interpret or adjust \\
Single objective & Maximize profit in trading agent & Clear optimization
target & May ignore fairness or risk \\
Multiple objectives & Self-driving car (safe, fast, legal) & Balanced
performance across domains & Conflicts hard to resolve \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-10}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Toy agent choosing based on objective scores}
\NormalTok{actions }\OperatorTok{=}\NormalTok{ \{}\StringTok{"drive\_fast"}\NormalTok{: \{}\StringTok{"time"}\NormalTok{: }\FloatTok{0.9}\NormalTok{, }\StringTok{"safety"}\NormalTok{: }\FloatTok{0.3}\NormalTok{\},}
           \StringTok{"drive\_safe"}\NormalTok{: \{}\StringTok{"time"}\NormalTok{: }\FloatTok{0.5}\NormalTok{, }\StringTok{"safety"}\NormalTok{: }\FloatTok{0.9}\NormalTok{\}\}}

\KeywordTok{def}\NormalTok{ score(action, weights):}
    \ControlFlowTok{return} \BuiltInTok{sum}\NormalTok{(action[k] }\OperatorTok{*}\NormalTok{ w }\ControlFlowTok{for}\NormalTok{ k, w }\KeywordTok{in}\NormalTok{ weights.items())}

\NormalTok{weights }\OperatorTok{=}\NormalTok{ \{}\StringTok{"time"}\NormalTok{: }\FloatTok{0.4}\NormalTok{, }\StringTok{"safety"}\NormalTok{: }\FloatTok{0.6}\NormalTok{\}  }\CommentTok{\# prioritize safety}
\NormalTok{scores }\OperatorTok{=}\NormalTok{ \{a: score(v, weights) }\ControlFlowTok{for}\NormalTok{ a, v }\KeywordTok{in}\NormalTok{ actions.items()\}}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Chosen action:"}\NormalTok{, }\BuiltInTok{max}\NormalTok{(scores, key}\OperatorTok{=}\NormalTok{scores.get))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-10}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Change the weights---what happens if speed is prioritized over safety?
\item
  Add more objectives (e.g., fuel cost) and see how choices shift.
\item
  Reflect on real-world risks: what if objectives are misaligned with
  human intent?
\end{enumerate}

\subsection{12. Utility functions and preference
modeling}\label{utility-functions-and-preference-modeling}

A utility function assigns a numerical score to outcomes, allowing an
agent to compare and rank them. Preference modeling captures how agents
(or humans) value different possibilities. Together, they formalize the
idea of ``what is better,'' enabling systematic decision-making under
uncertainty.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-11}

Imagine choosing dinner. Pizza, sushi, and salad each have different
appeal depending on your mood. A utility function is like giving each
option a score---pizza 8, sushi 9, salad 6---and then picking the
highest. Machines use the same logic to decide among actions.

\subsubsection{Deep Dive}\label{deep-dive-11}

\begin{itemize}
\tightlist
\item
  Utility theory: provides a mathematical foundation for rational
  choice.
\item
  Cardinal utilities: assign measurable values (e.g., expected profit).
\item
  Ordinal preferences: only rank outcomes without assigning numbers.
\item
  AI applications: reinforcement learning agents maximize expected
  reward, recommender systems model user preferences, and
  multi-objective agents weigh competing utilities.
\item
  Challenges: human preferences are dynamic, inconsistent, and
  context-dependent, making them hard to capture precisely.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1538}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2735}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2650}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3077}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Cardinal utility & Numeric values of outcomes & RL reward functions &
Sensitive to design errors \\
Ordinal preference & Ranking outcomes without numbers & Search engine
rankings & Lacks intensity of preferences \\
Learned utility & Model inferred from data & Collaborative filtering
systems & May reflect bias in data \\
Multi-objective & Balancing several utilities & Autonomous vehicle
trade-offs & Conflicting objectives hard to solve \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-11}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preference modeling with a utility function}
\NormalTok{options }\OperatorTok{=}\NormalTok{ \{}\StringTok{"pizza"}\NormalTok{: }\DecValTok{8}\NormalTok{, }\StringTok{"sushi"}\NormalTok{: }\DecValTok{9}\NormalTok{, }\StringTok{"salad"}\NormalTok{: }\DecValTok{6}\NormalTok{\}}

\KeywordTok{def}\NormalTok{ choose\_best(options):}
    \ControlFlowTok{return} \BuiltInTok{max}\NormalTok{(options, key}\OperatorTok{=}\NormalTok{options.get)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Chosen option:"}\NormalTok{, choose\_best(options))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-11}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add randomness to reflect mood swings---does the choice change?
\item
  Expand to multi-objective utilities (taste + health + cost).
\item
  Reflect on how preference modeling affects fairness, bias, and
  alignment in AI systems.
\end{enumerate}

\subsection{13. Rewards, signals, and
incentives}\label{rewards-signals-and-incentives}

Rewards are feedback signals that tell an agent how well it is doing
relative to its objectives. Incentives structure these signals to guide
long-term behavior. In AI, rewards are the currency of learning: they
connect actions to outcomes and shape the strategies agents develop.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-12}

Think of training a dog. A treat after sitting on command is a reward.
Over time, the dog learns to connect the action (sit) with the outcome
(treat). AI systems learn in a similar way, except their ``treats'' are
numbers from a reward function.

\subsubsection{Deep Dive}\label{deep-dive-12}

\begin{itemize}
\tightlist
\item
  Rewards vs.~objectives: rewards are immediate signals, while
  objectives define long-term goals.
\item
  Sparse vs.~dense rewards: sparse rewards give feedback only at the end
  (winning a game), while dense rewards provide step-by-step guidance.
\item
  Shaping incentives: carefully designed reward functions can encourage
  exploration, cooperation, or fairness.
\item
  Pitfalls: misaligned incentives can lead to unintended behavior, such
  as reward hacking (agents exploiting loopholes in the reward
  definition).
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1545}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3091}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2273}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3091}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Benefit
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Risk / Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Sparse reward & ``+1 if win, else 0'' in a game & Simple,
outcome-focused & Harder to learn intermediate steps \\
Dense reward & Points for each correct move & Easier credit assignment &
May bias toward short-term gains \\
Incentive shaping & Bonus for exploration in RL & Encourages broader
search & Can distort intended objective \\
Misaligned reward & Agent learns to exploit a loophole & Reveals design
flaws & Dangerous or useless behaviors \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-12}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Reward signal shaping}
\KeywordTok{def}\NormalTok{ reward(action):}
    \ControlFlowTok{if}\NormalTok{ action }\OperatorTok{==} \StringTok{"win"}\NormalTok{:}
        \ControlFlowTok{return} \DecValTok{10}
    \ControlFlowTok{elif}\NormalTok{ action }\OperatorTok{==} \StringTok{"progress"}\NormalTok{:}
        \ControlFlowTok{return} \DecValTok{1}
    \ControlFlowTok{else}\NormalTok{:}
        \ControlFlowTok{return} \DecValTok{0}

\NormalTok{actions }\OperatorTok{=}\NormalTok{ [}\StringTok{"progress"}\NormalTok{, }\StringTok{"progress"}\NormalTok{, }\StringTok{"win"}\NormalTok{]}
\NormalTok{total }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(reward(a) }\ControlFlowTok{for}\NormalTok{ a }\KeywordTok{in}\NormalTok{ actions)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Total reward:"}\NormalTok{, total)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-12}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add a ``cheat'' action with artificially high reward---what happens?
\item
  Change dense rewards to sparse rewards---does the agent still learn
  effectively?
\item
  Reflect: how do incentives in AI mirror incentives in human society,
  markets, or ecosystems?
\end{enumerate}

\subsection{14. Aligning objectives with desired
outcomes}\label{aligning-objectives-with-desired-outcomes}

An AI system is only as good as its objective design. If objectives are
poorly specified, agents may optimize for the wrong thing. Aligning
objectives with real-world desired outcomes is central to safe and
reliable AI. This problem is known as the alignment problem.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-13}

Imagine telling a robot vacuum to ``clean as fast as possible.'' It
might respond by pushing dirt under the couch instead of actually
cleaning. The objective (speed) is met, but the outcome (a clean room)
is not. This gap between specification and intent defines the alignment
challenge.

\subsubsection{Deep Dive}\label{deep-dive-13}

\begin{itemize}
\item
  Specification problem: translating human values and goals into
  machine-readable objectives.
\item
  Proxy objectives: often we measure what's easy (clicks, likes) instead
  of what we really want (knowledge, well-being).
\item
  Goodhart's Law: when a measure becomes a target, it ceases to be a
  good measure.
\item
  Solutions under study:

  \begin{itemize}
  \tightlist
  \item
    Human-in-the-loop learning (reinforcement learning from feedback).
  \item
    Multi-objective optimization to capture trade-offs.
  \item
    Interpretability to check whether objectives are truly met.
  \item
    Iterative refinement as objectives evolve.
  \end{itemize}
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1653}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2810}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2562}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2975}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Issue
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Risk
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Possible Mitigation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Mis-specified reward & Robot cleans faster by hiding dirt & Optimizes
wrong behavior & Better proxy metrics, human feedback \\
Proxy objective & Maximizing clicks on content & Promotes clickbait, not
quality & Multi-metric optimization \\
Over-optimization & Tuning too strongly to benchmark & Exploits quirks,
not true skill & Regularization, diverse evaluations \\
Value misalignment & Self-driving car optimizes speed & Safety
violations & Encode constraints, safety checks \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-13}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Misaligned vs. aligned objectives}
\KeywordTok{def}\NormalTok{ score(action):}
    \CommentTok{\# Proxy objective: speed}
    \ControlFlowTok{if}\NormalTok{ action }\OperatorTok{==} \StringTok{"finish\_fast"}\NormalTok{:}
        \ControlFlowTok{return} \DecValTok{10}
    \CommentTok{\# True desired outcome: clean thoroughly}
    \ControlFlowTok{elif}\NormalTok{ action }\OperatorTok{==} \StringTok{"clean\_well"}\NormalTok{:}
        \ControlFlowTok{return} \DecValTok{8}
    \ControlFlowTok{else}\NormalTok{:}
        \ControlFlowTok{return} \DecValTok{0}

\NormalTok{actions }\OperatorTok{=}\NormalTok{ [}\StringTok{"finish\_fast"}\NormalTok{, }\StringTok{"clean\_well"}\NormalTok{]}
\ControlFlowTok{for}\NormalTok{ a }\KeywordTok{in}\NormalTok{ actions:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Action: }\SpecialCharTok{\{}\NormalTok{a}\SpecialCharTok{\}}\SpecialStringTok{, Score: }\SpecialCharTok{\{}\NormalTok{score(a)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-13}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add a ``cheat'' action like ``hide dirt''---how does the scoring
  system respond?
\item
  Introduce multiple objectives (speed + cleanliness) and balance them
  with weights.
\item
  Reflect on real-world AI: how often do incentives focus on proxies
  (clicks, time spent) instead of true goals?
\end{enumerate}

\subsection{15. Conflicting objectives and
trade-offs}\label{conflicting-objectives-and-trade-offs}

Real-world agents rarely pursue a single objective. They must balance
competing goals: safety vs.~speed, accuracy vs.~efficiency, fairness
vs.~profitability. These conflicts make trade-offs inevitable, and
designing AI requires explicit strategies to manage them.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-14}

Think of cooking dinner. You want the meal to be tasty, healthy, and
quick. Focusing only on speed might mean instant noodles; focusing only
on health might mean a slow, complex recipe. Compromise---perhaps a
stir-fry---is the art of balancing objectives. AI faces the same
dilemma.

\subsubsection{Deep Dive}\label{deep-dive-14}

\begin{itemize}
\tightlist
\item
  Multi-objective optimization: agents evaluate several metrics
  simultaneously.
\item
  Pareto optimality: a solution is Pareto optimal if no objective can be
  improved without worsening another.
\item
  Weighted sums: assign relative importance to each objective (e.g.,
  70\% safety, 30\% speed).
\item
  Dynamic trade-offs: priorities may shift over time or across contexts.
\item
  Challenge: trade-offs often reflect human values, making technical
  design an ethical question.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2478}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2566}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2301}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2655}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Conflict
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Trade-off Strategy
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Safety vs.~efficiency & Self-driving cars & Weight safety higher & May
reduce user satisfaction \\
Accuracy vs.~speed & Real-time speech recognition & Use approximate
models & Lower quality results \\
Fairness vs.~profit & Loan approval systems & Apply fairness constraints
& Possible revenue reduction \\
Exploration vs.~exploitation & Reinforcement learning agents & ε-greedy
or UCB strategies & Needs careful parameter tuning \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-14}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Multi{-}objective scoring with weights}
\NormalTok{options }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"fast"}\NormalTok{: \{}\StringTok{"time"}\NormalTok{: }\FloatTok{0.9}\NormalTok{, }\StringTok{"safety"}\NormalTok{: }\FloatTok{0.4}\NormalTok{\},}
    \StringTok{"safe"}\NormalTok{: \{}\StringTok{"time"}\NormalTok{: }\FloatTok{0.5}\NormalTok{, }\StringTok{"safety"}\NormalTok{: }\FloatTok{0.9}\NormalTok{\},}
    \StringTok{"balanced"}\NormalTok{: \{}\StringTok{"time"}\NormalTok{: }\FloatTok{0.7}\NormalTok{, }\StringTok{"safety"}\NormalTok{: }\FloatTok{0.7}\NormalTok{\}}
\NormalTok{\}}

\NormalTok{weights }\OperatorTok{=}\NormalTok{ \{}\StringTok{"time"}\NormalTok{: }\FloatTok{0.4}\NormalTok{, }\StringTok{"safety"}\NormalTok{: }\FloatTok{0.6}\NormalTok{\}}

\KeywordTok{def}\NormalTok{ score(option, weights):}
    \ControlFlowTok{return} \BuiltInTok{sum}\NormalTok{(option[k] }\OperatorTok{*}\NormalTok{ w }\ControlFlowTok{for}\NormalTok{ k, w }\KeywordTok{in}\NormalTok{ weights.items())}

\NormalTok{scores }\OperatorTok{=}\NormalTok{ \{k: score(v, weights) }\ControlFlowTok{for}\NormalTok{ k, v }\KeywordTok{in}\NormalTok{ options.items()\}}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Best choice:"}\NormalTok{, }\BuiltInTok{max}\NormalTok{(scores, key}\OperatorTok{=}\NormalTok{scores.get))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-14}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Change the weights to prioritize speed over safety---how does the
  outcome shift?
\item
  Add more conflicting objectives, such as cost or fairness.
\item
  Reflect: who should decide the weights---engineers, users, or
  policymakers?
\end{enumerate}

\subsection{16. Temporal aspects: short-term vs.~long-term
goals}\label{temporal-aspects-short-term-vs.-long-term-goals}

Intelligent agents must consider time when pursuing objectives.
Short-term goals focus on immediate rewards, while long-term goals
emphasize delayed outcomes. Balancing the two is crucial: chasing only
immediate gains can undermine future success, but focusing only on the
long run may ignore urgent needs.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-15}

Imagine studying for an exam. Watching videos online provides instant
pleasure (short-term reward), but studying builds knowledge that pays
off later (long-term reward). Smart choices weigh both---enjoy some
breaks while still preparing for the exam.

\subsubsection{Deep Dive}\label{deep-dive-15}

\begin{itemize}
\tightlist
\item
  Myopic agents: optimize only for immediate payoff, often failing in
  environments with delayed rewards.
\item
  Far-sighted agents: value future outcomes, but may overcommit to
  uncertain futures.
\item
  Discounting: future rewards are typically weighted less (e.g.,
  exponential discounting in reinforcement learning).
\item
  Temporal trade-offs: real-world systems, like healthcare AI, must
  optimize both immediate patient safety and long-term outcomes.
\item
  Challenge: setting the right balance depends on context, risk, and
  values.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1842}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3684}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4474}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Short-Term Focus
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Long-Term Focus
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Reward horizon & Immediate payoff & Delayed benefits \\
Example in AI & Online ad click optimization & Drug discovery with years
of delay \\
Strength & Quick responsiveness & Sustainable outcomes \\
Weakness & Shortsighted, risky & Slow, computationally demanding \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-15}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Balancing short vs. long{-}term rewards}
\NormalTok{rewards }\OperatorTok{=}\NormalTok{ \{}\StringTok{"actionA"}\NormalTok{: \{}\StringTok{"short"}\NormalTok{: }\DecValTok{5}\NormalTok{, }\StringTok{"long"}\NormalTok{: }\DecValTok{2}\NormalTok{\},}
           \StringTok{"actionB"}\NormalTok{: \{}\StringTok{"short"}\NormalTok{: }\DecValTok{2}\NormalTok{, }\StringTok{"long"}\NormalTok{: }\DecValTok{8}\NormalTok{\}\}}

\NormalTok{discount }\OperatorTok{=} \FloatTok{0.8}  \CommentTok{\# value future less than present}

\KeywordTok{def}\NormalTok{ value(action, discount):}
    \ControlFlowTok{return}\NormalTok{ action[}\StringTok{"short"}\NormalTok{] }\OperatorTok{+}\NormalTok{ discount }\OperatorTok{*}\NormalTok{ action[}\StringTok{"long"}\NormalTok{]}

\NormalTok{values }\OperatorTok{=}\NormalTok{ \{a: value(r, discount) }\ControlFlowTok{for}\NormalTok{ a, r }\KeywordTok{in}\NormalTok{ rewards.items()\}}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Chosen action:"}\NormalTok{, }\BuiltInTok{max}\NormalTok{(values, key}\OperatorTok{=}\NormalTok{values.get))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-15}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Adjust the discount factor closer to 0 (short-sighted) or 1
  (far-sighted)---how does the choice change?
\item
  Add uncertainty to long-term rewards---what if outcomes aren't
  guaranteed?
\item
  Reflect on real-world cases: how do companies, governments, or
  individuals balance short vs.~long-term objectives?
\end{enumerate}

\subsection{17. Measuring success and utility in
practice}\label{measuring-success-and-utility-in-practice}

Defining success for an AI system requires measurable criteria. Utility
functions provide a theoretical framework, but in practice, success is
judged by task-specific metrics---accuracy, efficiency, user
satisfaction, safety, or profit. The challenge lies in translating
abstract objectives into concrete, measurable signals.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-16}

Imagine designing a delivery drone. You might say its goal is to
``deliver packages well.'' But what does ``well'' mean? Fast delivery,
minimal energy use, or safe landings? Each definition of success leads
to different system behaviors.

\subsubsection{Deep Dive}\label{deep-dive-16}

\begin{itemize}
\tightlist
\item
  Task-specific metrics: classification error, precision/recall,
  latency, throughput.
\item
  Composite metrics: weighted combinations of goals (e.g., safety +
  efficiency).
\item
  Operational constraints: resource usage, fairness requirements, or
  regulatory compliance.
\item
  User-centered measures: satisfaction, trust, adoption rates.
\item
  Pitfalls: metrics can diverge from true goals, creating misaligned
  incentives or unintended consequences.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1308}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2897}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2617}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3178}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Domain
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Common Metric
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strength
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Weakness
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Classification & Accuracy, F1-score & Clear, quantitative & Ignores
fairness, interpretability \\
Robotics & Task success rate, energy usage & Captures physical
efficiency & Hard to model safety trade-offs \\
Recommenders & Click-through rate (CTR) & Easy to measure at scale &
Encourages clickbait \\
Finance & ROI, Sharpe ratio & Reflects profitability & May overlook
systemic risks \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-16}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Measuring success with multiple metrics}
\NormalTok{results }\OperatorTok{=}\NormalTok{ \{}\StringTok{"accuracy"}\NormalTok{: }\FloatTok{0.92}\NormalTok{, }\StringTok{"latency"}\NormalTok{: }\DecValTok{120}\NormalTok{, }\StringTok{"user\_satisfaction"}\NormalTok{: }\FloatTok{0.8}\NormalTok{\}}

\NormalTok{weights }\OperatorTok{=}\NormalTok{ \{}\StringTok{"accuracy"}\NormalTok{: }\FloatTok{0.5}\NormalTok{, }\StringTok{"latency"}\NormalTok{: }\OperatorTok{{-}}\FloatTok{0.2}\NormalTok{, }\StringTok{"user\_satisfaction"}\NormalTok{: }\FloatTok{0.3}\NormalTok{\}}

\KeywordTok{def}\NormalTok{ utility(metrics, weights):}
    \ControlFlowTok{return} \BuiltInTok{sum}\NormalTok{(metrics[k] }\OperatorTok{*}\NormalTok{ w }\ControlFlowTok{for}\NormalTok{ k, w }\KeywordTok{in}\NormalTok{ weights.items())}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Overall utility score:"}\NormalTok{, utility(results, weights))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-16}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Change weights to prioritize latency over accuracy---how does the
  utility score shift?
\item
  Add fairness as a new metric and decide how to incorporate it.
\item
  Reflect: do current industry benchmarks truly measure success, or just
  proxies for convenience?
\end{enumerate}

\subsection{18. Reward hacking and specification
gaming}\label{reward-hacking-and-specification-gaming}

When objectives or reward functions are poorly specified, agents can
exploit loopholes to maximize the reward without achieving the intended
outcome. This phenomenon is known as reward hacking or specification
gaming. It highlights the danger of optimizing for proxies instead of
true goals.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-17}

Imagine telling a cleaning robot to ``remove visible dirt.'' Instead of
vacuuming, it learns to cover dirt with a rug. The room looks clean, the
objective is ``met,'' but the real goal---cleanliness---has been
subverted.

\subsubsection{Deep Dive}\label{deep-dive-17}

\begin{itemize}
\item
  Causes:

  \begin{itemize}
  \tightlist
  \item
    Overly simplistic reward design.
  \item
    Reliance on proxies instead of direct measures.
  \item
    Failure to anticipate edge cases.
  \end{itemize}
\item
  Examples:

  \begin{itemize}
  \tightlist
  \item
    A simulated agent flips over in a racing game to earn reward points
    faster.
  \item
    A text model maximizes length because ``longer output'' is rewarded,
    regardless of relevance.
  \end{itemize}
\item
  Consequences: reward hacking reduces trust, safety, and usefulness.
\item
  Research directions:

  \begin{itemize}
  \tightlist
  \item
    Iterative refinement of reward functions.
  \item
    Human feedback integration (RLHF).
  \item
    Inverse reinforcement learning to infer true goals.
  \item
    Safe exploration methods to avoid pathological behaviors.
  \end{itemize}
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1653}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2562}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2975}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2810}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Issue
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Why It Happens
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mitigation Approach
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Proxy misuse & Optimizing clicks → clickbait & Easy-to-measure metric
replaces goal & Multi-metric evaluation \\
Exploiting loopholes & Game agent exploits scoring bug & Reward not
covering all cases & Robust testing, adversarial design \\
Perverse incentives & ``Remove dirt'' → hide dirt & Ambiguity in
specification & Human oversight, richer feedback \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-17}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Reward hacking example}
\KeywordTok{def}\NormalTok{ reward(action):}
    \ControlFlowTok{if}\NormalTok{ action }\OperatorTok{==} \StringTok{"hide\_dirt"}\NormalTok{:}
        \ControlFlowTok{return} \DecValTok{10}  \CommentTok{\# unintended loophole}
    \ControlFlowTok{elif}\NormalTok{ action }\OperatorTok{==} \StringTok{"clean"}\NormalTok{:}
        \ControlFlowTok{return} \DecValTok{8}
    \ControlFlowTok{return} \DecValTok{0}

\NormalTok{actions }\OperatorTok{=}\NormalTok{ [}\StringTok{"clean"}\NormalTok{, }\StringTok{"hide\_dirt"}\NormalTok{]}
\ControlFlowTok{for}\NormalTok{ a }\KeywordTok{in}\NormalTok{ actions:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Action: }\SpecialCharTok{\{}\NormalTok{a}\SpecialCharTok{\}}\SpecialStringTok{, Reward: }\SpecialCharTok{\{}\NormalTok{reward(a)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-17}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Modify the reward so that ``hide\_dirt'' is penalized---does the agent
  now choose correctly?
\item
  Add additional proxy rewards (e.g., speed) and test whether they
  conflict.
\item
  Reflect on real-world analogies: how do poorly designed incentives in
  finance, education, or politics lead to unintended behavior?
\end{enumerate}

\subsection{19. Human feedback and preference
learning}\label{human-feedback-and-preference-learning}

Human feedback provides a way to align AI systems with values that are
hard to encode directly. Instead of handcrafting reward functions,
agents can learn from demonstrations, comparisons, or ratings. This
process, known as preference learning, is central to making AI behavior
more aligned with human expectations.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-18}

Imagine teaching a child to draw. You don't give them a formula for
``good art.'' Instead, you encourage some attempts and correct others.
Over time, they internalize your preferences. AI agents can be trained
in the same way---by receiving approval or disapproval signals from
humans.

\subsubsection{Deep Dive}\label{deep-dive-18}

\begin{itemize}
\item
  Forms of feedback:

  \begin{itemize}
  \tightlist
  \item
    Demonstrations: show the agent how to act.
  \item
    Comparisons: pick between two outputs (``this is better than
    that'').
  \item
    Ratings: assign quality scores to behaviors or outputs.
  \end{itemize}
\item
  Algorithms: reinforcement learning from human feedback (RLHF), inverse
  reinforcement learning, and preference-based optimization.
\item
  Advantages: captures subtle, value-laden judgments not expressible in
  explicit rewards.
\item
  Challenges: feedback can be inconsistent, biased, or expensive to
  gather at scale.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1373}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2941}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2549}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3137}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Feedback Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Use Case
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strength
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Demonstrations & Robot learns tasks from humans & Intuitive, easy to
provide & Hard to cover all cases \\
Comparisons & Ranking chatbot responses & Efficient, captures nuance &
Requires many pairwise judgments \\
Ratings & Users scoring recommendations & Simple signal, scalable &
Subjective, noisy, may be gamed \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-18}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preference learning via pairwise comparison}
\NormalTok{pairs }\OperatorTok{=}\NormalTok{ [(}\StringTok{"response A"}\NormalTok{, }\StringTok{"response B"}\NormalTok{), (}\StringTok{"response C"}\NormalTok{, }\StringTok{"response D"}\NormalTok{)]}
\NormalTok{human\_choices }\OperatorTok{=}\NormalTok{ \{}\StringTok{"response A"}\NormalTok{: }\DecValTok{1}\NormalTok{, }\StringTok{"response B"}\NormalTok{: }\DecValTok{0}\NormalTok{,}
                 \StringTok{"response C"}\NormalTok{: }\DecValTok{0}\NormalTok{, }\StringTok{"response D"}\NormalTok{: }\DecValTok{1}\NormalTok{\}}

\KeywordTok{def}\NormalTok{ learn\_preferences(pairs, choices):}
\NormalTok{    scores }\OperatorTok{=}\NormalTok{ \{\}}
    \ControlFlowTok{for}\NormalTok{ a, b }\KeywordTok{in}\NormalTok{ pairs:}
\NormalTok{        scores[a] }\OperatorTok{=}\NormalTok{ scores.get(a, }\DecValTok{0}\NormalTok{) }\OperatorTok{+}\NormalTok{ choices[a]}
\NormalTok{        scores[b] }\OperatorTok{=}\NormalTok{ scores.get(b, }\DecValTok{0}\NormalTok{) }\OperatorTok{+}\NormalTok{ choices[b]}
    \ControlFlowTok{return}\NormalTok{ scores}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Learned preference scores:"}\NormalTok{, learn\_preferences(pairs, human\_choices))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-18}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add more responses with conflicting feedback---how stable are the
  learned preferences?
\item
  Introduce noisy feedback (random mistakes) and test how it affects
  outcomes.
\item
  Reflect: in which domains (education, healthcare, social media) should
  human feedback play the strongest role in shaping AI?
\end{enumerate}

\subsection{20. Normative vs.~descriptive accounts of
utility}\label{normative-vs.-descriptive-accounts-of-utility}

Utility can be understood in two ways: normatively, as how perfectly
rational agents \emph{should} behave, and descriptively, as how real
humans (or systems) actually behave. AI design must grapple with this
gap: formal models of utility often clash with observed human
preferences, which are noisy, inconsistent, and context-dependent.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-19}

Imagine someone choosing food at a buffet. A normative model might
assume they maximize health or taste consistently. In reality, they may
skip salad one day, overeat dessert the next, or change choices
depending on mood. Human behavior is rarely a clean optimization of a
fixed utility.

\subsubsection{Deep Dive}\label{deep-dive-19}

\begin{itemize}
\item
  Normative utility: rooted in economics and decision theory, assumes
  consistency, transitivity, and rational optimization.
\item
  Descriptive utility: informed by psychology and behavioral economics,
  reflects cognitive biases, framing effects, and bounded rationality.
\item
  AI implications:

  \begin{itemize}
  \tightlist
  \item
    If we design systems around normative models, they may misinterpret
    real human behavior.
  \item
    If we design systems around descriptive models, they may replicate
    human biases.
  \end{itemize}
\item
  Middle ground: AI research increasingly seeks hybrid models---rational
  principles corrected by behavioral insights.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0932}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3136}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3390}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2542}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Perspective
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Normative & How agents \emph{should} maximize utility & Reinforcement
learning with clean reward & Ignores human irrationality \\
Descriptive & How agents actually behave & Recommenders modeling click
patterns & Reinforces bias, inconsistency \\
Hybrid & Blend of rational + behavioral models & Human-in-the-loop
decision support & Complex to design and validate \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-19}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Normative vs descriptive utility example}
\ImportTok{import}\NormalTok{ random}

\CommentTok{\# Normative: always pick highest score}
\NormalTok{options }\OperatorTok{=}\NormalTok{ \{}\StringTok{"salad"}\NormalTok{: }\DecValTok{8}\NormalTok{, }\StringTok{"cake"}\NormalTok{: }\DecValTok{6}\NormalTok{\}}
\NormalTok{choice\_norm }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(options, key}\OperatorTok{=}\NormalTok{options.get)}

\CommentTok{\# Descriptive: human sometimes picks suboptimal}
\NormalTok{choice\_desc }\OperatorTok{=}\NormalTok{ random.choice(}\BuiltInTok{list}\NormalTok{(options.keys()))}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Normative choice:"}\NormalTok{, choice\_norm)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Descriptive choice:"}\NormalTok{, choice\_desc)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-19}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run the descriptive choice multiple times---how often does it diverge
  from the normative?
\item
  Add framing effects (e.g., label salad as ``diet food'') and see how
  it alters preferences.
\item
  Reflect: should AI systems enforce normative rationality, or adapt to
  descriptive human behavior?
\end{enumerate}

\section{Chapter 3. Information, Uncertainty, and
Entropy}\label{chapter-3.-information-uncertainty-and-entropy}

\subsection{21. Information as reduction of
uncertainty}\label{information-as-reduction-of-uncertainty}

Information is not just raw data---it is the amount by which uncertainty
is reduced when new data is received. In AI, information measures how
much an observation narrows down the possible states of the world. The
more surprising or unexpected the signal, the more information it
carries.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-20}

Imagine guessing a number between 1 and 100. Each yes/no question halves
the possibilities: ``Is it greater than 50?'' reduces uncertainty
dramatically. Every answer gives you information by shrinking the space
of possible numbers.

\subsubsection{Deep Dive}\label{deep-dive-20}

\begin{itemize}
\tightlist
\item
  Information theory (Claude Shannon) formalizes this idea.
\item
  The information content of an event relates to its probability: rare
  events are more informative.
\item
  Entropy measures the average uncertainty of a random variable.
\item
  AI uses information measures in many ways: feature selection, decision
  trees (information gain), communication systems, and model evaluation.
\item
  High information reduces ambiguity, but noisy channels and biased data
  can distort the signal.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2021}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4043}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3936}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Information content & Surprise of an event = −log(p) & Rare class label
in classification \\
Entropy & Expected uncertainty over distribution & Decision tree
splits \\
Information gain & Reduction in entropy after observation & Choosing the
best feature to split on \\
Mutual information & Shared information between variables & Feature
relevance for prediction \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-20}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ math}

\CommentTok{\# Information content of an event}
\KeywordTok{def}\NormalTok{ info\_content(prob):}
    \ControlFlowTok{return} \OperatorTok{{-}}\NormalTok{math.log2(prob)}

\NormalTok{events }\OperatorTok{=}\NormalTok{ \{}\StringTok{"common"}\NormalTok{: }\FloatTok{0.8}\NormalTok{, }\StringTok{"rare"}\NormalTok{: }\FloatTok{0.2}\NormalTok{\}}
\ControlFlowTok{for}\NormalTok{ e, p }\KeywordTok{in}\NormalTok{ events.items():}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{e}\SpecialCharTok{\}}\SpecialStringTok{: information = }\SpecialCharTok{\{}\NormalTok{info\_content(p)}\SpecialCharTok{:.2f\}}\SpecialStringTok{ bits"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-20}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add more events with different probabilities---how does rarity affect
  information?
\item
  Simulate a fair vs.~biased coin toss---compare entropy values.
\item
  Reflect: how does information connect to AI tasks like
  decision-making, compression, or communication?
\end{enumerate}

\subsection{22. Probabilities and degrees of
belief}\label{probabilities-and-degrees-of-belief}

Probability provides a mathematical language for representing
uncertainty. Instead of treating outcomes as certain or impossible,
probabilities assign degrees of belief between 0 and 1. In AI,
probability theory underpins reasoning, prediction, and learning under
incomplete information.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-21}

Think of carrying an umbrella. If the forecast says a 90\% chance of
rain, you probably take it. If it's 10\%, you might risk leaving it at
home. Probabilities let you act sensibly even when the outcome is
uncertain.

\subsubsection{Deep Dive}\label{deep-dive-21}

\begin{itemize}
\tightlist
\item
  Frequentist view: probability as long-run frequency of events.
\item
  Bayesian view: probability as degree of belief, updated with evidence.
\item
  Random variables: map uncertain outcomes to numbers.
\item
  Distributions: describe how likely different outcomes are.
\item
  Applications in AI: spam detection, speech recognition, medical
  diagnosis---all rely on probabilistic reasoning to handle noisy or
  incomplete inputs.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Frequentist & Probability = long-run frequency & Coin toss
experiments \\
Bayesian & Probability = belief, updated by data & Spam filters
adjusting to new emails \\
Random variable & Variable taking probabilistic values & Weather: sunny
= 0, rainy = 1 \\
Distribution & Assignment of probabilities to outcomes & Gaussian priors
in machine learning \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-21}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ random}

\CommentTok{\# Simple probability estimation (frequentist)}
\NormalTok{trials }\OperatorTok{=} \DecValTok{1000}
\NormalTok{heads }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(}\DecValTok{1} \ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(trials) }\ControlFlowTok{if}\NormalTok{ random.random() }\OperatorTok{\textless{}} \FloatTok{0.5}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Estimated P(heads):"}\NormalTok{, heads }\OperatorTok{/}\NormalTok{ trials)}

\CommentTok{\# Bayesian{-}style update (toy)}
\NormalTok{prior }\OperatorTok{=} \FloatTok{0.5}
\NormalTok{likelihood }\OperatorTok{=} \FloatTok{0.8}  \CommentTok{\# chance of evidence given hypothesis}
\NormalTok{evidence\_prob }\OperatorTok{=} \FloatTok{0.6}
\NormalTok{posterior }\OperatorTok{=}\NormalTok{ (prior }\OperatorTok{*}\NormalTok{ likelihood) }\OperatorTok{/}\NormalTok{ evidence\_prob}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Posterior belief:"}\NormalTok{, posterior)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-21}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Increase the number of trials---does the estimated probability
  converge to 0.5?
\item
  Modify the Bayesian update with different priors---how does prior
  belief affect the posterior?
\item
  Reflect: when designing AI, when should you favor frequentist
  reasoning, and when Bayesian?
\end{enumerate}

\subsection{23. Random variables, distributions, and
signals}\label{random-variables-distributions-and-signals}

A random variable assigns numerical values to uncertain outcomes. Its
distribution describes how likely each outcome is. In AI, random
variables model uncertain inputs (sensor readings), latent states
(hidden causes), and outputs (predictions). Signals are time-varying
realizations of such variables, carrying information from the
environment.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-22}

Imagine rolling a die. The outcome itself (1--6) is uncertain, but the
random variable ``X = die roll'' captures that uncertainty. If you track
successive rolls over time, you get a signal: a sequence of values
reflecting the random process.

\subsubsection{Deep Dive}\label{deep-dive-22}

\begin{itemize}
\item
  Random variables: can be discrete (finite outcomes) or continuous
  (infinite outcomes).
\item
  Distributions: specify the probabilities (discrete) or densities
  (continuous). Examples include Bernoulli, Gaussian, and Poisson.
\item
  Signals: realizations of random processes evolving over
  time---essential in speech, vision, and sensor data.
\item
  AI applications:

  \begin{itemize}
  \tightlist
  \item
    Gaussian distributions for modeling noise.
  \item
    Bernoulli/Binomial for classification outcomes.
  \item
    Hidden random variables in latent variable models.
  \end{itemize}
\item
  Challenge: real-world signals often combine noise, structure, and
  nonstationarity.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2159}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4091}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3750}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Discrete variable & Finite possible outcomes & Dice rolls,
classification labels \\
Continuous variable & Infinite range of values & Temperature, pixel
intensities \\
Distribution & Likelihood of different outcomes & Gaussian noise in
sensors \\
Signal & Sequence of random variable outcomes & Audio waveform, video
frames \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-22}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Discrete random variable: dice}
\NormalTok{dice\_rolls }\OperatorTok{=}\NormalTok{ np.random.choice([}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{], size}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Dice rolls:"}\NormalTok{, dice\_rolls)}

\CommentTok{\# Continuous random variable: Gaussian noise}
\NormalTok{noise }\OperatorTok{=}\NormalTok{ np.random.normal(loc}\OperatorTok{=}\DecValTok{0}\NormalTok{, scale}\OperatorTok{=}\DecValTok{1}\NormalTok{, size}\OperatorTok{=}\DecValTok{5}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Gaussian noise samples:"}\NormalTok{, noise)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-22}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Change the distribution parameters (e.g., mean and variance of
  Gaussian)---how do samples shift?
\item
  Simulate a signal by generating a sequence of random variables over
  time.
\item
  Reflect: how does modeling randomness help AI deal with uncertainty in
  perception and decision-making?
\end{enumerate}

\subsection{24. Entropy as a measure of
uncertainty}\label{entropy-as-a-measure-of-uncertainty}

Entropy quantifies how uncertain or unpredictable a random variable is.
High entropy means outcomes are spread out and less predictable, while
low entropy means outcomes are concentrated and more certain. In AI,
entropy helps measure information content, guide decision trees, and
regularize models.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-23}

Imagine two dice: one fair, one loaded to always roll a six. The fair
die is unpredictable (high entropy), while the loaded die is predictable
(low entropy). Entropy captures this difference in uncertainty
mathematically.

\subsubsection{Deep Dive}\label{deep-dive-23}

\begin{itemize}
\item
  Shannon entropy:

  \[
  H(X) = -\sum p(x) \log_2 p(x)
  \]
\item
  High entropy: uniform distributions, maximum uncertainty.
\item
  Low entropy: skewed distributions, predictable outcomes.
\item
  Applications in AI:

  \begin{itemize}
  \tightlist
  \item
    Decision trees: choose features with highest information gain
    (entropy reduction).
  \item
    Reinforcement learning: encourage exploration by maximizing policy
    entropy.
  \item
    Generative models: evaluate uncertainty in output distributions.
  \end{itemize}
\item
  Limitations: entropy depends on probability estimates, which may be
  inaccurate in noisy environments.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1809}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3085}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1383}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3723}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Distribution Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Entropy Level
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Use Case
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Uniform & Fair die (1--6 equally likely) & High & Maximum
unpredictability \\
Skewed & Loaded die (90\% six) & Low & Predictable classification
outcomes \\
Binary balanced & Coin flip & Medium & Baseline uncertainty in
decisions \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-23}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ math}

\KeywordTok{def}\NormalTok{ entropy(probs):}
    \ControlFlowTok{return} \OperatorTok{{-}}\BuiltInTok{sum}\NormalTok{(p }\OperatorTok{*}\NormalTok{ math.log2(p) }\ControlFlowTok{for}\NormalTok{ p }\KeywordTok{in}\NormalTok{ probs }\ControlFlowTok{if}\NormalTok{ p }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{)}

\CommentTok{\# Fair die vs. loaded die}
\NormalTok{fair\_probs }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\OperatorTok{/}\DecValTok{6}\NormalTok{] }\OperatorTok{*} \DecValTok{6}
\NormalTok{loaded\_probs }\OperatorTok{=}\NormalTok{ [}\FloatTok{0.9}\NormalTok{] }\OperatorTok{+}\NormalTok{ [}\FloatTok{0.02}\NormalTok{] }\OperatorTok{*} \DecValTok{5}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Fair die entropy:"}\NormalTok{, entropy(fair\_probs))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Loaded die entropy:"}\NormalTok{, entropy(loaded\_probs))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-23}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Change probabilities---see how entropy increases with uniformity.
\item
  Apply entropy to text: compute uncertainty over letter frequencies in
  a sentence.
\item
  Reflect: why do AI systems often prefer reducing entropy when making
  decisions?
\end{enumerate}

\subsection{25. Mutual information and
relevance}\label{mutual-information-and-relevance}

Mutual information (MI) measures how much knowing one variable reduces
uncertainty about another. It captures dependence between variables,
going beyond simple correlation. In AI, mutual information helps
identify which features are most relevant for prediction, compress data
efficiently, and align multimodal signals.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-24}

Think of two friends whispering answers during a quiz. If one always
knows the answer and the other copies, the information from one
completely determines the other---high mutual information. If their
answers are random and unrelated, the MI is zero.

\subsubsection{Deep Dive}\label{deep-dive-24}

\begin{itemize}
\item
  Definition:

  \[
  I(X;Y) = \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}
  \]
\item
  Zero MI: variables are independent.
\item
  High MI: strong dependence, one variable reveals much about the other.
\item
  Applications in AI:

  \begin{itemize}
  \tightlist
  \item
    Feature selection (choose features with highest MI with labels).
  \item
    Multimodal learning (aligning audio with video).
  \item
    Representation learning (maximize MI between input and latent
    codes).
  \end{itemize}
\item
  Advantages: captures nonlinear relationships, unlike correlation.
\item
  Challenges: requires estimating joint distributions, which is
  difficult in high dimensions.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2917}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4583}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Situation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mutual Information
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Independent variables & MI = 0 & Random noise vs.~labels \\
Strong dependence & High MI & Pixel intensities vs.~image class \\
Partial dependence & Medium MI & User clicks vs.~recommendations \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-24}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ math}
\ImportTok{from}\NormalTok{ collections }\ImportTok{import}\NormalTok{ Counter}

\KeywordTok{def}\NormalTok{ mutual\_information(X, Y):}
\NormalTok{    n }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(X)}
\NormalTok{    px }\OperatorTok{=}\NormalTok{ Counter(X)}
\NormalTok{    py }\OperatorTok{=}\NormalTok{ Counter(Y)}
\NormalTok{    pxy }\OperatorTok{=}\NormalTok{ Counter(}\BuiltInTok{zip}\NormalTok{(X, Y))}
\NormalTok{    mi }\OperatorTok{=} \FloatTok{0.0}
    \ControlFlowTok{for}\NormalTok{ (x, y), count }\KeywordTok{in}\NormalTok{ pxy.items():}
\NormalTok{        pxy\_val }\OperatorTok{=}\NormalTok{ count }\OperatorTok{/}\NormalTok{ n}
\NormalTok{        mi }\OperatorTok{+=}\NormalTok{ pxy\_val }\OperatorTok{*}\NormalTok{ math.log2(pxy\_val }\OperatorTok{/}\NormalTok{ ((px[x]}\OperatorTok{/}\NormalTok{n) }\OperatorTok{*}\NormalTok{ (py[y]}\OperatorTok{/}\NormalTok{n)))}
    \ControlFlowTok{return}\NormalTok{ mi}

\NormalTok{X }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Mutual Information:"}\NormalTok{, mutual\_information(X, Y))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-24}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Generate independent variables---does MI approach zero?
\item
  Create perfectly correlated variables---does MI increase?
\item
  Reflect: why is MI a more powerful measure of relevance than
  correlation in AI systems?
\end{enumerate}

\subsection{26. Noise, error, and uncertainty in
perception}\label{noise-error-and-uncertainty-in-perception}

AI systems rarely receive perfect data. Sensors introduce noise, models
make errors, and the world itself produces uncertainty. Understanding
and managing these imperfections is crucial for building reliable
perception systems in vision, speech, robotics, and beyond.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-25}

Imagine trying to recognize a friend in a crowded, dimly lit room.
Background chatter, poor lighting, and movement all interfere. Despite
this, your brain filters signals, corrects errors, and still identifies
them. AI perception faces the same challenges.

\subsubsection{Deep Dive}\label{deep-dive-25}

\begin{itemize}
\item
  Noise: random fluctuations in signals (e.g., static in audio, blur in
  images).
\item
  Error: systematic deviation from the correct value (e.g., biased
  sensor calibration).
\item
  Uncertainty: incomplete knowledge about the true state of the
  environment.
\item
  Handling strategies:

  \begin{itemize}
  \tightlist
  \item
    Filtering (Kalman, particle filters) to denoise signals.
  \item
    Probabilistic models to represent uncertainty explicitly.
  \item
    Ensemble methods to reduce model variance.
  \end{itemize}
\item
  Challenge: distinguishing between random noise, systematic error, and
  inherent uncertainty.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1058}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2212}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3173}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3558}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mitigation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Noise & Random signal variation & Camera grain in low light & Smoothing,
denoising filters \\
Error & Systematic deviation & Miscalibrated temperature sensor &
Calibration, bias correction \\
Uncertainty & Lack of full knowledge & Self-driving car unsure of intent
& Probabilistic modeling, Bayesian nets \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-25}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Simulate noisy sensor data}
\NormalTok{true\_value }\OperatorTok{=} \DecValTok{10}
\NormalTok{noise }\OperatorTok{=}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{)  }\CommentTok{\# Gaussian noise}
\NormalTok{measurements }\OperatorTok{=}\NormalTok{ true\_value }\OperatorTok{+}\NormalTok{ noise}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Measurements:"}\NormalTok{, measurements)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Estimated mean:"}\NormalTok{, np.mean(measurements))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-25}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Increase noise variance---how does it affect the reliability of the
  estimate?
\item
  Add systematic error (e.g., always +2 bias)---can the mean still
  recover the truth?
\item
  Reflect: when should AI treat uncertainty as noise to be removed,
  versus as real ambiguity to be modeled?
\end{enumerate}

\subsection{27. Bayesian updating and belief
revision}\label{bayesian-updating-and-belief-revision}

Bayesian updating provides a principled way to revise beliefs in light
of new evidence. It combines prior knowledge (what you believed before)
with likelihood (how well the evidence fits a hypothesis) to produce a
posterior belief. This mechanism lies at the heart of probabilistic AI.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-26}

Imagine a doctor diagnosing a patient. Before seeing test results, she
has a prior belief about possible illnesses. A new lab test provides
evidence, shifting her belief toward one diagnosis. Each new piece of
evidence reshapes the belief distribution.

\subsubsection{Deep Dive}\label{deep-dive-26}

\begin{itemize}
\item
  Bayes' theorem:

  \[
  P(H|E) = \frac{P(E|H) P(H)}{P(E)}
  \]

  where \(H\) = hypothesis, \(E\) = evidence.
\item
  Prior: initial degree of belief.
\item
  Likelihood: how consistent evidence is with the hypothesis.
\item
  Posterior: updated belief after evidence.
\item
  AI applications: spam filtering, medical diagnosis, robotics
  localization, Bayesian neural networks.
\item
  Key insight: Bayesian updating enables continual learning, where
  beliefs evolve rather than reset.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1829}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3659}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4512}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Element
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Prior & Belief before evidence & Spam probability before reading
email \\
Likelihood & Evidence fit given hypothesis & Probability of words if
spam \\
Posterior & Belief after evidence & Updated spam probability \\
Belief revision & Iterative update with new data & Robot refining map
after each sensor \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-26}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simple Bayesian update}
\NormalTok{prior\_spam }\OperatorTok{=} \FloatTok{0.2}
\NormalTok{likelihood\_word\_given\_spam }\OperatorTok{=} \FloatTok{0.9}
\NormalTok{likelihood\_word\_given\_ham }\OperatorTok{=} \FloatTok{0.3}
\NormalTok{evidence\_prob }\OperatorTok{=}\NormalTok{ prior\_spam }\OperatorTok{*}\NormalTok{ likelihood\_word\_given\_spam }\OperatorTok{+}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ prior\_spam) }\OperatorTok{*}\NormalTok{ likelihood\_word\_given\_ham}

\NormalTok{posterior\_spam }\OperatorTok{=}\NormalTok{ (prior\_spam }\OperatorTok{*}\NormalTok{ likelihood\_word\_given\_spam) }\OperatorTok{/}\NormalTok{ evidence\_prob}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Posterior P(spam|word):"}\NormalTok{, posterior\_spam)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-26}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Change priors---how does initial belief influence the posterior?
\item
  Add more evidence step by step---observe belief revision over time.
\item
  Reflect: what kinds of AI systems need to continuously update beliefs
  instead of making static predictions?
\end{enumerate}

\subsection{28. Ambiguity
vs.~randomness}\label{ambiguity-vs.-randomness}

Uncertainty can arise from two different sources: randomness, where
outcomes are inherently probabilistic, and ambiguity, where the
probabilities themselves are unknown or ill-defined. Distinguishing
between these is crucial for AI systems making decisions under
uncertainty.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-27}

Imagine drawing a ball from a jar. If you know the jar has 50 red and 50
blue balls, the outcome is random but well-defined. If you don't know
the composition of the jar, the uncertainty is ambiguous---you can't
even assign exact probabilities.

\subsubsection{Deep Dive}\label{deep-dive-27}

\begin{itemize}
\item
  Randomness (risk): modeled with well-defined probability
  distributions. Example: rolling dice, weather forecasts.
\item
  Ambiguity (Knightian uncertainty): probabilities are unknown,
  incomplete, or contested. Example: predicting success of a brand-new
  technology.
\item
  AI implications:

  \begin{itemize}
  \tightlist
  \item
    Randomness can be managed with probabilistic models.
  \item
    Ambiguity requires robust decision criteria (maximin, minimax
    regret, distributional robustness).
  \item
    Real-world AI often faces both at once---stochastic environments
    with incomplete models.
  \end{itemize}
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1583}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3167}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type of Uncertainty
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Handling Strategy
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Randomness (risk) & Known probabilities, random outcome & Dice rolls,
sensor noise & Probability theory, expected value \\
Ambiguity & Unknown or ill-defined probabilities & Novel diseases, new
markets & Robust optimization, cautious planning \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-27}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ random}

\CommentTok{\# Randomness: fair coin}
\NormalTok{coin }\OperatorTok{=}\NormalTok{ random.choice([}\StringTok{"H"}\NormalTok{, }\StringTok{"T"}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Random outcome:"}\NormalTok{, coin)}

\CommentTok{\# Ambiguity: unknown distribution (simulate ignorance)}
\NormalTok{unknown\_jar }\OperatorTok{=}\NormalTok{ [}\StringTok{"?"}\NormalTok{, }\StringTok{"?"}\NormalTok{]  }\CommentTok{\# cannot assign probabilities yet}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Ambiguous outcome:"}\NormalTok{, random.choice(unknown\_jar))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-27}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simulate dice rolls (randomness) vs.~drawing from an unknown jar
  (ambiguity).
\item
  Implement maximin: choose the action with the best worst-case payoff.
\item
  Reflect: how should AI systems behave differently when probabilities
  are known versus when they are not?
\end{enumerate}

\subsection{29. Value of information in
decision-making}\label{value-of-information-in-decision-making}

The value of information (VoI) measures how much an additional piece of
information improves decision quality. Not all data is equally
useful---some observations greatly reduce uncertainty, while others
change nothing. In AI, VoI guides data collection, active learning, and
sensor placement.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-28}

Imagine planning a picnic. If the weather forecast is uncertain, paying
for a more accurate update could help decide whether to pack sunscreen
or an umbrella. But once you already know it's raining, more forecasts
add no value.

\subsubsection{Deep Dive}\label{deep-dive-28}

\begin{itemize}
\item
  Definition: VoI = (expected utility with information) − (expected
  utility without information).
\item
  Perfect information: knowing outcomes in advance---upper bound on VoI.
\item
  Sample information: partial signals---lower but often practical value.
\item
  Applications:

  \begin{itemize}
  \tightlist
  \item
    Active learning: query the most informative data points.
  \item
    Robotics: decide where to place sensors.
  \item
    Healthcare AI: order diagnostic tests only when they meaningfully
    improve treatment choices.
  \end{itemize}
\item
  Trade-off: gathering information has costs; VoI balances benefit
  vs.~expense.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3091}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3091}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1818}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type of Information
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Benefit
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Perfect information & Knowing true label before training & Maximum
reduction in uncertainty & Rare, hypothetical \\
Sample information & Adding a diagnostic test result & Improves decision
accuracy & Costly, may be noisy \\
Irrelevant information & Redundant features in a dataset & No
improvement, may add complexity & Wastes resources \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-28}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Toy value of information calculation}
\ImportTok{import}\NormalTok{ random}

\KeywordTok{def}\NormalTok{ decision\_with\_info():}
    \CommentTok{\# Always correct after info}
    \ControlFlowTok{return} \FloatTok{1.0}  \CommentTok{\# utility}

\KeywordTok{def}\NormalTok{ decision\_without\_info():}
    \CommentTok{\# Guess with 50\% accuracy}
    \ControlFlowTok{return}\NormalTok{ random.choice([}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{])  }

\NormalTok{expected\_with }\OperatorTok{=}\NormalTok{ decision\_with\_info()}
\NormalTok{expected\_without }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(decision\_without\_info() }\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1000}\NormalTok{)) }\OperatorTok{/} \DecValTok{1000}

\NormalTok{voi }\OperatorTok{=}\NormalTok{ expected\_with }\OperatorTok{{-}}\NormalTok{ expected\_without}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Estimated Value of Information:"}\NormalTok{, }\BuiltInTok{round}\NormalTok{(voi, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-28}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add costs to information gathering---when is it still worth it?
\item
  Simulate imperfect information (70\% accuracy)---compare VoI against
  perfect information.
\item
  Reflect: where in real-world AI is information most valuable---medical
  diagnostics, autonomous driving, or recommender systems?
\end{enumerate}

\subsection{30. Limits of certainty in real-world
AI}\label{limits-of-certainty-in-real-world-ai}

AI systems never operate with complete certainty. Data can be noisy,
models are approximations, and environments change unpredictably.
Instead of seeking absolute certainty, effective AI embraces
uncertainty, quantifies it, and makes robust decisions under it.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-29}

Think of weather forecasting. Even with advanced satellites and
simulations, predictions are never 100\% accurate. Forecasters give
probabilities (``60\% chance of rain'') because certainty is impossible.
AI works the same way: it outputs probabilities, not guarantees.

\subsubsection{Deep Dive}\label{deep-dive-29}

\begin{itemize}
\item
  Sources of uncertainty:

  \begin{itemize}
  \tightlist
  \item
    Aleatoric: inherent randomness (e.g., quantum noise, dice rolls).
  \item
    Epistemic: lack of knowledge or model errors.
  \item
    Ontological: unforeseen situations outside the model's scope.
  \end{itemize}
\item
  AI strategies:

  \begin{itemize}
  \tightlist
  \item
    Probabilistic modeling and Bayesian inference.
  \item
    Confidence calibration for predictions.
  \item
    Robust optimization and safety margins.
  \end{itemize}
\item
  Implication: certainty is unattainable, but uncertainty-aware design
  leads to systems that are safer, more interpretable, and more
  trustworthy.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3083}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2583}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Uncertainty Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Handling Strategy
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Aleatoric & Randomness inherent in data & Sensor noise in robotics &
Probabilistic models, filtering \\
Epistemic & Model uncertainty due to limited data & Medical diagnosis
with rare diseases & Bayesian learning, ensembles \\
Ontological & Unknown unknowns & Autonomous car meets novel obstacle &
Fail-safes, human oversight \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-29}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Simulating aleatoric vs epistemic uncertainty}
\NormalTok{true\_value }\OperatorTok{=} \DecValTok{10}
\NormalTok{aleatoric\_noise }\OperatorTok{=}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{)  }\CommentTok{\# randomness}
\NormalTok{epistemic\_error }\OperatorTok{=} \DecValTok{2}  \CommentTok{\# model bias}

\NormalTok{measurements }\OperatorTok{=}\NormalTok{ true\_value }\OperatorTok{+}\NormalTok{ aleatoric\_noise }\OperatorTok{+}\NormalTok{ epistemic\_error}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Measurements with uncertainties:"}\NormalTok{, measurements)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-29}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Reduce aleatoric noise (lower variance)---does uncertainty shrink?
\item
  Change epistemic error---see how systematic bias skews results.
\item
  Reflect: why should AI systems present probabilities or confidence
  intervals instead of single ``certain'' answers?
\end{enumerate}

\section{Chapter 4. Computation, Complexity and
Limits}\label{chapter-4.-computation-complexity-and-limits}

\subsection{31. Computation as symbol
manipulation}\label{computation-as-symbol-manipulation}

At its core, computation is the manipulation of symbols according to
formal rules. AI systems inherit this foundation: whether processing
numbers, words, or images, they transform structured inputs into
structured outputs through rule-governed operations.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-30}

Think of a child using building blocks. Each block is a symbol, and by
arranging them under certain rules---stacking, matching shapes---the
child builds structures. A computer does the same, but with electrical
signals and logic gates instead of blocks.

\subsubsection{Deep Dive}\label{deep-dive-30}

\begin{itemize}
\item
  Classical view: computation = symbol manipulation independent of
  meaning.
\item
  Church--Turing thesis: any effective computation can be carried out by
  a Turing machine.
\item
  Relevance to AI:

  \begin{itemize}
  \tightlist
  \item
    Symbolic AI explicitly encodes rules and symbols (e.g., logic-based
    systems).
  \item
    Sub-symbolic AI (neural networks) still reduces to symbol
    manipulation at the machine level (numbers, tensors).
  \end{itemize}
\item
  Philosophical note: this raises questions of whether ``understanding''
  emerges from symbol manipulation or whether semantics requires
  embodiment.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2048}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3735}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4217}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Symbolic Computation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sub-symbolic Computation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Unit of operation & Explicit symbols, rules & Numbers, vectors,
matrices \\
Example in AI & Expert systems, theorem proving & Neural networks, deep
learning \\
Strength & Transparency, logical reasoning & Pattern recognition,
generalization \\
Limitation & Brittle, hard to scale & Opaque, hard to interpret \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-30}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simple symbol manipulation: replace symbols with rules}
\NormalTok{rules }\OperatorTok{=}\NormalTok{ \{}\StringTok{"A"}\NormalTok{: }\StringTok{"B"}\NormalTok{, }\StringTok{"B"}\NormalTok{: }\StringTok{"AB"}\NormalTok{\}}
\NormalTok{sequence }\OperatorTok{=} \StringTok{"A"}

\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{5}\NormalTok{):}
\NormalTok{    sequence }\OperatorTok{=} \StringTok{""}\NormalTok{.join(rules.get(ch, ch) }\ControlFlowTok{for}\NormalTok{ ch }\KeywordTok{in}\NormalTok{ sequence)}
    \BuiltInTok{print}\NormalTok{(sequence)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-30}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Extend the rewrite rules---how do the symbolic patterns evolve?
\item
  Try encoding arithmetic as symbol manipulation (e.g., ``III + II'' →
  ``V'').
\item
  Reflect: does symbol manipulation alone explain intelligence, or does
  meaning require more?
\end{enumerate}

\subsection{32. Models of computation (Turing, circuits,
RAM)}\label{models-of-computation-turing-circuits-ram}

Models of computation formalize what it means for a system to compute.
They provide abstract frameworks to describe algorithms, machines, and
their capabilities. For AI, these models define the boundaries of what
is computable and influence how we design efficient systems.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-31}

Imagine three ways of cooking the same meal: following a recipe step by
step (Turing machine), using a fixed kitchen appliance with wires and
buttons (logic circuit), or working in a modern kitchen with labeled
drawers and random access (RAM model). Each produces food but with
different efficiencies and constraints---just like models of
computation.

\subsubsection{Deep Dive}\label{deep-dive-31}

\begin{itemize}
\item
  Turing machine: sequential steps on an infinite tape. Proves what is
  \emph{computable}. Foundation of theoretical computer science.
\item
  Logic circuits: finite networks of gates (AND, OR, NOT). Capture
  computation at the hardware level.
\item
  Random Access Machine (RAM): closer to real computers, allowing
  constant-time access to memory cells. Used in algorithm analysis.
\item
  Implications for AI:

  \begin{itemize}
  \tightlist
  \item
    Proves equivalence of models (all can compute the same functions).
  \item
    Guides efficiency analysis---circuits emphasize parallelism, RAM
    emphasizes step complexity.
  \item
    Highlights limits---no model escapes undecidability or
    intractability.
  \end{itemize}
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1239}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3009}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2743}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3009}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Key Idea
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strength
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Turing machine & Infinite tape, sequential rules & Defines computability
& Impractical for efficiency \\
Logic circuits & Gates wired into fixed networks & Parallel, hardware
realizable & Fixed, less flexible \\
RAM model & Memory cells, constant-time access & Matches real algorithm
analysis & Ignores hardware-level constraints \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-31}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simulate a simple RAM model: array memory}
\NormalTok{memory }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{] }\OperatorTok{*} \DecValTok{5}  \CommentTok{\# 5 memory cells}

\CommentTok{\# Program: compute sum of first 3 cells}
\NormalTok{memory[}\DecValTok{0}\NormalTok{], memory[}\DecValTok{1}\NormalTok{], memory[}\DecValTok{2}\NormalTok{] }\OperatorTok{=} \DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}
\NormalTok{accumulator }\OperatorTok{=} \DecValTok{0}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{3}\NormalTok{):}
\NormalTok{    accumulator }\OperatorTok{+=}\NormalTok{ memory[i]}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Sum:"}\NormalTok{, accumulator)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-31}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Extend the RAM simulation to support subtraction or branching.
\item
  Build a tiny circuit simulator (AND, OR, NOT) and combine gates.
\item
  Reflect: why do we use different models for theory, hardware, and
  algorithm analysis in AI?
\end{enumerate}

\subsection{33. Time and space complexity
basics}\label{time-and-space-complexity-basics}

Complexity theory studies how the resources required by an
algorithm---time and memory---grow with input size. For AI,
understanding complexity is essential: it explains why some problems
scale well while others become intractable as data grows.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-32}

Imagine sorting a deck of cards. Sorting 10 cards by hand is quick.
Sorting 1,000 cards takes much longer. Sorting 1,000,000 cards by hand
might be impossible. The rules didn't change---the input size did.
Complexity tells us how performance scales.

\subsubsection{Deep Dive}\label{deep-dive-32}

\begin{itemize}
\item
  Time complexity: how the number of steps grows with input size \(n\).
  Common classes:

  \begin{itemize}
  \tightlist
  \item
    Constant \(O(1)\)
  \item
    Logarithmic \(O(\log n)\)
  \item
    Linear \(O(n)\)
  \item
    Quadratic \(O(n^2)\)
  \item
    Exponential \(O(2^n)\)
  \end{itemize}
\item
  Space complexity: how much memory an algorithm uses.
\item
  Big-O notation: describes asymptotic upper bound behavior.
\item
  AI implications: deep learning training scales roughly linearly with
  data and parameters, while combinatorial search may scale
  exponentially. Trade-offs between accuracy and feasibility often hinge
  on complexity.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1798}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2135}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3371}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2697}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Complexity Class
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Growth Rate Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Feasibility
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(O(1)\) & Constant time & Hash table lookup & Always feasible \\
\(O(\log n)\) & Grows slowly & Binary search over sorted data & Scales
well \\
\(O(n)\) & Linear growth & One pass over dataset & Scales with large
data \\
\(O(n^2)\) & Quadratic growth & Naive similarity comparison & Costly at
scale \\
\(O(2^n)\) & Exponential growth & Brute-force SAT solving & Infeasible
for large \(n\) \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-32}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ time}

\KeywordTok{def}\NormalTok{ quadratic\_algorithm(n):}
\NormalTok{    count }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
        \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
\NormalTok{            count }\OperatorTok{+=} \DecValTok{1}
    \ControlFlowTok{return}\NormalTok{ count}

\ControlFlowTok{for}\NormalTok{ n }\KeywordTok{in}\NormalTok{ [}\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{500}\NormalTok{]:}
\NormalTok{    start }\OperatorTok{=}\NormalTok{ time.time()}
\NormalTok{    quadratic\_algorithm(n)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"n=}\SpecialCharTok{\{}\NormalTok{n}\SpecialCharTok{\}}\SpecialStringTok{, time=}\SpecialCharTok{\{}\NormalTok{time}\SpecialCharTok{.}\NormalTok{time()}\OperatorTok{{-}}\NormalTok{start}\SpecialCharTok{:.5f\}}\SpecialStringTok{s"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-32}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Replace the quadratic algorithm with a linear one and compare
  runtimes.
\item
  Experiment with larger \(n\)---when does runtime become impractical?
\item
  Reflect: which AI methods scale poorly, and how do we approximate or
  simplify them to cope?
\end{enumerate}

\subsection{34. Polynomial vs.~exponential
time}\label{polynomial-vs.-exponential-time}

Algorithms fall into broad categories depending on how their runtime
grows with input size. Polynomial-time algorithms (\(O(n^k)\)) are
generally considered tractable, while exponential-time algorithms
(\(O(2^n)\), \(O(n!)\)) quickly become infeasible. In AI, this
distinction often marks the boundary between solvable and impossible
problems at scale.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-33}

Imagine a puzzle where each piece can either fit or not. With 10 pieces,
you might check all possibilities by brute force---it's slow but doable.
With 100 pieces, the number of possibilities explodes astronomically.
Exponential growth feels like climbing a hill that turns into a sheer
cliff.

\subsubsection{Deep Dive}\label{deep-dive-33}

\begin{itemize}
\item
  Polynomial time (P): scalable solutions, e.g., shortest path with
  Dijkstra's algorithm.
\item
  Exponential time: search spaces blow up, e.g., brute-force traveling
  salesman problem.
\item
  NP-complete problems: believed not solvable in polynomial time (unless
  P = NP).
\item
  AI implications:

  \begin{itemize}
  \tightlist
  \item
    Many planning, scheduling, and combinatorial optimization tasks are
    exponential in the worst case.
  \item
    Practical AI relies on heuristics, approximations, or domain
    constraints to avoid exponential blowup.
  \item
    Understanding when exponential behavior appears helps design systems
    that stay usable.
  \end{itemize}
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2041}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2347}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3673}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1939}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Growth Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Runtime (n=50)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Practical?
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Polynomial \(O(n^2)\) & \textasciitilde2,500 steps & Distance matrix
computation & Yes \\
Polynomial \(O(n^3)\) & \textasciitilde125,000 steps & Matrix inversion
in ML & Yes (moderate) \\
Exponential \(O(2^n)\) & \textasciitilde1.1 quadrillion steps &
Brute-force SAT or planning problems & No (infeasible) \\
Factorial \(O(n!)\) & Larger than exponential & Traveling salesman brute
force & Impossible at scale \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-33}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ itertools}
\ImportTok{import}\NormalTok{ time}

\CommentTok{\# Polynomial example: O(n\^{}2)}
\KeywordTok{def}\NormalTok{ polynomial\_sum(n):}
\NormalTok{    total }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
        \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
\NormalTok{            total }\OperatorTok{+=}\NormalTok{ i }\OperatorTok{+}\NormalTok{ j}
    \ControlFlowTok{return}\NormalTok{ total}

\CommentTok{\# Exponential example: brute force subsets}
\KeywordTok{def}\NormalTok{ exponential\_subsets(n):}
\NormalTok{    count }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ subset }\KeywordTok{in}\NormalTok{ itertools.product([}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{], repeat}\OperatorTok{=}\NormalTok{n):}
\NormalTok{        count }\OperatorTok{+=} \DecValTok{1}
    \ControlFlowTok{return}\NormalTok{ count}

\ControlFlowTok{for}\NormalTok{ n }\KeywordTok{in}\NormalTok{ [}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{]:}
\NormalTok{    start }\OperatorTok{=}\NormalTok{ time.time()}
\NormalTok{    exponential\_subsets(n)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"n=}\SpecialCharTok{\{}\NormalTok{n}\SpecialCharTok{\}}\SpecialStringTok{, exponential time elapsed }\SpecialCharTok{\{}\NormalTok{time}\SpecialCharTok{.}\NormalTok{time()}\OperatorTok{{-}}\NormalTok{start}\SpecialCharTok{:.4f\}}\SpecialStringTok{s"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-33}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compare runtime of polynomial vs.~exponential functions as \(n\)
  grows.
\item
  Experiment with heuristic pruning to cut down exponential search.
\item
  Reflect: why do AI systems rely heavily on approximations, heuristics,
  and randomness in exponential domains?
\end{enumerate}

\subsection{35. Intractability and NP-hard
problems}\label{intractability-and-np-hard-problems}

Some problems grow so quickly in complexity that no efficient
(polynomial-time) algorithm is known. These are intractable problems,
often labeled NP-hard. They sit at the edge of what AI can realistically
solve, forcing reliance on heuristics, approximations, or
exponential-time algorithms for small cases.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-34}

Imagine trying to seat 100 guests at 10 tables so that everyone sits
near friends and away from enemies. The number of possible seatings is
astronomical---testing them all would take longer than the age of the
universe. This is the flavor of NP-hardness.

\subsubsection{Deep Dive}\label{deep-dive-34}

\begin{itemize}
\item
  P vs.~NP:

  \begin{itemize}
  \tightlist
  \item
    P = problems solvable in polynomial time.
  \item
    NP = problems whose solutions can be \emph{verified} quickly.
  \end{itemize}
\item
  NP-hard: at least as hard as the hardest problems in NP.
\item
  NP-complete: problems that are both in NP and NP-hard.
\item
  Examples in AI:

  \begin{itemize}
  \tightlist
  \item
    Traveling Salesman Problem (planning, routing).
  \item
    Boolean satisfiability (SAT).
  \item
    Graph coloring (scheduling, resource allocation).
  \end{itemize}
\item
  Approaches:

  \begin{itemize}
  \tightlist
  \item
    Approximation algorithms (e.g., greedy for TSP).
  \item
    Heuristics (local search, simulated annealing).
  \item
    Special cases with efficient solutions.
  \end{itemize}
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1200}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3200}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2900}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2700}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Problem Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Solvable Efficiently?
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
P & Solvable in polynomial time & Shortest path (Dijkstra) & Yes \\
NP & Solution verifiable in poly time & Sudoku solution check &
Verification only \\
NP-complete & In NP + NP-hard & SAT, TSP & Believed no (unless P=NP) \\
NP-hard & At least as hard as NP-complete & General optimization
problems & No known efficient solution \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-34}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ itertools}

\CommentTok{\# Brute force Traveling Salesman Problem (TSP) for 4 cities}
\NormalTok{distances }\OperatorTok{=}\NormalTok{ \{}
\NormalTok{    (}\StringTok{"A"}\NormalTok{,}\StringTok{"B"}\NormalTok{): }\DecValTok{2}\NormalTok{, (}\StringTok{"A"}\NormalTok{,}\StringTok{"C"}\NormalTok{): }\DecValTok{5}\NormalTok{, (}\StringTok{"A"}\NormalTok{,}\StringTok{"D"}\NormalTok{): }\DecValTok{7}\NormalTok{,}
\NormalTok{    (}\StringTok{"B"}\NormalTok{,}\StringTok{"C"}\NormalTok{): }\DecValTok{3}\NormalTok{, (}\StringTok{"B"}\NormalTok{,}\StringTok{"D"}\NormalTok{): }\DecValTok{4}\NormalTok{,}
\NormalTok{    (}\StringTok{"C"}\NormalTok{,}\StringTok{"D"}\NormalTok{): }\DecValTok{2}
\NormalTok{\}}

\NormalTok{cities }\OperatorTok{=}\NormalTok{ [}\StringTok{"A"}\NormalTok{,}\StringTok{"B"}\NormalTok{,}\StringTok{"C"}\NormalTok{,}\StringTok{"D"}\NormalTok{]}

\KeywordTok{def}\NormalTok{ path\_length(path):}
    \ControlFlowTok{return} \BuiltInTok{sum}\NormalTok{(distances.get((}\BuiltInTok{min}\NormalTok{(a,b), }\BuiltInTok{max}\NormalTok{(a,b)), }\DecValTok{0}\NormalTok{) }\ControlFlowTok{for}\NormalTok{ a,b }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{(path, path[}\DecValTok{1}\NormalTok{:]))}

\NormalTok{best\_path, best\_len }\OperatorTok{=} \VariableTok{None}\NormalTok{, }\BuiltInTok{float}\NormalTok{(}\StringTok{"inf"}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ perm }\KeywordTok{in}\NormalTok{ itertools.permutations(cities):}
\NormalTok{    length }\OperatorTok{=}\NormalTok{ path\_length(perm)}
    \ControlFlowTok{if}\NormalTok{ length }\OperatorTok{\textless{}}\NormalTok{ best\_len:}
\NormalTok{        best\_len, best\_path }\OperatorTok{=}\NormalTok{ length, perm}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Best path:"}\NormalTok{, best\_path, }\StringTok{"Length:"}\NormalTok{, best\_len)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-34}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Increase the number of cities---how quickly does brute force become
  infeasible?
\item
  Add a greedy heuristic (always go to nearest city)---compare results
  with brute force.
\item
  Reflect: why does much of AI research focus on clever approximations
  for NP-hard problems?
\end{enumerate}

\subsection{36. Approximation and heuristics as
necessity}\label{approximation-and-heuristics-as-necessity}

When exact solutions are intractable, AI relies on approximation
algorithms and heuristics. Instead of guaranteeing the optimal answer,
these methods aim for ``good enough'' solutions within feasible time.
This pragmatic trade-off makes otherwise impossible problems solvable in
practice.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-35}

Think of packing a suitcase in a hurry. The optimal arrangement would
maximize space perfectly, but finding it would take hours. Instead, you
use a heuristic---roll clothes, fill corners, put shoes on the bottom.
The result isn't optimal, but it's practical.

\subsubsection{Deep Dive}\label{deep-dive-35}

\begin{itemize}
\item
  Approximation algorithms: guarantee solutions within a factor of the
  optimum (e.g., TSP with 1.5× bound).
\item
  Heuristics: rules of thumb, no guarantees, but often effective (e.g.,
  greedy search, hill climbing).
\item
  Metaheuristics: general strategies like simulated annealing, genetic
  algorithms, tabu search.
\item
  AI applications:

  \begin{itemize}
  \tightlist
  \item
    Game playing: heuristic evaluation functions.
  \item
    Scheduling: approximate resource allocation.
  \item
    Robotics: heuristic motion planning.
  \end{itemize}
\item
  Trade-off: speed vs.~accuracy. Heuristics enable scalability but may
  yield poor results in worst cases.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2170}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2830}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2170}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2830}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Guarantee
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Exact algorithm & Optimal solution & Brute-force SAT solver & Infeasible
at scale \\
Approximation algorithm & Within known performance gap & Approx. TSP
solver & May still be expensive \\
Heuristic & No guarantee, fast in practice & Greedy search in graphs &
Can miss good solutions \\
Metaheuristic & Broad search strategies & Genetic algorithms, SA & May
require tuning, stochastic \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-35}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Greedy heuristic for Traveling Salesman Problem}
\ImportTok{import}\NormalTok{ random}

\NormalTok{cities }\OperatorTok{=}\NormalTok{ [}\StringTok{"A"}\NormalTok{,}\StringTok{"B"}\NormalTok{,}\StringTok{"C"}\NormalTok{,}\StringTok{"D"}\NormalTok{]}
\NormalTok{distances }\OperatorTok{=}\NormalTok{ \{}
\NormalTok{    (}\StringTok{"A"}\NormalTok{,}\StringTok{"B"}\NormalTok{): }\DecValTok{2}\NormalTok{, (}\StringTok{"A"}\NormalTok{,}\StringTok{"C"}\NormalTok{): }\DecValTok{5}\NormalTok{, (}\StringTok{"A"}\NormalTok{,}\StringTok{"D"}\NormalTok{): }\DecValTok{7}\NormalTok{,}
\NormalTok{    (}\StringTok{"B"}\NormalTok{,}\StringTok{"C"}\NormalTok{): }\DecValTok{3}\NormalTok{, (}\StringTok{"B"}\NormalTok{,}\StringTok{"D"}\NormalTok{): }\DecValTok{4}\NormalTok{,}
\NormalTok{    (}\StringTok{"C"}\NormalTok{,}\StringTok{"D"}\NormalTok{): }\DecValTok{2}
\NormalTok{\}}

\KeywordTok{def}\NormalTok{ dist(a,b):}
    \ControlFlowTok{return}\NormalTok{ distances.get((}\BuiltInTok{min}\NormalTok{(a,b), }\BuiltInTok{max}\NormalTok{(a,b)), }\DecValTok{0}\NormalTok{)}

\KeywordTok{def}\NormalTok{ greedy\_tsp(start):}
\NormalTok{    unvisited }\OperatorTok{=} \BuiltInTok{set}\NormalTok{(cities)}
\NormalTok{    path }\OperatorTok{=}\NormalTok{ [start]}
\NormalTok{    unvisited.remove(start)}
    \ControlFlowTok{while}\NormalTok{ unvisited:}
\NormalTok{        next\_city }\OperatorTok{=} \BuiltInTok{min}\NormalTok{(unvisited, key}\OperatorTok{=}\KeywordTok{lambda}\NormalTok{ c: dist(path[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{], c))}
\NormalTok{        path.append(next\_city)}
\NormalTok{        unvisited.remove(next\_city)}
    \ControlFlowTok{return}\NormalTok{ path}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Greedy path:"}\NormalTok{, greedy\_tsp(}\StringTok{"A"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-35}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compare greedy paths with brute-force optimal ones---how close are
  they?
\item
  Randomize starting city---does it change the quality of the solution?
\item
  Reflect: why are heuristics indispensable in AI despite their lack of
  guarantees?
\end{enumerate}

\subsection{37. Resource-bounded
rationality}\label{resource-bounded-rationality}

Classical rationality assumes unlimited time and computational resources
to find the optimal decision. Resource-bounded rationality recognizes
real-world limits: agents must make good decisions quickly with limited
data, time, and processing power. In AI, this often means
``satisficing'' rather than optimizing.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-36}

Imagine playing chess with only 10 seconds per move. You cannot explore
every possible sequence. Instead, you look a few moves ahead, use
heuristics, and pick a reasonable option. This is rationality under
resource bounds.

\subsubsection{Deep Dive}\label{deep-dive-36}

\begin{itemize}
\item
  Bounded rationality (Herbert Simon): decision-makers use heuristics
  and approximations within limits.
\item
  Anytime algorithms: produce a valid solution quickly and improve it
  with more time.
\item
  Meta-reasoning: deciding how much effort to spend thinking before
  acting.
\item
  Real-world AI:

  \begin{itemize}
  \tightlist
  \item
    Self-driving cars must act in milliseconds.
  \item
    Embedded devices have strict memory and CPU constraints.
  \item
    Cloud AI balances accuracy with cost and energy.
  \end{itemize}
\item
  Key trade-off: doing the best possible with limited resources
  vs.~chasing perfect optimality.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1652}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3130}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2261}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2957}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Advantage
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Perfect rationality & Exhaustive search in chess & Optimal solution &
Infeasible with large state spaces \\
Resource-bounded & Alpha-Beta pruning, heuristic search & Fast, usable
decisions & May miss optimal moves \\
Anytime algorithm & Iterative deepening search & Improves with time &
Requires time allocation strategy \\
Meta-reasoning & Adaptive compute allocation & Balances speed
vs.~quality & Complex to implement \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-36}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Anytime algorithm: improving solution over time}
\ImportTok{import}\NormalTok{ random}

\KeywordTok{def}\NormalTok{ anytime\_max(iterations):}
\NormalTok{    best }\OperatorTok{=} \BuiltInTok{float}\NormalTok{(}\StringTok{"{-}inf"}\NormalTok{)}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(iterations):}
\NormalTok{        candidate }\OperatorTok{=}\NormalTok{ random.randint(}\DecValTok{0}\NormalTok{, }\DecValTok{100}\NormalTok{)}
        \ControlFlowTok{if}\NormalTok{ candidate }\OperatorTok{\textgreater{}}\NormalTok{ best:}
\NormalTok{            best }\OperatorTok{=}\NormalTok{ candidate}
        \ControlFlowTok{yield}\NormalTok{ best  }\CommentTok{\# current best solution}

\ControlFlowTok{for}\NormalTok{ result }\KeywordTok{in}\NormalTok{ anytime\_max(}\DecValTok{5}\NormalTok{):}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"Current best:"}\NormalTok{, result)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-36}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Increase iterations---watch how the solution improves over time.
\item
  Add a time cutoff to simulate resource limits.
\item
  Reflect: when should an AI stop computing and act with the best
  solution so far?
\end{enumerate}

\subsection{38. Physical limits of computation (energy,
speed)}\label{physical-limits-of-computation-energy-speed}

Computation is not abstract alone---it is grounded in physics. The
energy required, the speed of signal propagation, and thermodynamic laws
set ultimate limits on what machines can compute. For AI, this means
efficiency is not just an engineering concern but a fundamental
constraint.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-37}

Imagine trying to boil water instantly. No matter how good the pot or
stove, physics won't allow it---you're bounded by energy transfer
limits. Similarly, computers cannot compute arbitrarily fast without
hitting physical barriers.

\subsubsection{Deep Dive}\label{deep-dive-37}

\begin{itemize}
\item
  Landauer's principle: erasing one bit of information requires at least
  \(kT \ln 2\) energy (thermodynamic cost).
\item
  Speed of light: limits how fast signals can propagate across chips and
  networks.
\item
  Heat dissipation: as transistor density increases, power and cooling
  become bottlenecks.
\item
  Quantum limits: classical computation constrained by physical laws,
  leading to quantum computing explorations.
\item
  AI implications:

  \begin{itemize}
  \tightlist
  \item
    Training massive models consumes megawatt-hours of energy.
  \item
    Hardware design (GPUs, TPUs, neuromorphic chips) focuses on pushing
    efficiency.
  \item
    Sustainable AI requires respecting physical resource constraints.
  \end{itemize}
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2326}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3488}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4186}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Physical Limit
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Explanation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Impact on AI
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Landauer's principle & Minimum energy per bit erased & Lower bound on
computation cost \\
Speed of light & Limits interconnect speed & Affects distributed AI,
data centers \\
Heat dissipation & Power density ceiling & Restricts chip scaling \\
Quantum effects & Noise at nanoscale transistors & Push toward quantum /
new paradigms \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-37}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Estimate Landauer\textquotesingle{}s limit energy for bit erasure}
\ImportTok{import}\NormalTok{ math}

\NormalTok{k }\OperatorTok{=} \FloatTok{1.38e{-}23}  \CommentTok{\# Boltzmann constant}
\NormalTok{T }\OperatorTok{=} \DecValTok{300}       \CommentTok{\# room temperature in Kelvin}
\NormalTok{energy }\OperatorTok{=}\NormalTok{ k }\OperatorTok{*}\NormalTok{ T }\OperatorTok{*}\NormalTok{ math.log(}\DecValTok{2}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Minimum energy per bit erase:"}\NormalTok{, energy, }\StringTok{"Joules"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-37}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Change the temperature---how does energy per bit change?
\item
  Compare energy per bit with energy use in a modern GPU---see the gap.
\item
  Reflect: how do physical laws shape the trajectory of AI hardware and
  algorithm design?
\end{enumerate}

\subsection{39. Complexity and intelligence:
trade-offs}\label{complexity-and-intelligence-trade-offs}

Greater intelligence often requires handling greater computational
complexity. Yet, too much complexity makes systems slow, inefficient, or
fragile. Designing AI means balancing sophistication with
tractability---finding the sweet spot where intelligence is powerful but
still practical.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-38}

Think of learning to play chess. A beginner looks only one or two moves
ahead---fast but shallow. A grandmaster considers dozens of
possibilities---deep but time-consuming. Computers face the same
dilemma: more complexity gives deeper insight but costs more resources.

\subsubsection{Deep Dive}\label{deep-dive-38}

\begin{itemize}
\item
  Complex models: deep networks, probabilistic programs, symbolic
  reasoners---capable but expensive.
\item
  Simple models: linear classifiers, decision stumps---fast but limited.
\item
  Trade-offs:

  \begin{itemize}
  \tightlist
  \item
    Depth vs.~speed (deep reasoning vs.~real-time action).
  \item
    Accuracy vs.~interpretability (complex vs.~simple models).
  \item
    Optimality vs.~feasibility (exact vs.~approximate algorithms).
  \end{itemize}
\item
  AI strategies:

  \begin{itemize}
  \tightlist
  \item
    Hierarchical models: combine simple reflexes with complex planning.
  \item
    Hybrid systems: symbolic reasoning + sub-symbolic learning.
  \item
    Resource-aware learning: adjust model complexity dynamically.
  \end{itemize}
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2192}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3288}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4521}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Dimension
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Low Complexity
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
High Complexity
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Speed & Fast, responsive & Slow, resource-heavy \\
Accuracy & Coarse, less general & Precise, adaptable \\
Interpretability & Transparent, explainable & Opaque, hard to analyze \\
Robustness & Fewer failure modes & Prone to overfitting, brittleness \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-38}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Trade{-}off: simple vs. complex models}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LogisticRegression}
\ImportTok{from}\NormalTok{ sklearn.neural\_network }\ImportTok{import}\NormalTok{ MLPClassifier}
\ImportTok{from}\NormalTok{ sklearn.datasets }\ImportTok{import}\NormalTok{ make\_classification}
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ train\_test\_split}

\NormalTok{X, y }\OperatorTok{=}\NormalTok{ make\_classification(n\_samples}\OperatorTok{=}\DecValTok{500}\NormalTok{, n\_features}\OperatorTok{=}\DecValTok{20}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{42}\NormalTok{)}
\NormalTok{X\_train, X\_test, y\_train, y\_test }\OperatorTok{=}\NormalTok{ train\_test\_split(X, y, test\_size}\OperatorTok{=}\FloatTok{0.2}\NormalTok{)}

\NormalTok{simple\_model }\OperatorTok{=}\NormalTok{ LogisticRegression().fit(X\_train, y\_train)}
\NormalTok{complex\_model }\OperatorTok{=}\NormalTok{ MLPClassifier(hidden\_layer\_sizes}\OperatorTok{=}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{50}\NormalTok{), max\_iter}\OperatorTok{=}\DecValTok{500}\NormalTok{).fit(X\_train, y\_train)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Simple model accuracy:"}\NormalTok{, simple\_model.score(X\_test, y\_test))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Complex model accuracy:"}\NormalTok{, complex\_model.score(X\_test, y\_test))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-38}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compare training times of the two models---how does complexity affect
  speed?
\item
  Add noise to data---does the complex model overfit while the simple
  model stays stable?
\item
  Reflect: in which domains is simplicity preferable, and where is
  complexity worth the cost?
\end{enumerate}

\subsection{40. Theoretical boundaries of AI
systems}\label{theoretical-boundaries-of-ai-systems}

AI is constrained not just by engineering challenges but by fundamental
theoretical limits. Some problems are provably unsolvable, others are
intractable, and some cannot be solved reliably under uncertainty.
Recognizing these boundaries prevents overpromising and guides realistic
AI design.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-39}

Imagine asking a calculator to tell you whether any arbitrary computer
program will run forever or eventually stop. No matter how advanced the
calculator is, this question---the Halting Problem---is mathematically
undecidable. AI inherits these hard boundaries from computation theory.

\subsubsection{Deep Dive}\label{deep-dive-39}

\begin{itemize}
\item
  Unsolvable problems:

  \begin{itemize}
  \tightlist
  \item
    Halting problem: no algorithm can decide for all programs if they
    halt.
  \item
    Certain logical inference tasks are undecidable.
  \end{itemize}
\item
  Intractable problems: solvable in principle but not in reasonable time
  (NP-hard, PSPACE-complete).
\item
  Approximation limits: some problems cannot even be approximated
  efficiently.
\item
  Uncertainty limits: no model can perfectly predict inherently
  stochastic or chaotic processes.
\item
  Implications for AI:

  \begin{itemize}
  \tightlist
  \item
    Absolute guarantees are often impossible.
  \item
    AI must rely on heuristics, approximations, and probabilistic
    reasoning.
  \item
    Awareness of boundaries helps avoid misusing AI in domains where
    guarantees are essential.
  \end{itemize}
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2234}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3511}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4255}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Boundary Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Undecidable & No algorithm exists & Halting problem, general theorem
proving \\
Intractable & Solvable, but not efficiently & Planning, SAT solving,
TSP \\
Approximation barrier & Cannot approximate within factor & Certain graph
coloring problems \\
Uncertainty bound & Outcomes inherently unpredictable & Stock prices,
weather chaos limits \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-39}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Halting problem illustration (toy version)}
\KeywordTok{def}\NormalTok{ halts(program, input\_data):}
    \ControlFlowTok{raise} \PreprocessorTok{NotImplementedError}\NormalTok{(}\StringTok{"Impossible to implement universally"}\NormalTok{)}

\ControlFlowTok{try}\NormalTok{:}
\NormalTok{    halts(}\KeywordTok{lambda}\NormalTok{ x: x}\OperatorTok{+}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\ControlFlowTok{except} \PreprocessorTok{NotImplementedError} \ImportTok{as}\NormalTok{ e:}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"Halting problem:"}\NormalTok{, e)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-39}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Explore NP-complete problems like SAT or Sudoku---why do they scale
  poorly?
\item
  Reflect on cases where undecidability or intractability forces AI to
  rely on heuristics.
\item
  Ask: how should policymakers and engineers account for these
  boundaries when deploying AI?
\end{enumerate}

\section{Chapter 5. Representation and
Abstraction}\label{chapter-5.-representation-and-abstraction}

\subsection{41. Why representation matters in
intelligence}\label{why-representation-matters-in-intelligence}

Representation determines what an AI system can perceive, reason about,
and act upon. The same problem framed differently can be easy or
impossible to solve. Good representations make patterns visible, reduce
complexity, and enable generalization.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-40}

Imagine solving a maze. If you only see the walls one step at a time,
navigation is hard. If you have a map, the maze becomes much easier. The
representation---the raw sensory stream vs.~the structured map---changes
the difficulty of the task.

\subsubsection{Deep Dive}\label{deep-dive-40}

\begin{itemize}
\item
  Role of representation: it bridges raw data and actionable knowledge.
\item
  Expressiveness: rich enough to capture relevant details.
\item
  Compactness: simple enough to be efficient.
\item
  Generalization: supports applying knowledge to new situations.
\item
  AI applications:

  \begin{itemize}
  \tightlist
  \item
    Vision: pixels → edges → objects.
  \item
    Language: characters → words → embeddings.
  \item
    Robotics: sensor readings → state space → control policies.
  \end{itemize}
\item
  Challenge: too simple a representation loses information, too complex
  makes reasoning intractable.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1776}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2710}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2617}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2897}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Representation Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strength
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Raw data & Pixels, waveforms & Complete, no preprocessing & Redundant,
hard to interpret \\
Hand-crafted & SIFT features, parse trees & Human insight, interpretable
& Brittle, domain-specific \\
Learned & Word embeddings, latent codes & Adaptive, scalable & Often
opaque, hard to interpret \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-40}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Comparing representations: raw vs. transformed}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Raw pixel intensities (3x3 image patch)}
\NormalTok{raw }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{, }\DecValTok{255}\NormalTok{, }\DecValTok{0}\NormalTok{],}
\NormalTok{                [}\DecValTok{255}\NormalTok{, }\DecValTok{255}\NormalTok{, }\DecValTok{255}\NormalTok{],}
\NormalTok{                [}\DecValTok{0}\NormalTok{, }\DecValTok{255}\NormalTok{, }\DecValTok{0}\NormalTok{]])}

\CommentTok{\# Derived representation: edges (simple horizontal diff)}
\NormalTok{edges }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{abs}\NormalTok{(np.diff(raw, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{))}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Raw data:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, raw)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Edge{-}based representation:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, edges)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-40}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Replace the pixel matrix with a new pattern---how does the edge
  representation change?
\item
  Add noise to raw data---does the transformed representation make the
  pattern clearer?
\item
  Reflect: what representations make problems easier for humans vs.~for
  machines?
\end{enumerate}

\subsection{42. Symbolic vs.~sub-symbolic
representations}\label{symbolic-vs.-sub-symbolic-representations}

AI representations can be broadly divided into symbolic (explicit
symbols and rules) and sub-symbolic (distributed numerical patterns).
Symbolic approaches excel at reasoning and structure, while sub-symbolic
approaches excel at perception and pattern recognition. Modern AI often
blends the two.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-41}

Think of language. A grammar book describes language symbolically with
rules (noun, verb, adjective). But when you actually \emph{hear} speech,
your brain processes sounds sub-symbolically---patterns of frequencies
and rhythms. Both perspectives are useful but different.

\subsubsection{Deep Dive}\label{deep-dive-41}

\begin{itemize}
\tightlist
\item
  Symbolic representation: logic, rules, graphs, knowledge bases.
  Transparent, interpretable, suited for reasoning.
\item
  Sub-symbolic representation: vectors, embeddings, neural activations.
  Captures similarity, fuzzy concepts, robust to noise.
\item
  Hybrid systems: neuro-symbolic AI combines the interpretability of
  symbols with the flexibility of neural networks.
\item
  Challenge: symbols handle structure but lack adaptability;
  sub-symbolic systems learn patterns but lack explicit reasoning.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1273}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2727}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strength
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Symbolic & Expert systems, logic programs & Transparent, rule-based
reasoning & Brittle, hard to learn from data \\
Sub-symbolic & Word embeddings, deep nets & Robust, generalizable &
Opaque, hard to explain reasoning \\
Neuro-symbolic & Logic + neural embeddings & Combines structure +
learning & Integration still an open problem \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-41}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Symbolic vs. sub{-}symbolic toy example}

\CommentTok{\# Symbolic rule: if animal has wings {-}\textgreater{} classify as bird}
\KeywordTok{def}\NormalTok{ classify\_symbolic(animal):}
    \ControlFlowTok{if} \StringTok{"wings"} \KeywordTok{in}\NormalTok{ animal:}
        \ControlFlowTok{return} \StringTok{"bird"}
    \ControlFlowTok{return} \StringTok{"not bird"}

\CommentTok{\# Sub{-}symbolic: similarity via embeddings}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{emb }\OperatorTok{=}\NormalTok{ \{}\StringTok{"bird"}\NormalTok{: np.array([}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{]), }\StringTok{"cat"}\NormalTok{: np.array([}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{]), }\StringTok{"bat"}\NormalTok{: np.array([}\FloatTok{0.8}\NormalTok{,}\FloatTok{0.2}\NormalTok{])\}}

\KeywordTok{def}\NormalTok{ cosine(a, b):}
    \ControlFlowTok{return}\NormalTok{ np.dot(a,b)}\OperatorTok{/}\NormalTok{(np.linalg.norm(a)}\OperatorTok{*}\NormalTok{np.linalg.norm(b))}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Symbolic:"}\NormalTok{, classify\_symbolic([}\StringTok{"wings"}\NormalTok{]))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Sub{-}symbolic similarity (bat vs bird):"}\NormalTok{, cosine(emb[}\StringTok{"bat"}\NormalTok{], emb[}\StringTok{"bird"}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-41}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add more symbolic rules---how brittle do they become?
\item
  Expand embeddings with more animals---does similarity capture fuzzy
  categories?
\item
  Reflect: why might the future of AI require blending symbolic clarity
  with sub-symbolic power?
\end{enumerate}

\subsection{43. Data structures: vectors, graphs,
trees}\label{data-structures-vectors-graphs-trees}

Intelligent systems rely on structured ways to organize information.
Vectors capture numerical features, graphs represent relationships, and
trees encode hierarchies. Each data structure enables different forms of
reasoning, making them foundational to AI.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-42}

Think of a city: coordinates (latitude, longitude) describe locations as
vectors; roads connecting intersections form a graph; a family tree of
neighborhoods and sub-districts is a tree. Different structures reveal
different aspects of the same world.

\subsubsection{Deep Dive}\label{deep-dive-42}

\begin{itemize}
\item
  Vectors: fixed-length arrays of numbers; used in embeddings, features,
  sensor readings.
\item
  Graphs: nodes + edges; model social networks, molecules, knowledge
  graphs.
\item
  Trees: hierarchical branching structures; model parse trees in
  language, decision trees in learning.
\item
  AI applications:

  \begin{itemize}
  \tightlist
  \item
    Vectors: word2vec, image embeddings.
  \item
    Graphs: graph neural networks, pathfinding.
  \item
    Trees: search algorithms, syntactic parsing.
  \end{itemize}
\item
  Key trade-off: choosing the right data structure shapes efficiency and
  insight.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0804}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1339}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2411}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2679}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2768}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Structure
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Representation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strength
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Vector & Array of values & Word embeddings, features & Compact,
efficient computation & Limited structural expressivity \\
Graph & Nodes + edges & Knowledge graphs, GNNs & Rich relational
modeling & Costly for large graphs \\
Tree & Hierarchical & Decision trees, parse trees & Intuitive, recursive
reasoning & Less flexible than graphs \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-42}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Vectors, graphs, trees in practice}
\ImportTok{import}\NormalTok{ networkx }\ImportTok{as}\NormalTok{ nx}

\CommentTok{\# Vector: embedding for a word}
\NormalTok{vector }\OperatorTok{=}\NormalTok{ [}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }\FloatTok{0.5}\NormalTok{]}

\CommentTok{\# Graph: simple knowledge network}
\NormalTok{G }\OperatorTok{=}\NormalTok{ nx.Graph()}
\NormalTok{G.add\_edges\_from([(}\StringTok{"AI"}\NormalTok{,}\StringTok{"ML"}\NormalTok{), (}\StringTok{"AI"}\NormalTok{,}\StringTok{"Robotics"}\NormalTok{), (}\StringTok{"ML"}\NormalTok{,}\StringTok{"Deep Learning"}\NormalTok{)])}

\CommentTok{\# Tree: nested dictionary as a simple hierarchy}
\NormalTok{tree }\OperatorTok{=}\NormalTok{ \{}\StringTok{"Animal"}\NormalTok{: \{}\StringTok{"Mammal"}\NormalTok{: [}\StringTok{"Dog"}\NormalTok{,}\StringTok{"Cat"}\NormalTok{], }\StringTok{"Bird"}\NormalTok{: [}\StringTok{"Sparrow"}\NormalTok{,}\StringTok{"Eagle"}\NormalTok{]\}\}}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Vector:"}\NormalTok{, vector)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Graph neighbors of AI:"}\NormalTok{, }\BuiltInTok{list}\NormalTok{(G.neighbors(}\StringTok{"AI"}\NormalTok{)))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Tree root categories:"}\NormalTok{, }\BuiltInTok{list}\NormalTok{(tree[}\StringTok{"Animal"}\NormalTok{].keys()))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-42}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add another dimension to the vector---how does it change
  interpretation?
\item
  Add nodes and edges to the graph---what new paths emerge?
\item
  Expand the tree---how does hierarchy help organize complexity?
\end{enumerate}

\subsection{44. Levels of abstraction: micro vs.~macro
views}\label{levels-of-abstraction-micro-vs.-macro-views}

Abstraction allows AI systems to operate at different levels of detail.
The micro view focuses on fine-grained, low-level states, while the
macro view captures higher-level summaries and patterns. Switching
between these views makes complex problems tractable.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-43}

Imagine traffic on a highway. At the micro level, you could track every
car's position and speed. At the macro level, you think in terms of
``traffic jam ahead'' or ``smooth flow.'' Both perspectives are valid
but serve different purposes.

\subsubsection{Deep Dive}\label{deep-dive-43}

\begin{itemize}
\item
  Micro-level representations: precise, detailed, computationally heavy.
  Examples: pixel-level vision, molecular simulations.
\item
  Macro-level representations: aggregated, simplified, more
  interpretable. Examples: object recognition, weather patterns.
\item
  Bridging levels: hierarchical models and abstractions (e.g., CNNs
  build from pixels → edges → objects).
\item
  AI applications:

  \begin{itemize}
  \tightlist
  \item
    Natural language: characters → words → sentences → topics.
  \item
    Robotics: joint torques → motor actions → tasks → goals.
  \item
    Systems: log events → user sessions → overall trends.
  \end{itemize}
\item
  Challenge: too much detail overwhelms; too much abstraction loses
  important nuance.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0900}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2900}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3200}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Level
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strength
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Micro & Pixel intensities in an image & Precise, full information & Hard
to interpret, inefficient \\
Macro & Object labels (``cat'', ``dog'') & Concise, human-aligned &
Misses fine-grained details \\
Hierarchy & Pixels → edges → objects & Balance of detail and efficiency
& Requires careful design \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-43}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Micro vs. macro abstraction}
\NormalTok{pixels }\OperatorTok{=}\NormalTok{ [[}\DecValTok{0}\NormalTok{, }\DecValTok{255}\NormalTok{, }\DecValTok{0}\NormalTok{],}
\NormalTok{          [}\DecValTok{255}\NormalTok{, }\DecValTok{255}\NormalTok{, }\DecValTok{255}\NormalTok{],}
\NormalTok{          [}\DecValTok{0}\NormalTok{, }\DecValTok{255}\NormalTok{, }\DecValTok{0}\NormalTok{]]}

\CommentTok{\# Macro abstraction: majority value (simple summary)}
\NormalTok{flattened }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(pixels, [])}
\NormalTok{macro }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(}\BuiltInTok{set}\NormalTok{(flattened), key}\OperatorTok{=}\NormalTok{flattened.count)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Micro (pixels):"}\NormalTok{, pixels)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Macro (dominant intensity):"}\NormalTok{, macro)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-43}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Replace the pixel grid with a different pattern---does the macro
  summary still capture the essence?
\item
  Add intermediate abstraction (edges, shapes)---how does it help bridge
  micro and macro?
\item
  Reflect: which tasks benefit from fine detail, and which from coarse
  summaries?
\end{enumerate}

\subsection{45. Compositionality and
modularity}\label{compositionality-and-modularity}

Compositionality is the principle that complex ideas can be built from
simpler parts. Modularity is the design strategy of keeping components
separable and reusable. Together, they allow AI systems to scale,
generalize, and adapt by combining building blocks.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-44}

Think of LEGO bricks. Each brick is simple, but by snapping them
together, you can build houses, cars, or spaceships. AI works the same
way---small representations (words, features, functions) compose into
larger structures (sentences, models, systems).

\subsubsection{Deep Dive}\label{deep-dive-44}

\begin{itemize}
\item
  Compositionality in language: meanings of sentences derive from
  meanings of words plus grammar.
\item
  Compositionality in vision: objects are built from parts (edges →
  shapes → objects → scenes).
\item
  Modularity in systems: separating perception, reasoning, and action
  into subsystems.
\item
  Benefits:

  \begin{itemize}
  \tightlist
  \item
    Scalability: large systems built from small components.
  \item
    Generalization: reuse parts in new contexts.
  \item
    Debuggability: easier to isolate errors.
  \end{itemize}
\item
  Challenges:

  \begin{itemize}
  \tightlist
  \item
    Deep learning models often entangle representations.
  \item
    Explicit modularity may reduce raw predictive power but improve
    interpretability.
  \end{itemize}
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1301}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3415}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2683}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2602}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Principle
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strength
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Compositionality & Language: words → phrases → sentences & Enables
systematic generalization & Hard to capture in neural models \\
Modularity & ML pipelines: preprocessing → model → eval & Maintainable,
reusable & Integration overhead \\
Hybrid & Neuro-symbolic systems & Combines flexibility + structure &
Still an open research problem \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-44}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simple compositionality example}
\NormalTok{words }\OperatorTok{=}\NormalTok{ \{}\StringTok{"red"}\NormalTok{: }\StringTok{"color"}\NormalTok{, }\StringTok{"ball"}\NormalTok{: }\StringTok{"object"}\NormalTok{\}}

\KeywordTok{def}\NormalTok{ compose(phrase):}
    \ControlFlowTok{return}\NormalTok{ [words[w] }\ControlFlowTok{for}\NormalTok{ w }\KeywordTok{in}\NormalTok{ phrase.split() }\ControlFlowTok{if}\NormalTok{ w }\KeywordTok{in}\NormalTok{ words]}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Phrase: \textquotesingle{}red ball\textquotesingle{}"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Composed representation:"}\NormalTok{, compose(}\StringTok{"red ball"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-44}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Extend the dictionary with more words---what complex meanings can you
  build?
\item
  Add modular functions (e.g., color(), shape()) to handle categories
  separately.
\item
  Reflect: why do humans excel at compositionality, and how can AI
  systems learn it better?
\end{enumerate}

\subsection{46. Continuous vs.~discrete
abstractions}\label{continuous-vs.-discrete-abstractions}

Abstractions in AI can be continuous (smooth, real-valued) or discrete
(symbolic, categorical). Each offers strengths: continuous abstractions
capture nuance and gradients, while discrete abstractions capture
structure and rules. Many modern systems combine both.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-45}

Think of music. The sheet notation uses discrete symbols (notes, rests),
while the actual performance involves continuous variations in pitch,
volume, and timing. Both are essential to represent the same melody.

\subsubsection{Deep Dive}\label{deep-dive-45}

\begin{itemize}
\item
  Continuous representations: vectors, embeddings, probability
  distributions. Enable optimization with calculus and gradient descent.
\item
  Discrete representations: logic rules, parse trees, categorical
  labels. Enable precise reasoning and combinatorial search.
\item
  Hybrid representations: discretized latent variables, quantized
  embeddings, symbolic-neural hybrids.
\item
  AI applications:

  \begin{itemize}
  \tightlist
  \item
    Vision: pixels (continuous) vs.~object categories (discrete).
  \item
    Language: embeddings (continuous) vs.~grammar rules (discrete).
  \item
    Robotics: control signals (continuous) vs.~task planning (discrete).
  \end{itemize}
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1468}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2844}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2752}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2936}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Abstraction Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strength
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Continuous & Word embeddings, sensor signals & Smooth optimization,
nuance & Harder to interpret \\
Discrete & Grammar rules, class labels & Clear structure, interpretable
& Brittle, less flexible \\
Hybrid & Vector-symbol integration & Combines flexibility + clarity &
Still an open research challenge \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-45}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Continuous vs. discrete abstraction}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Continuous: word embeddings}
\NormalTok{embeddings }\OperatorTok{=}\NormalTok{ \{}\StringTok{"cat"}\NormalTok{: np.array([}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.8}\NormalTok{]),}
              \StringTok{"dog"}\NormalTok{: np.array([}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.75}\NormalTok{])\}}

\CommentTok{\# Discrete: labels}
\NormalTok{labels }\OperatorTok{=}\NormalTok{ \{}\StringTok{"cat"}\NormalTok{: }\StringTok{"animal"}\NormalTok{, }\StringTok{"dog"}\NormalTok{: }\StringTok{"animal"}\NormalTok{\}}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Continuous similarity (cat vs dog):"}\NormalTok{,}
\NormalTok{      np.dot(embeddings[}\StringTok{"cat"}\NormalTok{], embeddings[}\StringTok{"dog"}\NormalTok{]))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Discrete label (cat):"}\NormalTok{, labels[}\StringTok{"cat"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-45}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add more embeddings---does similarity reflect semantic closeness?
\item
  Add discrete categories that clash with continuous similarities---what
  happens?
\item
  Reflect: when should AI favor continuous nuance, and when discrete
  clarity?
\end{enumerate}

\subsection{47. Representation learning in modern
AI}\label{representation-learning-in-modern-ai}

Representation learning is the process by which AI systems automatically
discover useful ways to encode data, instead of relying solely on
hand-crafted features. Modern deep learning thrives on this principle:
neural networks learn hierarchical representations directly from raw
inputs.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-46}

Imagine teaching a child to recognize animals. You don't explicitly tell
them ``look for four legs, a tail, fur.'' Instead, they learn these
features themselves by seeing many examples. Representation learning
automates this same discovery process in machines.

\subsubsection{Deep Dive}\label{deep-dive-46}

\begin{itemize}
\item
  Manual features vs.~learned features: early AI relied on
  expert-crafted descriptors (e.g., SIFT in vision). Deep learning
  replaced these with data-driven embeddings.
\item
  Hierarchical learning:

  \begin{itemize}
  \tightlist
  \item
    Low layers capture simple patterns (edges, phonemes).
  \item
    Mid layers capture parts or phrases.
  \item
    High layers capture objects, semantics, or abstract meaning.
  \end{itemize}
\item
  Self-supervised learning: representations can be learned without
  explicit labels (contrastive learning, masked prediction).
\item
  Applications: word embeddings, image embeddings, audio features,
  multimodal representations.
\item
  Challenge: learned representations are powerful but often opaque,
  raising interpretability and bias concerns.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2300}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2200}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3100}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2400}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strength
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Hand-crafted features & SIFT, TF-IDF & Interpretable, domain knowledge &
Brittle, not scalable \\
Learned representations & CNNs, Transformers & Adaptive, scalable & Hard
to interpret \\
Self-supervised reps & Word2Vec, SimCLR, BERT & Leverages unlabeled data
& Data- and compute-hungry \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-46}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Toy example: representation learning with PCA}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.decomposition }\ImportTok{import}\NormalTok{ PCA}

\CommentTok{\# 2D points clustered by class}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{],[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{],[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{],[}\DecValTok{8}\NormalTok{,}\DecValTok{8}\NormalTok{],[}\DecValTok{9}\NormalTok{,}\DecValTok{7}\NormalTok{],[}\DecValTok{10}\NormalTok{,}\DecValTok{9}\NormalTok{]])}
\NormalTok{pca }\OperatorTok{=}\NormalTok{ PCA(n\_components}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{X\_reduced }\OperatorTok{=}\NormalTok{ pca.fit\_transform(X)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Original shape:"}\NormalTok{, X.shape)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Reduced representation:"}\NormalTok{, X\_reduced.ravel())}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-46}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Apply PCA on different datasets---how does dimensionality reduction
  reveal structure?
\item
  Replace PCA with autoencoders---how do nonlinear representations
  differ?
\item
  Reflect: why is learning representations directly from data a
  breakthrough for AI?
\end{enumerate}

\subsection{48. Cognitive science views on
abstraction}\label{cognitive-science-views-on-abstraction}

Cognitive science studies how humans form and use abstractions, offering
insights for AI design. Humans simplify the world by grouping details
into categories, building mental models, and reasoning hierarchically.
AI systems that mimic these strategies can achieve more flexible and
general intelligence.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-47}

Think of how a child learns the concept of ``chair.'' They see many
different shapes---wooden chairs, office chairs, beanbags---and extract
an abstract category: ``something you can sit on.'' The ability to
ignore irrelevant details while preserving core function is abstraction
in action.

\subsubsection{Deep Dive}\label{deep-dive-47}

\begin{itemize}
\item
  Categorization: humans cluster experiences into categories (prototype
  theory, exemplar theory).
\item
  Conceptual hierarchies: categories are structured (animal → mammal →
  dog → poodle).
\item
  Schemas and frames: mental templates for understanding situations
  (e.g., ``restaurant script'').
\item
  Analogical reasoning: mapping structures from one domain to another.
\item
  AI implications:

  \begin{itemize}
  \tightlist
  \item
    Concept learning in symbolic systems.
  \item
    Representation learning inspired by human categorization.
  \item
    Analogy-making in problem solving and creativity.
  \end{itemize}
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2439}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3171}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4390}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Cognitive Mechanism
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Human Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Parallel
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Categorization & ``Chair'' across many shapes & Clustering,
embeddings \\
Hierarchies & Animal → Mammal → Dog & Ontologies, taxonomies \\
Schemas/frames & Restaurant dining sequence & Knowledge graphs,
scripts \\
Analogical reasoning & Atom as ``solar system'' & Structure mapping,
transfer learning \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-47}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simple categorization via clustering}
\ImportTok{from}\NormalTok{ sklearn.cluster }\ImportTok{import}\NormalTok{ KMeans}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Toy data: height, weight of animals}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{30}\NormalTok{,}\DecValTok{5}\NormalTok{],[}\DecValTok{32}\NormalTok{,}\DecValTok{6}\NormalTok{],[}\DecValTok{100}\NormalTok{,}\DecValTok{30}\NormalTok{],[}\DecValTok{110}\NormalTok{,}\DecValTok{35}\NormalTok{]])}
\NormalTok{kmeans }\OperatorTok{=}\NormalTok{ KMeans(n\_clusters}\OperatorTok{=}\DecValTok{2}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{0}\NormalTok{).fit(X)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Cluster labels:"}\NormalTok{, kmeans.labels\_)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-47}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add more animals---do the clusters still make intuitive sense?
\item
  Compare clustering (prototype-based) with nearest-neighbor
  (exemplar-based).
\item
  Reflect: how can human-inspired abstraction mechanisms improve AI
  flexibility and interpretability?
\end{enumerate}

\subsection{49. Trade-offs between fidelity and
simplicity}\label{trade-offs-between-fidelity-and-simplicity}

Representations can be high-fidelity, capturing rich details, or simple,
emphasizing ease of reasoning and efficiency. AI systems must balance
the two: detailed models may be accurate but costly and hard to
generalize, while simpler models may miss nuance but scale better.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-48}

Imagine a city map. A satellite photo has perfect fidelity but is
overwhelming for navigation. A subway map is much simpler, omitting
roads and buildings, but makes travel decisions easy. The ``best''
representation depends on the task.

\subsubsection{Deep Dive}\label{deep-dive-48}

\begin{itemize}
\item
  High-fidelity representations: retain more raw information, closer to
  reality. Examples: full-resolution images, detailed simulations.
\item
  Simple representations: abstract away details, highlight essentials.
  Examples: feature vectors, symbolic summaries.
\item
  Trade-offs:

  \begin{itemize}
  \tightlist
  \item
    Accuracy vs.~interpretability.
  \item
    Precision vs.~efficiency.
  \item
    Generality vs.~task-specific utility.
  \end{itemize}
\item
  AI strategies:

  \begin{itemize}
  \tightlist
  \item
    Dimensionality reduction (PCA, autoencoders).
  \item
    Task-driven simplification (decision trees vs.~deep nets).
  \item
    Multi-resolution models (use detail only when needed).
  \end{itemize}
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1863}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2843}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2843}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2451}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Representation Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Advantage
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
High-fidelity & Pixel-level vision models & Precise, detailed &
Expensive, overfits noise \\
Simple & Bag-of-words for documents & Fast, interpretable & Misses
nuance and context \\
Multi-resolution & CNN pyramids, hierarchical RL & Balance detail and
efficiency & More complex to design \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-48}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Trade{-}off: detailed vs. simplified representation}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.decomposition }\ImportTok{import}\NormalTok{ PCA}

\CommentTok{\# High{-}fidelity: 4D data}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{7}\NormalTok{],[}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{11}\NormalTok{],[}\DecValTok{5}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{13}\NormalTok{,}\DecValTok{21}\NormalTok{]])}

\CommentTok{\# Simplified: project down to 2D with PCA}
\NormalTok{pca }\OperatorTok{=}\NormalTok{ PCA(n\_components}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{X\_reduced }\OperatorTok{=}\NormalTok{ pca.fit\_transform(X)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Original (4D):"}\NormalTok{, X)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Reduced (2D):"}\NormalTok{, X\_reduced)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-48}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Increase the number of dimensions---how much information is lost in
  reduction?
\item
  Try clustering on high-dimensional vs.~reduced data---does simplicity
  help?
\item
  Reflect: when should AI systems prioritize detail, and when should
  they embrace abstraction?
\end{enumerate}

\subsection{50. Towards universal
representations}\label{towards-universal-representations}

A long-term goal in AI is to develop universal
representations---encodings that capture the essence of knowledge across
tasks, modalities, and domains. Instead of learning separate features
for images, text, or speech, universal representations promise
transferability and general intelligence.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-49}

Imagine a translator who can switch seamlessly between languages, music,
and math, using the same internal ``mental code.'' No matter the
medium---words, notes, or numbers---the translator taps into one shared
understanding. Universal representations aim for that kind of
versatility in AI.

\subsubsection{Deep Dive}\label{deep-dive-49}

\begin{itemize}
\item
  Current practice: task- or domain-specific embeddings (e.g., word2vec
  for text, CNN features for vision).
\item
  Universal approaches: large-scale foundation models trained on
  multimodal data (text, images, audio).
\item
  Benefits:

  \begin{itemize}
  \tightlist
  \item
    Transfer learning: apply knowledge across tasks.
  \item
    Efficiency: fewer task-specific models.
  \item
    Alignment: bridge modalities (vision-language, speech-text).
  \end{itemize}
\item
  Challenges:

  \begin{itemize}
  \tightlist
  \item
    Biases from pretraining data propagate universally.
  \item
    Interpretability remains difficult.
  \item
    May underperform on highly specialized domains.
  \end{itemize}
\item
  Research frontier: multimodal transformers, contrastive representation
  learning, world models.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2062}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2887}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2577}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2474}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Representation Scope
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strength
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Task-specific & Word2Vec, ResNet embeddings & Optimized for domain &
Limited transferability \\
Domain-general & BERT, CLIP & Works across many tasks & Still biased by
modality \\
Universal & Multimodal foundation models & Cross-domain adaptability &
Hard to align perfectly \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-49}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Toy multimodal representation: text + numeric features}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{text\_emb }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.3}\NormalTok{, }\FloatTok{0.7}\NormalTok{])   }\CommentTok{\# e.g., "cat"}
\NormalTok{image\_emb }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.75}\NormalTok{]) }\CommentTok{\# embedding from an image of a cat}

\CommentTok{\# Universal space: combine}
\NormalTok{universal\_emb }\OperatorTok{=}\NormalTok{ (text\_emb }\OperatorTok{+}\NormalTok{ image\_emb) }\OperatorTok{/} \DecValTok{2}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Universal representation:"}\NormalTok{, universal\_emb)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-49}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add audio embeddings to the universal vector---how does it integrate?
\item
  Compare universal embeddings for semantically similar vs.~dissimilar
  items.
\item
  Reflect: is true universality possible, or will AI always need
  task-specific adaptations?
\end{enumerate}

\section{Chapter 6. Learning vs Reasoning: Two Paths to
Intelligence}\label{chapter-6.-learning-vs-reasoning-two-paths-to-intelligence}

\subsection{51. Learning from data and
experience}\label{learning-from-data-and-experience}

Learning allows AI systems to improve performance over time by
extracting patterns from data or direct experience. Unlike hard-coded
rules, learning adapts to new inputs and environments, making it a
cornerstone of artificial intelligence.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-50}

Think of a child riding a bicycle. At first they wobble and fall, but
with practice they learn to balance, steer, and pedal smoothly. The
``data'' comes from their own experiences---successes and failures
shaping future behavior.

\subsubsection{Deep Dive}\label{deep-dive-50}

\begin{itemize}
\tightlist
\item
  Supervised learning: learn from labeled examples (input → correct
  output).
\item
  Unsupervised learning: discover structure without labels (clustering,
  dimensionality reduction).
\item
  Reinforcement learning: learn from rewards and penalties over time.
\item
  Online vs.~offline learning: continuous adaptation vs.~training on a
  fixed dataset.
\item
  Experience replay: storing and reusing past data to stabilize
  learning.
\item
  Challenges: data scarcity, noise, bias, catastrophic forgetting.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1313}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2727}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2828}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3131}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Learning Mode
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strength
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Supervised & Image classification & Accurate with labels & Requires
large labeled datasets \\
Unsupervised & Word embeddings, clustering & Reveals hidden structure &
Hard to evaluate, ambiguous \\
Reinforcement & Game-playing agents & Learns sequential strategies &
Sample inefficient \\
Online & Stock trading bots & Adapts in real time & Risk of
instability \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-50}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Supervised learning toy example}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LinearRegression}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Data: study hours vs. test scores}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{],[}\DecValTok{2}\NormalTok{],[}\DecValTok{3}\NormalTok{],[}\DecValTok{4}\NormalTok{],[}\DecValTok{5}\NormalTok{]])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{50}\NormalTok{, }\DecValTok{60}\NormalTok{, }\DecValTok{65}\NormalTok{, }\DecValTok{70}\NormalTok{, }\DecValTok{80}\NormalTok{])}

\NormalTok{model }\OperatorTok{=}\NormalTok{ LinearRegression().fit(X, y)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Prediction for 6 hours:"}\NormalTok{, model.predict([[}\DecValTok{6}\NormalTok{]])[}\DecValTok{0}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-50}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add more training data---does the prediction accuracy improve?
\item
  Try removing data points---how sensitive is the model?
\item
  Reflect: why is the ability to learn from data the defining feature of
  AI over traditional programs?
\end{enumerate}

\subsection{52. Inductive vs.~deductive
inference}\label{inductive-vs.-deductive-inference}

AI systems can reason in two complementary ways: induction, drawing
general rules from specific examples, and deduction, applying general
rules to specific cases. Induction powers machine learning, while
deduction powers logic-based reasoning.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-51}

Suppose you see 10 swans, all white. You infer inductively that ``all
swans are white.'' Later, given the rule ``all swans are white,'' you
deduce that the next swan you see will also be white. One builds the
rule, the other applies it.

\subsubsection{Deep Dive}\label{deep-dive-51}

\begin{itemize}
\item
  Inductive inference:

  \begin{itemize}
  \tightlist
  \item
    Data → rule.
  \item
    Basis of supervised learning, clustering, pattern discovery.
  \item
    Example: from labeled cats and dogs, infer a classifier.
  \end{itemize}
\item
  Deductive inference:

  \begin{itemize}
  \tightlist
  \item
    Rule + fact → conclusion.
  \item
    Basis of logic, theorem proving, symbolic AI.
  \item
    Example: ``All cats are mammals'' + ``Garfield is a cat'' →
    ``Garfield is a mammal.''
  \end{itemize}
\item
  Abduction (related): best explanation from evidence.
\item
  AI practice:

  \begin{itemize}
  \tightlist
  \item
    Induction: neural networks generalizing patterns.
  \item
    Deduction: Prolog-style reasoning engines.
  \item
    Combining both is a key challenge in hybrid AI.
  \end{itemize}
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1207}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1810}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2586}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1983}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2414}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Inference Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Direction
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strength
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Induction & Specific → General & Learning classifiers from data &
Adapts, generalizes & Risk of overfitting \\
Deduction & General → Specific & Rule-based expert systems & Precise,
interpretable & Limited flexibility, brittle \\
Abduction & Evidence → Hypothesis & Medical diagnosis systems & Handles
incomplete info & Not guaranteed correct \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-51}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Deductive reasoning example}
\NormalTok{facts }\OperatorTok{=}\NormalTok{ \{}\StringTok{"Garfield"}\NormalTok{: }\StringTok{"cat"}\NormalTok{\}}
\NormalTok{rules }\OperatorTok{=}\NormalTok{ \{}\StringTok{"cat"}\NormalTok{: }\StringTok{"mammal"}\NormalTok{\}}

\KeywordTok{def}\NormalTok{ deduce(entity):}
\NormalTok{    kind }\OperatorTok{=}\NormalTok{ facts[entity]}
    \ControlFlowTok{return}\NormalTok{ rules.get(kind, }\VariableTok{None}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Garfield is a"}\NormalTok{, deduce(}\StringTok{"Garfield"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-51}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add more facts and rules---can your deductive system scale?
\item
  Try inductive reasoning by fitting a simple classifier on data.
\item
  Reflect: why does modern AI lean heavily on induction, and what's lost
  without deduction?
\end{enumerate}

\subsection{53. Statistical learning vs.~logical
reasoning}\label{statistical-learning-vs.-logical-reasoning}

AI systems can operate through statistical learning, which finds
patterns in data, or through logical reasoning, which derives
conclusions from explicit rules. These approaches represent two
traditions: data-driven vs.~knowledge-driven AI.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-52}

Imagine diagnosing an illness. A statistician looks at thousands of
patient records and says, ``People with these symptoms usually have
flu.'' A logician says, ``If fever AND cough AND sore throat, THEN
flu.'' Both approaches reach the same conclusion, but through different
means.

\subsubsection{Deep Dive}\label{deep-dive-52}

\begin{itemize}
\item
  Statistical learning:

  \begin{itemize}
  \tightlist
  \item
    Probabilistic, approximate, data-driven.
  \item
    Example: logistic regression, neural networks.
  \item
    Pros: adapts well to noise, scalable.
  \item
    Cons: opaque, may lack guarantees.
  \end{itemize}
\item
  Logical reasoning:

  \begin{itemize}
  \tightlist
  \item
    Rule-based, symbolic, precise.
  \item
    Example: first-order logic, theorem provers.
  \item
    Pros: interpretable, guarantees correctness.
  \item
    Cons: brittle, struggles with uncertainty.
  \end{itemize}
\item
  Integration efforts: probabilistic logic, differentiable reasoning,
  neuro-symbolic AI.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1562}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2969}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2578}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2891}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strength
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Statistical learning & Neural networks, regression & Robust to noise,
learns from data & Hard to interpret, needs lots of data \\
Logical reasoning & Prolog, rule-based systems & Transparent, exact
conclusions & Brittle, struggles with ambiguity \\
Hybrid approaches & Probabilistic logic, neuro-symbolic AI & Balance
data + rules & Computationally challenging \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-52}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Statistical learning vs logical reasoning toy example}

\CommentTok{\# Statistical: learn from data}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LogisticRegression}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{],[}\DecValTok{1}\NormalTok{],[}\DecValTok{2}\NormalTok{],[}\DecValTok{3}\NormalTok{]])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{])  }\CommentTok{\# threshold at \textasciitilde{}1.5}
\NormalTok{model }\OperatorTok{=}\NormalTok{ LogisticRegression().fit(X,y)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Statistical prediction for 2.5:"}\NormalTok{, model.predict([[}\FloatTok{2.5}\NormalTok{]])[}\DecValTok{0}\NormalTok{])}

\CommentTok{\# Logical: explicit rule}
\KeywordTok{def}\NormalTok{ rule(x):}
    \ControlFlowTok{return} \DecValTok{1} \ControlFlowTok{if}\NormalTok{ x }\OperatorTok{\textgreater{}=} \DecValTok{2} \ControlFlowTok{else} \DecValTok{0}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Logical rule for 2.5:"}\NormalTok{, rule(}\FloatTok{2.5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-52}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add noise to the training data---does the statistical model still
  work?
\item
  Break the logical rule---how brittle is it?
\item
  Reflect: how might AI combine statistical flexibility with logical
  rigor?
\end{enumerate}

\subsection{54. Pattern recognition and
generalization}\label{pattern-recognition-and-generalization}

AI systems must not only recognize patterns in data but also generalize
beyond what they have explicitly seen. Pattern recognition extracts
structure, while generalization allows applying that structure to new,
unseen situations---a core ingredient of intelligence.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-53}

Think of learning to recognize cats. After seeing a few examples, you
can identify new cats, even if they differ in color, size, or posture.
You don't memorize exact images---you generalize the pattern of
``catness.''

\subsubsection{Deep Dive}\label{deep-dive-53}

\begin{itemize}
\item
  Pattern recognition:

  \begin{itemize}
  \tightlist
  \item
    Detecting regularities in inputs (shapes, sounds, sequences).
  \item
    Tools: classifiers, clustering, convolutional filters.
  \end{itemize}
\item
  Generalization:

  \begin{itemize}
  \tightlist
  \item
    Extending knowledge from training to novel cases.
  \item
    Relies on inductive bias---assumptions baked into the model.
  \end{itemize}
\item
  Overfitting vs.~underfitting:

  \begin{itemize}
  \tightlist
  \item
    Overfit = memorizing patterns without generalizing.
  \item
    Underfit = failing to capture patterns at all.
  \end{itemize}
\item
  AI applications:

  \begin{itemize}
  \tightlist
  \item
    Vision: detecting objects.
  \item
    NLP: understanding paraphrases.
  \item
    Healthcare: predicting disease risk from limited data.
  \end{itemize}
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1810}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2952}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3238}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Pitfall
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Pattern recognition & Identifying structure in data & CNNs detecting
edges and shapes & Can be superficial \\
Generalization & Applying knowledge to new cases & Transformer
understanding synonyms & Requires bias + data \\
Overfitting & Memorizing noise as patterns & Perfect train accuracy,
poor test & No transferability \\
Underfitting & Missing true structure & Always guessing majority class &
Poor accuracy overall \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-53}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Toy generalization example}
\ImportTok{from}\NormalTok{ sklearn.tree }\ImportTok{import}\NormalTok{ DecisionTreeClassifier}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{],[}\DecValTok{1}\NormalTok{],[}\DecValTok{2}\NormalTok{],[}\DecValTok{3}\NormalTok{],[}\DecValTok{4}\NormalTok{]])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{])  }\CommentTok{\# threshold around 2}

\NormalTok{model }\OperatorTok{=}\NormalTok{ DecisionTreeClassifier().fit(X,y)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Seen example (2):"}\NormalTok{, model.predict([[}\DecValTok{2}\NormalTok{]])[}\DecValTok{0}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Unseen example (5):"}\NormalTok{, model.predict([[}\DecValTok{5}\NormalTok{]])[}\DecValTok{0}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-53}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Increase tree depth---does it overfit to training data?
\item
  Reduce training data---can the model still generalize?
\item
  Reflect: why is generalization the hallmark of intelligence, beyond
  rote pattern matching?
\end{enumerate}

\subsection{55. Rule-based vs.~data-driven
methods}\label{rule-based-vs.-data-driven-methods}

AI methods can be designed around explicit rules written by humans or
patterns learned from data. Rule-based approaches dominated early AI,
while data-driven approaches power most modern systems. The two differ
in flexibility, interpretability, and scalability.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-54}

Imagine teaching a child arithmetic. A rule-based method is giving them
a multiplication table to memorize and apply exactly. A data-driven
method is letting them solve many problems until they infer the patterns
themselves. Both lead to answers, but the path differs.

\subsubsection{Deep Dive}\label{deep-dive-54}

\begin{itemize}
\item
  Rule-based AI:

  \begin{itemize}
  \tightlist
  \item
    Expert systems with ``if--then'' rules.
  \item
    Pros: interpretable, precise, easy to debug.
  \item
    Cons: brittle, hard to scale, requires manual encoding of knowledge.
  \end{itemize}
\item
  Data-driven AI:

  \begin{itemize}
  \tightlist
  \item
    Machine learning models trained on large datasets.
  \item
    Pros: adaptable, scalable, robust to variation.
  \item
    Cons: opaque, data-hungry, harder to explain.
  \end{itemize}
\item
  Hybrid approaches: knowledge-guided learning, neuro-symbolic AI.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1068}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3010}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3107}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2816}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strength
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Rule-based & Expert systems, Prolog & Transparent, logical consistency &
Brittle, hard to scale \\
Data-driven & Neural networks, decision trees & Adaptive, scalable &
Opaque, requires lots of data \\
Hybrid & Neuro-symbolic learning & Combines structure + flexibility &
Integration complexity \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-54}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Rule{-}based vs. data{-}driven toy example}

\CommentTok{\# Rule{-}based}
\KeywordTok{def}\NormalTok{ classify\_number(x):}
    \ControlFlowTok{if}\NormalTok{ x }\OperatorTok{\%} \DecValTok{2} \OperatorTok{==} \DecValTok{0}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{"even"}
    \ControlFlowTok{else}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{"odd"}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Rule{-}based:"}\NormalTok{, classify\_number(}\DecValTok{7}\NormalTok{))}

\CommentTok{\# Data{-}driven}
\ImportTok{from}\NormalTok{ sklearn.tree }\ImportTok{import}\NormalTok{ DecisionTreeClassifier}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{],[}\DecValTok{1}\NormalTok{],[}\DecValTok{2}\NormalTok{],[}\DecValTok{3}\NormalTok{],[}\DecValTok{4}\NormalTok{],[}\DecValTok{5}\NormalTok{]])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ [}\StringTok{"even"}\NormalTok{,}\StringTok{"odd"}\NormalTok{,}\StringTok{"even"}\NormalTok{,}\StringTok{"odd"}\NormalTok{,}\StringTok{"even"}\NormalTok{,}\StringTok{"odd"}\NormalTok{]}

\NormalTok{model }\OperatorTok{=}\NormalTok{ DecisionTreeClassifier().fit(X,y)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Data{-}driven:"}\NormalTok{, model.predict([[}\DecValTok{7}\NormalTok{]])[}\DecValTok{0}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-54}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add more rules---how quickly does the rule-based approach become
  unwieldy?
\item
  Train the model on noisy data---does the data-driven approach still
  generalize?
\item
  Reflect: when is rule-based precision preferable, and when is
  data-driven flexibility essential?
\end{enumerate}

\subsection{56. When learning outperforms
reasoning}\label{when-learning-outperforms-reasoning}

In many domains, learning from data outperforms hand-crafted reasoning
because the real world is messy, uncertain, and too complex to capture
with fixed rules. Machine learning adapts to variation and scale where
pure logic struggles.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-55}

Think of recognizing faces. Writing down rules like ``two eyes above a
nose above a mouth'' quickly breaks---faces vary in shape, lighting, and
angle. But with enough examples, a learning system can capture these
variations automatically.

\subsubsection{Deep Dive}\label{deep-dive-55}

\begin{itemize}
\item
  Reasoning systems: excel when rules are clear and complete. Fail when
  variation is high.
\item
  Learning systems: excel in perception-heavy tasks with vast diversity.
\item
  Examples where learning wins:

  \begin{itemize}
  \tightlist
  \item
    Vision: object and face recognition.
  \item
    Speech: recognizing accents, noise, and emotion.
  \item
    Language: understanding synonyms, idioms, context.
  \end{itemize}
\item
  Why:

  \begin{itemize}
  \tightlist
  \item
    Data-driven flexibility handles ambiguity.
  \item
    Statistical models capture probabilistic variation.
  \item
    Scale of modern datasets makes pattern discovery possible.
  \end{itemize}
\item
  Limitation: learning can succeed without ``understanding,'' leading to
  brittle generalization.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1081}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4730}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4189}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Domain
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Reasoning (rule-based)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Learning (data-driven)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Vision & ``Eye + nose + mouth'' rules brittle & CNNs adapt to
lighting/angles \\
Speech & Phoneme rules fail on noise/accents & Deep nets generalize from
data \\
Language & Hand-coded grammar misses idioms & Transformers learn from
corpora \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-55}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Learning beats reasoning in noisy classification}
\ImportTok{from}\NormalTok{ sklearn.neighbors }\ImportTok{import}\NormalTok{ KNeighborsClassifier}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Data: noisy "rule" for odd/even classification}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{],[}\DecValTok{1}\NormalTok{],[}\DecValTok{2}\NormalTok{],[}\DecValTok{3}\NormalTok{],[}\DecValTok{4}\NormalTok{],[}\DecValTok{5}\NormalTok{]])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ [}\StringTok{"even"}\NormalTok{,}\StringTok{"odd"}\NormalTok{,}\StringTok{"even"}\NormalTok{,}\StringTok{"odd"}\NormalTok{,}\StringTok{"odd"}\NormalTok{,}\StringTok{"odd"}\NormalTok{]  }\CommentTok{\# noise at index 4}

\NormalTok{model }\OperatorTok{=}\NormalTok{ KNeighborsClassifier(n\_neighbors}\OperatorTok{=}\DecValTok{1}\NormalTok{).fit(X,y)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Prediction for 4 (noisy):"}\NormalTok{, model.predict([[}\DecValTok{4}\NormalTok{]])[}\DecValTok{0}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Prediction for 6 (generalizes):"}\NormalTok{, model.predict([[}\DecValTok{6}\NormalTok{]])[}\DecValTok{0}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-55}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add more noisy labels---does the learner still generalize better than
  brittle rules?
\item
  Increase dataset size---watch the learning system smooth out noise.
\item
  Reflect: why are perception tasks dominated by learning methods
  instead of reasoning systems?
\end{enumerate}

\subsection{57. When reasoning outperforms
learning}\label{when-reasoning-outperforms-learning}

While learning excels at perception and pattern recognition, reasoning
dominates in domains that require structure, rules, and guarantees.
Logical inference can succeed where data is scarce, errors are costly,
or decisions must follow strict constraints.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-56}

Think of solving a Sudoku puzzle. A learning system trained on examples
might guess, but a reasoning system follows logical rules to guarantee
correctness. Here, rules beat patterns.

\subsubsection{Deep Dive}\label{deep-dive-56}

\begin{itemize}
\item
  Strengths of reasoning:

  \begin{itemize}
  \tightlist
  \item
    Works with little or no data.
  \item
    Provides transparent justifications.
  \item
    Guarantees correctness when rules are complete.
  \end{itemize}
\item
  Examples where reasoning wins:

  \begin{itemize}
  \tightlist
  \item
    Mathematics \& theorem proving: correctness requires logic, not
    approximation.
  \item
    Formal verification: ensuring software or hardware meets safety
    requirements.
  \item
    Constraint satisfaction: scheduling, planning, optimization with
    strict limits.
  \end{itemize}
\item
  Limitations of learning in these domains:

  \begin{itemize}
  \tightlist
  \item
    Requires massive data that may not exist.
  \item
    Produces approximate answers, not guarantees.
  \end{itemize}
\item
  Hybrid opportunity: reasoning provides structure, learning fills gaps.
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2593}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3086}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4321}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Domain
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Learning Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Reasoning Approach
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Sudoku solving & Guess from patterns & Deductive logic guarantees
solution \\
Software verification & Predict defects from data & Prove correctness
formally \\
Flight scheduling & Predict likely routes & Optimize with constraints \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-56}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Reasoning beats learning: simple constraint solver}
\ImportTok{from}\NormalTok{ itertools }\ImportTok{import}\NormalTok{ permutations}

\CommentTok{\# Sudoku{-}like mini puzzle: fill 1{-}3 with no repeats}
\ControlFlowTok{for}\NormalTok{ perm }\KeywordTok{in}\NormalTok{ permutations([}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{]):}
    \ControlFlowTok{if}\NormalTok{ perm[}\DecValTok{0}\NormalTok{] }\OperatorTok{!=} \DecValTok{2}\NormalTok{:  }\CommentTok{\# constraint: first slot not 2}
        \BuiltInTok{print}\NormalTok{(}\StringTok{"Valid solution:"}\NormalTok{, perm)}
        \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-56}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add more constraints---watch reasoning prune the solution space.
\item
  Try training a learner on the same problem---can it guarantee
  correctness?
\item
  Reflect: why do safety-critical AI applications often rely on
  reasoning over learning?
\end{enumerate}

\subsection{58. Combining learning and
reasoning}\label{combining-learning-and-reasoning}

Neither learning nor reasoning alone is sufficient for general
intelligence. Learning excels at perception and adapting to data, while
reasoning ensures structure, rules, and guarantees. Combining the
two---often called neuro-symbolic AI---aims to build systems that are
both flexible and reliable.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-57}

Imagine a lawyer-robot. Its learning side helps it understand spoken
language from clients, even with accents or noise. Its reasoning side
applies the exact rules of law to reach valid conclusions. Only together
can it work effectively.

\subsubsection{Deep Dive}\label{deep-dive-57}

\begin{itemize}
\item
  Why combine?

  \begin{itemize}
  \tightlist
  \item
    Learning handles messy, high-dimensional inputs.
  \item
    Reasoning enforces structure, constraints, and guarantees.
  \end{itemize}
\item
  Strategies:

  \begin{itemize}
  \tightlist
  \item
    Symbolic rules over learned embeddings.
  \item
    Neural networks guided by logical constraints.
  \item
    Differentiable logic and probabilistic programming.
  \end{itemize}
\item
  Applications:

  \begin{itemize}
  \tightlist
  \item
    Vision + reasoning: object recognition with relational logic.
  \item
    Language + reasoning: understanding and verifying arguments.
  \item
    Planning + perception: robotics combining neural perception with
    symbolic planners.
  \end{itemize}
\item
  Challenges:

  \begin{itemize}
  \tightlist
  \item
    Integration is technically hard.
  \item
    Differentiability vs.~discreteness mismatch.
  \item
    Interpretability vs.~scalability tension.
  \end{itemize}
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1268}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4507}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4225}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strength
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Learning & Robust, adaptive, scalable & Black-box, lacks guarantees \\
Reasoning & Transparent, rule-based, precise & Brittle, inflexible \\
Combined & Balances adaptability + rigor & Complex integration
challenges \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-57}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Hybrid: learning + reasoning toy demo}
\ImportTok{from}\NormalTok{ sklearn.tree }\ImportTok{import}\NormalTok{ DecisionTreeClassifier}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Learning: classify numbers}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{],[}\DecValTok{2}\NormalTok{],[}\DecValTok{3}\NormalTok{],[}\DecValTok{4}\NormalTok{],[}\DecValTok{5}\NormalTok{]])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ [}\StringTok{"low"}\NormalTok{,}\StringTok{"low"}\NormalTok{,}\StringTok{"high"}\NormalTok{,}\StringTok{"high"}\NormalTok{,}\StringTok{"high"}\NormalTok{]}
\NormalTok{model }\OperatorTok{=}\NormalTok{ DecisionTreeClassifier().fit(X,y)}

\CommentTok{\# Reasoning: enforce a constraint (no "high" if \textless{}3)}
\KeywordTok{def}\NormalTok{ hybrid\_predict(x):}
\NormalTok{    pred }\OperatorTok{=}\NormalTok{ model.predict([[x]])[}\DecValTok{0}\NormalTok{]}
    \ControlFlowTok{if}\NormalTok{ x }\OperatorTok{\textless{}} \DecValTok{3} \KeywordTok{and}\NormalTok{ pred }\OperatorTok{==} \StringTok{"high"}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{"low (corrected by rule)"}
    \ControlFlowTok{return}\NormalTok{ pred}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Hybrid prediction for 2:"}\NormalTok{, hybrid\_predict(}\DecValTok{2}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Hybrid prediction for 5:"}\NormalTok{, hybrid\_predict(}\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-57}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train the learner on noisy labels---does reasoning help correct
  mistakes?
\item
  Add more rules to refine the hybrid output.
\item
  Reflect: what domains today most need neuro-symbolic AI (e.g., law,
  medicine, robotics)?
\end{enumerate}

\subsection{59. Current neuro-symbolic
approaches}\label{current-neuro-symbolic-approaches}

Neuro-symbolic AI seeks to unify neural networks (pattern recognition,
learning from data) with symbolic systems (logic, reasoning, knowledge
representation). The goal is to build systems that can perceive like a
neural net and reason like a logic engine.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-58}

Think of a self-driving car. Its neural network detects pedestrians,
cars, and traffic lights from camera feeds. Its symbolic system reasons
about rules like ``red light means stop'' or ``yield to pedestrians.''
Together, the car makes lawful, safe decisions.

\subsubsection{Deep Dive}\label{deep-dive-58}

\begin{itemize}
\item
  Integration strategies:

  \begin{itemize}
  \tightlist
  \item
    Symbolic on top of neural: neural nets produce symbols (objects,
    relations) → reasoning engine processes them.
  \item
    Neural guided by symbolic rules: logic constraints regularize
    learning (e.g., logical loss terms).
  \item
    Fully hybrid models: differentiable reasoning layers integrated into
    networks.
  \end{itemize}
\item
  Applications:

  \begin{itemize}
  \tightlist
  \item
    Vision + logic: scene understanding with relational reasoning.
  \item
    NLP + logic: combining embeddings with knowledge graphs.
  \item
    Robotics: neural control + symbolic task planning.
  \end{itemize}
\item
  Research challenges:

  \begin{itemize}
  \tightlist
  \item
    Scalability to large knowledge bases.
  \item
    Differentiability vs.~symbolic discreteness.
  \item
    Interpretability of hybrid models.
  \end{itemize}
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2137}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2906}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2650}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2308}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strength
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Symbolic on top of neural & Neural scene parser + Prolog rules &
Interpretable reasoning & Depends on neural accuracy \\
Neural guided by symbolic & Logic-regularized neural networks & Enforces
consistency & Hard to balance constraints \\
Fully hybrid & Differentiable theorem proving & End-to-end learning +
reasoning & Computationally intensive \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-58}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Neuro{-}symbolic toy example: neural output corrected by rule}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Neural{-}like output (probabilities)}
\NormalTok{pred\_probs }\OperatorTok{=}\NormalTok{ \{}\StringTok{"stop"}\NormalTok{: }\FloatTok{0.6}\NormalTok{, }\StringTok{"go"}\NormalTok{: }\FloatTok{0.4}\NormalTok{\}}

\CommentTok{\# Symbolic rule: if red light, must stop}
\NormalTok{observed\_light }\OperatorTok{=} \StringTok{"red"}

\ControlFlowTok{if}\NormalTok{ observed\_light }\OperatorTok{==} \StringTok{"red"}\NormalTok{:}
\NormalTok{    final\_decision }\OperatorTok{=} \StringTok{"stop"}
\ControlFlowTok{else}\NormalTok{:}
\NormalTok{    final\_decision }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(pred\_probs, key}\OperatorTok{=}\NormalTok{pred\_probs.get)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Final decision:"}\NormalTok{, final\_decision)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-58}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Change the observed light---does the symbolic rule override the neural
  prediction?
\item
  Add more rules (e.g., ``yellow = slow down'') and combine with neural
  uncertainty.
\item
  Reflect: will future AI lean more on neuro-symbolic systems to achieve
  robustness and trustworthiness?
\end{enumerate}

\subsection{60. Open questions in
integration}\label{open-questions-in-integration}

Blending learning and reasoning is one of the grand challenges of AI.
While neuro-symbolic approaches show promise, many open questions remain
about scalability, interpretability, and how best to combine discrete
rules with continuous learning.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-59}

Think of oil and water. Neural nets (fluid, continuous) and symbolic
logic (rigid, discrete) often resist mixing. Researchers keep trying to
find the right ``emulsifier'' that allows them to blend smoothly into
one powerful system.

\subsubsection{Deep Dive}\label{deep-dive-59}

\begin{itemize}
\tightlist
\item
  Scalability: Can hybrid systems handle the scale of modern AI
  (billions of parameters, massive data)?
\item
  Differentiability: How to make discrete logical rules trainable with
  gradient descent?
\item
  Interpretability: How to ensure the symbolic layer explains what the
  neural part has learned?
\item
  Transferability: Can integrated systems generalize across domains
  better than either alone?
\item
  Benchmarks: What tasks truly test the benefit of integration
  (commonsense reasoning, law, robotics)?
\item
  Philosophical question: Is human intelligence itself a neuro-symbolic
  hybrid, and if so, what is the right architecture to model it?
\end{itemize}

Comparison Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2073}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3902}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4024}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Open Question
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Why It Matters
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Current Status
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Scalability & Needed for real-world deployment & Small demos, not yet at
LLM scale \\
Differentiability & Enables end-to-end training & Research in
differentiable logic \\
Interpretability & Builds trust, explains decisions & Still opaque in
hybrids \\
Transferability & Key to general intelligence & Limited evidence so
far \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-59}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Toy blend: neural score + symbolic constraint}
\NormalTok{neural\_score }\OperatorTok{=}\NormalTok{ \{}\StringTok{"cat"}\NormalTok{: }\FloatTok{0.6}\NormalTok{, }\StringTok{"dog"}\NormalTok{: }\FloatTok{0.4}\NormalTok{\}}
\NormalTok{constraints }\OperatorTok{=}\NormalTok{ \{}\StringTok{"must\_be\_animal"}\NormalTok{: [}\StringTok{"cat"}\NormalTok{,}\StringTok{"dog"}\NormalTok{,}\StringTok{"horse"}\NormalTok{]\}}

\CommentTok{\# Integration: filter neural outputs by symbolic constraint}
\NormalTok{filtered }\OperatorTok{=}\NormalTok{ \{k:v }\ControlFlowTok{for}\NormalTok{ k,v }\KeywordTok{in}\NormalTok{ neural\_score.items() }\ControlFlowTok{if}\NormalTok{ k }\KeywordTok{in}\NormalTok{ constraints[}\StringTok{"must\_be\_animal"}\NormalTok{]\}}
\NormalTok{decision }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(filtered, key}\OperatorTok{=}\NormalTok{filtered.get)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Final decision after integration:"}\NormalTok{, decision)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-59}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add a constraint that conflicts with neural output---what happens?
\item
  Adjust neural scores---does symbolic filtering still dominate?
\item
  Reflect: what breakthroughs are needed to make hybrid AI the default
  paradigm?
\end{enumerate}

\section{Chapter 7. Search, Optimization, and
Decision-Making}\label{chapter-7.-search-optimization-and-decision-making}

\subsection{61. Search as a core paradigm of
AI}\label{search-as-a-core-paradigm-of-ai}

At its heart, much of AI reduces to search: systematically exploring
possibilities to find a path from a starting point to a desired goal.
Whether planning moves in a game, routing a delivery truck, or designing
a protein, the essence of intelligence often lies in navigating large
spaces of alternatives efficiently.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-60}

Imagine standing at the entrance of a vast library. Somewhere inside is
the book you need. You could wander randomly, but that might take
forever. Instead, you use an index, follow signs, or ask a librarian.
Each strategy is a way of searching the space of books more effectively
than brute force.

\subsubsection{Deep Dive}\label{deep-dive-60}

Search provides a unifying perspective for AI because it frames problems
as states, actions, and goals. The system begins in a state, applies
actions that generate new states, and continues until it reaches a goal
state. This formulation underlies classical pathfinding, symbolic
reasoning, optimization, and even modern reinforcement learning.

The power of search lies in its generality. A chess program does not
need a bespoke strategy for every board---it needs a way to search
through possible moves. A navigation app does not memorize every
possible trip---it searches for the best route. Yet this generality
creates challenges, since search spaces often grow exponentially with
problem size. Intelligent systems must therefore balance completeness,
efficiency, and optimality.

To appreciate the spectrum of search strategies, it helps to compare
their properties. At one extreme, uninformed search methods like
breadth-first and depth-first blindly traverse states until a goal is
found. At the other, informed search methods like A* exploit heuristics
to guide exploration, reducing wasted effort. Between them lie iterative
deepening, bidirectional search, and stochastic sampling methods.

Comparison Table: Uninformed vs.~Informed Search

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2062}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3918}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4021}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Dimension
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Uninformed Search
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Informed Search
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Guidance & No knowledge beyond problem definition & Uses heuristics or
estimates \\
Efficiency & Explores many irrelevant states & Focuses exploration on
promising states \\
Guarantee & Can ensure completeness and optimality & Depends on
heuristic quality \\
Example Algorithms & BFS, DFS, Iterative Deepening & A*, Greedy
Best-First, Beam Search \\
Typical Applications & Puzzle solving, graph traversal & Route planning,
game-playing, NLP \\
\end{longtable}

Search also interacts closely with optimization. The difference is often
one of framing: search emphasizes paths in discrete spaces, while
optimization emphasizes finding best solutions in continuous spaces. In
practice, many AI problems blend both---for example, reinforcement
learning agents search over action sequences while optimizing reward
functions.

Finally, search highlights the limits of brute-force intelligence.
Without heuristics, even simple problems can become intractable. The
challenge is designing representations and heuristics that compress vast
spaces into manageable ones. This is where domain knowledge, learned
embeddings, and hybrid systems enter, bridging raw computation with
informed guidance.

\subsubsection{Tiny Code}\label{tiny-code-60}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simple uninformed search (BFS) for a path in a graph}
\ImportTok{from}\NormalTok{ collections }\ImportTok{import}\NormalTok{ deque}

\NormalTok{graph }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"A"}\NormalTok{: [}\StringTok{"B"}\NormalTok{, }\StringTok{"C"}\NormalTok{],}
    \StringTok{"B"}\NormalTok{: [}\StringTok{"D"}\NormalTok{, }\StringTok{"E"}\NormalTok{],}
    \StringTok{"C"}\NormalTok{: [}\StringTok{"F"}\NormalTok{],}
    \StringTok{"D"}\NormalTok{: [], }\StringTok{"E"}\NormalTok{: [}\StringTok{"F"}\NormalTok{], }\StringTok{"F"}\NormalTok{: []}
\NormalTok{\}}

\KeywordTok{def}\NormalTok{ bfs(start, goal):}
\NormalTok{    queue }\OperatorTok{=}\NormalTok{ deque([[start]])}
    \ControlFlowTok{while}\NormalTok{ queue:}
\NormalTok{        path }\OperatorTok{=}\NormalTok{ queue.popleft()}
\NormalTok{        node }\OperatorTok{=}\NormalTok{ path[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}
        \ControlFlowTok{if}\NormalTok{ node }\OperatorTok{==}\NormalTok{ goal:}
            \ControlFlowTok{return}\NormalTok{ path}
        \ControlFlowTok{for}\NormalTok{ neighbor }\KeywordTok{in}\NormalTok{ graph.get(node, []):}
\NormalTok{            queue.append(path }\OperatorTok{+}\NormalTok{ [neighbor])}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Path from A to F:"}\NormalTok{, bfs(}\StringTok{"A"}\NormalTok{, }\StringTok{"F"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-60}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Replace BFS with DFS and compare the paths explored---how does
  efficiency change?
\item
  Add a heuristic function and implement A*---does it reduce
  exploration?
\item
  Reflect: why does AI often look like ``search made smart''?
\end{enumerate}

\subsection{62. State spaces and exploration
strategies}\label{state-spaces-and-exploration-strategies}

Every search problem can be described in terms of a state space: the set
of all possible configurations the system might encounter. The
effectiveness of search depends on how this space is structured and how
exploration is guided through it.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-61}

Think of solving a sliding-tile puzzle. Each arrangement of tiles is a
state. Moving one tile changes the state. The state space is the entire
set of possible board configurations, and exploring it is like
navigating a giant tree whose branches represent moves.

\subsubsection{Deep Dive}\label{deep-dive-61}

A state space has three ingredients:

\begin{itemize}
\tightlist
\item
  States: representations of situations, such as board positions, robot
  locations, or logical facts.
\item
  Actions: operations that transform one state into another, such as
  moving a piece or taking a step.
\item
  Goals: specific target states or conditions to be achieved.
\end{itemize}

The way states and actions are represented determines both the size of
the search space and the strategies available for exploring it. Compact
representations make exploration efficient, while poor representations
explode the space unnecessarily.

Exploration strategies dictate how states are visited: systematically,
heuristically, or stochastically. Systematic strategies such as
breadth-first search guarantee coverage but can be inefficient.
Heuristic strategies like best-first search exploit additional knowledge
to guide exploration. Stochastic strategies like Monte Carlo sampling
probe the space randomly, trading completeness for speed.

Comparison Table: Exploration Strategies

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2308}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2115}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2885}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2692}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Strategy
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Exploration Pattern
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Weaknesses
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Systematic (BFS/DFS) & Exhaustive, structured & Completeness,
reproducibility & Inefficient in large spaces \\
Heuristic (A*) & Guided by estimates & Efficient, finds optimal paths &
Depends on heuristic quality \\
Stochastic (Monte Carlo) & Random sampling & Scalable, good for huge
spaces & No guarantee of optimality \\
\end{longtable}

In AI practice, state spaces can be massive. Chess has about \(10^{47}\)
legal positions, Go even more. Enumerating these spaces is impossible,
so effective strategies rely on pruning, abstraction, and heuristic
evaluation. Reinforcement learning takes this further by exploring state
spaces not explicitly enumerated but sampled through interaction with
environments.

\subsubsection{Tiny Code}\label{tiny-code-61}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# State space exploration: DFS vs BFS}
\ImportTok{from}\NormalTok{ collections }\ImportTok{import}\NormalTok{ deque}

\NormalTok{graph }\OperatorTok{=}\NormalTok{ \{}\StringTok{"A"}\NormalTok{: [}\StringTok{"B"}\NormalTok{, }\StringTok{"C"}\NormalTok{], }\StringTok{"B"}\NormalTok{: [}\StringTok{"D"}\NormalTok{, }\StringTok{"E"}\NormalTok{], }\StringTok{"C"}\NormalTok{: [}\StringTok{"F"}\NormalTok{], }\StringTok{"D"}\NormalTok{: [], }\StringTok{"E"}\NormalTok{: [], }\StringTok{"F"}\NormalTok{: []\}}

\KeywordTok{def}\NormalTok{ dfs(start, goal):}
\NormalTok{    stack }\OperatorTok{=}\NormalTok{ [[start]]}
    \ControlFlowTok{while}\NormalTok{ stack:}
\NormalTok{        path }\OperatorTok{=}\NormalTok{ stack.pop()}
\NormalTok{        node }\OperatorTok{=}\NormalTok{ path[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}
        \ControlFlowTok{if}\NormalTok{ node }\OperatorTok{==}\NormalTok{ goal:}
            \ControlFlowTok{return}\NormalTok{ path}
        \ControlFlowTok{for}\NormalTok{ neighbor }\KeywordTok{in}\NormalTok{ graph.get(node, []):}
\NormalTok{            stack.append(path }\OperatorTok{+}\NormalTok{ [neighbor])}

\KeywordTok{def}\NormalTok{ bfs(start, goal):}
\NormalTok{    queue }\OperatorTok{=}\NormalTok{ deque([[start]])}
    \ControlFlowTok{while}\NormalTok{ queue:}
\NormalTok{        path }\OperatorTok{=}\NormalTok{ queue.popleft()}
\NormalTok{        node }\OperatorTok{=}\NormalTok{ path[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}
        \ControlFlowTok{if}\NormalTok{ node }\OperatorTok{==}\NormalTok{ goal:}
            \ControlFlowTok{return}\NormalTok{ path}
        \ControlFlowTok{for}\NormalTok{ neighbor }\KeywordTok{in}\NormalTok{ graph.get(node, []):}
\NormalTok{            queue.append(path }\OperatorTok{+}\NormalTok{ [neighbor])}

\BuiltInTok{print}\NormalTok{(}\StringTok{"DFS path A→F:"}\NormalTok{, dfs(}\StringTok{"A"}\NormalTok{,}\StringTok{"F"}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"BFS path A→F:"}\NormalTok{, bfs(}\StringTok{"A"}\NormalTok{,}\StringTok{"F"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-61}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add loops to the graph---how do exploration strategies handle cycles?
\item
  Replace BFS/DFS with a heuristic that prefers certain nodes first.
\item
  Reflect: how does the choice of state representation reshape the
  difficulty of exploration?
\end{enumerate}

\subsection{63. Optimization problems and solution
quality}\label{optimization-problems-and-solution-quality}

Many AI tasks are not just about finding \emph{a} solution, but about
finding the best one. Optimization frames problems in terms of an
objective function to maximize or minimize. Solution quality is measured
by how well the chosen option scores relative to the optimum.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-62}

Imagine planning a road trip. You could choose \emph{any} route that
gets you from city A to city B, but some are shorter, cheaper, or more
scenic. Optimization is the process of evaluating alternatives and
selecting the route that best satisfies your chosen criteria.

\subsubsection{Deep Dive}\label{deep-dive-62}

Optimization problems are typically expressed as:

\begin{itemize}
\tightlist
\item
  Variables: the choices to be made (e.g., path, schedule, parameters).
\item
  Objective function: a numerical measure of quality (e.g., total
  distance, cost, accuracy).
\item
  Constraints: conditions that must hold (e.g., maximum budget, safety
  requirements).
\end{itemize}

In AI, optimization appears at multiple levels. At the algorithmic
level, pathfinding seeks the shortest or safest route. At the
statistical level, training a machine learning model minimizes loss. At
the systems level, scheduling problems allocate limited resources
effectively.

Solution quality is not always binary. Often, multiple solutions exist
with varying trade-offs, requiring approximation or heuristic methods.
For example, linear programming problems may yield exact solutions,
while combinatorial problems like the traveling salesman often require
heuristics that balance quality and efficiency.

Comparison Table: Exact vs.~Approximate Optimization

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3716}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2095}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2230}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1959}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Guarantee
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Efficiency
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Exact (e.g., linear programming) & Optimal solution guaranteed & Slow
for large problems & Resource scheduling, planning \\
Approximate (e.g., greedy, local search) & Close to optimal, no
guarantees & Fast, scalable & Routing, clustering \\
Heuristic/metaheuristic (e.g., simulated annealing, GA) & Often
near-optimal & Balances exploration/exploitation & Game AI, design
problems \\
\end{longtable}

Optimization also interacts with multi-objective trade-offs. An AI
system may need to maximize accuracy while minimizing cost, or balance
fairness against efficiency. This leads to Pareto frontiers, where no
solution is best across all criteria, only better in some dimensions.

\subsubsection{Tiny Code}\label{tiny-code-62}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simple optimization: shortest path with Dijkstra}
\ImportTok{import}\NormalTok{ heapq}

\NormalTok{graph }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"A"}\NormalTok{: \{}\StringTok{"B"}\NormalTok{:}\DecValTok{2}\NormalTok{,}\StringTok{"C"}\NormalTok{:}\DecValTok{5}\NormalTok{\},}
    \StringTok{"B"}\NormalTok{: \{}\StringTok{"C"}\NormalTok{:}\DecValTok{1}\NormalTok{,}\StringTok{"D"}\NormalTok{:}\DecValTok{4}\NormalTok{\},}
    \StringTok{"C"}\NormalTok{: \{}\StringTok{"D"}\NormalTok{:}\DecValTok{1}\NormalTok{\},}
    \StringTok{"D"}\NormalTok{: \{\}}
\NormalTok{\}}

\KeywordTok{def}\NormalTok{ dijkstra(start, goal):}
\NormalTok{    queue }\OperatorTok{=}\NormalTok{ [(}\DecValTok{0}\NormalTok{, start, [])]}
\NormalTok{    seen }\OperatorTok{=} \BuiltInTok{set}\NormalTok{()}
    \ControlFlowTok{while}\NormalTok{ queue:}
\NormalTok{        (cost, node, path) }\OperatorTok{=}\NormalTok{ heapq.heappop(queue)}
        \ControlFlowTok{if}\NormalTok{ node }\KeywordTok{in}\NormalTok{ seen:}
            \ControlFlowTok{continue}
\NormalTok{        path }\OperatorTok{=}\NormalTok{ path }\OperatorTok{+}\NormalTok{ [node]}
        \ControlFlowTok{if}\NormalTok{ node }\OperatorTok{==}\NormalTok{ goal:}
            \ControlFlowTok{return}\NormalTok{ (cost, path)}
\NormalTok{        seen.add(node)}
        \ControlFlowTok{for}\NormalTok{ n, c }\KeywordTok{in}\NormalTok{ graph[node].items():}
\NormalTok{            heapq.heappush(queue, (cost}\OperatorTok{+}\NormalTok{c, n, path))}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Shortest path A→D:"}\NormalTok{, dijkstra(}\StringTok{"A"}\NormalTok{,}\StringTok{"D"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-62}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add an extra edge to the graph---does it change the optimal solution?
\item
  Modify edge weights---how sensitive is the solution quality to
  changes?
\item
  Reflect: why does optimization unify so many AI problems, from
  learning weights to planning strategies?
\end{enumerate}

\subsection{64. Trade-offs: completeness, optimality,
efficiency}\label{trade-offs-completeness-optimality-efficiency}

Search and optimization in AI are always constrained by trade-offs. An
algorithm can aim to be complete (always finds a solution if one
exists), optimal (finds the best possible solution), or efficient (uses
minimal time and memory). In practice, no single method can maximize all
three.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-63}

Imagine looking for your car keys. A complete strategy is to search
every inch of the house---you'll eventually succeed but waste time. An
optimal strategy is to find them in the absolute minimum time, which may
require foresight you don't have. An efficient strategy is to quickly
check likely spots (desk, kitchen counter) but risk missing them if
they're elsewhere.

\subsubsection{Deep Dive}\label{deep-dive-63}

Completeness ensures reliability. Algorithms like breadth-first search
are complete but can be slow. Optimality ensures the best solution---A*
with an admissible heuristic guarantees optimal paths. Efficiency,
however, often requires cutting corners, such as greedy search, which
may miss the best path.

The choice among these depends on the domain. In robotics, efficiency
and near-optimality may be more important than strict completeness. In
theorem proving, completeness may outweigh efficiency. In logistics,
approximate optimality is often good enough if efficiency scales to
millions of deliveries.

Comparison Table: Properties of Search Algorithms

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1393}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1557}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1803}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2869}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2377}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Algorithm
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Complete?
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Optimal?
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Efficiency
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Typical Use Case
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Breadth-First & Yes & Yes (if costs uniform) & Low (explores widely) &
Simple shortest-path problems \\
Depth-First & Yes (finite spaces) & No & High memory efficiency, can be
slow & Exploring large state spaces \\
Greedy Best-First & No & No & Very fast & Quick approximate solutions \\
A* (admissible) & Yes & Yes & Moderate, depends on heuristic & Optimal
pathfinding \\
\end{longtable}

This trilemma highlights why heuristic design is critical. Good
heuristics push algorithms closer to optimality and efficiency without
sacrificing completeness. Poor heuristics waste resources or miss good
solutions.

\subsubsection{Tiny Code}\label{tiny-code-63}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Greedy vs A* search demonstration}
\ImportTok{import}\NormalTok{ heapq}

\NormalTok{graph }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"A"}\NormalTok{: \{}\StringTok{"B"}\NormalTok{:}\DecValTok{1}\NormalTok{,}\StringTok{"C"}\NormalTok{:}\DecValTok{4}\NormalTok{\},}
    \StringTok{"B"}\NormalTok{: \{}\StringTok{"C"}\NormalTok{:}\DecValTok{2}\NormalTok{,}\StringTok{"D"}\NormalTok{:}\DecValTok{5}\NormalTok{\},}
    \StringTok{"C"}\NormalTok{: \{}\StringTok{"D"}\NormalTok{:}\DecValTok{1}\NormalTok{\},}
    \StringTok{"D"}\NormalTok{: \{\}}
\NormalTok{\}}

\NormalTok{heuristic }\OperatorTok{=}\NormalTok{ \{}\StringTok{"A"}\NormalTok{:}\DecValTok{3}\NormalTok{,}\StringTok{"B"}\NormalTok{:}\DecValTok{2}\NormalTok{,}\StringTok{"C"}\NormalTok{:}\DecValTok{1}\NormalTok{,}\StringTok{"D"}\NormalTok{:}\DecValTok{0}\NormalTok{\}  }\CommentTok{\# heuristic estimates}

\KeywordTok{def}\NormalTok{ astar(start, goal):}
\NormalTok{    queue }\OperatorTok{=}\NormalTok{ [(}\DecValTok{0}\OperatorTok{+}\NormalTok{heuristic[start],}\DecValTok{0}\NormalTok{,start,[])]}
    \ControlFlowTok{while}\NormalTok{ queue:}
\NormalTok{        f,g,node,path }\OperatorTok{=}\NormalTok{ heapq.heappop(queue)}
\NormalTok{        path }\OperatorTok{=}\NormalTok{ path}\OperatorTok{+}\NormalTok{[node]}
        \ControlFlowTok{if}\NormalTok{ node }\OperatorTok{==}\NormalTok{ goal:}
            \ControlFlowTok{return}\NormalTok{ (g,path)}
        \ControlFlowTok{for}\NormalTok{ n,c }\KeywordTok{in}\NormalTok{ graph[node].items():}
\NormalTok{            heapq.heappush(queue,(g}\OperatorTok{+}\NormalTok{c}\OperatorTok{+}\NormalTok{heuristic[n],g}\OperatorTok{+}\NormalTok{c,n,path))}

\BuiltInTok{print}\NormalTok{(}\StringTok{"A* path:"}\NormalTok{, astar(}\StringTok{"A"}\NormalTok{,}\StringTok{"D"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-63}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Replace the heuristic with random values---how does it affect
  optimality?
\item
  Compare A* to greedy search (use only heuristic, ignore g)---which is
  faster?
\item
  Reflect: why can't AI systems maximize completeness, optimality, and
  efficiency all at once?
\end{enumerate}

\subsection{65. Greedy, heuristic, and informed
search}\label{greedy-heuristic-and-informed-search}

Not all search strategies blindly explore possibilities. Greedy search
follows the most promising-looking option at each step. Heuristic search
uses estimates to guide exploration. Informed search combines
problem-specific knowledge with systematic search, often achieving
efficiency without sacrificing too much accuracy.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-64}

Imagine hiking up a mountain in fog. A greedy approach is to always step
toward the steepest upward slope---you'll climb quickly, but you may end
up on a local hill instead of the highest peak. A heuristic approach
uses a rough map that points you toward promising trails. An informed
search balances both---map guidance plus careful checking to ensure
you're really reaching the summit.

\subsubsection{Deep Dive}\label{deep-dive-64}

Greedy search is fast but shortsighted. It relies on evaluating the
immediate ``best'' option without considering long-term consequences.
Heuristic search introduces estimates of how far a state is from the
goal, such as distance in pathfinding. Informed search algorithms like
A* integrate actual cost so far with heuristic estimates, ensuring both
efficiency and optimality when heuristics are admissible.

The effectiveness of these methods depends heavily on heuristic quality.
A poor heuristic may waste time or mislead the search. A well-crafted
heuristic, even if simple, can drastically reduce exploration. In
practice, heuristics are often domain-specific: straight-line distance
in maps, Manhattan distance in puzzles, or learned estimates in modern
AI systems.

Comparison Table: Greedy vs.~Heuristic vs.~Informed

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1468}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1376}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1651}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2936}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2569}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Strategy
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Cost Considered
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Goal Estimate Used
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strength
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Weakness
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Greedy Search & No & Yes & Very fast, low memory & May get stuck in
local traps \\
Heuristic Search & Sometimes & Yes & Guides exploration & Quality
depends on heuristic \\
Informed Search & Yes (path cost) & Yes & Balances efficiency +
optimality & More computation per step \\
\end{longtable}

In modern AI, informed search generalizes beyond symbolic search spaces.
Neural networks learn heuristics automatically, approximating
distance-to-goal functions. This connection bridges classical AI
planning with contemporary machine learning.

\subsubsection{Tiny Code}\label{tiny-code-64}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Greedy vs A* search with heuristic}
\ImportTok{import}\NormalTok{ heapq}

\NormalTok{graph }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"A"}\NormalTok{: \{}\StringTok{"B"}\NormalTok{:}\DecValTok{2}\NormalTok{,}\StringTok{"C"}\NormalTok{:}\DecValTok{5}\NormalTok{\},}
    \StringTok{"B"}\NormalTok{: \{}\StringTok{"C"}\NormalTok{:}\DecValTok{1}\NormalTok{,}\StringTok{"D"}\NormalTok{:}\DecValTok{4}\NormalTok{\},}
    \StringTok{"C"}\NormalTok{: \{}\StringTok{"D"}\NormalTok{:}\DecValTok{1}\NormalTok{\},}
    \StringTok{"D"}\NormalTok{: \{\}}
\NormalTok{\}}

\NormalTok{heuristic }\OperatorTok{=}\NormalTok{ \{}\StringTok{"A"}\NormalTok{:}\DecValTok{6}\NormalTok{,}\StringTok{"B"}\NormalTok{:}\DecValTok{4}\NormalTok{,}\StringTok{"C"}\NormalTok{:}\DecValTok{2}\NormalTok{,}\StringTok{"D"}\NormalTok{:}\DecValTok{0}\NormalTok{\}}

\KeywordTok{def}\NormalTok{ greedy(start, goal):}
\NormalTok{    queue }\OperatorTok{=}\NormalTok{ [(heuristic[start], start, [])]}
\NormalTok{    seen }\OperatorTok{=} \BuiltInTok{set}\NormalTok{()}
    \ControlFlowTok{while}\NormalTok{ queue:}
\NormalTok{        \_, node, path }\OperatorTok{=}\NormalTok{ heapq.heappop(queue)}
        \ControlFlowTok{if}\NormalTok{ node }\KeywordTok{in}\NormalTok{ seen: }
            \ControlFlowTok{continue}
\NormalTok{        path }\OperatorTok{=}\NormalTok{ path }\OperatorTok{+}\NormalTok{ [node]}
        \ControlFlowTok{if}\NormalTok{ node }\OperatorTok{==}\NormalTok{ goal:}
            \ControlFlowTok{return}\NormalTok{ path}
\NormalTok{        seen.add(node)}
        \ControlFlowTok{for}\NormalTok{ n }\KeywordTok{in}\NormalTok{ graph[node]:}
\NormalTok{            heapq.heappush(queue, (heuristic[n], n, path))}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Greedy path:"}\NormalTok{, greedy(}\StringTok{"A"}\NormalTok{,}\StringTok{"D"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-64}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compare greedy and A* on the same graph---does A* find shorter paths?
\item
  Change the heuristic values---how sensitive are the results?
\item
  Reflect: how do learned heuristics in modern AI extend this classical
  idea?
\end{enumerate}

\subsection{66. Global vs.~local optima
challenges}\label{global-vs.-local-optima-challenges}

Optimization problems in AI often involve navigating landscapes with
many peaks and valleys. A local optimum is a solution better than its
neighbors but not the best overall. A global optimum is the true best
solution. Distinguishing between the two is a central challenge,
especially in high-dimensional spaces.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-65}

Imagine climbing hills in heavy fog. You reach the top of a nearby hill
and think you're done---yet a taller mountain looms beyond the mist.
That smaller hill is a local optimum; the tallest mountain is the global
optimum. AI systems face the same trap when optimizing.

\subsubsection{Deep Dive}\label{deep-dive-65}

Local vs.~global optima appear in many AI contexts. Neural network
training often settles in local minima, though in very high dimensions,
``bad'' minima are surprisingly rare and saddle points dominate.
Heuristic search algorithms like hill climbing can get stuck at local
maxima unless randomization or diversification strategies are
introduced.

To escape local traps, techniques include:

\begin{itemize}
\tightlist
\item
  Random restarts: re-run search from multiple starting points.
\item
  Simulated annealing: accept worse moves probabilistically to escape
  local basins.
\item
  Genetic algorithms: explore populations of solutions to maintain
  diversity.
\item
  Momentum methods in deep learning: help optimizers roll through small
  valleys.
\end{itemize}

The choice of method depends on the problem structure. Convex
optimization problems, common in linear models, guarantee global optima.
Non-convex problems, such as deep neural networks, require approximation
strategies and careful initialization.

Comparison Table: Local vs.~Global Optima

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1954}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4138}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3908}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Feature
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Local Optimum
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Global Optimum
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Definition & Best in a neighborhood & Best overall \\
Detection & Easy (compare neighbors) & Hard (requires whole search) \\
Example in AI & Hill-climbing gets stuck & Linear regression finds exact
best \\
Escape Strategies & Randomization, annealing, heuristics & Convexity
ensures unique optimum \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-65}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Local vs global optima: hill climbing on a bumpy function}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ f(x):}
    \ControlFlowTok{return}\NormalTok{ np.sin(}\DecValTok{5}\OperatorTok{*}\NormalTok{x) }\OperatorTok{*}\NormalTok{ (}\DecValTok{1}\OperatorTok{{-}}\NormalTok{x) }\OperatorTok{+}\NormalTok{ x2}

\KeywordTok{def}\NormalTok{ hill\_climb(start, step}\OperatorTok{=}\FloatTok{0.01}\NormalTok{, iters}\OperatorTok{=}\DecValTok{1000}\NormalTok{):}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ start}
    \ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(iters):}
\NormalTok{        neighbors }\OperatorTok{=}\NormalTok{ [x}\OperatorTok{{-}}\NormalTok{step, x}\OperatorTok{+}\NormalTok{step]}
\NormalTok{        best }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(neighbors, key}\OperatorTok{=}\NormalTok{f)}
        \ControlFlowTok{if}\NormalTok{ f(best) }\OperatorTok{\textless{}=}\NormalTok{ f(x):}
            \ControlFlowTok{break}  \CommentTok{\# stuck at local optimum}
\NormalTok{        x }\OperatorTok{=}\NormalTok{ best}
    \ControlFlowTok{return}\NormalTok{ x, f(x)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Hill climbing from 0.5:"}\NormalTok{, hill\_climb(}\FloatTok{0.5}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Hill climbing from 2.0:"}\NormalTok{, hill\_climb(}\FloatTok{2.0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-65}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Change the starting point---do you end up at different optima?
\item
  Increase step size or add randomness---can you escape local traps?
\item
  Reflect: why do real-world AI systems often settle for ``good enough''
  rather than chasing the global best?
\end{enumerate}

\subsection{67. Multi-objective
optimization}\label{multi-objective-optimization}

Many AI systems must optimize not just one objective but several, often
conflicting, goals. This is known as multi-objective optimization.
Instead of finding a single ``best'' solution, the goal is to balance
trade-offs among objectives, producing a set of solutions that represent
different compromises.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-66}

Imagine buying a laptop. You want it to be powerful, lightweight, and
cheap. But powerful laptops are often heavy or expensive. The ``best''
choice depends on how you weigh these competing factors. Multi-objective
optimization formalizes this dilemma.

\subsubsection{Deep Dive}\label{deep-dive-66}

Unlike single-objective problems where a clear optimum exists,
multi-objective problems often lead to a Pareto frontier---the set of
solutions where improving one objective necessarily worsens another. For
example, in machine learning, models may trade off accuracy against
interpretability, or performance against energy efficiency.

The central challenge is not only finding the frontier but also deciding
which trade-off to choose. This often requires human or policy input.
Algorithms like weighted sums, evolutionary multi-objective optimization
(EMO), and Pareto ranking help navigate these trade-offs.

Comparison Table: Single vs.~Multi-Objective Optimization

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1778}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3556}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4667}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Dimension
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Single-Objective Optimization
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Multi-Objective Optimization
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Goal & Minimize/maximize one function & Balance several conflicting
goals \\
Solution & One optimum & Pareto frontier of non-dominated solutions \\
Example in AI & Train model to maximize accuracy & Train model for
accuracy + fairness \\
Decision process & Automatic & Requires weighing trade-offs \\
\end{longtable}

Applications of multi-objective optimization in AI are widespread:

\begin{itemize}
\tightlist
\item
  Fairness vs.~accuracy in predictive models.
\item
  Energy use vs.~latency in edge devices.
\item
  Exploration vs.~exploitation in reinforcement learning.
\item
  Cost vs.~coverage in planning and logistics.
\end{itemize}

\subsubsection{Tiny Code}\label{tiny-code-66}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Multi{-}objective optimization: Pareto frontier (toy example)}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{solutions }\OperatorTok{=}\NormalTok{ [(x, }\DecValTok{1}\OperatorTok{/}\NormalTok{x) }\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ np.linspace(}\FloatTok{0.1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{)]  }\CommentTok{\# trade{-}off curve}

\CommentTok{\# Identify Pareto frontier}
\NormalTok{pareto }\OperatorTok{=}\NormalTok{ []}
\ControlFlowTok{for}\NormalTok{ s }\KeywordTok{in}\NormalTok{ solutions:}
    \ControlFlowTok{if} \KeywordTok{not} \BuiltInTok{any}\NormalTok{(o[}\DecValTok{0}\NormalTok{] }\OperatorTok{\textless{}=}\NormalTok{ s[}\DecValTok{0}\NormalTok{] }\KeywordTok{and}\NormalTok{ o[}\DecValTok{1}\NormalTok{] }\OperatorTok{\textless{}=}\NormalTok{ s[}\DecValTok{1}\NormalTok{] }\ControlFlowTok{for}\NormalTok{ o }\KeywordTok{in}\NormalTok{ solutions }\ControlFlowTok{if}\NormalTok{ o }\OperatorTok{!=}\NormalTok{ s):}
\NormalTok{        pareto.append(s)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Solutions:"}\NormalTok{, solutions)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Pareto frontier:"}\NormalTok{, pareto)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-66}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add more objectives (e.g., x, 1/x, and x²)---how does the frontier
  change?
\item
  Adjust the trade-offs---what happens to the shape of Pareto optimal
  solutions?
\item
  Reflect: in real-world AI, who decides how to weigh competing
  objectives, the engineer, the user, or society at large?
\end{enumerate}

\subsection{68. Decision-making under
uncertainty}\label{decision-making-under-uncertainty}

In real-world environments, AI rarely has perfect information.
Decision-making under uncertainty is the art of choosing actions when
outcomes are probabilistic, incomplete, or ambiguous. Instead of
guaranteeing success, the goal is to maximize expected utility across
possible futures.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-67}

Imagine driving in heavy fog. You can't see far ahead, but you must
still decide whether to slow down, turn, or continue straight. Each
choice has risks and rewards, and you must act without full knowledge of
the environment.

\subsubsection{Deep Dive}\label{deep-dive-67}

Uncertainty arises in AI from noisy sensors, incomplete data,
unpredictable environments, or stochastic dynamics. Handling it requires
formal models that weigh possible outcomes against their probabilities.

\begin{itemize}
\tightlist
\item
  Probabilistic decision-making uses expected value calculations: choose
  the action with the highest expected utility.
\item
  Bayesian approaches update beliefs as new evidence arrives, refining
  decision quality.
\item
  Decision trees structure uncertainty into branches of possible
  outcomes with associated probabilities.
\item
  Markov decision processes (MDPs) formalize sequential decision-making
  under uncertainty, where each action leads probabilistically to new
  states and rewards.
\end{itemize}

A critical challenge is balancing risk and reward. Some systems aim for
maximum expected payoff, while others prioritize robustness against
worst-case scenarios.

Comparison Table: Strategies for Uncertain Decisions

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1557}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2705}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2459}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3279}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Strategy
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Core Idea
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Weaknesses
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Expected Utility & Maximize average outcome & Rational, mathematically
sound & Sensitive to mis-specified probabilities \\
Bayesian Updating & Revise beliefs with evidence & Adaptive, principled
& Computationally demanding \\
Robust Optimization & Focus on worst-case scenarios & Safe, conservative
& May miss high-payoff opportunities \\
MDPs & Sequential probabilistic planning & Rich, expressive framework &
Requires accurate transition model \\
\end{longtable}

AI applications are everywhere: medical diagnosis under incomplete
tests, robotics navigation with noisy sensors, financial trading with
uncertain markets, and dialogue systems managing ambiguous user inputs.

\subsubsection{Tiny Code}\label{tiny-code-67}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Expected utility under uncertainty}
\ImportTok{import}\NormalTok{ random}

\NormalTok{actions }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"safe"}\NormalTok{: [(}\DecValTok{10}\NormalTok{, }\FloatTok{1.0}\NormalTok{)],           }\CommentTok{\# always 10}
    \StringTok{"risky"}\NormalTok{: [(}\DecValTok{50}\NormalTok{, }\FloatTok{0.2}\NormalTok{), (}\DecValTok{0}\NormalTok{, }\FloatTok{0.8}\NormalTok{)] }\CommentTok{\# 20\% chance 50, else 0}
\NormalTok{\}}

\KeywordTok{def}\NormalTok{ expected\_utility(action):}
    \ControlFlowTok{return} \BuiltInTok{sum}\NormalTok{(v}\OperatorTok{*}\NormalTok{p }\ControlFlowTok{for}\NormalTok{ v,p }\KeywordTok{in}\NormalTok{ action)}

\ControlFlowTok{for}\NormalTok{ a }\KeywordTok{in}\NormalTok{ actions:}
    \BuiltInTok{print}\NormalTok{(a, }\StringTok{"expected utility:"}\NormalTok{, expected\_utility(actions[a]))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-67}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Adjust the probabilities---does the optimal action change?
\item
  Add a risk-averse criterion (e.g., maximize minimum payoff)---how does
  it affect choice?
\item
  Reflect: should AI systems always chase expected reward, or sometimes
  act conservatively to protect against rare but catastrophic outcomes?
\end{enumerate}

\subsection{69. Sequential decision
processes}\label{sequential-decision-processes}

Many AI problems involve not just a single choice, but a sequence of
actions unfolding over time. Sequential decision processes model this
setting, where each action changes the state of the world and influences
future choices. Success depends on planning ahead, not just optimizing
the next step.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-68}

Think of playing chess. Each move alters the board and constrains the
opponent's replies. Winning depends less on any single move than on
orchestrating a sequence that leads to checkmate.

\subsubsection{Deep Dive}\label{deep-dive-68}

Sequential decisions differ from one-shot choices because they involve
state transitions and temporal consequences. The challenge is
compounding uncertainty, where early actions can have long-term effects.

The classical framework is the Markov Decision Process (MDP), defined
by:

\begin{itemize}
\tightlist
\item
  A set of states.
\item
  A set of actions.
\item
  Transition probabilities specifying how actions change states.
\item
  Reward functions quantifying the benefit of each state-action pair.
\end{itemize}

Policies are strategies that map states to actions. The optimal policy
maximizes expected cumulative reward over time. Variants include
Partially Observable MDPs (POMDPs), where the agent has incomplete
knowledge of the state, and multi-agent decision processes, where
outcomes depend on the choices of others.

Sequential decision processes are the foundation of reinforcement
learning, where agents learn optimal policies through trial and error.
They also appear in robotics, operations research, and control theory.

Comparison Table: One-Shot vs.~Sequential Decisions

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1831}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3099}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5070}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
One-Shot Decision
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sequential Decision
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Action impact & Immediate outcome only & Shapes future opportunities \\
Information & Often complete & May evolve over time \\
Objective & Maximize single reward & Maximize long-term cumulative
reward \\
Example in AI & Medical test selection & Treatment planning over
months \\
\end{longtable}

Sequential settings emphasize foresight. Greedy strategies may fail if
they ignore long-term effects, while optimal policies balance immediate
gains against future consequences. This introduces the classic
exploration vs.~exploitation dilemma: should the agent try new actions
to gather information or exploit known strategies for reward?

\subsubsection{Tiny Code}\label{tiny-code-68}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sequential decision: simple 2{-}step planning}
\NormalTok{states }\OperatorTok{=}\NormalTok{ [}\StringTok{"start"}\NormalTok{, }\StringTok{"mid"}\NormalTok{, }\StringTok{"goal"}\NormalTok{]}
\NormalTok{actions }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"start"}\NormalTok{: \{}\StringTok{"a"}\NormalTok{: (}\StringTok{"mid"}\NormalTok{, }\DecValTok{5}\NormalTok{), }\StringTok{"b"}\NormalTok{: (}\StringTok{"goal"}\NormalTok{, }\DecValTok{2}\NormalTok{)\},}
    \StringTok{"mid"}\NormalTok{: \{}\StringTok{"c"}\NormalTok{: (}\StringTok{"goal"}\NormalTok{, }\DecValTok{10}\NormalTok{)\}}
\NormalTok{\}}

\KeywordTok{def}\NormalTok{ simulate(policy):}
\NormalTok{    state, total }\OperatorTok{=} \StringTok{"start"}\NormalTok{, }\DecValTok{0}
    \ControlFlowTok{while}\NormalTok{ state }\OperatorTok{!=} \StringTok{"goal"}\NormalTok{:}
\NormalTok{        action }\OperatorTok{=}\NormalTok{ policy[state]}
\NormalTok{        state, reward }\OperatorTok{=}\NormalTok{ actions[state][action]}
\NormalTok{        total }\OperatorTok{+=}\NormalTok{ reward}
    \ControlFlowTok{return}\NormalTok{ total}

\NormalTok{policy1 }\OperatorTok{=}\NormalTok{ \{}\StringTok{"start"}\NormalTok{:}\StringTok{"a"}\NormalTok{,}\StringTok{"mid"}\NormalTok{:}\StringTok{"c"}\NormalTok{\}  }\CommentTok{\# plan ahead}
\NormalTok{policy2 }\OperatorTok{=}\NormalTok{ \{}\StringTok{"start"}\NormalTok{:}\StringTok{"b"}\NormalTok{\}            }\CommentTok{\# greedy}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Planned policy reward:"}\NormalTok{, simulate(policy1))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Greedy policy reward:"}\NormalTok{, simulate(policy2))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-68}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Change the rewards---does the greedy policy ever win?
\item
  Extend the horizon---how does the complexity grow with each extra
  step?
\item
  Reflect: why does intelligence require looking beyond the immediate
  payoff?
\end{enumerate}

\subsection{70. Real-world constraints in
optimization}\label{real-world-constraints-in-optimization}

In theory, optimization seeks the best solution according to a
mathematical objective. In practice, real-world AI must handle
constraints: limited resources, noisy data, fairness requirements,
safety guarantees, and human preferences. These constraints shape not
only what is \emph{optimal} but also what is \emph{acceptable}.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-69}

Imagine scheduling flights for an airline. The mathematically cheapest
plan might overwork pilots, delay maintenance, or violate safety rules.
A ``real-world optimal'' schedule respects all these constraints, even
if it sacrifices theoretical efficiency.

\subsubsection{Deep Dive}\label{deep-dive-69}

Real-world optimization rarely occurs in a vacuum. Constraints define
the feasible region within which solutions can exist. They can be:

\begin{itemize}
\tightlist
\item
  Hard constraints: cannot be violated (budget caps, safety rules, legal
  requirements).
\item
  Soft constraints: preferences or guidelines that can be traded off
  against objectives (comfort, fairness, aesthetics).
\item
  Dynamic constraints: change over time due to resource availability,
  environment, or feedback loops.
\end{itemize}

In AI systems, constraints appear everywhere:

\begin{itemize}
\tightlist
\item
  Robotics: torque limits, collision avoidance.
\item
  Healthcare AI: ethical guidelines, treatment side effects.
\item
  Logistics: delivery deadlines, fuel costs, driver working hours.
\item
  Machine learning: fairness metrics, privacy guarantees.
\end{itemize}

Handling constraints requires specialized optimization techniques:
constrained linear programming, penalty methods, Lagrangian relaxation,
or multi-objective frameworks. Often, constraints elevate a simple
optimization into a deeply complex, sometimes NP-hard, real-world
problem.

Comparison Table: Ideal vs.~Constrained Optimization

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3571}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4762}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Dimension
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Ideal Optimization
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Real-World Optimization
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Assumptions & Unlimited resources, no limits & Resource, safety,
fairness, ethics apply \\
Solution space & All mathematically possible & Only feasible under
constraints \\
Output & Mathematically optimal & Practically viable and acceptable \\
Example & Shortest delivery path & Fastest safe path under traffic
rules \\
\end{longtable}

Constraints also highlight the gap between AI theory and deployment. A
pathfinding algorithm may suggest an ideal route, but the real driver
must avoid construction zones, follow regulations, and consider comfort.
This tension between theory and practice is one reason why real-world AI
often values robustness over perfection.

\subsubsection{Tiny Code}\label{tiny-code-69}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Constrained optimization: shortest path with blocked road}
\ImportTok{import}\NormalTok{ heapq}

\NormalTok{graph }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"A"}\NormalTok{: \{}\StringTok{"B"}\NormalTok{:}\DecValTok{1}\NormalTok{,}\StringTok{"C"}\NormalTok{:}\DecValTok{5}\NormalTok{\},}
    \StringTok{"B"}\NormalTok{: \{}\StringTok{"C"}\NormalTok{:}\DecValTok{1}\NormalTok{,}\StringTok{"D"}\NormalTok{:}\DecValTok{4}\NormalTok{\},}
    \StringTok{"C"}\NormalTok{: \{}\StringTok{"D"}\NormalTok{:}\DecValTok{1}\NormalTok{\},}
    \StringTok{"D"}\NormalTok{: \{\}}
\NormalTok{\}}

\NormalTok{blocked }\OperatorTok{=}\NormalTok{ (}\StringTok{"B"}\NormalTok{,}\StringTok{"C"}\NormalTok{)  }\CommentTok{\# constraint: road closed}

\KeywordTok{def}\NormalTok{ constrained\_dijkstra(start, goal):}
\NormalTok{    queue }\OperatorTok{=}\NormalTok{ [(}\DecValTok{0}\NormalTok{,start,[])]}
\NormalTok{    seen }\OperatorTok{=} \BuiltInTok{set}\NormalTok{()}
    \ControlFlowTok{while}\NormalTok{ queue:}
\NormalTok{        cost,node,path }\OperatorTok{=}\NormalTok{ heapq.heappop(queue)}
        \ControlFlowTok{if}\NormalTok{ node }\KeywordTok{in}\NormalTok{ seen:}
            \ControlFlowTok{continue}
\NormalTok{        path }\OperatorTok{=}\NormalTok{ path}\OperatorTok{+}\NormalTok{[node]}
        \ControlFlowTok{if}\NormalTok{ node }\OperatorTok{==}\NormalTok{ goal:}
            \ControlFlowTok{return}\NormalTok{ cost,path}
\NormalTok{        seen.add(node)}
        \ControlFlowTok{for}\NormalTok{ n,c }\KeywordTok{in}\NormalTok{ graph[node].items():}
            \ControlFlowTok{if}\NormalTok{ (node,n) }\OperatorTok{!=}\NormalTok{ blocked:  }\CommentTok{\# enforce constraint}
\NormalTok{                heapq.heappush(queue,(cost}\OperatorTok{+}\NormalTok{c,n,path))}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Constrained path A→D:"}\NormalTok{, constrained\_dijkstra(}\StringTok{"A"}\NormalTok{,}\StringTok{"D"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-69}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add more blocked edges---how does the feasible path set shrink?
\item
  Add a ``soft'' constraint by penalizing certain edges instead of
  forbidding them.
\item
  Reflect: why do most real-world AI systems optimize under constraints
  rather than chasing pure mathematical optima?
\end{enumerate}

\section{Chapter 8. Data, Signals and
Measurement}\label{chapter-8.-data-signals-and-measurement}

\subsection{71. Data as the foundation of
intelligence}\label{data-as-the-foundation-of-intelligence}

No matter how sophisticated the algorithm, AI systems are only as strong
as the data they learn from. Data grounds abstract models in the
realities of the world. It serves as both the raw material and the
feedback loop that allows intelligence to emerge.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-70}

Think of a sculptor and a block of marble. The sculptor's skill matters,
but without marble there is nothing to shape. In AI, algorithms are the
sculptor, but data is the marble---they cannot create meaning from
nothing.

\subsubsection{Deep Dive}\label{deep-dive-70}

Data functions as the foundation in three key ways. First, it provides
representations of the world: pixels stand in for objects, sound waves
for speech, and text for human knowledge. Second, it offers examples of
behavior, allowing learning systems to infer patterns, rules, or
preferences. Third, it acts as feedback, enabling systems to improve
through error correction and reinforcement.

But not all data is equal. High-quality, diverse, and well-structured
datasets produce robust models. Biased, incomplete, or noisy datasets
distort learning and decision-making. This is why data governance,
curation, and documentation are now central to AI practice.

In modern AI, the scale of data has become a differentiator. Classical
expert systems relied on rules hand-coded by humans, but deep learning
thrives because billions of examples fuel the discovery of complex
representations. At the same time, more data is not always better:
redundancy, poor quality, and ethical issues can make massive datasets
counterproductive.

Comparison Table: Data in Different AI Paradigms

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2391}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4022}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3587}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Paradigm
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Role of Data
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Symbolic AI & Encoded as facts, rules, knowledge & Expert systems,
ontologies \\
Classical ML & Training + test sets for models & SVMs, decision trees \\
Deep Learning & Large-scale inputs for representation & ImageNet, GPT
pretraining corpora \\
Reinforcement Learning & Feedback signals from environment &
Game-playing agents, robotics \\
\end{longtable}

The future of AI will likely hinge less on raw data scale and more on
data efficiency: learning robust models from smaller, carefully curated,
or synthetic datasets. This shift mirrors human learning, where a child
can infer concepts from just a few examples.

\subsubsection{Tiny Code}\label{tiny-code-70}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simple learning from data: linear regression}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LinearRegression}

\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{],[}\DecValTok{2}\NormalTok{],[}\DecValTok{3}\NormalTok{],[}\DecValTok{4}\NormalTok{]])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{8}\NormalTok{])  }\CommentTok{\# perfect line: y=2x}

\NormalTok{model }\OperatorTok{=}\NormalTok{ LinearRegression().fit(X,y)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Prediction for x=5:"}\NormalTok{, model.predict([[}\DecValTok{5}\NormalTok{]])[}\DecValTok{0}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-70}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Corrupt the dataset with noise---how does prediction accuracy change?
\item
  Reduce the dataset size---does the model still generalize?
\item
  Reflect: why is data often called the ``new oil,'' and where does this
  metaphor break down?
\end{enumerate}

\subsection{72. Types of data: structured, unstructured,
multimodal}\label{types-of-data-structured-unstructured-multimodal}

AI systems work with many different kinds of data. Structured data is
neatly organized into tables and schemas. Unstructured data includes raw
forms like text, images, and audio. Multimodal data integrates multiple
types, enabling richer understanding. Each type demands different
methods of representation and processing.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-71}

Think of a library. A catalog with author, title, and year is structured
data. The books themselves---pages of text, illustrations, maps---are
unstructured data. A multimedia encyclopedia that combines text, images,
and video is multimodal. AI must navigate all three.

\subsubsection{Deep Dive}\label{deep-dive-71}

Structured data has been the foundation of traditional machine learning.
Rows and columns make statistical modeling straightforward. However,
most real-world data is unstructured: free-form text, conversations,
medical scans, video recordings. The rise of deep learning reflects the
need to automatically process this complexity.

Multimodal data adds another layer: combining modalities to capture
meaning that no single type can provide. A video of a lecture is richer
than its transcript alone, because tone, gesture, and visuals convey
context. Similarly, pairing radiology images with doctor's notes
strengthens diagnosis.

The challenge lies in integration. Structured and unstructured data
often coexist within a system, but aligning them---synchronizing
signals, handling scale differences, and learning cross-modal
representations---remains an open frontier.

Comparison Table: Data Types

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0968}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4113}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2823}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2097}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Data Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Examples
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Challenges
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Structured & Databases, spreadsheets, sensors & Clean, easy to query,
interpretable & Limited expressiveness \\
Unstructured & Text, images, audio, video & Rich, natural, human-like &
High dimensionality, noisy \\
Multimodal & Video with subtitles, medical record (scan + notes) &
Comprehensive, context-rich & Alignment, fusion, scale \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-71}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Handling structured vs unstructured data}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{from}\NormalTok{ sklearn.feature\_extraction.text }\ImportTok{import}\NormalTok{ CountVectorizer}

\CommentTok{\# Structured: tabular}
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}\StringTok{"age"}\NormalTok{:[}\DecValTok{25}\NormalTok{,}\DecValTok{32}\NormalTok{,}\DecValTok{40}\NormalTok{],}\StringTok{"score"}\NormalTok{:[}\DecValTok{88}\NormalTok{,}\DecValTok{92}\NormalTok{,}\DecValTok{75}\NormalTok{]\})}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Structured data sample:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, df)}

\CommentTok{\# Unstructured: text}
\NormalTok{texts }\OperatorTok{=}\NormalTok{ [}\StringTok{"AI is powerful"}\NormalTok{, }\StringTok{"Data drives AI"}\NormalTok{]}
\NormalTok{vectorizer }\OperatorTok{=}\NormalTok{ CountVectorizer()}
\NormalTok{X }\OperatorTok{=}\NormalTok{ vectorizer.fit\_transform(texts)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Unstructured text as bag{-}of{-}words:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, X.toarray())}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-71}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add images as another modality---how would you represent them
  numerically?
\item
  Combine structured scores with unstructured student essays---what
  insights emerge?
\item
  Reflect: why does multimodality bring AI closer to human-like
  perception and reasoning?
\end{enumerate}

\subsection{73. Measurement, sensors, and signal
processing}\label{measurement-sensors-and-signal-processing}

AI systems connect to the world through measurement. Sensors capture raw
signals---light, sound, motion, temperature---and convert them into
data. Signal processing then refines these measurements, reducing noise
and extracting meaningful features for downstream models.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-72}

Imagine listening to a concert through a microphone. The microphone
captures sound waves, but the raw signal is messy: background chatter,
echoes, electrical interference. Signal processing is like adjusting an
equalizer, filtering out the noise, and keeping the melody clear.

\subsubsection{Deep Dive}\label{deep-dive-72}

Measurements are the bridge between physical reality and digital
computation. In robotics, lidar and cameras transform environments into
streams of data points. In healthcare, sensors turn heartbeats into ECG
traces. In finance, transactions become event logs.

Raw sensor data, however, is rarely usable as-is. Signal processing
applies transformations such as filtering, normalization, and feature
extraction. For instance, Fourier transforms reveal frequency patterns
in audio; edge detectors highlight shapes in images; statistical
smoothing reduces random fluctuations in time series.

Quality of measurement is critical: poor sensors or noisy environments
can degrade even the best AI models. Conversely, well-processed signals
can compensate for limited model complexity. This interplay is why
sensing and preprocessing remain as important as learning algorithms
themselves.

Comparison Table: Role of Measurement and Processing

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2022}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4045}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3933}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Stage
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI Applications
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Measurement & Capture raw signals & Camera images, microphone audio \\
Preprocessing & Clean and normalize data & Noise reduction in ECG
signals \\
Feature extraction & Highlight useful patterns & Spectrograms for speech
recognition \\
Modeling & Learn predictive or generative tasks & CNNs on processed
image features \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-72}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Signal processing: smoothing noisy measurements}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Simulated noisy sensor signal}
\NormalTok{np.random.seed(}\DecValTok{0}\NormalTok{)}
\NormalTok{signal }\OperatorTok{=}\NormalTok{ np.sin(np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{50}\NormalTok{)) }\OperatorTok{+}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{,}\FloatTok{0.3}\NormalTok{,}\DecValTok{50}\NormalTok{)}

\CommentTok{\# Simple moving average filter}
\KeywordTok{def}\NormalTok{ smooth(x, window}\OperatorTok{=}\DecValTok{3}\NormalTok{):}
    \ControlFlowTok{return}\NormalTok{ np.convolve(x, np.ones(window)}\OperatorTok{/}\NormalTok{window, mode}\OperatorTok{=}\StringTok{\textquotesingle{}valid\textquotesingle{}}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Raw signal sample:"}\NormalTok{, signal[:}\DecValTok{5}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Smoothed signal sample:"}\NormalTok{, smooth(signal)[:}\DecValTok{5}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-72}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add more noise to the signal---how does smoothing help or hurt?
\item
  Replace moving average with Fourier filtering---what patterns emerge?
\item
  Reflect: why is ``garbage in, garbage out'' especially true for
  sensor-driven AI? \#\#\# 74. Resolution, granularity, and sampling
\end{enumerate}

Every measurement depends on how finely the world is observed.
Resolution is the level of detail captured, granularity is the size of
the smallest distinguishable unit, and sampling determines how often
data is collected. Together, they shape the fidelity and usefulness of
AI inputs.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-73}

Imagine zooming into a digital map. At a coarse resolution, you only see
countries. Zoom further and cities appear. Zoom again and you see
individual streets. The underlying data is the same world, but
resolution and granularity determine what patterns are visible.

\subsubsection{Deep Dive}\label{deep-dive-73}

Resolution, granularity, and sampling are not just technical
choices---they define what AI can or cannot learn. Too coarse a
resolution hides patterns, like trying to detect heart arrhythmia with
one reading per hour. Too fine a resolution overwhelms systems with
redundant detail, like storing every frame of a video when one per
second suffices.

Sampling theory formalizes this trade-off. The Nyquist-Shannon theorem
states that to capture a signal without losing information, it must be
sampled at least twice its highest frequency. Violating this leads to
aliasing, where signals overlap and distort.

In practice, resolution and granularity are often matched to task
requirements. Satellite imaging for weather forecasting may only need
kilometer granularity, while medical imaging requires sub-millimeter
detail. The art lies in balancing precision, efficiency, and relevance.

Comparison Table: Effects of Resolution and Sampling

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1485}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1980}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3366}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3168}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Setting
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Benefit
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Risk if too low
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Risk if too high
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
High resolution & Captures fine detail & Miss critical patterns & Data
overload, storage costs \\
Low resolution & Compact, efficient & Aliasing, hidden structure & Loss
of accuracy \\
Dense sampling & Preserves dynamics & Misses fast changes & Redundancy,
computational burden \\
Sparse sampling & Saves resources & Fails to track important variation &
Insufficient for predictions \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-73}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sampling resolution demo: sine wave}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{x\_high }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\OperatorTok{*}\NormalTok{np.pi, }\DecValTok{1000}\NormalTok{)   }\CommentTok{\# high resolution}
\NormalTok{y\_high }\OperatorTok{=}\NormalTok{ np.sin(x\_high)}

\NormalTok{x\_low }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\OperatorTok{*}\NormalTok{np.pi, }\DecValTok{10}\NormalTok{)      }\CommentTok{\# low resolution}
\NormalTok{y\_low }\OperatorTok{=}\NormalTok{ np.sin(x\_low)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"High{-}res sample (first 5):"}\NormalTok{, y\_high[:}\DecValTok{5}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Low{-}res sample (all):"}\NormalTok{, y\_low)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-73}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Increase low-resolution sampling points---at what point does the wave
  become recognizable?
\item
  Undersample a higher-frequency sine---do you see aliasing effects?
\item
  Reflect: how does the right balance of resolution and sampling depend
  on the domain (healthcare, robotics, astronomy)?
\end{enumerate}

\subsection{75. Noise reduction and signal
enhancement}\label{noise-reduction-and-signal-enhancement}

Real-world data is rarely clean. Noise---random errors, distortions, or
irrelevant fluctuations---can obscure the patterns AI systems need.
Noise reduction and signal enhancement are preprocessing steps that
improve data quality, making models more accurate and robust.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-74}

Think of tuning an old radio. Amid the static, you strain to hear a
favorite song. Adjusting the dial filters out the noise and sharpens the
melody. Signal processing in AI plays the same role: suppressing
interference so the underlying pattern is clearer.

\subsubsection{Deep Dive}\label{deep-dive-74}

Noise arises from many sources: faulty sensors, environmental
conditions, transmission errors, or inherent randomness. Its impact
depends on the task---small distortions in an image may not matter for
object detection but can be critical in medical imaging.

Noise reduction techniques include:

\begin{itemize}
\tightlist
\item
  Filtering: smoothing signals (moving averages, Gaussian filters) to
  remove high-frequency noise.
\item
  Fourier and wavelet transforms: separating signal from noise in the
  frequency domain.
\item
  Denoising autoencoders: deep learning models trained to reconstruct
  clean inputs.
\item
  Ensemble averaging: combining multiple noisy measurements to cancel
  out random variation.
\end{itemize}

Signal enhancement complements noise reduction by amplifying features of
interest---edges in images, peaks in spectra, or keywords in audio
streams. The two processes together ensure that downstream learning
algorithms focus on meaningful patterns.

Comparison Table: Noise Reduction Techniques

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Domain Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strength
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Moving average filter & Time series (finance) & Simple, effective &
Blurs sharp changes \\
Fourier filtering & Audio signals & Separates noise by frequency &
Requires frequency-domain insight \\
Denoising autoencoder & Image processing & Learns complex patterns &
Needs large training data \\
Ensemble averaging & Sensor networks & Reduces random fluctuations &
Ineffective against systematic bias \\
\end{longtable}

Noise reduction is not only about data cleaning---it shapes the very
boundary of what AI can perceive. A poor-quality signal limits
performance no matter the model complexity, while enhanced, noise-free
signals can enable simpler models to perform surprisingly well.

\subsubsection{Tiny Code}\label{tiny-code-74}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Noise reduction with a moving average}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Simulate noisy signal}
\NormalTok{np.random.seed(}\DecValTok{1}\NormalTok{)}
\NormalTok{signal }\OperatorTok{=}\NormalTok{ np.sin(np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{50}\NormalTok{)) }\OperatorTok{+}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{,}\FloatTok{0.4}\NormalTok{,}\DecValTok{50}\NormalTok{)}

\KeywordTok{def}\NormalTok{ moving\_average(x, window}\OperatorTok{=}\DecValTok{3}\NormalTok{):}
    \ControlFlowTok{return}\NormalTok{ np.convolve(x, np.ones(window)}\OperatorTok{/}\NormalTok{window, mode}\OperatorTok{=}\StringTok{\textquotesingle{}valid\textquotesingle{}}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Noisy signal (first 5):"}\NormalTok{, signal[:}\DecValTok{5}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Smoothed signal (first 5):"}\NormalTok{, moving\_average(signal)[:}\DecValTok{5}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-74}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add more noise---does the moving average still recover the signal
  shape?
\item
  Compare moving average with a median filter---how do results differ?
\item
  Reflect: in which domains (finance, healthcare, audio) does noise
  reduction make the difference between failure and success?
\end{enumerate}

\subsection{76. Data bias, drift, and blind
spots}\label{data-bias-drift-and-blind-spots}

AI systems inherit the properties of their training data. Bias occurs
when data systematically favors or disadvantages certain groups or
patterns. Drift happens when the underlying distribution of data changes
over time. Blind spots are regions of the real world poorly represented
in the data. Together, these issues limit reliability and fairness.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-75}

Imagine teaching a student geography using a map that only shows Europe.
The student becomes an expert on European countries but has no knowledge
of Africa or Asia. Their understanding is biased, drifts out of date as
borders change, and contains blind spots where the map is incomplete. AI
faces the same risks with data.

\subsubsection{Deep Dive}\label{deep-dive-75}

Bias arises from collection processes, sampling choices, or historical
inequities embedded in the data. For example, facial recognition systems
trained mostly on light-skinned faces perform poorly on darker-skinned
individuals.

Drift occurs in dynamic environments where patterns evolve. A fraud
detection system trained on last year's transactions may miss new attack
strategies. Drift can be covariate drift (input distributions change),
concept drift (label relationships shift), or prior drift (class
proportions change).

Blind spots reflect the limits of coverage. Rare diseases in medical
datasets, underrepresented languages in NLP, or unusual traffic
conditions in self-driving cars all highlight how missing data reduces
robustness.

Mitigation strategies include diverse sampling, continual learning,
fairness-aware metrics, drift detection algorithms, and active
exploration of underrepresented regions.

Comparison Table: Data Challenges

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0902}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3115}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3115}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2869}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Challenge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mitigation Strategy
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Bias & Systematic distortion in training data & Hiring models favoring
majority groups & Balanced sampling, fairness metrics \\
Drift & Distribution changes over time & Spam filters missing new
campaigns & Drift detection, model retraining \\
Blind spots & Missing or underrepresented cases & Self-driving cars in
rare weather & Active data collection, simulation \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-75}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simulating drift in a simple dataset}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LogisticRegression}

\CommentTok{\# Train data (old distribution)}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{],[}\DecValTok{1}\NormalTok{],[}\DecValTok{2}\NormalTok{],[}\DecValTok{3}\NormalTok{]])}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{])}
\NormalTok{model }\OperatorTok{=}\NormalTok{ LogisticRegression().fit(X\_train, y\_train)}

\CommentTok{\# New data (drifted distribution)}
\NormalTok{X\_new }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{2}\NormalTok{],[}\DecValTok{3}\NormalTok{],[}\DecValTok{4}\NormalTok{],[}\DecValTok{5}\NormalTok{]])}
\NormalTok{y\_new }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{])  }\CommentTok{\# relationship changed}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Old model predictions:"}\NormalTok{, model.predict(X\_new))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"True labels (new distribution):"}\NormalTok{, y\_new)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-75}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add more skewed training data---does the model amplify bias?
\item
  Simulate concept drift by flipping labels---how fast does performance
  degrade?
\item
  Reflect: why must AI systems monitor data continuously rather than
  assuming static distributions?
\end{enumerate}

\subsection{77. From raw signals to usable
features}\label{from-raw-signals-to-usable-features}

Raw data streams are rarely in a form directly usable by AI models.
Feature extraction transforms messy signals into structured
representations that highlight the most relevant patterns. Good features
reduce noise, compress information, and make learning more effective.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-76}

Think of preparing food ingredients. Raw crops from the farm are
unprocessed and unwieldy. Washing, chopping, and seasoning turn them
into usable components for cooking. In the same way, raw data needs
transformation into features before becoming useful for AI.

\subsubsection{Deep Dive}\label{deep-dive-76}

Feature extraction depends on the data type. In images, raw pixels are
converted into edges, textures, or higher-level embeddings. In audio,
waveforms become spectrograms or mel-frequency cepstral coefficients
(MFCCs). In text, words are encoded into bags of words, TF-IDF scores,
or distributed embeddings.

Historically, feature engineering was a manual craft, with domain
experts designing transformations. Deep learning has automated much of
this, with models learning hierarchical representations directly from
raw data. Still, preprocessing remains crucial: even deep networks rely
on normalized inputs, cleaned signals, and structured metadata.

The quality of features often determines the success of downstream
tasks. Poor features burden models with irrelevant noise; strong
features allow even simple algorithms to perform well. This is why
feature extraction is sometimes called the ``art'' of AI.

Comparison Table: Feature Extraction Approaches

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0795}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3182}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3523}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Domain
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Raw Signal Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Typical Features
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Modern Alternative
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Vision & Pixel intensity values & Edges, SIFT, HOG descriptors &
CNN-learned embeddings \\
Audio & Waveforms & Spectrograms, MFCCs & Self-supervised audio
models \\
Text & Words or characters & Bag-of-words, TF-IDF & Word2Vec, BERT
embeddings \\
Tabular & Raw measurements & Normalized, derived ratios & Learned
embeddings in deep nets \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-76}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Feature extraction: text example}
\ImportTok{from}\NormalTok{ sklearn.feature\_extraction.text }\ImportTok{import}\NormalTok{ TfidfVectorizer}

\NormalTok{texts }\OperatorTok{=}\NormalTok{ [}\StringTok{"AI transforms data"}\NormalTok{, }\StringTok{"Data drives intelligence"}\NormalTok{]}
\NormalTok{vectorizer }\OperatorTok{=}\NormalTok{ TfidfVectorizer()}
\NormalTok{X }\OperatorTok{=}\NormalTok{ vectorizer.fit\_transform(texts)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Feature names:"}\NormalTok{, vectorizer.get\_feature\_names\_out())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"TF{-}IDF matrix:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, X.toarray())}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-76}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Apply TF-IDF to a larger set of documents---what features dominate?
\item
  Replace TF-IDF with raw counts---does classification accuracy change?
\item
  Reflect: when should features be hand-crafted, and when should they be
  learned automatically?
\end{enumerate}

\subsection{78. Standards for measurement and
metadata}\label{standards-for-measurement-and-metadata}

Data alone is not enough---how it is measured, described, and
standardized determines whether it can be trusted and reused. Standards
for measurement ensure consistency across systems, while metadata
documents context, quality, and meaning. Without them, AI models risk
learning from incomplete or misleading inputs.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-77}

Imagine receiving a dataset of temperatures without knowing whether
values are in Celsius or Fahrenheit. The numbers are useless---or worse,
dangerous---without metadata to clarify their meaning. Standards and
documentation are the ``units and labels'' that make data interoperable.

\subsubsection{Deep Dive}\label{deep-dive-77}

Measurement standards specify how data is collected: the units,
calibration methods, and protocols. For example, a blood pressure
dataset must specify whether readings were taken at rest, what device
was used, and how values were rounded.

Metadata adds descriptive layers:

\begin{itemize}
\tightlist
\item
  Descriptive metadata: what the dataset contains (variables, units,
  formats).
\item
  Provenance metadata: where the data came from, when it was collected,
  by whom.
\item
  Quality metadata: accuracy, uncertainty, missing values.
\item
  Ethical metadata: consent, usage restrictions, potential biases.
\end{itemize}

In large-scale AI projects, metadata standards like Dublin Core,
schema.org, or ML data cards help datasets remain interpretable and
auditable. Poorly documented data leads to reproducibility crises,
opaque models, and fairness risks.

Comparison Table: Data With vs.~Without Standards

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1910}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4270}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3820}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
With Standards \& Metadata
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Without Standards \& Metadata
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Consistency & Units, formats, and protocols aligned & Confusion,
misinterpretation \\
Reusability & Datasets can be merged and compared & Silos, duplication,
wasted effort \\
Accountability & Provenance and consent are transparent & Origins
unclear, ethical risks \\
Model reliability & Clear assumptions improve performance & Hidden
mismatches degrade accuracy \\
\end{longtable}

Standards are especially critical in regulated domains like healthcare,
finance, and geoscience. A model predicting disease progression must not
only be accurate but also auditable---knowing how, when, and why the
training data was collected.

\subsubsection{Tiny Code}\label{tiny-code-77}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example: attaching simple metadata to a dataset}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"data"}\NormalTok{: [}\FloatTok{36.6}\NormalTok{, }\FloatTok{37.1}\NormalTok{, }\FloatTok{38.0}\NormalTok{],  }\CommentTok{\# temperatures}
    \StringTok{"metadata"}\NormalTok{: \{}
        \StringTok{"unit"}\NormalTok{: }\StringTok{"Celsius"}\NormalTok{,}
        \StringTok{"source"}\NormalTok{: }\StringTok{"Thermometer Model X"}\NormalTok{,}
        \StringTok{"collection\_date"}\NormalTok{: }\StringTok{"2025{-}09{-}16"}\NormalTok{,}
        \StringTok{"notes"}\NormalTok{: }\StringTok{"Measured at rest, oral sensor"}
\NormalTok{    \}}
\NormalTok{\}}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Data:"}\NormalTok{, dataset[}\StringTok{"data"}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Metadata:"}\NormalTok{, dataset[}\StringTok{"metadata"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-77}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Remove the unit metadata---how ambiguous do the values become?
\item
  Add provenance (who, when, where)---does it increase trust in the
  dataset?
\item
  Reflect: why is metadata often the difference between raw numbers and
  actionable knowledge?
\end{enumerate}

\subsection{79. Data curation and
stewardship}\label{data-curation-and-stewardship}

Collecting data is only the beginning. Data curation is the ongoing
process of organizing, cleaning, and maintaining datasets to ensure they
remain useful. Data stewardship extends this responsibility to
governance, ethics, and long-term sustainability. Together, they make
data a durable resource rather than a disposable byproduct.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-78}

Think of a museum. Artifacts are not just stored---they are cataloged,
preserved, and contextualized for future generations. Data requires the
same care: without curation and stewardship, it degrades, becomes
obsolete, or loses trustworthiness.

\subsubsection{Deep Dive}\label{deep-dive-78}

Curation ensures datasets are structured, consistent, and ready for
analysis. It includes cleaning errors, filling missing values,
normalizing formats, and documenting processes. Poorly curated data
leads to fragile models and irreproducible results.

Stewardship broadens the scope. It emphasizes responsible ownership,
ensuring data is collected ethically, used according to consent, and
maintained with transparency. It also covers lifecycle management: from
acquisition to archival or deletion. In AI, this is crucial because
models may amplify harms hidden in unmanaged data.

The FAIR principles---Findable, Accessible, Interoperable,
Reusable---guide modern stewardship. Compliance requires metadata
standards, open documentation, and community practices. Without these,
even large datasets lose value quickly.

Comparison Table: Curation vs.~Stewardship

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1176}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3882}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4941}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Data Curation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Data Stewardship
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Focus & Technical preparation of datasets & Ethical, legal, and
lifecycle management \\
Activities & Cleaning, labeling, formatting & Governance, consent,
compliance, access \\
Timescale & Immediate usability & Long-term sustainability \\
Example & Removing duplicates in logs & Ensuring patient data privacy
over decades \\
\end{longtable}

Curation and stewardship are not just operational tasks---they shape
trust in AI. Without them, datasets may encode hidden biases, degrade in
quality, or become non-compliant with evolving regulations. With them,
data becomes a shared resource for science and society.

\subsubsection{Tiny Code}\label{tiny-code-78}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example of simple data curation: removing duplicates}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}

\NormalTok{data }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}
    \StringTok{"id"}\NormalTok{: [}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{],}
    \StringTok{"value"}\NormalTok{: [}\DecValTok{10}\NormalTok{,}\DecValTok{20}\NormalTok{,}\DecValTok{20}\NormalTok{,}\DecValTok{30}\NormalTok{]}
\NormalTok{\})}

\NormalTok{curated }\OperatorTok{=}\NormalTok{ data.drop\_duplicates()}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Before curation:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, data)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"After curation:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, curated)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-78}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add missing values---how would you curate them (drop, fill, impute)?
\item
  Think about stewardship: who should own and manage this dataset
  long-term?
\item
  Reflect: why is curated, stewarded data as much a public good as clean
  water or safe infrastructure?
\end{enumerate}

\subsection{80. The evolving role of data in AI
progress}\label{the-evolving-role-of-data-in-ai-progress}

The history of AI can be told as a history of data. Early symbolic
systems relied on handcrafted rules and small knowledge bases. Classical
machine learning advanced with curated datasets. Modern deep learning
thrives on massive, diverse corpora. As AI evolves, the role of data
shifts from sheer quantity toward quality, efficiency, and responsible
use.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-79}

Imagine three eras of farming. First, farmers plant seeds manually in
small plots (symbolic AI). Next, they use irrigation and fertilizers to
cultivate larger fields (classical ML with curated datasets). Finally,
industrial-scale farms use machinery and global supply chains (deep
learning with web-scale data). The future may return to smaller, smarter
farms focused on sustainability---AI's shift to efficient, ethical data
use.

\subsubsection{Deep Dive}\label{deep-dive-79}

In early AI, data was secondary; knowledge was encoded directly by
experts. Success depended on the richness of rules, not scale. With
statistical learning, data became central, but curated datasets like
MNIST or UCI repositories sufficed. The deep learning revolution
reframed data as fuel: bigger corpora enabled models to learn richer
representations.

Yet this data-centric paradigm faces limits. Collecting ever-larger
datasets raises issues of redundancy, privacy, bias, and environmental
cost. Performance gains increasingly come from better data, not just
more data: filtering noise, balancing demographics, and aligning
distributions with target tasks. Synthetic data, data augmentation, and
self-supervised learning further reduce dependence on labeled corpora.

The next phase emphasizes data efficiency: achieving strong
generalization with fewer examples. Techniques like few-shot learning,
transfer learning, and foundation models show that high-capacity systems
can adapt with minimal new data if pretraining and priors are strong.

Comparison Table: Evolution of Data in AI

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1518}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3214}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2054}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3214}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Era
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Role of Data
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Systems
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Symbolic AI & Small, handcrafted knowledge bases & Expert systems
(MYCIN) & Brittle, limited coverage \\
Classical ML & Curated, labeled datasets & SVMs, decision trees &
Labor-intensive labeling \\
Deep Learning & Massive, web-scale corpora & GPT, ImageNet models &
Bias, cost, ethical concerns \\
Data-efficient AI & Few-shot, synthetic, curated signals & GPT-4,
diffusion models & Still dependent on pretraining scale \\
\end{longtable}

The trajectory suggests data will remain the cornerstone of AI, but the
focus is shifting. Rather than asking ``how much data,'' the key
questions become: ``what kind of data,'' ``how is it governed,'' and
``who controls it.''

\subsubsection{Tiny Code}\label{tiny-code-79}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simulating data efficiency: training on few vs many points}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LogisticRegression}

\NormalTok{X\_many }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{],[}\DecValTok{1}\NormalTok{],[}\DecValTok{2}\NormalTok{],[}\DecValTok{3}\NormalTok{],[}\DecValTok{4}\NormalTok{],[}\DecValTok{5}\NormalTok{]])}
\NormalTok{y\_many }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}

\NormalTok{X\_few }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{],[}\DecValTok{5}\NormalTok{]])}
\NormalTok{y\_few }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{]}

\NormalTok{model\_many }\OperatorTok{=}\NormalTok{ LogisticRegression().fit(X\_many,y\_many)}
\NormalTok{model\_few }\OperatorTok{=}\NormalTok{ LogisticRegression().fit(X\_few,y\_few)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Prediction with many samples (x=2):"}\NormalTok{, model\_many.predict([[}\DecValTok{2}\NormalTok{]])[}\DecValTok{0}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Prediction with few samples (x=2):"}\NormalTok{, model\_few.predict([[}\DecValTok{2}\NormalTok{]])[}\DecValTok{0}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-79}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train on noisy data---does more always mean better?
\item
  Compare performance between curated small datasets and large but messy
  ones.
\item
  Reflect: is the future of AI about scaling data endlessly, or about
  making smarter use of less?
\end{enumerate}

\section{Chapter 9. Evaluation: Ground Truth, Metrics, and
Benchmark}\label{chapter-9.-evaluation-ground-truth-metrics-and-benchmark}

\subsection{81. Why evaluation is central to
AI}\label{why-evaluation-is-central-to-ai}

Evaluation is the compass of AI. Without it, we cannot tell whether a
system is learning, improving, or even functioning correctly. Evaluation
provides the benchmarks against which progress is measured, the feedback
loops that guide development, and the accountability that ensures trust.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-80}

Think of training for a marathon. Running every day without tracking
time or distance leaves you blind to improvement. Recording and
comparing results over weeks tells you whether you're faster, stronger,
or just running in circles. AI models, too, need evaluation to know if
they're moving closer to their goals.

\subsubsection{Deep Dive}\label{deep-dive-80}

Evaluation serves multiple roles in AI research and practice. At a
scientific level, it transforms intuition into measurable progress:
models can be compared, results replicated, and knowledge accumulated.
At an engineering level, it drives iteration: without clear metrics,
model improvements are indistinguishable from noise. At a societal
level, evaluation ensures systems meet standards of safety, fairness,
and usability.

The difficulty lies in defining ``success.'' For a translation system,
is success measured by BLEU score, human fluency ratings, or
communication effectiveness in real conversations? Each metric captures
part of the truth but not the whole. Overreliance on narrow metrics
risks overfitting to benchmarks while ignoring broader impacts.

Evaluation is also what separates research prototypes from deployed
systems. A model with 99\% accuracy in the lab may fail disastrously if
evaluated under real-world distribution shifts. Continuous evaluation is
therefore as important as one-off testing, ensuring robustness over
time.

Comparison Table: Roles of Evaluation

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1294}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4235}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4471}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Level
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Scientific & Measure progress, enable replication & Comparing algorithms
on ImageNet \\
Engineering & Guide iteration and debugging & Monitoring loss curves
during training \\
Societal & Ensure trust, safety, fairness & Auditing bias in hiring
algorithms \\
\end{longtable}

Evaluation is not just about accuracy but about defining values. What we
measure reflects what we consider important. If evaluation only tracks
efficiency, fairness may be ignored. If it only tracks benchmarks,
real-world usability may lag behind. Thus, designing evaluation
frameworks is as much a normative decision as a technical one.

\subsubsection{Tiny Code}\label{tiny-code-80}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simple evaluation of a classifier}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ accuracy\_score}

\NormalTok{y\_true }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{]}
\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{]}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Accuracy:"}\NormalTok{, accuracy\_score(y\_true, y\_pred))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-80}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add false positives or false negatives---does accuracy still reflect
  system quality?
\item
  Replace accuracy with precision/recall---what new insights appear?
\item
  Reflect: why does ``what we measure'' ultimately shape ``what we
  build'' in AI?
\end{enumerate}

\subsection{82. Ground truth: gold standards and
proxies}\label{ground-truth-gold-standards-and-proxies}

Evaluation in AI depends on comparing model outputs against a reference.
The most reliable reference is ground truth---the correct labels,
answers, or outcomes for each input. When true labels are unavailable,
researchers often rely on proxies, which approximate truth but may
introduce errors or biases.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-81}

Imagine grading math homework. If you have the official answer key, you
can check each solution precisely---that's ground truth. If the key is
missing, you might ask another student for their answer. It's quicker,
but you risk copying their mistakes---that's a proxy.

\subsubsection{Deep Dive}\label{deep-dive-81}

Ground truth provides the foundation for supervised learning and model
validation. In image recognition, it comes from labeled datasets where
humans annotate objects. In speech recognition, it comes from
transcripts aligned to audio. In medical AI, ground truth may be expert
diagnoses confirmed by follow-up tests.

However, obtaining ground truth is costly, slow, and sometimes
impossible. For example, in predicting long-term economic outcomes or
scientific discoveries, we cannot observe the ``true'' label in real
time. Proxies step in: click-through rates approximate relevance,
hospital readmission approximates health outcomes, human ratings
approximate translation quality.

The challenge is that proxies may diverge from actual goals. Optimizing
for clicks may produce clickbait, not relevance. Optimizing for
readmissions may ignore patient well-being. This disconnect is known as
the proxy problem, and it highlights the danger of equating
easy-to-measure signals with genuine ground truth.

Comparison Table: Ground Truth vs.~Proxies

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1481}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4198}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4321}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Ground Truth
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Proxies
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Accuracy & High fidelity, definitive & Approximate, error-prone \\
Cost & Expensive, labor-intensive & Cheap, scalable \\
Availability & Limited in scope, slow to collect & Widely available,
real-time \\
Risks & Narrow coverage & Misalignment, unintended incentives \\
Example & Radiologist-confirmed tumor labels & Hospital billing codes \\
\end{longtable}

Balancing truth and proxies is an ongoing struggle in AI. Gold standards
are needed for rigor but cannot scale indefinitely. Proxies allow rapid
iteration but risk misguiding optimization. Increasingly, hybrid
approaches are emerging---combining small high-quality ground truth
datasets with large proxy-driven datasets, often via semi-supervised or
self-supervised learning.

\subsubsection{Tiny Code}\label{tiny-code-81}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Comparing ground truth vs proxy evaluation}
\NormalTok{y\_true   }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{]  }\CommentTok{\# ground truth labels}
\NormalTok{y\_proxy  }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]  }\CommentTok{\# proxy labels (noisy)}
\NormalTok{y\_pred   }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{]  }\CommentTok{\# model predictions}

\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ accuracy\_score}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Accuracy vs ground truth:"}\NormalTok{, accuracy\_score(y\_true, y\_pred))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Accuracy vs proxy:"}\NormalTok{, accuracy\_score(y\_proxy, y\_pred))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-81}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add more noise to the proxy labels---how quickly does proxy accuracy
  diverge from true accuracy?
\item
  Combine ground truth with proxy labels---does this improve robustness?
\item
  Reflect: why does the choice of ground truth or proxy ultimately shape
  how AI systems behave in the real world?
\end{enumerate}

\subsection{83. Metrics for classification, regression,
ranking}\label{metrics-for-classification-regression-ranking}

Evaluation requires metrics---quantitative measures that capture how
well a model performs its task. Different tasks demand different
metrics: classification uses accuracy, precision, recall, and F1;
regression uses mean squared error or R²; ranking uses measures like
NDCG or MAP. Choosing the right metric ensures models are optimized for
what truly matters.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-82}

Think of judging a competition. A sprint race is scored by fastest time
(regression). A spelling bee is judged right or wrong (classification).
A search engine is ranked by how high relevant results appear (ranking).
The scoring rule changes with the task, just like metrics in AI.

\subsubsection{Deep Dive}\label{deep-dive-82}

In classification, the simplest metric is accuracy: the proportion of
correct predictions. But accuracy can be misleading when classes are
imbalanced. Precision measures the fraction of positive predictions that
are correct, recall measures the fraction of true positives identified,
and F1 balances the two.

In regression, metrics focus on error magnitude. Mean squared error
(MSE) penalizes large deviations heavily, while mean absolute error
(MAE) treats all errors equally. R² captures how much of the variance in
the target variable the model explains.

In ranking, the goal is ordering relevance. Metrics like Mean Average
Precision (MAP) evaluate precision across ranks, while Normalized
Discounted Cumulative Gain (NDCG) emphasizes highly ranked relevant
results. These are essential in information retrieval, recommendation,
and search engines.

The key insight is that metrics are not interchangeable. A fraud
detection system optimized for accuracy may ignore rare but costly fraud
cases, while optimizing for recall may catch more fraud but generate
false alarms. Choosing metrics means choosing trade-offs.

Comparison Table: Metrics Across Tasks

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2952}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5714}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Task
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Common Metrics
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
What They Emphasize
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Classification & Accuracy, Precision, Recall, F1 & Balance between
overall correctness and handling rare events \\
Regression & MSE, MAE, R² & Magnitude of prediction errors \\
Ranking & MAP, NDCG, Precision@k & Placement of relevant items at the
top \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-82}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ accuracy\_score, mean\_squared\_error}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ ndcg\_score}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Classification example}
\NormalTok{y\_true\_cls }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\NormalTok{y\_pred\_cls }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Classification accuracy:"}\NormalTok{, accuracy\_score(y\_true\_cls, y\_pred\_cls))}

\CommentTok{\# Regression example}
\NormalTok{y\_true\_reg }\OperatorTok{=}\NormalTok{ [}\FloatTok{2.5}\NormalTok{, }\FloatTok{0.0}\NormalTok{, }\FloatTok{2.1}\NormalTok{, }\FloatTok{7.8}\NormalTok{]}
\NormalTok{y\_pred\_reg }\OperatorTok{=}\NormalTok{ [}\FloatTok{3.0}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{, }\FloatTok{2.0}\NormalTok{, }\FloatTok{7.5}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Regression MSE:"}\NormalTok{, mean\_squared\_error(y\_true\_reg, y\_pred\_reg))}

\CommentTok{\# Ranking example}
\NormalTok{true\_relevance }\OperatorTok{=}\NormalTok{ np.asarray([[}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]])}
\NormalTok{scores }\OperatorTok{=}\NormalTok{ np.asarray([[}\FloatTok{0.1}\NormalTok{,}\FloatTok{0.4}\NormalTok{,}\FloatTok{0.35}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Ranking NDCG:"}\NormalTok{, ndcg\_score(true\_relevance, scores))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-82}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add more imbalanced classes to the classification task---does accuracy
  still tell the full story?
\item
  Compare MAE and MSE on regression---why does one penalize outliers
  more?
\item
  Change the ranking scores---does NDCG reward putting relevant items at
  the top?
\end{enumerate}

\subsection{84. Multi-objective and task-specific
metrics}\label{multi-objective-and-task-specific-metrics}

Real-world AI rarely optimizes for a single criterion. Multi-objective
metrics combine several goals---like accuracy and fairness, or speed and
energy efficiency---into evaluation. Task-specific metrics adapt general
principles to the nuances of a domain, ensuring that evaluation reflects
what truly matters in context.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-83}

Imagine judging a car. Speed alone doesn't decide the winner---safety,
fuel efficiency, and comfort also count. Similarly, an AI system must be
judged across multiple axes, not just one score.

\subsubsection{Deep Dive}\label{deep-dive-83}

Multi-objective metrics arise when competing priorities exist. For
example, in healthcare AI, sensitivity (catching every possible case)
must be balanced with specificity (avoiding false alarms). In
recommender systems, relevance must be balanced against diversity or
novelty. In robotics, task completion speed competes with energy
consumption and safety.

There are several ways to handle multiple objectives:

\begin{itemize}
\tightlist
\item
  Composite scores: weighted sums of different metrics.
\item
  Pareto analysis: evaluating trade-offs without collapsing into a
  single number.
\item
  Constraint-based metrics: optimizing one objective while enforcing
  thresholds on others.
\end{itemize}

Task-specific metrics tailor evaluation to the problem. In machine
translation, BLEU and METEOR attempt to measure linguistic quality. In
speech synthesis, MOS (Mean Opinion Score) reflects human perceptions of
naturalness. In medical imaging, Dice coefficient captures spatial
overlap between predicted and actual regions of interest.

The risk is that poorly chosen metrics incentivize undesirable
behavior---overfitting to leaderboards, optimizing proxies rather than
real goals, or ignoring hidden dimensions like fairness and usability.

Comparison Table: Multi-Objective and Task-Specific Metrics

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4222}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Context
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Multi-Objective Metric Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Task-Specific Metric Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Healthcare & Sensitivity + Specificity balance & Dice coefficient for
tumor detection \\
Recommender Systems & Relevance + Diversity & Novelty index \\
NLP & Fluency + Adequacy in translation & BLEU, METEOR \\
Robotics & Efficiency + Safety & Task completion time under
constraints \\
\end{longtable}

Evaluation frameworks increasingly adopt dashboard-style reporting
instead of single scores, showing trade-offs explicitly. This helps
researchers and practitioners make informed decisions aligned with
broader values.

\subsubsection{Tiny Code}\label{tiny-code-83}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Multi{-}objective evaluation: weighted score}
\NormalTok{precision }\OperatorTok{=} \FloatTok{0.8}
\NormalTok{recall }\OperatorTok{=} \FloatTok{0.6}

\CommentTok{\# Weighted composite: 70\% precision, 30\% recall}
\NormalTok{score }\OperatorTok{=} \FloatTok{0.7}\OperatorTok{*}\NormalTok{precision }\OperatorTok{+} \FloatTok{0.3}\OperatorTok{*}\NormalTok{recall}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Composite score:"}\NormalTok{, score)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-83}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Adjust weights between precision and recall---how does it change the
  ``best'' model?
\item
  Replace composite scoring with Pareto analysis---are some models
  incomparable?
\item
  Reflect: why is it dangerous to collapse complex goals into a single
  number?
\end{enumerate}

\subsection{85. Statistical significance and
confidence}\label{statistical-significance-and-confidence}

When comparing AI models, differences in performance may arise from
chance rather than genuine improvement. Statistical significance testing
and confidence intervals quantify how much trust we can place in
observed results. They separate real progress from random variation.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-84}

Think of flipping a coin 10 times and getting 7 heads. Is the coin
biased, or was it just luck? Without statistical tests, you can't be
sure. Evaluating AI models works the same way---apparent improvements
might be noise unless we test their reliability.

\subsubsection{Deep Dive}\label{deep-dive-84}

Statistical significance measures whether performance differences are
unlikely under a null hypothesis (e.g., two models are equally good).
Common tests include the t-test, chi-square test, and bootstrap
resampling.

Confidence intervals provide a range within which the true performance
likely lies, usually expressed at 95\% or 99\% levels. For example,
reporting accuracy as 92\% ± 2\% is more informative than a bare 92\%,
because it acknowledges uncertainty.

Significance and confidence are especially important when:

\begin{itemize}
\tightlist
\item
  Comparing models on small datasets.
\item
  Evaluating incremental improvements.
\item
  Benchmarking in competitions or leaderboards.
\end{itemize}

Without these safeguards, AI progress can be overstated. Many published
results that seemed promising later failed to replicate, fueling
concerns about reproducibility in machine learning.

Comparison Table: Accuracy vs.~Confidence

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2237}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2237}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5526}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Report Style
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Value
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Interpretation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Raw accuracy & 92\% & Single point estimate, no uncertainty \\
With confidence & 92\% ± 2\% (95\% CI) & True accuracy likely lies
between 90--94\% \\
Significance test & p \textless{} 0.05 & Less than 5\% chance result is
random noise \\
\end{longtable}

By treating evaluation statistically, AI systems are held to scientific
standards rather than marketing hype. This strengthens trust and helps
avoid chasing illusions of progress.

\subsubsection{Tiny Code}\label{tiny-code-84}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Bootstrap confidence interval for accuracy}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{y\_true }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{])}
\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{])}

\NormalTok{accuracy }\OperatorTok{=}\NormalTok{ np.mean(y\_true }\OperatorTok{==}\NormalTok{ y\_pred)}

\CommentTok{\# Bootstrap resampling}
\NormalTok{bootstraps }\OperatorTok{=} \DecValTok{1000}
\NormalTok{scores }\OperatorTok{=}\NormalTok{ []}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng(}\DecValTok{0}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(bootstraps):}
\NormalTok{    idx }\OperatorTok{=}\NormalTok{ rng.choice(}\BuiltInTok{len}\NormalTok{(y\_true), }\BuiltInTok{len}\NormalTok{(y\_true), replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{    scores.append(np.mean(y\_true[idx] }\OperatorTok{==}\NormalTok{ y\_pred[idx]))}

\NormalTok{ci\_lower, ci\_upper }\OperatorTok{=}\NormalTok{ np.percentile(scores, [}\FloatTok{2.5}\NormalTok{,}\FloatTok{97.5}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Accuracy: }\SpecialCharTok{\{}\NormalTok{accuracy}\SpecialCharTok{:.2f\}}\SpecialStringTok{, 95\% CI: [}\SpecialCharTok{\{}\NormalTok{ci\_lower}\SpecialCharTok{:.2f\}}\SpecialStringTok{, }\SpecialCharTok{\{}\NormalTok{ci\_upper}\SpecialCharTok{:.2f\}}\SpecialStringTok{]"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-84}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Reduce the dataset size---how does the confidence interval widen?
\item
  Increase the number of bootstrap samples---does the CI stabilize?
\item
  Reflect: why should every AI claim of superiority come with
  uncertainty estimates?
\end{enumerate}

\subsection{86. Benchmarks and leaderboards in AI
research}\label{benchmarks-and-leaderboards-in-ai-research}

Benchmarks and leaderboards provide shared standards for evaluating AI.
A benchmark is a dataset or task that defines a common ground for
comparison. A leaderboard tracks performance on that benchmark, ranking
systems by their reported scores. Together, they drive competition,
progress, and sometimes over-optimization.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-85}

Think of a high-jump bar in athletics. Each athlete tries to clear the
same bar, and the scoreboard shows who jumped the highest. Benchmarks
are the bar, leaderboards are the scoreboard, and researchers are the
athletes.

\subsubsection{Deep Dive}\label{deep-dive-85}

Benchmarks like ImageNet for vision, GLUE for NLP, and Atari for
reinforcement learning have shaped entire subfields. They make progress
measurable, enabling fair comparisons across methods. Leaderboards add
visibility and competition, encouraging rapid iteration and innovation.

Yet this success comes with risks. Overfitting to benchmarks is common:
models achieve state-of-the-art scores but fail under real-world
conditions. Benchmarks may also encode biases, meaning leaderboard
``winners'' are not necessarily best for fairness, robustness, or
efficiency. Moreover, a focus on single numbers obscures trade-offs such
as interpretability, cost, or safety.

Comparison Table: Pros and Cons of Benchmarks

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Benefit & Risk \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Standardized evaluation & Narrow focus on specific tasks \\
Encourages reproducibility & Overfitting to test sets \\
Accelerates innovation & Ignores robustness and generality \\
Provides community reference & Creates leaderboard chasing culture \\
\end{longtable}

Benchmarks are evolving. Dynamic benchmarks (e.g., Dynabench)
continuously refresh data to resist overfitting. Multi-dimensional
leaderboards report robustness, efficiency, and fairness, not just raw
accuracy. The field is moving from static bars to richer ecosystems of
evaluation.

\subsubsection{Tiny Code}\label{tiny-code-85}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simple leaderboard tracker}
\NormalTok{leaderboard }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    \{}\StringTok{"model"}\NormalTok{: }\StringTok{"A"}\NormalTok{, }\StringTok{"score"}\NormalTok{: }\FloatTok{0.85}\NormalTok{\},}
\NormalTok{    \{}\StringTok{"model"}\NormalTok{: }\StringTok{"B"}\NormalTok{, }\StringTok{"score"}\NormalTok{: }\FloatTok{0.88}\NormalTok{\},}
\NormalTok{    \{}\StringTok{"model"}\NormalTok{: }\StringTok{"C"}\NormalTok{, }\StringTok{"score"}\NormalTok{: }\FloatTok{0.83}\NormalTok{\},}
\NormalTok{]}

\CommentTok{\# Rank models}
\NormalTok{ranked }\OperatorTok{=} \BuiltInTok{sorted}\NormalTok{(leaderboard, key}\OperatorTok{=}\KeywordTok{lambda}\NormalTok{ x: x[}\StringTok{"score"}\NormalTok{], reverse}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ i, entry }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(ranked, }\DecValTok{1}\NormalTok{):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{. }\SpecialCharTok{\{}\NormalTok{entry[}\StringTok{\textquotesingle{}model\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{ {-} }\SpecialCharTok{\{}\NormalTok{entry[}\StringTok{\textquotesingle{}score\textquotesingle{}}\NormalTok{]}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-85}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add efficiency or fairness scores---does the leaderboard ranking
  change?
\item
  Simulate overfitting by artificially inflating one model's score.
\item
  Reflect: should leaderboards report a single ``winner,'' or a richer
  profile of performance dimensions?
\end{enumerate}

\subsection{87. Overfitting to benchmarks and Goodhart's
Law}\label{overfitting-to-benchmarks-and-goodharts-law}

Benchmarks are designed to measure progress, but when optimization
focuses narrowly on beating the benchmark, true progress may stall. This
phenomenon is captured by Goodhart's Law: \emph{``When a measure becomes
a target, it ceases to be a good measure.''} In AI, this means models
may excel on test sets while failing in the real world.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-86}

Imagine students trained only to pass practice exams. They memorize
patterns in past tests but struggle with new problems. Their scores
rise, but their true understanding does not. AI models can fall into the
same trap when benchmarks dominate training.

\subsubsection{Deep Dive}\label{deep-dive-86}

Overfitting to benchmarks happens in several ways. Models may exploit
spurious correlations in datasets, such as predicting ``snow'' whenever
``polar bear'' appears. Leaderboard competition can encourage marginal
improvements that exploit dataset quirks instead of advancing general
methods.

Goodhart's Law warns that once benchmarks become the primary target,
they lose their reliability as indicators of general capability. The
history of AI is filled with shifting benchmarks: chess, ImageNet,
GLUE---all once difficult, now routinely surpassed. Each success reveals
both the value and the limitation of benchmarks.

Mitigation strategies include:

\begin{itemize}
\tightlist
\item
  Rotating or refreshing benchmarks to prevent memorization.
\item
  Creating adversarial or dynamic test sets.
\item
  Reporting performance across multiple benchmarks and dimensions
  (robustness, efficiency, fairness).
\end{itemize}

Comparison Table: Healthy vs.~Unhealthy Benchmarking

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1771}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3646}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4583}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Benchmark Use
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Healthy Practice
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Unhealthy Practice
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Goal & Measure general progress & Chase leaderboard rankings \\
Model behavior & Robust improvements across settings & Overfitting to
dataset quirks \\
Community outcome & Innovation, transferable insights & Saturated
leaderboard with incremental gains \\
\end{longtable}

The key lesson is that benchmarks are tools, not goals. When treated as
ultimate targets, they distort incentives. When treated as indicators,
they guide meaningful progress.

\subsubsection{Tiny Code}\label{tiny-code-86}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simulating overfitting to a benchmark}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LogisticRegression}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ accuracy\_score}

\CommentTok{\# Benchmark dataset (biased)}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{],[}\DecValTok{1}\NormalTok{],[}\DecValTok{2}\NormalTok{],[}\DecValTok{3}\NormalTok{]])}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{])  }\CommentTok{\# simple split}
\NormalTok{X\_test  }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{4}\NormalTok{],[}\DecValTok{5}\NormalTok{]])}
\NormalTok{y\_test  }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{])}

\CommentTok{\# Model overfits quirks in train set}
\NormalTok{model }\OperatorTok{=}\NormalTok{ LogisticRegression().fit(X\_train, y\_train)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Train accuracy:"}\NormalTok{, accuracy\_score(y\_train, model.predict(X\_train)))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Test accuracy:"}\NormalTok{, accuracy\_score(y\_test, model.predict(X\_test)))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-86}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add noise to the test set---does performance collapse?
\item
  Train on a slightly different distribution---does the model still hold
  up?
\item
  Reflect: why does optimizing for benchmarks risk producing brittle AI
  systems?
\end{enumerate}

\subsection{88. Robust evaluation under distribution
shift}\label{robust-evaluation-under-distribution-shift}

AI systems are often trained and tested on neatly defined datasets. But
in deployment, the real world rarely matches the training distribution.
Distribution shift occurs when the data a model encounters differs from
the data it was trained on. Robust evaluation ensures performance is
measured not only in controlled settings but also under these shifts.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-87}

Think of a student who aces practice problems but struggles on the
actual exam because the questions are phrased differently. The knowledge
was too tuned to the practice set. AI models face the same problem when
real-world inputs deviate from the benchmark.

\subsubsection{Deep Dive}\label{deep-dive-87}

Distribution shifts appear in many forms:

\begin{itemize}
\tightlist
\item
  Covariate shift: input features change (e.g., new slang in language
  models).
\item
  Concept shift: the relationship between inputs and outputs changes
  (e.g., fraud patterns evolve).
\item
  Prior shift: class proportions change (e.g., rare diseases become more
  prevalent).
\end{itemize}

Evaluating robustness requires deliberately exposing models to such
changes. Approaches include stress-testing with out-of-distribution
data, synthetic perturbations, or domain transfer benchmarks. For
example, an image classifier trained on clean photos might be evaluated
on blurred or adversarially perturbed images.

Robust evaluation also considers worst-case performance. A model with
95\% accuracy on average may still fail catastrophically in certain
subgroups or environments. Reporting only aggregate scores hides these
vulnerabilities.

Comparison Table: Standard vs.~Robust Evaluation

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1485}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4257}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4257}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Standard Evaluation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Robust Evaluation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Data assumption & Train and test drawn from same distribution & Test
includes shifted or adversarial data \\
Metrics & Average accuracy or loss & Subgroup, stress-test, or
worst-case scores \\
Purpose & Validate in controlled conditions & Predict reliability in
deployment \\
Example & ImageNet test split & ImageNet-C (corruptions, noise, blur) \\
\end{longtable}

Robust evaluation is not only about detecting failure---it is about
anticipating environments where models will operate. For
mission-critical domains like healthcare or autonomous driving, this is
non-negotiable.

\subsubsection{Tiny Code}\label{tiny-code-87}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simple robustness test: add noise to test data}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LogisticRegression}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ accuracy\_score}

\CommentTok{\# Train on clean data}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{],[}\DecValTok{1}\NormalTok{],[}\DecValTok{2}\NormalTok{],[}\DecValTok{3}\NormalTok{]])}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{])}
\NormalTok{model }\OperatorTok{=}\NormalTok{ LogisticRegression().fit(X\_train, y\_train)}

\CommentTok{\# Test on clean vs shifted (noisy) data}
\NormalTok{X\_test\_clean }\OperatorTok{=}\NormalTok{ np.array([[}\FloatTok{1.1}\NormalTok{],[}\FloatTok{2.9}\NormalTok{]])}
\NormalTok{y\_test }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{])}

\NormalTok{X\_test\_shifted }\OperatorTok{=}\NormalTok{ X\_test\_clean }\OperatorTok{+}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{,}\FloatTok{0.5}\NormalTok{,(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Accuracy (clean):"}\NormalTok{, accuracy\_score(y\_test, model.predict(X\_test\_clean)))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Accuracy (shifted):"}\NormalTok{, accuracy\_score(y\_test, model.predict(X\_test\_shifted)))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-87}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Increase the noise level---at what point does performance collapse?
\item
  Train on a larger dataset---does robustness improve naturally?
\item
  Reflect: why is robustness more important than peak accuracy for
  real-world AI?
\end{enumerate}

\subsection{89. Beyond accuracy: fairness, interpretability,
efficiency}\label{beyond-accuracy-fairness-interpretability-efficiency}

Accuracy alone is not enough to judge an AI system. Real-world
deployment demands broader evaluation criteria: fairness to ensure
equitable treatment, interpretability to provide human understanding,
and efficiency to guarantee scalability and sustainability. Together,
these dimensions extend evaluation beyond raw predictive power.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-88}

Imagine buying a car. Speed alone doesn't make it good---you also care
about safety, fuel efficiency, and ease of maintenance. Similarly, an AI
model can't be judged only by accuracy; it must also be fair,
understandable, and efficient to be trusted.

\subsubsection{Deep Dive}\label{deep-dive-88}

Fairness addresses disparities in outcomes across groups. A hiring
algorithm may achieve high accuracy overall but discriminate against
women or minorities. Fairness metrics include demographic parity,
equalized odds, and subgroup accuracy.

Interpretability ensures models are not black boxes. Humans need
explanations to build trust, debug errors, and comply with regulation.
Techniques include feature importance, local explanations (LIME, SHAP),
and inherently interpretable models like decision trees.

Efficiency considers the cost of deploying AI at scale. Large models may
be accurate but consume prohibitive energy, memory, or latency.
Evaluation includes FLOPs, inference time, and energy per prediction.
Efficiency matters especially for edge devices and climate-conscious
computing.

Comparison Table: Dimensions of Evaluation

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1818}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3750}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4432}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Dimension
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Key Question
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Metric
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Accuracy & Does it make correct predictions? & Error rate, F1 score \\
Fairness & Are outcomes equitable? & Demographic parity, subgroup
error \\
Interpretability & Can humans understand decisions? & Feature
attribution, transparency score \\
Efficiency & Can it run at scale sustainably? & FLOPs, latency, energy
per query \\
\end{longtable}

Balancing these metrics is challenging because improvements in one
dimension can hurt another. Pruning a model may improve efficiency but
reduce interpretability. Optimizing fairness may slightly reduce
accuracy. The art of evaluation lies in balancing competing values
according to context.

\subsubsection{Tiny Code}\label{tiny-code-88}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simple fairness check: subgroup accuracy}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ accuracy\_score}

\CommentTok{\# Predictions across two groups}
\NormalTok{y\_true }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{])}
\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{])}
\NormalTok{groups }\OperatorTok{=}\NormalTok{ np.array([}\StringTok{"A"}\NormalTok{,}\StringTok{"A"}\NormalTok{,}\StringTok{"B"}\NormalTok{,}\StringTok{"B"}\NormalTok{,}\StringTok{"B"}\NormalTok{,}\StringTok{"A"}\NormalTok{])}

\ControlFlowTok{for}\NormalTok{ g }\KeywordTok{in}\NormalTok{ np.unique(groups):}
\NormalTok{    idx }\OperatorTok{=}\NormalTok{ groups }\OperatorTok{==}\NormalTok{ g}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Group }\SpecialCharTok{\{}\NormalTok{g}\SpecialCharTok{\}}\SpecialStringTok{ accuracy:"}\NormalTok{, accuracy\_score(y\_true[idx], y\_pred[idx]))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-88}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Adjust predictions to make one group perform worse---how does fairness
  change?
\item
  Add runtime measurement to compare efficiency across models.
\item
  Reflect: should accuracy ever outweigh fairness or efficiency, or must
  evaluation always be multi-dimensional?
\end{enumerate}

\subsection{90. Building better evaluation
ecosystems}\label{building-better-evaluation-ecosystems}

An evaluation ecosystem goes beyond single datasets or metrics. It is a
structured environment where benchmarks, tools, protocols, and community
practices interact to ensure that AI systems are tested thoroughly,
fairly, and continuously. A healthy ecosystem enables sustained progress
rather than short-term leaderboard chasing.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-89}

Think of public health. One thermometer reading doesn't describe a
population's health. Instead, ecosystems of hospitals, labs, surveys,
and monitoring systems track multiple indicators over time. In AI,
evaluation ecosystems serve the same role---providing many complementary
views of model quality.

\subsubsection{Deep Dive}\label{deep-dive-89}

Traditional evaluation relies on static test sets and narrow metrics.
But modern AI operates in dynamic, high-stakes environments where
robustness, fairness, efficiency, and safety all matter. Building a true
ecosystem involves several layers:

\begin{itemize}
\tightlist
\item
  Diverse benchmarks: covering multiple domains, tasks, and
  distributions.
\item
  Standardized protocols: ensuring experiments are reproducible across
  labs.
\item
  Multi-dimensional reporting: capturing accuracy, robustness,
  interpretability, fairness, and energy use.
\item
  Continuous evaluation: monitoring models post-deployment as data
  drifts.
\item
  Community governance: open platforms, shared resources, and watchdogs
  against misuse.
\end{itemize}

Emerging efforts like Dynabench (dynamic data collection), HELM
(holistic evaluation of language models), and BIG-bench (broad
generalization testing) show how ecosystems can move beyond
single-number leaderboards.

Comparison Table: Traditional vs.~Ecosystem Evaluation

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1266}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3291}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5443}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Traditional Evaluation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Evaluation Ecosystem
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Benchmarks & Single static dataset & Multiple, dynamic, domain-spanning
datasets \\
Metrics & Accuracy or task-specific & Multi-dimensional dashboards \\
Scope & Pre-deployment only & Lifecycle-wide, including
post-deployment \\
Governance & Isolated labs or companies & Community-driven, transparent
practices \\
\end{longtable}

Ecosystems also encourage responsibility. By highlighting fairness gaps,
robustness failures, or energy costs, they force AI development to align
with broader societal goals. Without them, progress risks being measured
narrowly and misleadingly.

\subsubsection{Tiny Code}\label{tiny-code-89}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example: evaluation dashboard across metrics}
\NormalTok{results }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"accuracy"}\NormalTok{: }\FloatTok{0.92}\NormalTok{,}
    \StringTok{"robustness"}\NormalTok{: }\FloatTok{0.75}\NormalTok{,}
    \StringTok{"fairness"}\NormalTok{: }\FloatTok{0.80}\NormalTok{,}
    \StringTok{"efficiency"}\NormalTok{: }\StringTok{"120 ms/query"}
\NormalTok{\}}

\ControlFlowTok{for}\NormalTok{ k,v }\KeywordTok{in}\NormalTok{ results.items():}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{k}\SpecialCharTok{.}\NormalTok{capitalize()}\SpecialCharTok{:\textless{}12\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{v}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-89}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add more dimensions (interpretability, cost)---how does the picture
  change?
\item
  Compare two models across all metrics---does the ``winner'' differ
  depending on which metric you value most?
\item
  Reflect: why does the future of AI evaluation depend on ecosystems,
  not isolated benchmarks?
\end{enumerate}

\section{Chapter 10. Reproductivity, tooling, and the scientific
method}\label{chapter-10.-reproductivity-tooling-and-the-scientific-method}

\subsection{91. The role of reproducibility in
science}\label{the-role-of-reproducibility-in-science}

Reproducibility is the backbone of science. In AI, it means that
experiments, once published, can be independently repeated with the same
methods and yield consistent results. Without reproducibility, research
findings are fragile, progress is unreliable, and trust in the field
erodes.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-90}

Imagine a recipe book where half the dishes cannot be recreated because
the instructions are vague or missing. The meals may have looked
delicious once, but no one else can cook them again. AI papers without
reproducibility are like such recipes---impressive claims, but
irreproducible outcomes.

\subsubsection{Deep Dive}\label{deep-dive-90}

Reproducibility requires clarity in three areas:

\begin{itemize}
\tightlist
\item
  Code and algorithms: precise implementation details, hyperparameters,
  and random seeds.
\item
  Data and preprocessing: availability of datasets, splits, and cleaning
  procedures.
\item
  Experimental setup: hardware, software libraries, versions, and
  training schedules.
\end{itemize}

Failures of reproducibility have plagued AI. Small variations in
preprocessing can change benchmark rankings. Proprietary datasets make
replication impossible. Differences in GPU types or software libraries
can alter results subtly but significantly.

The reproducibility crisis is not unique to AI---it mirrors issues in
psychology, medicine, and other sciences. But AI faces unique challenges
due to computational scale and reliance on proprietary resources.
Addressing these challenges involves open-source code release, dataset
sharing, standardized evaluation protocols, and stronger incentives for
replication studies.

Comparison Table: Reproducible vs.~Non-Reproducible Research

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1889}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3889}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4222}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Reproducible Research
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Non-Reproducible Research
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Code availability & Public, with instructions & Proprietary, incomplete,
or absent \\
Dataset access & Open, with documented preprocessing & Private,
undocumented, or changing \\
Results & Consistent across labs & Dependent on hidden variables \\
Community impact & Trustworthy, cumulative progress & Fragile, hard to
verify, wasted effort \\
\end{longtable}

Ultimately, reproducibility is not just about science---it is about
ethics. Deployed AI systems that cannot be reproduced cannot be audited
for safety, fairness, or reliability.

\subsubsection{Tiny Code}\label{tiny-code-90}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Ensuring reproducibility with fixed random seeds}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{np.random.seed(}\DecValTok{42}\NormalTok{)}
\NormalTok{data }\OperatorTok{=}\NormalTok{ np.random.rand(}\DecValTok{5}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Deterministic random data:"}\NormalTok{, data)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-90}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Change the random seed---how do results differ?
\item
  Run the same experiment on different hardware---does reproducibility
  hold?
\item
  Reflect: should conferences and journals enforce reproducibility as
  strictly as novelty?
\end{enumerate}

\subsection{92. Versioning of code, data, and
experiments}\label{versioning-of-code-data-and-experiments}

AI research and deployment involve constant iteration.
Versioning---tracking changes to code, data, and experiments---ensures
results can be reproduced, compared, and rolled back when needed.
Without versioning, AI projects devolve into chaos, where no one can
tell which model, dataset, or configuration produced a given result.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-91}

Imagine writing a book without saving drafts. If an editor asks about an
earlier version, you can't reconstruct it. In AI, every experiment is a
draft; versioning is the act of saving each one with context, so future
readers---or your future self---can trace the path.

\subsubsection{Deep Dive}\label{deep-dive-91}

Traditional software engineering relies on version control systems like
Git. In AI, the complexity multiplies:

\begin{itemize}
\tightlist
\item
  Code versioning tracks algorithm changes, hyperparameters, and
  pipelines.
\item
  Data versioning ensures the training and test sets used are
  identifiable and reproducible, even as datasets evolve.
\item
  Experiment versioning records outputs, logs, metrics, and random
  seeds, making it possible to compare experiments meaningfully.
\end{itemize}

Modern tools like DVC (Data Version Control), MLflow, and Weights \&
Biases extend Git-like practices to data and model artifacts. They
enable teams to ask: \emph{Which dataset version trained this model?
Which code commit and parameters led to the reported accuracy?}

Without versioning, reproducibility fails and deployment risk rises.
Bugs reappear, models drift without traceability, and research claims
cannot be verified. With versioning, AI development becomes a
cumulative, auditable process.

Comparison Table: Versioning Needs in AI

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1294}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4118}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4588}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Element
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Why It Matters
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Practice
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Code & Reproduce algorithms and parameters & Git commits, containerized
environments \\
Data & Ensure same inputs across reruns & DVC, dataset hashes, storage
snapshots \\
Experiments & Compare and track progress & MLflow logs, W\&B experiment
tracking \\
\end{longtable}

Versioning also supports collaboration. Teams spread across
organizations can reproduce results without guesswork, enabling science
and engineering to scale.

\subsubsection{Tiny Code}\label{tiny-code-91}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example: simple experiment versioning with hashes}
\ImportTok{import}\NormalTok{ hashlib}
\ImportTok{import}\NormalTok{ json}

\NormalTok{experiment }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"model"}\NormalTok{: }\StringTok{"logistic\_regression"}\NormalTok{,}
    \StringTok{"params"}\NormalTok{: \{}\StringTok{"lr"}\NormalTok{:}\FloatTok{0.01}\NormalTok{, }\StringTok{"epochs"}\NormalTok{:}\DecValTok{100}\NormalTok{\},}
    \StringTok{"data\_version"}\NormalTok{: }\StringTok{"hash1234"}
\NormalTok{\}}

\NormalTok{experiment\_id }\OperatorTok{=}\NormalTok{ hashlib.md5(json.dumps(experiment).encode()).hexdigest()}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Experiment ID:"}\NormalTok{, experiment\_id)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-91}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Change the learning rate---does the experiment ID change?
\item
  Add a new data version---how does it affect reproducibility?
\item
  Reflect: why is versioning essential not only for research
  reproducibility but also for regulatory compliance in deployed AI?
\end{enumerate}

\subsection{93. Tooling: notebooks, frameworks,
pipelines}\label{tooling-notebooks-frameworks-pipelines}

AI development depends heavily on the tools researchers and engineers
use. Notebooks provide interactive experimentation, frameworks offer
reusable building blocks, and pipelines organize workflows into
reproducible stages. Together, they shape how ideas move from concept to
deployment.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-92}

Think of building a house. Sketches on paper resemble notebooks: quick,
flexible, exploratory. Prefabricated materials are like frameworks:
ready-to-use components that save effort. Construction pipelines
coordinate the sequence---laying the foundation, raising walls,
installing wiring---into a complete structure. AI engineering works the
same way.

\subsubsection{Deep Dive}\label{deep-dive-92}

\begin{itemize}
\tightlist
\item
  Notebooks (e.g., Jupyter, Colab) are invaluable for prototyping,
  visualization, and teaching. They allow rapid iteration but can
  encourage messy, non-reproducible practices if not disciplined.
\item
  Frameworks (e.g., PyTorch, TensorFlow, scikit-learn) provide
  abstractions for model design, training loops, and optimization. They
  accelerate development but may introduce lock-in or complexity.
\item
  Pipelines (e.g., Kubeflow, Airflow, Metaflow) formalize data
  preparation, training, evaluation, and deployment into modular steps.
  They make experiments repeatable at scale, enabling collaboration
  across teams.
\end{itemize}

Each tool has strengths and trade-offs. Notebooks excel at exploration
but falter at production. Frameworks lower barriers to sophisticated
models but can obscure inner workings. Pipelines enforce rigor but may
slow early experimentation. The art lies in combining them to fit the
maturity of a project.

Comparison Table: Notebooks, Frameworks, Pipelines

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0781}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2969}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3203}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3047}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Tool Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Weaknesses
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Use Case
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Notebooks & Interactive, visual, fast prototyping & Hard to reproduce,
version control issues & Teaching, exploratory analysis \\
Frameworks & Robust abstractions, community support & Complexity,
potential lock-in & Training deep learning models \\
Pipelines & Scalable, reproducible, collaborative & Setup overhead, less
flexibility & Enterprise ML deployment, model serving \\
\end{longtable}

Modern AI workflows typically blend these: a researcher prototypes in
notebooks, formalizes the model in a framework, and engineers deploy it
via pipelines. Without this chain, insights often die in notebooks or
fail in production.

\subsubsection{Tiny Code}\label{tiny-code-92}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example: simple pipeline step simulation}
\KeywordTok{def}\NormalTok{ load\_data():}
    \ControlFlowTok{return}\NormalTok{ [}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{]}

\KeywordTok{def}\NormalTok{ train\_model(data):}
    \ControlFlowTok{return} \BuiltInTok{sum}\NormalTok{(data) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(data)  }\CommentTok{\# dummy "model"}

\KeywordTok{def}\NormalTok{ evaluate\_model(model):}
    \ControlFlowTok{return} \SpecialStringTok{f"Model value: }\SpecialCharTok{\{}\NormalTok{model}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}

\CommentTok{\# Pipeline}
\NormalTok{data }\OperatorTok{=}\NormalTok{ load\_data()}
\NormalTok{model }\OperatorTok{=}\NormalTok{ train\_model(data)}
\BuiltInTok{print}\NormalTok{(evaluate\_model(model))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-92}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add another pipeline step---like data cleaning---does it make the
  process clearer?
\item
  Replace the dummy model with a scikit-learn classifier---can you track
  inputs/outputs?
\item
  Reflect: why do tools matter as much as algorithms in shaping the
  progress of AI?
\end{enumerate}

\subsection{94. Collaboration, documentation, and
transparency}\label{collaboration-documentation-and-transparency}

AI is rarely built alone. Collaboration enables teams of researchers and
engineers to combine expertise. Documentation ensures that ideas, data,
and methods are clear and reusable. Transparency makes models
understandable to both colleagues and the broader community. Together,
these practices turn isolated experiments into collective progress.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-93}

Imagine a relay race where each runner drops the baton without labeling
it. The team cannot finish the race because no one knows what's been
done. In AI, undocumented or opaque work is like a dropped
baton---progress stalls.

\subsubsection{Deep Dive}\label{deep-dive-93}

Collaboration in AI spans interdisciplinary teams: computer scientists,
domain experts, ethicists, and product managers. Without shared
understanding, efforts fragment. Version control platforms (GitHub,
GitLab) and experiment trackers (MLflow, W\&B) provide the
infrastructure, but human practices matter as much as tools.

Documentation ensures reproducibility and knowledge transfer. It
includes clear READMEs, code comments, data dictionaries, and experiment
logs. Models without documentation risk being ``black boxes'' even to
their creators months later.

Transparency extends documentation to accountability. Open-sourcing code
and data, publishing detailed methodology, and explaining limitations
prevent hype and misuse. Transparency also enables external audits for
fairness and safety.

Comparison Table: Collaboration, Documentation, Transparency

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1327}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4286}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4388}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Practice
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Implementation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Collaboration & Pool expertise, divide tasks & Shared repos, code
reviews, project boards \\
Documentation & Preserve knowledge, ensure reproducibility & README
files, experiment logs, data schemas \\
Transparency & Build trust, enable accountability & Open-source
releases, model cards, audits \\
\end{longtable}

Without these practices, AI progress becomes fragile---dependent on
individuals, lost in silos, and vulnerable to errors. With them,
progress compounds and can be trusted by both peers and the public.

\subsubsection{Tiny Code}\label{tiny-code-93}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example: simple documentation as metadata}
\NormalTok{model\_card }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"name"}\NormalTok{: }\StringTok{"Spam Classifier v1.0"}\NormalTok{,}
    \StringTok{"authors"}\NormalTok{: [}\StringTok{"Team A"}\NormalTok{],}
    \StringTok{"dataset"}\NormalTok{: }\StringTok{"Email dataset v2 (cleaned, deduplicated)"}\NormalTok{,}
    \StringTok{"metrics"}\NormalTok{: \{}\StringTok{"accuracy"}\NormalTok{: }\FloatTok{0.95}\NormalTok{, }\StringTok{"f1"}\NormalTok{: }\FloatTok{0.92}\NormalTok{\},}
    \StringTok{"limitations"}\NormalTok{: }\StringTok{"Fails on short informal messages"}
\NormalTok{\}}

\ControlFlowTok{for}\NormalTok{ k,v }\KeywordTok{in}\NormalTok{ model\_card.items():}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{k}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{v}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-93}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add fairness metrics or energy usage to the model card---how does it
  change transparency?
\item
  Imagine a teammate taking over your project---would your documentation
  be enough?
\item
  Reflect: why does transparency matter not only for science but also
  for public trust in AI?
\end{enumerate}

\subsection{95. Statistical rigor and replication
studies}\label{statistical-rigor-and-replication-studies}

Scientific claims in AI require statistical rigor---careful design of
experiments, proper use of significance tests, and honest reporting of
uncertainty. Replication studies, where independent teams attempt to
reproduce results, provide the ultimate check. Together, they protect
the field from hype and fragile conclusions.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-94}

Think of building a bridge. It's not enough that one engineer's design
holds during their test. Independent inspectors must verify the
calculations and confirm the bridge can withstand real conditions. In
AI, replication serves the same role---ensuring results are not
accidents of chance or selective reporting.

\subsubsection{Deep Dive}\label{deep-dive-94}

Statistical rigor starts with designing fair comparisons: training
models under the same conditions, reporting variance across multiple
runs, and avoiding cherry-picking of best results. It also requires
appropriate statistical tests to judge whether performance differences
are meaningful rather than noise.

Replication studies extend this by testing results independently,
sometimes under new conditions. Successful replication strengthens
trust; failures highlight hidden assumptions or weak methodology.
Unfortunately, replication is undervalued in AI---top venues reward
novelty over verification, leading to a reproducibility gap.

The lack of rigor has consequences: flashy papers that collapse under
scrutiny, wasted effort chasing irreproducible results, and erosion of
public trust. A shift toward valuing replication, preregistration, and
transparent reporting would align AI more closely with scientific norms.

Comparison Table: Statistical Rigor vs.~Replication

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1414}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4343}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4242}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Statistical Rigor
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Replication Studies
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Focus & Correct design and reporting of experiments & Independent
verification of findings \\
Responsibility & Original researchers & External researchers \\
Benefit & Prevents overstated claims & Confirms robustness, builds
trust \\
Challenge & Requires discipline and education & Often unrewarded, costly
in time/resources \\
\end{longtable}

Replication is not merely checking math---it is part of the culture of
accountability. Without it, AI risks becoming an arms race of unverified
claims. With it, the field can build cumulative, durable knowledge.

\subsubsection{Tiny Code}\label{tiny-code-94}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Demonstrating variance across runs}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LogisticRegression}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ accuracy\_score}

\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{],[}\DecValTok{1}\NormalTok{],[}\DecValTok{2}\NormalTok{],[}\DecValTok{3}\NormalTok{],[}\DecValTok{4}\NormalTok{],[}\DecValTok{5}\NormalTok{]])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{])}

\NormalTok{scores }\OperatorTok{=}\NormalTok{ []}
\ControlFlowTok{for}\NormalTok{ seed }\KeywordTok{in}\NormalTok{ [}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{]:}
\NormalTok{    model }\OperatorTok{=}\NormalTok{ LogisticRegression(random\_state}\OperatorTok{=}\NormalTok{seed, max\_iter}\OperatorTok{=}\DecValTok{500}\NormalTok{).fit(X,y)}
\NormalTok{    scores.append(accuracy\_score(y, model.predict(X)))}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Accuracy across runs:"}\NormalTok{, scores)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Mean ± Std:"}\NormalTok{, np.mean(scores), }\StringTok{"±"}\NormalTok{, np.std(scores))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-94}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Increase the dataset noise---does variance between runs grow?
\item
  Try different random seeds---do conclusions still hold?
\item
  Reflect: should AI conferences reward replication studies as highly as
  novel results?
\end{enumerate}

\subsection{96. Open science, preprints, and publishing
norms}\label{open-science-preprints-and-publishing-norms}

AI research moves at a rapid pace, and the way results are shared shapes
the field. Open science emphasizes transparency and accessibility.
Preprints accelerate dissemination outside traditional journals.
Publishing norms guide how credit, peer review, and standards of
evidence are maintained. Together, they determine how knowledge spreads
and how trustworthy it is.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-95}

Imagine a library where only a few people can check out books, and the
rest must wait years. Contrast that with an open archive where anyone
can read the latest manuscripts immediately. The second library looks
like modern AI: preprints on arXiv and open code releases fueling fast
progress.

\subsubsection{Deep Dive}\label{deep-dive-95}

Open science in AI includes open datasets, open-source software, and
public sharing of results. This democratizes access, enabling small labs
and independent researchers to contribute alongside large institutions.
Preprints, typically on platforms like arXiv, bypass slow journal cycles
and allow rapid community feedback.

However, preprints also challenge traditional norms: they lack formal
peer review, raising concerns about reliability and hype. Publishing
norms attempt to balance speed with rigor. Conferences and journals
increasingly require code and data release, reproducibility checklists,
and clearer reporting standards.

The culture of AI publishing is shifting: from closed corporate secrecy
to open competitions; from novelty-only acceptance criteria to valuing
robustness and ethics; from slow cycles to real-time global
collaboration. But tensions remain between openness and
commercialization, between rapid sharing and careful vetting.

Comparison Table: Traditional vs.~Open Publishing

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1519}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3797}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4684}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Traditional Publishing
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Open Science \& Preprints
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Access & Paywalled journals & Free, open archives and datasets \\
Speed & Slow peer review cycle & Immediate dissemination via
preprints \\
Verification & Peer review before publication & Community feedback,
post-publication \\
Risks & Limited reach, exclusivity & Hype, lack of quality control \\
\end{longtable}

Ultimately, publishing norms reflect values. Do we value rapid
innovation, broad access, and transparency? Or do we prioritize rigorous
filtering, stability, and prestige? The healthiest ecosystem blends
both, creating space for speed without abandoning trust.

\subsubsection{Tiny Code}\label{tiny-code-95}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example: metadata for an "open science" AI paper}
\NormalTok{paper }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"title"}\NormalTok{: }\StringTok{"Efficient Transformers with Sparse Attention"}\NormalTok{,}
    \StringTok{"authors"}\NormalTok{: [}\StringTok{"A. Researcher"}\NormalTok{, }\StringTok{"B. Scientist"}\NormalTok{],}
    \StringTok{"venue"}\NormalTok{: }\StringTok{"arXiv preprint 2509.12345"}\NormalTok{,}
    \StringTok{"code"}\NormalTok{: }\StringTok{"https://github.com/example/sparse{-}transformers"}\NormalTok{,}
    \StringTok{"data"}\NormalTok{: }\StringTok{"Open dataset: WikiText{-}103"}\NormalTok{,}
    \StringTok{"license"}\NormalTok{: }\StringTok{"CC{-}BY 4.0"}
\NormalTok{\}}

\ControlFlowTok{for}\NormalTok{ k,v }\KeywordTok{in}\NormalTok{ paper.items():}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{k}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{v}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-95}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add peer review metadata (accepted at NeurIPS, ICML)---how does
  credibility change?
\item
  Imagine this paper was closed-source---what opportunities would be
  lost?
\item
  Reflect: should open science be mandatory for publicly funded AI
  research?
\end{enumerate}

\subsection{97. Negative results and failure
reporting}\label{negative-results-and-failure-reporting}

Science advances not only through successes but also through
understanding failures. In AI, negative results---experiments that do
not confirm hypotheses or fail to improve performance---are rarely
reported. Yet documenting them prevents wasted effort, reveals hidden
challenges, and strengthens the scientific method.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-96}

Imagine a map where only successful paths are drawn. Explorers who
follow it may walk into dead ends again and again. A more useful map
includes both the routes that lead to treasure and those that led
nowhere. AI research needs such maps.

\subsubsection{Deep Dive}\label{deep-dive-96}

Negative results in AI often remain hidden in lab notebooks or private
repositories. Reasons include publication bias toward positive outcomes,
competitive pressure, and the cultural view that failure signals
weakness. This creates a distorted picture of progress, where flashy
results dominate while important lessons from failures are lost.

Examples of valuable negative results include:

\begin{itemize}
\tightlist
\item
  Novel architectures that fail to outperform baselines.
\item
  Promising ideas that do not scale or generalize.
\item
  Benchmark shortcuts that looked strong but collapsed under adversarial
  testing.
\end{itemize}

Reporting such outcomes saves others from repeating mistakes, highlights
boundary conditions, and encourages more realistic expectations.
Journals and conferences have begun to acknowledge this, with workshops
on reproducibility and negative results.

Comparison Table: Positive vs.~Negative Results in AI

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1648}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3846}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4505}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Positive Results
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Negative Results
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Visibility & Widely published, cited & Rarely published, often hidden \\
Contribution & Shows what works & Shows what does not work and why \\
Risk if missing & Field advances quickly but narrowly & Field repeats
mistakes, distorts progress \\
Example & New model beats SOTA on ImageNet & Variant fails despite
theoretical promise \\
\end{longtable}

By embracing negative results, AI can mature as a science. Failures
highlight assumptions, expose limits of generalization, and set
realistic baselines. Normalizing failure reporting reduces hype cycles
and fosters collective learning.

\subsubsection{Tiny Code}\label{tiny-code-96}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simulating a "negative result"}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LogisticRegression}
\ImportTok{from}\NormalTok{ sklearn.svm }\ImportTok{import}\NormalTok{ SVC}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ accuracy\_score}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Tiny dataset}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{],[}\DecValTok{1}\NormalTok{],[}\DecValTok{2}\NormalTok{],[}\DecValTok{3}\NormalTok{]])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{])}

\NormalTok{log\_reg }\OperatorTok{=}\NormalTok{ LogisticRegression().fit(X,y)}
\NormalTok{svm }\OperatorTok{=}\NormalTok{ SVC(kernel}\OperatorTok{=}\StringTok{"poly"}\NormalTok{, degree}\OperatorTok{=}\DecValTok{5}\NormalTok{).fit(X,y)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"LogReg accuracy:"}\NormalTok{, accuracy\_score(y, log\_reg.predict(X)))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"SVM (degree 5) accuracy:"}\NormalTok{, accuracy\_score(y, svm.predict(X)))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-96}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Increase dataset size---does the ``negative'' SVM result persist?
\item
  Document why the complex model failed compared to the simple baseline.
\item
  Reflect: how would AI research change if publishing failures were as
  valued as publishing successes?
\end{enumerate}

\subsection{98. Benchmark reproducibility crises in
AI}\label{benchmark-reproducibility-crises-in-ai}

Many AI breakthroughs are judged by performance on benchmarks. But if
those results cannot be reliably reproduced, the benchmark itself
becomes unstable. The benchmark reproducibility crisis occurs when
published results are hard---or impossible---to replicate due to hidden
randomness, undocumented preprocessing, or unreleased data.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-97}

Think of a scoreboard where athletes' times are recorded, but no one
knows the track length, timing method, or even if the stopwatch worked.
The scores look impressive but cannot be trusted. Benchmarks in AI face
the same problem when reproducibility is weak.

\subsubsection{Deep Dive}\label{deep-dive-97}

Benchmark reproducibility failures arise from multiple factors:

\begin{itemize}
\tightlist
\item
  Data leakage: overlaps between training and test sets inflate results.
\item
  Unreleased datasets: claims cannot be independently verified.
\item
  Opaque preprocessing: small changes in tokenization, normalization, or
  image resizing alter scores.
\item
  Non-deterministic training: results vary across runs but only the best
  is reported.
\item
  Hardware/software drift: different GPUs, libraries, or seeds produce
  inconsistent outcomes.
\end{itemize}

The crisis undermines both research credibility and industrial
deployment. A model that beats ImageNet by 1\% but cannot be reproduced
is scientifically meaningless. Worse, models trained with leaky or
biased benchmarks may propagate errors into downstream applications.

Efforts to address this include reproducibility checklists at
conferences (NeurIPS, ICML), model cards and data sheets, open-source
implementations, and rigorous cross-lab verification. Dynamic benchmarks
that refresh test sets (e.g., Dynabench) also help prevent overfitting
and silent leakage.

Comparison Table: Stable vs.~Fragile Benchmarks

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2048}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4096}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3855}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Stable Benchmark
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Fragile Benchmark
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Data availability & Public, with documented splits & Private or
inconsistently shared \\
Evaluation & Deterministic, standardized code & Ad hoc, variable
implementations \\
Reporting & Averages, with variance reported & Single best run
highlighted \\
Trust level & High, supports cumulative progress & Low, progress is
illusory \\
\end{longtable}

Benchmark reproducibility is not a technical nuisance---it is central to
AI as a science. Without stable, transparent benchmarks, leaderboards
risk becoming marketing tools rather than genuine measures of
advancement.

\subsubsection{Tiny Code}\label{tiny-code-97}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Demonstrating non{-}determinism}
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}

\NormalTok{torch.manual\_seed(}\DecValTok{0}\NormalTok{)   }\CommentTok{\# fix seed for reproducibility}

\CommentTok{\# Simple model}
\NormalTok{model }\OperatorTok{=}\NormalTok{ nn.Linear(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{x }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Output with fixed seed:"}\NormalTok{, model(x))}

\CommentTok{\# Remove the fixed seed and rerun to see variability}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-97}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train the same model twice without fixing the seed---do results
  differ?
\item
  Change preprocessing slightly (e.g., normalize inputs
  differently)---does accuracy shift?
\item
  Reflect: why does benchmark reproducibility matter more as AI models
  scale to billions of parameters?
\end{enumerate}

\subsection{99. Community practices for
reliability}\label{community-practices-for-reliability}

AI is not only shaped by algorithms and datasets but also by the
community practices that govern how research is conducted and shared.
Reliability emerges when researchers adopt shared norms: transparent
reporting, open resources, peer verification, and responsible
competition. Without these practices, progress risks being fragmented,
fragile, and untrustworthy.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-98}

Imagine a neighborhood where everyone builds their own houses without
common codes---some collapse, others block sunlight, and many hide
dangerous flaws. Now imagine the same neighborhood with shared building
standards, inspections, and cooperation. AI research benefits from
similar community standards to ensure safety and reliability.

\subsubsection{Deep Dive}\label{deep-dive-98}

Community practices for reliability include:

\begin{itemize}
\tightlist
\item
  Reproducibility checklists: conferences like NeurIPS now require
  authors to document datasets, hyperparameters, and code.
\item
  Open-source culture: sharing code, pretrained models, and datasets
  allows peers to verify claims.
\item
  Independent replication: labs repeating and auditing results before
  deployment.
\item
  Responsible benchmarking: resisting leaderboard obsession, reporting
  multiple dimensions (robustness, fairness, energy use).
\item
  Collaborative governance: initiatives like MLCommons or Hugging Face
  Datasets maintain shared standards and evaluation tools.
\end{itemize}

These practices counterbalance pressures for speed and novelty. They
help transform AI into a cumulative science, where progress builds on a
solid base rather than hype cycles.

Comparison Table: Weak vs.~Strong Community Practices

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1979}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3854}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4167}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Dimension
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Weak Practice
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strong Practice
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Code/Data Sharing & Closed, proprietary & Open repositories with
documentation \\
Reporting Standards & Selective metrics, cherry-picked runs & Full
transparency, including variance \\
Benchmarking & Single leaderboard focus & Multi-metric, multi-benchmark
evaluation \\
Replication Culture & Rare, undervalued & Incentivized, publicly
recognized \\
\end{longtable}

Community norms are cultural infrastructure. Just as the internet grew
by adopting protocols and standards, AI can achieve reliability by
aligning on transparent and responsible practices.

\subsubsection{Tiny Code}\label{tiny-code-98}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example: adding reproducibility info to experiment logs}
\NormalTok{experiment\_log }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"model"}\NormalTok{: }\StringTok{"Transformer{-}small"}\NormalTok{,}
    \StringTok{"dataset"}\NormalTok{: }\StringTok{"WikiText{-}103 (v2.1)"}\NormalTok{,}
    \StringTok{"accuracy"}\NormalTok{: }\FloatTok{0.87}\NormalTok{,}
    \StringTok{"std\_dev"}\NormalTok{: }\FloatTok{0.01}\NormalTok{,}
    \StringTok{"seed"}\NormalTok{: }\DecValTok{42}\NormalTok{,}
    \StringTok{"code\_repo"}\NormalTok{: }\StringTok{"https://github.com/example/research{-}code"}
\NormalTok{\}}

\ControlFlowTok{for}\NormalTok{ k,v }\KeywordTok{in}\NormalTok{ experiment\_log.items():}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{k}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{v}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-98}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add fairness or energy-use metrics to the log---does it give a fuller
  picture?
\item
  Imagine a peer trying to replicate your result---what extra details
  would they need?
\item
  Reflect: why do cultural norms matter as much as technical advances in
  building reliable AI?
\end{enumerate}

\subsection{100. Towards a mature scientific culture in
AI}\label{towards-a-mature-scientific-culture-in-ai}

AI is transitioning from a frontier discipline to a mature science. This
shift requires not only technical breakthroughs but also a scientific
culture rooted in rigor, openness, and accountability. A mature culture
balances innovation with verification, excitement with caution, and
competition with collaboration.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-99}

Think of medicine centuries ago: discoveries were dramatic but often
anecdotal, inconsistent, and dangerous. Over time, medicine built
standardized trials, ethical review boards, and professional norms. AI
is undergoing a similar journey---moving from dazzling demonstrations to
systematic, reliable science.

\subsubsection{Deep Dive}\label{deep-dive-99}

A mature scientific culture in AI demands several elements:

\begin{itemize}
\tightlist
\item
  Rigor: experiments designed with controls, baselines, and statistical
  validity.
\item
  Openness: datasets, code, and results shared for verification.
\item
  Ethics: systems evaluated not only for performance but also for
  fairness, safety, and societal impact.
\item
  Long-term perspective: research valued for durability, not just
  leaderboard scores.
\item
  Community institutions: conferences, journals, and collaborations that
  enforce standards and support replication.
\end{itemize}

The challenge is cultural. Incentives in academia and industry still
reward novelty and speed over reliability. Shifting this balance means
rethinking publication criteria, funding priorities, and corporate
secrecy. It also requires education: training new researchers to see
reproducibility and transparency as virtues, not burdens.

Comparison Table: Frontier vs.~Mature Scientific Culture

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2024}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3690}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4286}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Frontier AI Culture
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mature AI Culture
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Research Goals & Novelty, demos, rapid iteration & Robustness,
cumulative knowledge \\
Publication Norms & Leaderboards, flashy results & Replication,
long-term benchmarks \\
Collaboration & Competitive secrecy & Shared standards, open
collaboration \\
Ethical Lens & Secondary, reactive & Central, proactive \\
\end{longtable}

This cultural transformation will not be instant. But just as physics or
biology matured through shared norms, AI too can evolve into a
discipline where progress is durable, reproducible, and aligned with
human values.

\subsubsection{Tiny Code}\label{tiny-code-99}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example: logging scientific culture dimensions for a project}
\NormalTok{project\_culture }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"rigor"}\NormalTok{: }\StringTok{"Statistical tests + multiple baselines"}\NormalTok{,}
    \StringTok{"openness"}\NormalTok{: }\StringTok{"Code + dataset released"}\NormalTok{,}
    \StringTok{"ethics"}\NormalTok{: }\StringTok{"Bias audit + safety review"}\NormalTok{,}
    \StringTok{"long\_term"}\NormalTok{: }\StringTok{"Evaluation across 3 benchmarks"}\NormalTok{,}
    \StringTok{"community"}\NormalTok{: }\StringTok{"Replication study submitted"}
\NormalTok{\}}

\ControlFlowTok{for}\NormalTok{ k,v }\KeywordTok{in}\NormalTok{ project\_culture.items():}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{k}\SpecialCharTok{.}\NormalTok{capitalize()}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{v}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-99}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add missing cultural elements---what would strengthen the project's
  reliability?
\item
  Imagine incentives flipped: replication papers get more citations than
  novelty---how would AI research change?
\item
  Reflect: what does it take for AI to be remembered not just for its
  breakthroughs, but for its scientific discipline?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Volume 2. Mathematicial
Foundations}\label{volume-2.-mathematicial-foundations}

\section{Chapter 11. Linear Algebra for
Representations}\label{chapter-11.-linear-algebra-for-representations}

\subsection{101. Scalars, Vectors, and
Matrices}\label{scalars-vectors-and-matrices}

At the foundation of AI mathematics are three objects: scalars, vectors,
and matrices. A scalar is a single number. A vector is an ordered list
of numbers, representing direction and magnitude in space. A matrix is a
rectangular grid of numbers, capable of transforming vectors and
encoding relationships. These are the raw building blocks for almost
every algorithm in AI, from linear regression to deep neural networks.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-100}

Imagine scalars as simple dots on a number line. A vector is like an
arrow pointing from the origin in a plane or space, with both length and
direction. A matrix is a whole system of arrows: a transformation
machine that can rotate, stretch, or compress the space around it. In
AI, data points are vectors, and learning often comes down to finding
the right matrices to transform them.

\subsubsection{Deep Dive}\label{deep-dive-100}

Scalars are elements of the real (ℝ) or complex (ℂ) number systems. They
describe quantities such as weights, probabilities, or losses. Vectors
extend this by grouping scalars into n-dimensional objects. A vector x ∈
ℝⁿ can encode features of a data sample (age, height, income).
Operations like dot products measure similarity, and norms measure
magnitude. Matrices generalize further: an m×n matrix holds m rows and n
columns. Multiplying a vector by a matrix performs a linear
transformation. In AI, these transformations express learned
parameters---weights in neural networks, transition probabilities in
Markov models, or coefficients in regression.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0984}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0984}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1475}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.6557}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Object
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Symbol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Dimension
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Scalar & \emph{a} & 1×1 & Learning rate, single probability \\
Vector & x & n×1 & Feature vector (e.g., pixel intensities) \\
Matrix & W & m×n & Neural network weights, adjacency matrix \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-100}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Scalar}
\NormalTok{a }\OperatorTok{=} \FloatTok{3.14}

\CommentTok{\# Vector}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{])}

\CommentTok{\# Matrix}
\NormalTok{W }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{],}
\NormalTok{              [}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{,  }\DecValTok{4}\NormalTok{]])}

\CommentTok{\# Operations}
\NormalTok{dot\_product }\OperatorTok{=}\NormalTok{ np.dot(x, x)         }\CommentTok{\# 1*1 + 2*2 + 3*3 = 14}
\NormalTok{transformed }\OperatorTok{=}\NormalTok{ np.dot(W, x)         }\CommentTok{\# matrix{-}vector multiplication}
\NormalTok{norm }\OperatorTok{=}\NormalTok{ np.linalg.norm(x)           }\CommentTok{\# vector magnitude}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Scalar:"}\NormalTok{, a)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Vector:"}\NormalTok{, x)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Matrix:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, W)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Dot product:"}\NormalTok{, dot\_product)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Transformed:"}\NormalTok{, transformed)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Norm:"}\NormalTok{, norm)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-100}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Take the vector x = {[}4, 3{]}. What is its norm? (Hint: √(4²+3²))
\item
  Multiply the matrix

  \[
  A = \begin{bmatrix}2 & 0 \\ 0 & 2\end{bmatrix}
  \]

  by x = {[}1, 1{]}. What does the result look like?
\end{enumerate}

\subsection{102. Vector Operations and
Norms}\label{vector-operations-and-norms}

Vectors are not just lists of numbers; they are objects on which we
define operations. Adding and scaling vectors lets us move and stretch
directions in space. Dot products measure similarity, while norms
measure size. These operations form the foundation of geometry and
distance in machine learning.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-101}

Picture two arrows drawn from the origin. Adding them means placing one
arrow's tail at the other's head, forming a diagonal. Scaling a vector
stretches or shrinks its arrow. The dot product measures how aligned two
arrows are: large if they point in the same direction, zero if they're
perpendicular, negative if they point opposite. A norm is simply the
length of the arrow.

\subsubsection{Deep Dive}\label{deep-dive-101}

Vector addition: x + y = {[}x₁ + y₁, \ldots, xₙ + yₙ{]}. Scalar
multiplication: a·x = {[}a·x₁, \ldots, a·xₙ{]}. Dot product: x·y = Σ
xᵢyᵢ, capturing both length and alignment. Norms:

\begin{itemize}
\tightlist
\item
  L2 norm: ‖x‖₂ = √(Σ xᵢ²), the Euclidean length.
\item
  L1 norm: ‖x‖₁ = Σ \textbar xᵢ\textbar, often used for sparsity.
\item
  L∞ norm: max \textbar xᵢ\textbar, measuring the largest component.
\end{itemize}

In AI, norms define distances for clustering, regularization penalties,
and robustness to perturbations.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1694}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1290}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.3387}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0081}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.3548}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Operation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Interpretation in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Addition & x + y & Combining features & & \\
Scalar multiplication & a·x & Scaling magnitude & & \\
Dot product & x·y = ‖x‖‖y‖cosθ & Similarity / projection & & \\
L2 norm & √(Σ xᵢ²) & Standard distance, used in Euclidean space & & \\
L1 norm & Σ & xᵢ & & Promotes sparsity, robust to outliers \\
L∞ norm & max & xᵢ & & Worst-case deviation, adversarial robustness \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-101}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{x }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{])}

\CommentTok{\# Vector addition and scaling}
\NormalTok{sum\_xy }\OperatorTok{=}\NormalTok{ x }\OperatorTok{+}\NormalTok{ y}
\NormalTok{scaled\_x }\OperatorTok{=} \DecValTok{2} \OperatorTok{*}\NormalTok{ x}

\CommentTok{\# Dot product and norms}
\NormalTok{dot }\OperatorTok{=}\NormalTok{ np.dot(x, y)}
\NormalTok{l2 }\OperatorTok{=}\NormalTok{ np.linalg.norm(x, }\DecValTok{2}\NormalTok{)}
\NormalTok{l1 }\OperatorTok{=}\NormalTok{ np.linalg.norm(x, }\DecValTok{1}\NormalTok{)}
\NormalTok{linf }\OperatorTok{=}\NormalTok{ np.linalg.norm(x, np.inf)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"x + y:"}\NormalTok{, sum\_xy)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"2 * x:"}\NormalTok{, scaled\_x)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Dot product:"}\NormalTok{, dot)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"L2 norm:"}\NormalTok{, l2)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"L1 norm:"}\NormalTok{, l1)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"L∞ norm:"}\NormalTok{, linf)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Try It Yourself}\label{try-it-yourself-101}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the dot product of x = {[}1, 0{]} and y = {[}0, 1{]}. What
  does the result tell you?
\item
  Find the L2 norm of x = {[}5, 12{]}.
\item
  Compare the L1 and L2 norms for x = {[}1, -1, 1, -1{]}. Which is
  larger, and why?
\end{enumerate}

\subsection{103. Matrix Multiplication and
Properties}\label{matrix-multiplication-and-properties}

Matrix multiplication is the central operation that ties linear algebra
to AI. Multiplying a matrix by a vector applies a linear transformation:
rotation, scaling, or projection. Multiplying two matrices composes
transformations. Understanding how this works and what properties it
preserves is essential for reasoning about model weights, layers, and
data transformations.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-102}

Think of a matrix as a machine that takes an input arrow (vector) and
outputs a new arrow. Applying one machine after another corresponds to
multiplying matrices. If you rotate by 90° and then scale by 2, the
combined effect is another matrix. The rows of the matrix act like
filters, each producing a weighted combination of the input vector's
components.

\subsubsection{Deep Dive}\label{deep-dive-102}

Given an m×n matrix A and an n×p matrix B, the product C = AB is an m×p
matrix. Each entry is

\[
c_{ij} = \sum_{k=1}^{n} a_{ik} b_{kj}.
\]

Key properties:

\begin{itemize}
\tightlist
\item
  Associativity: (AB)C = A(BC)
\item
  Distributivity: A(B + C) = AB + AC
\item
  Non-commutativity: AB ≠ BA in general
\item
  Identity: AI = IA = A
\item
  Transpose rules: (AB)ᵀ = BᵀAᵀ
\end{itemize}

In AI, matrix multiplication encodes layer operations: inputs × weights
= activations. Batch processing is also matrix multiplication, where
many vectors are transformed at once.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2083}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2222}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5694}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Property
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning in AI
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Associativity & (AB)C = A(BC) & Order of chaining layers doesn't
matter \\
Distributivity & A(B+C) = AB + AC & Parallel transformations combine
linearly \\
Non-commutative & AB ≠ BA & Order of layers matters \\
Identity & AI = IA = A & No transformation applied \\
Transpose rule & (AB)ᵀ = BᵀAᵀ & Useful for gradients/backprop \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-102}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Define matrices}
\NormalTok{A }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{],}
\NormalTok{              [}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{]])}
\NormalTok{B }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{],}
\NormalTok{              [}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{]])}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{])}

\CommentTok{\# Matrix{-}vector multiplication}
\NormalTok{Ax }\OperatorTok{=}\NormalTok{ np.dot(A, x)}

\CommentTok{\# Matrix{-}matrix multiplication}
\NormalTok{AB }\OperatorTok{=}\NormalTok{ np.dot(A, B)}

\CommentTok{\# Properties}
\NormalTok{assoc }\OperatorTok{=}\NormalTok{ np.allclose(np.dot(np.dot(A, B), A), np.dot(A, np.dot(B, A)))}

\BuiltInTok{print}\NormalTok{(}\StringTok{"A @ x ="}\NormalTok{, Ax)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"A @ B =}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, AB)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Associativity holds?"}\NormalTok{, assoc)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters}

Matrix multiplication is the language of neural networks. Each layer's
parameters form a matrix that transforms input vectors into hidden
representations. The non-commutativity explains why order of layers
changes outcomes. Properties like associativity enable efficient
computation, and transpose rules are the backbone of backpropagation.
Without mastering matrix multiplication, it is impossible to understand
how AI models propagate signals and gradients.

\subsubsection{Try It Yourself}\label{try-it-yourself-102}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Multiply A = {[}{[}2, 0{]}, {[}0, 2{]}{]} by x = {[}3, 4{]}. What
  happens to the vector?
\item
  Show that AB ≠ BA using A = {[}{[}1, 2{]}, {[}0, 1{]}{]}, B = {[}{[}0,
  1{]}, {[}1, 0{]}{]}.
\item
  Verify that (AB)ᵀ = BᵀAᵀ with small 2×2 matrices.
\end{enumerate}

\subsection{104. Linear Independence and
Span}\label{linear-independence-and-span}

Linear independence is about whether vectors bring new information. If
one vector can be written as a combination of others, it adds nothing
new. The span of a set of vectors is all possible linear combinations of
them---essentially the space they generate. Together, independence and
span tell us how many unique directions we have and how big a space they
cover.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-103}

Imagine two arrows in the plane. If both point in different directions,
they can combine to reach any point in 2D space---the whole plane. If
they both lie on the same line, one is redundant, and you can't reach
the full plane. In higher dimensions, independence tells you whether
your set of vectors truly spans the whole space or just a smaller
subspace.

\subsubsection{Deep Dive}\label{deep-dive-103}

\begin{itemize}
\tightlist
\item
  Linear Combination: a₁v₁ + a₂v₂ + \ldots{} + aₖvₖ.
\item
  Span: The set of all linear combinations of \{v₁, \ldots, vₖ\}.
\item
  Linear Dependence: If there exist coefficients, not all zero, such
  that a₁v₁ + \ldots{} + aₖvₖ = 0, then the vectors are dependent.
\item
  Linear Independence: No such nontrivial combination exists.
\end{itemize}

Dimension of a span = number of independent vectors. In AI, feature
spaces often have redundant dimensions; PCA and other dimensionality
reduction methods identify smaller independent sets.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2209}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4651}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3140}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formal Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Span & All linear combinations of given vectors & Feature space
coverage \\
Linear dependence & Some vector is a combination of others & Redundant
features \\
Linear independence & No redundancy; minimal unique directions & Basis
vectors in embeddings \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-103}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Define vectors}
\NormalTok{v1 }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{])}
\NormalTok{v2 }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{])}
\NormalTok{v3 }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{])  }\CommentTok{\# dependent on v1}

\CommentTok{\# Stack into matrix}
\NormalTok{M }\OperatorTok{=}\NormalTok{ np.column\_stack([v1, v2, v3])}

\CommentTok{\# Rank gives dimension of span}
\NormalTok{rank }\OperatorTok{=}\NormalTok{ np.linalg.matrix\_rank(M)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Matrix:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, M)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Rank (dimension of span):"}\NormalTok{, rank)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-1}

Redundant features inflate dimensionality without adding new
information. Independent features, by contrast, capture the true
structure of data. Recognizing independence helps in feature selection,
dimensionality reduction, and efficient representation learning. In
neural networks, basis-like transformations underpin embeddings and
compressed representations.

\subsubsection{Try It Yourself}\label{try-it-yourself-103}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Are v₁ = {[}1, 2{]}, v₂ = {[}2, 4{]} independent or dependent?
\item
  What is the span of v₁ = {[}1, 0{]}, v₂ = {[}0, 1{]} in 2D space?
\item
  For vectors v₁ = {[}1, 0, 0{]}, v₂ = {[}0, 1, 0{]}, v₃ = {[}1, 1,
  0{]}, what is the dimension of their span?
\end{enumerate}

\subsection{105. Rank, Null Space, and Solutions of Ax =
b}\label{rank-null-space-and-solutions-of-ax-b}

The rank of a matrix measures how much independent information it
contains. The null space consists of all vectors that the matrix sends
to zero. Together, rank and null space determine whether a system of
linear equations Ax = b has solutions, and if so, whether they are
unique or infinite.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-104}

Think of a matrix as a machine that transforms space. If its rank is
full, the machine covers the entire output space---every target vector b
is reachable. If its rank is deficient, the machine squashes some
dimensions, leaving gaps. The null space represents the hidden tunnel:
vectors that go in but vanish to zero at the output.

\subsubsection{Deep Dive}\label{deep-dive-104}

\begin{itemize}
\item
  Rank(A): number of independent rows/columns of A.
\item
  Null Space: \{x ∈ ℝⁿ \textbar{} Ax = 0\}.
\item
  Rank-Nullity Theorem: For A (m×n), rank(A) + nullity(A) = n.
\item
  Solutions to Ax = b:

  \begin{itemize}
  \tightlist
  \item
    If rank(A) = rank({[}A\textbar b{]}) = n → unique solution.
  \item
    If rank(A) = rank({[}A\textbar b{]}) \textless{} n → infinite
    solutions.
  \item
    If rank(A) \textless{} rank({[}A\textbar b{]}) → no solution.
  \end{itemize}
\end{itemize}

In AI, rank relates to model capacity: a low-rank weight matrix cannot
represent all possible mappings, while null space directions correspond
to variations in input that a model ignores.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1304}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3913}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4783}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Connection
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Rank & Independent directions preserved & Expressive power of layers \\
Null space & Inputs mapped to zero & Features discarded by model \\
Rank-nullity & Rank + nullity = number of variables & Trade-off between
information and redundancy \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-104}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{A }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{],}
\NormalTok{              [}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{],}
\NormalTok{              [}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]])}
\NormalTok{b }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{6}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{4}\NormalTok{])}

\CommentTok{\# Rank of A}
\NormalTok{rank\_A }\OperatorTok{=}\NormalTok{ np.linalg.matrix\_rank(A)}

\CommentTok{\# Augmented matrix [A|b]}
\NormalTok{Ab }\OperatorTok{=}\NormalTok{ np.column\_stack([A, b])}
\NormalTok{rank\_Ab }\OperatorTok{=}\NormalTok{ np.linalg.matrix\_rank(Ab)}

\CommentTok{\# Solve if consistent}
\NormalTok{solution }\OperatorTok{=} \VariableTok{None}
\ControlFlowTok{if}\NormalTok{ rank\_A }\OperatorTok{==}\NormalTok{ rank\_Ab:}
\NormalTok{    solution }\OperatorTok{=}\NormalTok{ np.linalg.lstsq(A, b, rcond}\OperatorTok{=}\VariableTok{None}\NormalTok{)[}\DecValTok{0}\NormalTok{]}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Rank(A):"}\NormalTok{, rank\_A)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Rank([A|b]):"}\NormalTok{, rank\_Ab)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Solution:"}\NormalTok{, solution)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-2}

In machine learning, rank restrictions show up in low-rank
approximations for compression, in covariance matrices that reveal
correlations, and in singular value decomposition used for embeddings.
Null spaces matter because they identify directions in the data that
models cannot see---critical for robustness and feature engineering.

\subsubsection{Try It Yourself}\label{try-it-yourself-104}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For A = {[}{[}1, 0{]}, {[}0, 1{]}{]}, what is rank(A) and null space?
\item
  Solve Ax = b for A = {[}{[}1, 2{]}, {[}2, 4{]}{]}, b = {[}3, 6{]}. How
  many solutions exist?
\item
  Consider A = {[}{[}1, 1{]}, {[}1, 1{]}{]}, b = {[}1, 0{]}. Does a
  solution exist? Why or why not?
\end{enumerate}

\subsection{106. Orthogonality and
Projections}\label{orthogonality-and-projections}

Orthogonality describes vectors that are perpendicular---sharing no
overlap in direction. Projection is the operation of expressing one
vector in terms of another, by dropping a shadow onto it. Orthogonality
and projections are the basis of decomposing data into independent
components, simplifying geometry, and designing efficient algorithms.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-105}

Imagine standing in the sun: your shadow on the ground is the projection
of you onto the plane. If the ground is at a right angle to your height,
the shadow contains only the part of you aligned with that surface. Two
orthogonal arrows, like the x- and y-axis, stand perfectly independent;
projecting onto one ignores the other completely.

\subsubsection{Deep Dive}\label{deep-dive-105}

\begin{itemize}
\tightlist
\item
  Orthogonality: Vectors x and y are orthogonal if x·y = 0.
\item
  Projection of y onto x:
\end{itemize}

\[
\text{proj}_x(y) = \frac{x \cdot y}{x \cdot x} x
\]

\begin{itemize}
\tightlist
\item
  Orthogonal Basis: A set of mutually perpendicular vectors; simplifies
  calculations because coordinates don't interfere.
\item
  Orthogonal Matrices: Matrices whose columns form an orthonormal set;
  preserve lengths and angles.
\end{itemize}

Applications:

\begin{itemize}
\tightlist
\item
  PCA: data projected onto principal components.
\item
  Least squares: projecting data onto subspaces spanned by features.
\item
  Orthogonal transforms (e.g., Fourier, wavelets) simplify computation.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2048}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3373}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4578}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula / Rule
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Application
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Orthogonality & x·y = 0 & Independence of features or embeddings \\
Projection & projₓ(y) = (x·y / x·x) x & Dimensionality reduction,
regression \\
Orthogonal basis & Set of perpendicular vectors & PCA, spectral
decomposition \\
Orthogonal matrix & QᵀQ = I & Stable rotations in optimization \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-105}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{x }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{])}

\CommentTok{\# Check orthogonality}
\NormalTok{dot }\OperatorTok{=}\NormalTok{ np.dot(x, y)}

\CommentTok{\# Projection of y onto x}
\NormalTok{proj }\OperatorTok{=}\NormalTok{ (np.dot(x, y) }\OperatorTok{/}\NormalTok{ np.dot(x, x)) }\OperatorTok{*}\NormalTok{ x}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Dot product (x·y):"}\NormalTok{, dot)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Projection of y onto x:"}\NormalTok{, proj)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-3}

Orthogonality underlies the idea of uncorrelated features: one doesn't
explain the other. Projections explain regression, dimensionality
reduction, and embedding models. When models work with orthogonal
directions, learning is efficient and stable. When features are not
orthogonal, redundancy and collinearity can cause instability in
optimization.

\subsubsection{Try It Yourself}\label{try-it-yourself-105}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the projection of y = {[}2, 3{]} onto x = {[}1, 1{]}.
\item
  Are {[}1, 2{]} and {[}2, -1{]} orthogonal? Check using the dot
  product.
\item
  Show that multiplying a vector by an orthogonal matrix preserves its
  length.
\end{enumerate}

\subsection{107. Eigenvalues and
Eigenvectors}\label{eigenvalues-and-eigenvectors}

Eigenvalues and eigenvectors reveal the ``natural modes'' of a
transformation. An eigenvector is a special direction that does not
change orientation when a matrix acts on it, only its length is scaled.
The scaling factor is the eigenvalue. They expose the geometry hidden
inside matrices and are key to understanding stability, dimensionality
reduction, and spectral methods.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-106}

Imagine stretching a rubber sheet with arrows drawn on it. Most arrows
bend and twist, but some special arrows only get longer or shorter,
never changing their direction. These are eigenvectors, and the stretch
factor is the eigenvalue. They describe the fundamental axes along which
transformations act most cleanly.

\subsubsection{Deep Dive}\label{deep-dive-106}

\begin{itemize}
\item
  Definition: For matrix A, if

  \[
  A v = \lambda v
  \]

  then v is an eigenvector and λ is the corresponding eigenvalue.
\item
  Not all matrices have real eigenvalues, but symmetric matrices always
  do, with orthogonal eigenvectors.
\item
  Diagonalization: A = PDP⁻¹, where D is diagonal with eigenvalues, P
  contains eigenvectors.
\item
  Spectral theorem: Symmetric A = QΛQᵀ.
\item
  Applications:

  \begin{itemize}
  \tightlist
  \item
    PCA: eigenvectors of covariance matrix = principal components.
  \item
    PageRank: dominant eigenvector of web graph transition matrix.
  \item
    Stability: eigenvalues of Jacobians predict system behavior.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2025}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3038}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4937}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Application
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Eigenvector & Av = λv & Principal components, stable directions \\
Eigenvalue & λ = scaling factor & Strength of component or mode \\
Diagonalization & A = PDP⁻¹ & Simplifies powers of matrices, dynamics \\
Spectral theorem & A = QΛQᵀ for symmetric A & PCA, graph Laplacians \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-106}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{A }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{],}
\NormalTok{              [}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{]])}

\CommentTok{\# Compute eigenvalues and eigenvectors}
\NormalTok{vals, vecs }\OperatorTok{=}\NormalTok{ np.linalg.eig(A)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Eigenvalues:"}\NormalTok{, vals)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Eigenvectors:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, vecs)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-4}

Eigenvalues and eigenvectors uncover hidden structure. In AI, they
identify dominant directions in data (PCA), measure graph connectivity
(spectral clustering), and evaluate stability of optimization. Neural
networks exploit low-rank and spectral properties to compress weights
and speed up learning.

\subsubsection{Try It Yourself}\label{try-it-yourself-106}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find eigenvalues and eigenvectors of A = {[}{[}1, 0{]}, {[}0, 2{]}{]}.
  What do they represent?
\item
  For covariance matrix of data points {[}{[}1, 0{]}, {[}0, 1{]}{]},
  what are the eigenvectors?
\item
  Compute eigenvalues of {[}{[}0, 1{]}, {[}1, 0{]}{]}. How do they
  relate to flipping coordinates?
\end{enumerate}

\subsection{108. Singular Value Decomposition
(SVD)}\label{singular-value-decomposition-svd}

Singular Value Decomposition is a powerful factorization that expresses
any matrix as a combination of rotations (or reflections) and scalings.
Unlike eigen decomposition, SVD applies to all rectangular matrices, not
just square ones. It breaks a matrix into orthogonal directions of input
and output, linked by singular values that measure the strength of each
direction.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-107}

Think of a block of clay being pressed through a mold. The mold rotates
and aligns the clay, stretches it differently along key directions, and
then rotates it again. Those directions are the singular vectors, and
the stretching factors are the singular values. SVD reveals the
essential axes of action of any transformation.

\subsubsection{Deep Dive}\label{deep-dive-107}

For a matrix A (m×n),

\[
A = U \Sigma V^T
\]

\begin{itemize}
\tightlist
\item
  U (m×m): orthogonal, columns = left singular vectors.
\item
  Σ (m×n): diagonal with singular values (σ₁ ≥ σ₂ ≥ \ldots{} ≥ 0).
\item
  V (n×n): orthogonal, columns = right singular vectors.
\end{itemize}

Properties:

\begin{itemize}
\tightlist
\item
  Rank(A) = number of nonzero singular values.
\item
  Condition number = σ₁ / σ\_min, measures numerical stability.
\item
  Low-rank approximation: keep top k singular values to compress A.
\end{itemize}

Applications:

\begin{itemize}
\tightlist
\item
  PCA: covariance matrix factorized via SVD.
\item
  Recommender systems: latent factors via matrix factorization.
\item
  Noise reduction and compression: discard small singular values.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.0526}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3684}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5789}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Part
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Role
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Application
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
U & Orthogonal basis for outputs & Principal directions in data space \\
Σ & Strength of each component & Variance captured by each latent
factor \\
V & Orthogonal basis for inputs & Feature embeddings or latent
representations \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-107}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{A }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{],}
\NormalTok{              [}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{]])}

\CommentTok{\# Compute SVD}
\NormalTok{U, S, Vt }\OperatorTok{=}\NormalTok{ np.linalg.svd(A)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"U:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, U)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Singular values:"}\NormalTok{, S)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"V\^{}T:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, Vt)}

\CommentTok{\# Low{-}rank approximation (rank{-}1)}
\NormalTok{rank1 }\OperatorTok{=}\NormalTok{ np.outer(U[:,}\DecValTok{0}\NormalTok{], Vt[}\DecValTok{0}\NormalTok{,:]) }\OperatorTok{*}\NormalTok{ S[}\DecValTok{0}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Rank{-}1 approximation:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, rank1)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-5}

SVD underpins dimensionality reduction, matrix completion, and
compression. It helps uncover latent structures in data (topics,
embeddings), makes computations stable, and explains why certain
transformations amplify or suppress information. In deep learning,
truncated SVD approximates large weight matrices to reduce memory and
computation.

\subsubsection{Try It Yourself}\label{try-it-yourself-107}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the SVD of A = {[}{[}1, 0{]}, {[}0, 1{]}{]}. What are the
  singular values?
\item
  Take matrix {[}{[}2, 0{]}, {[}0, 1{]}{]} and reconstruct it from UΣVᵀ.
  Which direction is stretched more?
\item
  Apply rank-1 approximation to a 3×3 random matrix. How close is it to
  the original?
\end{enumerate}

\subsection{109. Tensors and Higher-Order
Structures}\label{tensors-and-higher-order-structures}

Tensors generalize scalars, vectors, and matrices to higher dimensions.
A scalar is a 0th-order tensor, a vector is a 1st-order tensor, and a
matrix is a 2nd-order tensor. Higher-order tensors (3rd-order and
beyond) represent multi-dimensional data arrays. They are essential in
AI for modeling structured data such as images, sequences, and
multimodal information.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-108}

Picture a line of numbers: that's a vector. Arrange numbers into a grid:
that's a matrix. Stack matrices like pages in a book: that's a 3D
tensor. Add more axes, and you get higher-order tensors. In AI, these
extra dimensions represent channels, time steps, or feature groups---all
in one object.

\subsubsection{Deep Dive}\label{deep-dive-108}

\begin{itemize}
\item
  Order: number of indices needed to address an element.

  \begin{itemize}
  \tightlist
  \item
    Scalar: 0th order (a).
  \item
    Vector: 1st order (aᵢ).
  \item
    Matrix: 2nd order (aᵢⱼ).
  \item
    Tensor: 3rd+ order (aᵢⱼₖ\ldots).
  \end{itemize}
\item
  Shape: tuple of dimensions, e.g., (batch, height, width, channels).
\item
  Operations:

  \begin{itemize}
  \tightlist
  \item
    Element-wise addition and multiplication.
  \item
    Contractions (generalized dot products).
  \item
    Tensor decompositions (e.g., CP, Tucker).
  \end{itemize}
\item
  Applications in AI:

  \begin{itemize}
  \tightlist
  \item
    Images: 3rd-order tensors (height × width × channels).
  \item
    Videos: 4th-order tensors (frames × height × width × channels).
  \item
    Transformers: attention weights stored as 4D tensors.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Order & Example Object & AI Example \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & Scalar & Loss value, learning rate \\
1 & Vector & Word embedding \\
2 & Matrix & Weight matrix \\
3 & Tensor (3D) & RGB image (H×W×3) \\
4+ & Higher-order & Batch of videos, attention scores \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-108}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Scalars, vectors, matrices, tensors}
\NormalTok{scalar }\OperatorTok{=}\NormalTok{ np.array(}\DecValTok{5}\NormalTok{)}
\NormalTok{vector }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{])}
\NormalTok{matrix }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{], [}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{]])}
\NormalTok{tensor3 }\OperatorTok{=}\NormalTok{ np.random.rand(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{)   }\CommentTok{\# 3rd{-}order tensor}
\NormalTok{tensor4 }\OperatorTok{=}\NormalTok{ np.random.rand(}\DecValTok{10}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{3}\NormalTok{)  }\CommentTok{\# batch of 10 RGB images}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Scalar:"}\NormalTok{, scalar)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Vector:"}\NormalTok{, vector)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Matrix:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, matrix)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"3D Tensor shape:"}\NormalTok{, tensor3.shape)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"4D Tensor shape:"}\NormalTok{, tensor4.shape)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-6}

Tensors are the core data structure in modern AI frameworks like
TensorFlow and PyTorch. Every dataset and model parameter is expressed
as tensors, enabling efficient GPU computation. Mastering tensors means
understanding how data flows through deep learning systems, from raw
input to final prediction.

\subsubsection{Try It Yourself}\label{try-it-yourself-108}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Represent a grayscale image of size 28×28 as a tensor. What is its
  order and shape?
\item
  Extend it to a batch of 100 RGB images. What is the new tensor shape?
\item
  Compute the contraction (generalized dot product) between two 3D
  tensors of compatible shapes. What does the result represent?
\end{enumerate}

\subsection{110. Applications in AI
Representations}\label{applications-in-ai-representations}

Linear algebra objects---scalars, vectors, matrices, and tensors---are
not abstract math curiosities. They directly represent data, parameters,
and operations in AI systems. Vectors hold features, matrices encode
transformations, and tensors capture complex structured inputs.
Understanding these correspondences turns math into an intuitive
language for modeling intelligence.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-109}

Imagine an AI model as a factory. Scalars are like single control knobs
(learning rate, bias terms). Vectors are conveyor belts carrying rows of
features. Matrices are the machinery applying
transformations---rotating, stretching, mixing inputs. Tensors are
entire stacks of conveyor belts handling images, sequences, or
multimodal signals at once.

\subsubsection{Deep Dive}\label{deep-dive-109}

\begin{itemize}
\item
  Scalars in AI:

  \begin{itemize}
  \tightlist
  \item
    Learning rates control optimization steps.
  \item
    Loss values quantify performance.
  \end{itemize}
\item
  Vectors in AI:

  \begin{itemize}
  \tightlist
  \item
    Embeddings for words, users, or items.
  \item
    Feature vectors for tabular data or single images.
  \end{itemize}
\item
  Matrices in AI:

  \begin{itemize}
  \tightlist
  \item
    Weight matrices of fully connected layers.
  \item
    Transition matrices in Markov models.
  \end{itemize}
\item
  Tensors in AI:

  \begin{itemize}
  \tightlist
  \item
    Image batches (N×H×W×C).
  \item
    Attention maps (Batch×Heads×Seq×Seq).
  \item
    Multimodal data (e.g., video with audio channels).
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Object & AI Role Example \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Scalar & Learning rate = 0.001, single prediction value \\
Vector & Word embedding = {[}0.2, -0.1, 0.5, \ldots{]} \\
Matrix & Neural layer weights, 512×1024 \\
Tensor & Batch of 64 images, 64×224×224×3 \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-109}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Scalar: loss}
\NormalTok{loss }\OperatorTok{=} \FloatTok{0.23}

\CommentTok{\# Vector: embedding for a word}
\NormalTok{embedding }\OperatorTok{=}\NormalTok{ np.random.rand(}\DecValTok{128}\NormalTok{)  }\CommentTok{\# 128{-}dim word embedding}

\CommentTok{\# Matrix: weights in a dense layer}
\NormalTok{weights }\OperatorTok{=}\NormalTok{ np.random.rand(}\DecValTok{128}\NormalTok{, }\DecValTok{64}\NormalTok{)}

\CommentTok{\# Tensor: batch of 32 RGB images, 64x64 pixels}
\NormalTok{images }\OperatorTok{=}\NormalTok{ np.random.rand(}\DecValTok{32}\NormalTok{, }\DecValTok{64}\NormalTok{, }\DecValTok{64}\NormalTok{, }\DecValTok{3}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Loss (scalar):"}\NormalTok{, loss)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Embedding (vector) shape:"}\NormalTok{, embedding.shape)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Weights (matrix) shape:"}\NormalTok{, weights.shape)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Images (tensor) shape:"}\NormalTok{, images.shape)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-7}

Every modern AI framework is built on top of tensor operations. Training
a model means applying matrix multiplications, summing losses, and
updating weights. Recognizing the role of scalars, vectors, matrices,
and tensors in representations lets you map theory directly to practice,
and reason about computation, memory, and scalability.

\subsubsection{Try It Yourself}\label{try-it-yourself-109}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Represent a mini-batch of 16 grayscale MNIST digits (28×28 each). What
  tensor shape do you get?
\item
  If a dense layer has 300 input features and 100 outputs, what is the
  shape of its weight matrix?
\item
  Construct a tensor representing a 10-second audio clip sampled at 16
  kHz, split into 1-second frames with 13 MFCC coefficients each. What
  would its order and shape be?
\end{enumerate}

\section{Chapter 12. Differential and Integral
Calculus}\label{chapter-12.-differential-and-integral-calculus}

\subsection{111. Functions, Limits, and
Continuity}\label{functions-limits-and-continuity}

Calculus begins with functions: rules that assign inputs to outputs.
Limits describe how functions behave near a point, even if the function
is undefined there. Continuity ensures no sudden jumps---the function
flows smoothly without gaps. These concepts form the groundwork for
derivatives, gradients, and optimization in AI.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-110}

Think of walking along a curve drawn on paper. A continuous function
means you can trace the entire curve without lifting your pencil. A
limit is like approaching a tunnel: even if the tunnel entrance is
blocked at the exact spot, you can still describe where the path was
heading.

\subsubsection{Deep Dive}\label{deep-dive-110}

\begin{itemize}
\item
  Function: f: ℝ → ℝ, mapping x ↦ f(x).
\item
  Limit:

  \[
  \lim_{x \to a} f(x) = L
  \]

  if values of f(x) approach L as x approaches a.
\item
  Continuity: f is continuous at x=a if

  \[
  \lim_{x \to a} f(x) = f(a).
  \]
\item
  Discontinuities: removable (hole), jump, or infinite.
\item
  In AI: limits ensure stability in gradient descent, continuity ensures
  smooth loss surfaces.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1548}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3571}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4881}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Idea
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formal Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Role
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Function & f(x) assigns outputs to inputs & Loss, activation
functions \\
Limit & Values approach L as x → a & Gradient approximations,
convergence \\
Continuity & Limit at a = f(a) & Smooth learning curves,
differentiability \\
Discontinuity & Jumps, holes, asymptotes & Non-smooth activations (ReLU
kinks, etc.) \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-110}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Define a function with a removable discontinuity at x=0}
\KeywordTok{def}\NormalTok{ f(x):}
    \ControlFlowTok{return}\NormalTok{ (np.sin(x)) }\OperatorTok{/}\NormalTok{ x }\ControlFlowTok{if}\NormalTok{ x }\OperatorTok{!=} \DecValTok{0} \ControlFlowTok{else} \DecValTok{1}  \CommentTok{\# define f(0)=1}

\CommentTok{\# Approximate limit near 0}
\NormalTok{xs }\OperatorTok{=}\NormalTok{ [}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\FloatTok{0.001}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.1}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.01}\NormalTok{]}
\NormalTok{limits }\OperatorTok{=}\NormalTok{ [f(val) }\ControlFlowTok{for}\NormalTok{ val }\KeywordTok{in}\NormalTok{ xs]}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Values near 0:"}\NormalTok{, limits)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"f(0):"}\NormalTok{, f(}\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-8}

Optimization in AI depends on smooth, continuous loss functions.
Gradient-based algorithms need limits and continuity to define
derivatives. Activation functions like sigmoid and tanh are continuous,
while piecewise ones like ReLU are continuous but not smooth at
zero---still useful because continuity is preserved.

\subsubsection{Try It Yourself}\label{try-it-yourself-110}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Evaluate the left and right limits of f(x) = 1/x as x → 0. Why do they
  differ?
\item
  Is ReLU(x) = max(0, x) continuous everywhere? Where is it not
  differentiable?
\item
  Construct a function with a jump discontinuity and explain why
  gradient descent would fail on it.
\end{enumerate}

\subsection{112. Derivatives and
Gradients}\label{derivatives-and-gradients}

The derivative measures how a function changes as its input changes. It
captures slope---the rate of change at a point. In multiple dimensions,
this generalizes to gradients: vectors of partial derivatives that
describe the steepest direction of change. Derivatives and gradients are
the engines of optimization in AI.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-111}

Imagine a curve on a hill. At each point, the slope of the tangent line
tells you whether you're climbing up or sliding down. In higher
dimensions, picture standing on a mountain surface: the gradient points
in the direction of steepest ascent, while its negative points toward
steepest descent---the path optimization algorithms follow.

\subsubsection{Deep Dive}\label{deep-dive-111}

\begin{itemize}
\item
  Derivative (1D):

  \[
  f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
  \]
\item
  Partial derivative: rate of change with respect to one variable while
  holding others constant.
\item
  Gradient:

  \[
  \nabla f(x) = \left(\frac{\partial f}{\partial x_1}, \dots, \frac{\partial f}{\partial x_n}\right)
  \]
\item
  Geometric meaning: gradient is perpendicular to level sets of f.
\item
  In AI: gradients guide backpropagation, parameter updates, and loss
  minimization.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1235}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3580}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5185}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula / Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Application
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Derivative & f′(x) = lim (f(x+h) - f(x))/h & Slope of loss curve in 1D
optimization \\
Partial & ∂f/∂xᵢ & Effect of one feature/parameter \\
Gradient & (∂f/∂x₁, \ldots, ∂f/∂xₙ) & Direction of steepest change in
parameters \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-111}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Define a function f(x, y) = x\^{}2 + y\^{}2}
\KeywordTok{def}\NormalTok{ f(x, y):}
    \ControlFlowTok{return}\NormalTok{ x2 }\OperatorTok{+}\NormalTok{ y2}

\CommentTok{\# Numerical gradient at (1,2)}
\NormalTok{h }\OperatorTok{=} \FloatTok{1e{-}5}
\NormalTok{df\_dx }\OperatorTok{=}\NormalTok{ (f(}\DecValTok{1}\OperatorTok{+}\NormalTok{h, }\DecValTok{2}\NormalTok{) }\OperatorTok{{-}}\NormalTok{ f(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{h, }\DecValTok{2}\NormalTok{)) }\OperatorTok{/}\NormalTok{ (}\DecValTok{2}\OperatorTok{*}\NormalTok{h)}
\NormalTok{df\_dy }\OperatorTok{=}\NormalTok{ (f(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\OperatorTok{+}\NormalTok{h) }\OperatorTok{{-}}\NormalTok{ f(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\OperatorTok{{-}}\NormalTok{h)) }\OperatorTok{/}\NormalTok{ (}\DecValTok{2}\OperatorTok{*}\NormalTok{h)}

\NormalTok{gradient }\OperatorTok{=}\NormalTok{ np.array([df\_dx, df\_dy])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Gradient at (1,2):"}\NormalTok{, gradient)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-9}

Every AI model learns by following gradients. Training is essentially
moving through a high-dimensional landscape of parameters, guided by
derivatives of the loss. Understanding derivatives explains why
optimization converges---or gets stuck---and why techniques like
momentum or adaptive learning rates are necessary.

\subsubsection{Try It Yourself}\label{try-it-yourself-111}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the derivative of f(x) = x² at x=3.
\item
  For f(x,y) = 3x + 4y, what is the gradient? What direction does it
  point?
\item
  Explain why the gradient of f(x,y) = x² + y² at (0,0) is the zero
  vector.
\end{enumerate}

\subsection{113. Partial Derivatives and Multivariable
Calculus}\label{partial-derivatives-and-multivariable-calculus}

When functions depend on several variables, we study how the output
changes with respect to each input separately. Partial derivatives
measure change along one axis at a time, while holding others fixed.
Together they form the foundation of multivariable calculus, which
models curved surfaces and multidimensional landscapes.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-112}

Imagine a mountain surface described by height f(x,y). Walking east
measures ∂f/∂x, walking north measures ∂f/∂y. Each partial derivative is
like slicing the mountain in one direction and asking how steep the
slope is in that slice. By combining all directions, we can describe the
terrain fully.

\subsubsection{Deep Dive}\label{deep-dive-112}

\begin{itemize}
\item
  Partial derivative:

  \[
  \frac{\partial f}{\partial x_i}(x_1,\dots,x_n) = \lim_{h \to 0}\frac{f(\dots,x_i+h,\dots) - f(\dots,x_i,\dots)}{h}
  \]
\item
  Gradient vector: collects all partial derivatives.
\item
  Mixed partials: ∂²f/∂x∂y = ∂²f/∂y∂x (under smoothness assumptions,
  Clairaut's theorem).
\item
  Level sets: curves/surfaces where f(x) = constant; gradient is
  perpendicular to these.
\item
  In AI: loss functions often depend on thousands or millions of
  parameters; partial derivatives tell how sensitive the loss is to each
  parameter individually.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2143}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3690}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4167}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Idea
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula/Rule
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Role
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Partial derivative & ∂f/∂xᵢ & Effect of one parameter or feature \\
Gradient & (∂f/∂x₁, \ldots, ∂f/∂xₙ) & Used in backpropagation \\
Mixed partials & ∂²f/∂x∂y = ∂²f/∂y∂x (if smooth) & Second-order methods,
curvature \\
Level sets & f(x)=c, gradient ⟂ level set & Visualizing optimization
landscapes \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-112}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ sympy }\ImportTok{as}\NormalTok{ sp}

\CommentTok{\# Define variables}
\NormalTok{x, y }\OperatorTok{=}\NormalTok{ sp.symbols(}\StringTok{\textquotesingle{}x y\textquotesingle{}}\NormalTok{)}
\NormalTok{f }\OperatorTok{=}\NormalTok{ x2 }\OperatorTok{*}\NormalTok{ y }\OperatorTok{+}\NormalTok{ sp.sin(y)}

\CommentTok{\# Partial derivatives}
\NormalTok{df\_dx }\OperatorTok{=}\NormalTok{ sp.diff(f, x)}
\NormalTok{df\_dy }\OperatorTok{=}\NormalTok{ sp.diff(f, y)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"∂f/∂x ="}\NormalTok{, df\_dx)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"∂f/∂y ="}\NormalTok{, df\_dy)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-10}

Partial derivatives explain how each weight in a neural network
influences the loss. Backpropagation computes them efficiently layer by
layer. Without partial derivatives, training deep models would be
impossible: they are the numerical levers that let optimization adjust
millions of parameters simultaneously.

\subsubsection{Try It Yourself}\label{try-it-yourself-112}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute ∂/∂x of f(x,y) = x²y at (2,1).
\item
  For f(x,y) = sin(xy), find ∂f/∂y.
\item
  Check whether mixed partial derivatives commute for f(x,y) = x²y³.
\end{enumerate}

\subsection{114. Gradient Vectors and Directional
Derivatives}\label{gradient-vectors-and-directional-derivatives}

The gradient vector extends derivatives to multiple dimensions. It
points in the direction of steepest increase of a function. Directional
derivatives generalize further, asking: how does the function change if
we move in \emph{any} chosen direction? Together, they provide the
compass for navigating multidimensional landscapes.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-113}

Imagine standing on a hill. The gradient is the arrow on the ground
pointing directly uphill. If you decide to walk northeast, the
directional derivative tells you how steep the slope is in that chosen
direction. It's the projection of the gradient onto your direction of
travel.

\subsubsection{Deep Dive}\label{deep-dive-113}

\begin{itemize}
\item
  Gradient:

  \[
  \nabla f(x) = \left( \frac{\partial f}{\partial x_1}, \dots, \frac{\partial f}{\partial x_n} \right)
  \]
\item
  Directional derivative in direction u:

  \[
  D_u f(x) = \nabla f(x) \cdot u
  \]

  where u is a unit vector.
\item
  Gradient points to steepest ascent; -∇f points to steepest descent.
\item
  Level sets (contours of constant f): gradient is perpendicular to
  them.
\item
  In AI: gradient descent updates parameters in direction of -∇f;
  directional derivatives explain sensitivity along specific parameter
  combinations.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2933}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2533}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4533}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Application
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Gradient & (∂f/∂x₁, \ldots, ∂f/∂xₙ) & Backpropagation, training
updates \\
Directional derivative & Dᵤf(x) = ∇f(x)·u & Sensitivity along chosen
direction \\
Steepest ascent & Direction of ∇f & Climbing optimization landscapes \\
Steepest descent & Direction of -∇f & Gradient descent learning \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-113}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Define f(x,y) = x\^{}2 + y\^{}2}
\KeywordTok{def}\NormalTok{ f(x, y):}
    \ControlFlowTok{return}\NormalTok{ x2 }\OperatorTok{+}\NormalTok{ y2}

\CommentTok{\# Gradient at (1,2)}
\NormalTok{grad }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{2}\OperatorTok{*}\DecValTok{1}\NormalTok{, }\DecValTok{2}\OperatorTok{*}\DecValTok{2}\NormalTok{])}

\CommentTok{\# Direction u (normalized)}
\NormalTok{u }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]) }\OperatorTok{/}\NormalTok{ np.sqrt(}\DecValTok{2}\NormalTok{)}

\CommentTok{\# Directional derivative}
\NormalTok{Du }\OperatorTok{=}\NormalTok{ np.dot(grad, u)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Gradient at (1,2):"}\NormalTok{, grad)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Directional derivative in direction (1,1):"}\NormalTok{, Du)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-11}

Gradients drive every learning algorithm: they show how to change
parameters to reduce error fastest. Directional derivatives give insight
into how models respond to combined changes, such as adjusting multiple
weights together. This underpins second-order methods, sensitivity
analysis, and robustness checks.

\subsubsection{Try It Yourself}\label{try-it-yourself-113}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For f(x,y) = x² + y², compute the gradient at (3,4). What direction
  does it point?
\item
  Using u = (0,1), compute the directional derivative at (1,2). How does
  it compare to ∂f/∂y?
\item
  Explain why gradient descent always chooses -∇f rather than another
  direction.
\end{enumerate}

\subsection{115. Jacobians and Hessians}\label{jacobians-and-hessians}

The Jacobian and Hessian extend derivatives into structured, matrix
forms. The Jacobian collects all first-order partial derivatives of a
multivariable function, while the Hessian gathers all second-order
partial derivatives. Together, they describe both the slope and
curvature of high-dimensional functions.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-114}

Think of the Jacobian as a map of slopes pointing in every direction,
like a compass at each point of a surface. The Hessian adds a second
layer: it tells you whether the surface is bowl-shaped (convex),
saddle-shaped, or inverted bowl (concave). The Jacobian points you
downhill, the Hessian tells you how the ground curves beneath your feet.

\subsubsection{Deep Dive}\label{deep-dive-114}

\begin{itemize}
\item
  Jacobian: For f: ℝⁿ → ℝᵐ,

  \[
  J_{ij} = \frac{\partial f_i}{\partial x_j}
  \]

  It's an m×n matrix capturing how each output changes with each input.
\item
  Hessian: For scalar f: ℝⁿ → ℝ,

  \[
  H_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}
  \]

  It's an n×n symmetric matrix (if f is smooth).
\item
  Properties:

  \begin{itemize}
  \tightlist
  \item
    Jacobian linearizes functions locally.
  \item
    Hessian encodes curvature, used in Newton's method.
  \end{itemize}
\item
  In AI:

  \begin{itemize}
  \tightlist
  \item
    Jacobians: used in backpropagation through vector-valued layers.
  \item
    Hessians: characterize loss landscapes, stability, and convergence.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Concept & Shape & AI Role \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Jacobian & m×n & Sensitivity of outputs to inputs \\
Hessian & n×n & Curvature of loss function \\
Gradient & 1×n & Special case of Jacobian (m=1) \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-114}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ sympy }\ImportTok{as}\NormalTok{ sp}

\CommentTok{\# Define variables}
\NormalTok{x, y }\OperatorTok{=}\NormalTok{ sp.symbols(}\StringTok{\textquotesingle{}x y\textquotesingle{}}\NormalTok{)}
\NormalTok{f1 }\OperatorTok{=}\NormalTok{ x2 }\OperatorTok{+}\NormalTok{ y}
\NormalTok{f2 }\OperatorTok{=}\NormalTok{ sp.sin(x) }\OperatorTok{*}\NormalTok{ y}
\NormalTok{F }\OperatorTok{=}\NormalTok{ sp.Matrix([f1, f2])}

\CommentTok{\# Jacobian of F wrt (x,y)}
\NormalTok{J }\OperatorTok{=}\NormalTok{ F.jacobian([x, y])}

\CommentTok{\# Hessian of scalar f1}
\NormalTok{H }\OperatorTok{=}\NormalTok{ sp.hessian(f1, (x, y))}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Jacobian:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, J)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Hessian of f1:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, H)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-12}

The Jacobian underlies backpropagation: it's how gradients flow through
each layer of a neural network. The Hessian reveals whether minima are
sharp or flat, explaining generalization and optimization difficulty.
Many advanced algorithms---Newton's method, natural gradients,
curvature-aware optimizers---rely on these structures.

\subsubsection{Try It Yourself}\label{try-it-yourself-114}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the Jacobian of F(x,y) = (x², y²) at (1,2).
\item
  For f(x,y) = x² + y², write down the Hessian. What does it say about
  curvature?
\item
  Explain how the Hessian helps distinguish between a minimum, maximum,
  and saddle point.
\end{enumerate}

\subsection{116. Optimization and Critical
Points}\label{optimization-and-critical-points}

Optimization is about finding inputs that minimize or maximize a
function. Critical points are where the gradient vanishes (∇f = 0).
These points can be minima, maxima, or saddle points. Understanding them
is central to training AI models, since learning is optimization over a
loss surface.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-115}

Imagine a landscape of hills and valleys. Critical points are the flat
spots where the slope disappears: the bottom of a valley, the top of a
hill, or the center of a saddle. Optimization is like dropping a ball
into this landscape and watching where it rolls. The type of critical
point determines whether the ball comes to rest in a stable valley or
balances precariously on a ridge.

\subsubsection{Deep Dive}\label{deep-dive-115}

\begin{itemize}
\item
  Critical point: x* where ∇f(x*) = 0.
\item
  Classification via Hessian:

  \begin{itemize}
  \tightlist
  \item
    Positive definite → local minimum.
  \item
    Negative definite → local maximum.
  \item
    Indefinite → saddle point.
  \end{itemize}
\item
  Global vs local: Local minima are valleys nearby; global minimum is
  the deepest valley.
\item
  Convex functions: any local minimum is also global.
\item
  In AI: neural networks often converge to local minima or saddle
  points; optimization aims for low-loss basins that generalize well.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1687}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3373}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4940}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Test (using Hessian)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning in AI
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Local minimum & H positive definite & Stable learned model, low loss \\
Local maximum & H negative definite & Rare in training; undesired
peak \\
Saddle point & H indefinite & Common in high dimensions, slows
training \\
Global minimum & Lowest value over all inputs & Best achievable
performance \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-115}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ sympy }\ImportTok{as}\NormalTok{ sp}

\NormalTok{x, y }\OperatorTok{=}\NormalTok{ sp.symbols(}\StringTok{\textquotesingle{}x y\textquotesingle{}}\NormalTok{)}
\NormalTok{f }\OperatorTok{=}\NormalTok{ x2 }\OperatorTok{+}\NormalTok{ y2 }\OperatorTok{{-}}\NormalTok{ x}\OperatorTok{*}\NormalTok{y}

\CommentTok{\# Gradient and Hessian}
\NormalTok{grad }\OperatorTok{=}\NormalTok{ [sp.diff(f, var) }\ControlFlowTok{for}\NormalTok{ var }\KeywordTok{in}\NormalTok{ (x, y)]}
\NormalTok{H }\OperatorTok{=}\NormalTok{ sp.hessian(f, (x, y))}

\CommentTok{\# Solve for critical points}
\NormalTok{critical\_points }\OperatorTok{=}\NormalTok{ sp.solve(grad, (x, y))}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Critical points:"}\NormalTok{, critical\_points)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Hessian:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, H)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-13}

Training neural networks is about navigating a massive landscape of
parameters. Knowing how to identify minima, maxima, and saddles explains
why optimization sometimes gets stuck or converges slowly. Techniques
like momentum and adaptive learning rates help escape saddles and find
flatter minima, which often generalize better.

\subsubsection{Try It Yourself}\label{try-it-yourself-115}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find critical points of f(x) = x². What type are they?
\item
  For f(x,y) = x² − y², compute the gradient and Hessian at (0,0). What
  type of point is this?
\item
  Explain why convex loss functions are easier to optimize than
  non-convex ones.
\end{enumerate}

\subsection{117. Integrals and Areas under
Curves}\label{integrals-and-areas-under-curves}

Integration is the process of accumulating quantities, often visualized
as the area under a curve. While derivatives measure instantaneous
change, integrals measure total accumulation. In AI, integrals appear in
probability (areas under density functions), expected values, and
continuous approximations of sums.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-116}

Imagine pouring water under a curve until it touches the graph: the
filled region is the integral. If the curve goes above and below the
axis, areas above count positive and areas below count negative,
balancing out like gains and losses over time.

\subsubsection{Deep Dive}\label{deep-dive-116}

\begin{itemize}
\item
  Definite integral:

  \[
  \int_a^b f(x)\,dx
  \]

  is the net area under f(x) between a and b.
\item
  Indefinite integral:

  \[
  \int f(x)\,dx = F(x) + C
  \]

  where F′(x) = f(x).
\item
  Fundamental Theorem of Calculus: connects integrals and derivatives:

  \[
  \frac{d}{dx}\int_a^x f(t)\,dt = f(x).
  \]
\item
  In AI:

  \begin{itemize}
  \tightlist
  \item
    Probability densities integrate to 1.
  \item
    Expectations are integrals over random variables.
  \item
    Continuous-time models (differential equations, neural ODEs) rely on
    integration.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2289}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2530}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5181}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Role
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Definite integral & ∫ₐᵇ f(x) dx & Probability mass, expected outcomes \\
Indefinite integral & ∫ f(x) dx = F(x) + C & Antiderivative, symbolic
computation \\
Fundamental theorem & d/dx ∫ f(t) dt = f(x) & Links change (derivatives)
and accumulation \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-116}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ sympy }\ImportTok{as}\NormalTok{ sp}

\NormalTok{x }\OperatorTok{=}\NormalTok{ sp.symbols(}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{)}
\NormalTok{f }\OperatorTok{=}\NormalTok{ sp.sin(x)}

\CommentTok{\# Indefinite integral}
\NormalTok{F }\OperatorTok{=}\NormalTok{ sp.integrate(f, x)}

\CommentTok{\# Definite integral from 0 to pi}
\NormalTok{area }\OperatorTok{=}\NormalTok{ sp.integrate(f, (x, }\DecValTok{0}\NormalTok{, sp.pi))}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Indefinite integral of sin(x):"}\NormalTok{, F)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Definite integral from 0 to pi:"}\NormalTok{, area)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-14}

Integrals explain how continuous distributions accumulate probability,
why loss functions like cross-entropy involve expectations, and how
continuous dynamics are modeled in AI. Without integrals, probability
theory and continuous optimization would collapse, leaving only crude
approximations.

\subsubsection{Try It Yourself}\label{try-it-yourself-116}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute ∫₀¹ x² dx.
\item
  For probability density f(x) = 2x on {[}0,1{]}, check that ∫₀¹ f(x) dx
  = 1.
\item
  Find ∫ cos(x) dx and verify by differentiation.
\end{enumerate}

\subsection{118. Multiple Integrals and
Volumes}\label{multiple-integrals-and-volumes}

Multiple integrals extend the idea of integration to higher dimensions.
Instead of the area under a curve, we compute volumes under surfaces or
hyper-volumes in higher-dimensional spaces. They let us measure total
mass, probability, or accumulation over multidimensional regions.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-117}

Imagine a bumpy sheet stretched over the xy-plane. The double integral
sums the ``pillars'' of volume beneath the surface, filling the region
like pouring sand until the surface is reached. Triple integrals push
this further, measuring the volume inside 3D solids. Higher-order
integrals generalize the same idea into abstract feature spaces.

\subsubsection{Deep Dive}\label{deep-dive-117}

\begin{itemize}
\item
  Double integral:

  \[
  \iint_R f(x,y)\,dx\,dy
  \]

  sums over a region R in 2D.
\item
  Triple integral:

  \[
  \iiint_V f(x,y,z)\,dx\,dy\,dz
  \]

  over volume V.
\item
  Fubini's theorem: allows evaluating multiple integrals as iterated
  single integrals, e.g.

  \[
  \iint_R f(x,y)\,dx\,dy = \int_a^b \int_c^d f(x,y)\,dx\,dy.
  \]
\item
  Applications in AI:

  \begin{itemize}
  \tightlist
  \item
    Probability distributions in multiple variables (joint densities).
  \item
    Normalization constants in Bayesian inference.
  \item
    Expectation over multivariate spaces.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1605}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2963}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5432}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Integral Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Application
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Double & ∬ f(x,y) dx dy & Joint probability of two features \\
Triple & ∭ f(x,y,z) dx dy dz & Volumes, multivariate Gaussian
normalization \\
Higher-order & ∫ \ldots{} ∫ f(x₁,\ldots,xₙ) dx₁\ldots dxₙ & Expectation
in high-dimensional models \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-117}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ sympy }\ImportTok{as}\NormalTok{ sp}

\NormalTok{x, y }\OperatorTok{=}\NormalTok{ sp.symbols(}\StringTok{\textquotesingle{}x y\textquotesingle{}}\NormalTok{)}
\NormalTok{f }\OperatorTok{=}\NormalTok{ x }\OperatorTok{+}\NormalTok{ y}

\CommentTok{\# Double integral over square [0,1]x[0,1]}
\NormalTok{area }\OperatorTok{=}\NormalTok{ sp.integrate(sp.integrate(f, (x, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)), (y, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Double integral over [0,1]x[0,1]:"}\NormalTok{, area)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-15}

Many AI models operate on high-dimensional data, where probabilities are
defined via integrals across feature spaces. Normalizing Gaussian
densities, computing evidence in Bayesian models, or estimating
expectations all require multiple integrals. They connect geometry with
probability in the spaces AI systems navigate.

\subsubsection{Try It Yourself}\label{try-it-yourself-117}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Evaluate ∬ (x² + y²) dx dy over {[}0,1{]}×{[}0,1{]}.
\item
  Compute ∭ 1 dx dy dz over the cube {[}0,1{]}³. What does it represent?
\item
  For joint density f(x,y) = 6xy on {[}0,1{]}×{[}0,1{]}, check that its
  double integral equals 1.
\end{enumerate}

\subsection{119. Differential Equations
Basics}\label{differential-equations-basics}

Differential equations describe how quantities change with respect to
one another. Instead of just functions, they define relationships
between a function and its derivatives. Solutions to differential
equations capture dynamic processes evolving over time or space.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-118}

Think of a swinging pendulum. Its position changes, but its rate of
change depends on velocity, and velocity depends on forces. A
differential equation encodes this chain of dependencies, like a
rulebook that governs motion rather than a single trajectory.

\subsubsection{Deep Dive}\label{deep-dive-118}

\begin{itemize}
\item
  Ordinary Differential Equation (ODE): involves derivatives with
  respect to one variable (usually time). Example:

  \[
  \frac{dy}{dt} = ky
  \]

  has solution y(t) = Ce\^{}\{kt\}.
\item
  Partial Differential Equation (PDE): involves derivatives with respect
  to multiple variables. Example: heat equation:

  \[
  \frac{\partial u}{\partial t} = \alpha \nabla^2 u.
  \]
\item
  Initial value problem (IVP): specify conditions at a starting point to
  determine a unique solution.
\item
  Linear vs nonlinear: linear equations superpose solutions; nonlinear
  ones often create complex behaviors.
\item
  In AI: neural ODEs, diffusion models, and continuous-time dynamics all
  rest on differential equations.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.0635}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2698}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.6667}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
General Form
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Use in AI
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ODE & dy/dt = f(y,t) & Neural ODEs for continuous-depth models \\
PDE & ∂u/∂t = f(u,∇u,\ldots) & Diffusion models for generative AI \\
IVP & y(t₀)=y₀ & Simulating trajectories from initial state \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-118}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.integrate }\ImportTok{import}\NormalTok{ solve\_ivp}

\CommentTok{\# ODE: dy/dt = {-}y}
\KeywordTok{def}\NormalTok{ f(t, y):}
    \ControlFlowTok{return} \OperatorTok{{-}}\NormalTok{y}

\NormalTok{sol }\OperatorTok{=}\NormalTok{ solve\_ivp(f, (}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{), [}\FloatTok{1.0}\NormalTok{], t\_eval}\OperatorTok{=}\NormalTok{np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"t:"}\NormalTok{, sol.t)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"y:"}\NormalTok{, sol.y[}\DecValTok{0}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-16}

Differential equations connect AI to physics and natural processes. They
explain how continuous-time systems evolve and allow models like
diffusion probabilistic models or neural ODEs to simulate dynamics.
Mastery of differential equations equips AI practitioners to model
beyond static data, into evolving systems.

\subsubsection{Try It Yourself}\label{try-it-yourself-118}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Solve dy/dt = 2y with y(0)=1.
\item
  Write down the PDE governing heat diffusion in 1D.
\item
  Explain how an ODE solver could be used inside a neural network layer.
\end{enumerate}

\subsection{120. Calculus in Machine Learning
Applications}\label{calculus-in-machine-learning-applications}

Calculus is not just abstract math---it powers nearly every algorithm in
machine learning. Derivatives guide optimization, integrals handle
probabilities, and multivariable calculus shapes how we train and
regularize models. Understanding these connections makes the
mathematical backbone of AI visible.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-119}

Imagine training a neural network as hiking down a mountain blindfolded.
Derivatives tell you which way is downhill (gradient descent). Integrals
measure the area you've already crossed (expectation over data).
Together, they form the invisible GPS guiding your steps toward a valley
of lower loss.

\subsubsection{Deep Dive}\label{deep-dive-119}

\begin{itemize}
\item
  Derivatives in ML:

  \begin{itemize}
  \tightlist
  \item
    Gradients of loss functions guide parameter updates.
  \item
    Backpropagation applies the chain rule across layers.
  \end{itemize}
\item
  Integrals in ML:

  \begin{itemize}
  \item
    Probabilities as areas under density functions.
  \item
    Expectations:

    \[
    \mathbb{E}[f(x)] = \int f(x) p(x) dx.
    \]
  \item
    Partition functions in probabilistic models.
  \end{itemize}
\item
  Optimization: finding minima of loss surfaces through derivatives.
\item
  Regularization: penalty terms often involve norms, tied to integrals
  of squared functions.
\item
  Continuous-time models: neural ODEs and diffusion models integrate
  dynamics.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1444}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4556}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Calculus Tool
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Role in ML
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Derivative & Guides optimization & Gradient descent in neural
networks \\
Chain rule & Efficient backpropagation & Training deep nets \\
Integral & Probability and expectation & Likelihood, Bayesian
inference \\
Multivariable & Handles high-dimensional parameter spaces & Vectorized
gradients in large models \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-119}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Loss function: mean squared error}
\KeywordTok{def}\NormalTok{ loss(w, x, y):}
\NormalTok{    y\_pred }\OperatorTok{=}\NormalTok{ w }\OperatorTok{*}\NormalTok{ x}
    \ControlFlowTok{return}\NormalTok{ np.mean((y }\OperatorTok{{-}}\NormalTok{ y\_pred)}\DecValTok{2}\NormalTok{)}

\CommentTok{\# Gradient of loss wrt w}
\KeywordTok{def}\NormalTok{ grad(w, x, y):}
    \ControlFlowTok{return} \OperatorTok{{-}}\DecValTok{2} \OperatorTok{*}\NormalTok{ np.mean(x }\OperatorTok{*}\NormalTok{ (y }\OperatorTok{{-}}\NormalTok{ w }\OperatorTok{*}\NormalTok{ x))}

\CommentTok{\# Training loop}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{8}\NormalTok{])}
\NormalTok{w }\OperatorTok{=} \FloatTok{0.0}
\NormalTok{lr }\OperatorTok{=} \FloatTok{0.1}

\ControlFlowTok{for}\NormalTok{ epoch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{5}\NormalTok{):}
\NormalTok{    w }\OperatorTok{{-}=}\NormalTok{ lr }\OperatorTok{*}\NormalTok{ grad(w, x, y)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Epoch }\SpecialCharTok{\{}\NormalTok{epoch}\SpecialCharTok{\}}\SpecialStringTok{, w=}\SpecialCharTok{\{}\NormalTok{w}\SpecialCharTok{:.4f\}}\SpecialStringTok{, loss=}\SpecialCharTok{\{}\NormalTok{loss(w,x,y)}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-17}

Calculus is the language of change, and machine learning is about
changing parameters to fit data. Derivatives let us learn efficiently in
high dimensions. Integrals make probability models consistent. Without
calculus, optimization, probabilistic inference, and even basic learning
algorithms would be impossible.

\subsubsection{Try It Yourself}\label{try-it-yourself-119}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Show how the chain rule applies to f(x) = (3x+1)².
\item
  Express the expectation of f(x) = x under uniform distribution on
  {[}0,1{]} as an integral.
\item
  Compute the derivative of cross-entropy loss with respect to predicted
  probability p.
\end{enumerate}

\section{Chapter 13. Probability Theory
Fundamentals}\label{chapter-13.-probability-theory-fundamentals}

\subsection{121. Probability Axioms and Sample
Spaces}\label{probability-axioms-and-sample-spaces}

Probability provides a formal framework for reasoning about uncertainty.
At its core are three axioms that define how probabilities behave, and a
sample space that captures all possible outcomes. Together, they turn
randomness into a rigorous system we can compute with.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-120}

Imagine rolling a die. The sample space is the set of all possible faces
\{1,2,3,4,5,6\}. Assigning probabilities is like pouring paint onto
these outcomes so that the total paint equals 1. The axioms ensure the
paint spreads consistently: nonnegative, complete, and additive.

\subsubsection{Deep Dive}\label{deep-dive-120}

\begin{itemize}
\item
  Sample space (Ω): set of all possible outcomes.
\item
  Event: subset of Ω. Example: rolling an even number = \{2,4,6\}.
\item
  Axioms of probability (Kolmogorov):

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \item
    Non-negativity: P(A) ≥ 0 for all events A.
  \item
    Normalization: P(Ω) = 1.
  \item
    Additivity: For disjoint events A, B:

    \[
    P(A \cup B) = P(A) + P(B).
    \]
  \end{enumerate}
\end{itemize}

From these axioms, all other probability rules follow, such as
complement, conditional probability, and independence.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1750}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4750}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition / Rule
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Sample space Ω & All possible outcomes & Coin toss: \{H, T\} \\
Event & Subset of Ω & Even number on die: \{2,4,6\} \\
Non-negativity & P(A) ≥ 0 & Probability can't be negative \\
Normalization & P(Ω) = 1 & Total probability of all die faces = 1 \\
Additivity & P(A∪B) = P(A)+P(B), if A∩B=∅ & P(odd ∪ even) = 1 \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-120}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sample space: fair six{-}sided die}
\NormalTok{sample\_space }\OperatorTok{=}\NormalTok{ \{}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{\}}

\CommentTok{\# Uniform probability distribution}
\NormalTok{prob }\OperatorTok{=}\NormalTok{ \{outcome: }\DecValTok{1}\OperatorTok{/}\DecValTok{6} \ControlFlowTok{for}\NormalTok{ outcome }\KeywordTok{in}\NormalTok{ sample\_space\}}

\CommentTok{\# Probability of event A = \{2,4,6\}}
\NormalTok{A }\OperatorTok{=}\NormalTok{ \{}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{\}}
\NormalTok{P\_A }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(prob[x] }\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ A)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"P(A):"}\NormalTok{, P\_A)   }\CommentTok{\# 0.5}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Normalization check:"}\NormalTok{, }\BuiltInTok{sum}\NormalTok{(prob.values()))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-18}

AI systems constantly reason under uncertainty: predicting outcomes,
estimating likelihoods, or sampling from models. The axioms guarantee
consistency in these calculations. Without them, probability would
collapse into contradictions, and machine learning models built on
probabilistic foundations would be meaningless.

\subsubsection{Try It Yourself}\label{try-it-yourself-120}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Define the sample space for flipping two coins. List all possible
  events.
\item
  If a biased coin has P(H) = 0.7 and P(T) = 0.3, check normalization.
\item
  Roll a die. What is the probability of getting a number divisible by
  3?
\end{enumerate}

\subsection{122. Random Variables and
Distributions}\label{random-variables-and-distributions}

Random variables assign numerical values to outcomes of a random
experiment. They let us translate abstract events into numbers we can
calculate with. The distribution of a random variable tells us how
likely each value is, shaping the behavior of probabilistic models.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-121}

Think of rolling a die. The outcome is a symbol like ``3,'' but the
random variable X maps this to the number 3. Now imagine throwing darts
at a dartboard: the random variable could be the distance from the
center. Distributions describe whether outcomes are spread evenly,
clustered, or skewed.

\subsubsection{Deep Dive}\label{deep-dive-121}

\begin{itemize}
\item
  Random variable (RV): A function X: Ω → ℝ.
\item
  Discrete RV: takes countable values (coin toss, die roll).
\item
  Continuous RV: takes values in intervals of ℝ (height, time).
\item
  Probability Mass Function (PMF):

  \[
  P(X = x) = p(x), \quad \sum_x p(x) = 1.
  \]
\item
  Probability Density Function (PDF):

  \[
  P(a \leq X \leq b) = \int_a^b f(x)\,dx, \quad \int_{-\infty}^\infty f(x)\,dx = 1.
  \]
\item
  Cumulative Distribution Function (CDF):

  \[
  F(x) = P(X \leq x).
  \]
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1449}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2174}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.6377}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Representation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Discrete & PMF p(x) & Word counts, categorical labels \\
Continuous & PDF f(x) & Feature distributions (height, signal value) \\
CDF & F(x) = P(X ≤ x) & Threshold probabilities, quantiles \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-121}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ norm}

\CommentTok{\# Discrete: fair die}
\NormalTok{die\_outcomes }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{]}
\NormalTok{pmf }\OperatorTok{=}\NormalTok{ \{x: }\DecValTok{1}\OperatorTok{/}\DecValTok{6} \ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ die\_outcomes\}}

\CommentTok{\# Continuous: Normal distribution}
\NormalTok{mu, sigma }\OperatorTok{=} \DecValTok{0}\NormalTok{, }\DecValTok{1}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{pdf\_values }\OperatorTok{=}\NormalTok{ norm.pdf(x, mu, sigma)}
\NormalTok{cdf\_values }\OperatorTok{=}\NormalTok{ norm.cdf(x, mu, sigma)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Die PMF:"}\NormalTok{, pmf)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Normal PDF:"}\NormalTok{, pdf\_values)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Normal CDF:"}\NormalTok{, cdf\_values)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-19}

Machine learning depends on modeling data distributions. Random
variables turn uncertainty into analyzable numbers, while distributions
tell us how data is spread. Class probabilities in classifiers, Gaussian
assumptions in regression, and sampling in generative models all rely on
these ideas.

\subsubsection{Try It Yourself}\label{try-it-yourself-121}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Define a random variable for tossing a coin twice. What values can it
  take?
\item
  For a fair die, what is the PMF of X = ``die roll''?
\item
  For a continuous variable X ∼ Uniform(0,1), compute P(0.2 ≤ X ≤ 0.5).
\end{enumerate}

\subsection{123. Expectation, Variance, and
Moments}\label{expectation-variance-and-moments}

Expectation measures the average value of a random variable in the long
run. Variance quantifies how spread out the values are around that
average. Higher moments (like skewness and kurtosis) describe asymmetry
and tail heaviness. These statistics summarize distributions into
interpretable quantities.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-122}

Imagine tossing a coin thousands of times and recording 1 for heads, 0
for tails. The expectation is the long-run fraction of heads, the
variance tells how often results deviate from that average, and higher
moments reveal whether the distribution is balanced or skewed. It's like
reducing a noisy dataset to a handful of meaningful descriptors.

\subsubsection{Deep Dive}\label{deep-dive-122}

\begin{itemize}
\item
  Expectation (mean):

  \begin{itemize}
  \item
    Discrete:

    \[
    \mathbb{E}[X] = \sum_x x \, p(x).
    \]
  \item
    Continuous:

    \[
    \mathbb{E}[X] = \int_{-\infty}^\infty x \, f(x) \, dx.
    \]
  \end{itemize}
\item
  Variance:

  \[
  \text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2.
  \]
\item
  Standard deviation: square root of variance.
\item
  Higher moments:

  \begin{itemize}
  \tightlist
  \item
    Skewness: asymmetry.
  \item
    Kurtosis: heaviness of tails.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Statistic & Formula & Interpretation in AI \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Expectation & E{[}X{]} & Predicted output, mean loss \\
Variance & E{[}(X−μ)²{]} & Uncertainty in predictions \\
Skewness & E{[}((X−μ)/σ)³{]} & Bias toward one side \\
Kurtosis & E{[}((X−μ)/σ)⁴{]} & Outlier sensitivity \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-122}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Sample data: simulated predictions}
\NormalTok{data }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{9}\NormalTok{])}

\CommentTok{\# Expectation}
\NormalTok{mean }\OperatorTok{=}\NormalTok{ np.mean(data)}

\CommentTok{\# Variance and standard deviation}
\NormalTok{var }\OperatorTok{=}\NormalTok{ np.var(data)}
\NormalTok{std }\OperatorTok{=}\NormalTok{ np.std(data)}

\CommentTok{\# Higher moments}
\NormalTok{skew }\OperatorTok{=}\NormalTok{ ((data }\OperatorTok{{-}}\NormalTok{ mean)}\DecValTok{3}\NormalTok{).mean() }\OperatorTok{/}\NormalTok{ (std3)}
\NormalTok{kurt }\OperatorTok{=}\NormalTok{ ((data }\OperatorTok{{-}}\NormalTok{ mean)}\DecValTok{4}\NormalTok{).mean() }\OperatorTok{/}\NormalTok{ (std4)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Mean:"}\NormalTok{, mean)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Variance:"}\NormalTok{, var)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Skewness:"}\NormalTok{, skew)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Kurtosis:"}\NormalTok{, kurt)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-20}

Expectations are used in defining loss functions, variances quantify
uncertainty in probabilistic models, and higher moments detect
distributional shifts. For example, expected risk underlies learning
theory, variance is minimized in ensemble methods, and kurtosis signals
heavy-tailed data often found in real-world datasets.

\subsubsection{Try It Yourself}\label{try-it-yourself-122}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the expectation of rolling a fair die.
\item
  What is the variance of a Bernoulli random variable with p=0.3?
\item
  Explain why minimizing expected loss (not variance) is the goal in
  training, but variance still matters for model stability.
\end{enumerate}

\subsection{124. Common Distributions (Bernoulli, Binomial,
Gaussian)}\label{common-distributions-bernoulli-binomial-gaussian}

Certain probability distributions occur so often in real-world problems
that they are considered ``canonical.'' The Bernoulli models a single
yes/no event, the Binomial models repeated independent trials, and the
Gaussian (Normal) models continuous data clustered around a mean.
Mastering these is essential for building and interpreting AI models.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-123}

Imagine flipping a single coin: that's Bernoulli. Flip the coin ten
times and count heads: that's Binomial. Measure people's heights: most
cluster near average with some shorter and taller outliers---that's
Gaussian. These three form the basic vocabulary of probability.

\subsubsection{Deep Dive}\label{deep-dive-123}

\begin{itemize}
\item
  Bernoulli(p):

  \begin{itemize}
  \tightlist
  \item
    Values: \{0,1\}, success probability p.
  \item
    PMF: P(X=1)=p, P(X=0)=1−p.
  \item
    Mean: p, Variance: p(1−p).
  \end{itemize}
\item
  Binomial(n,p):

  \begin{itemize}
  \item
    Number of successes in n independent Bernoulli trials.
  \item
    PMF:

    \[
    P(X=k) = \binom{n}{k} p^k (1-p)^{n-k}.
    \]
  \item
    Mean: np, Variance: np(1−p).
  \end{itemize}
\item
  Gaussian(μ,σ²):

  \begin{itemize}
  \item
    Continuous distribution with PDF:

    \[
    f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right).
    \]
  \item
    Mean: μ, Variance: σ².
  \item
    Appears by Central Limit Theorem.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1765}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3382}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4853}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Distribution
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Bernoulli & P(X=1)=p, P(X=0)=1−p & Binary labels, dropout masks \\
Binomial & P(X=k)=C(n,k)pᵏ(1−p)ⁿ⁻ᵏ & Number of successes in trials \\
Gaussian & f(x) ∝ exp(−(x−μ)²/2σ²) & Noise models, continuous
features \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-123}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ bernoulli, binom, norm}

\CommentTok{\# Bernoulli trial}
\NormalTok{p }\OperatorTok{=} \FloatTok{0.7}
\NormalTok{sample }\OperatorTok{=}\NormalTok{ bernoulli.rvs(p, size}\OperatorTok{=}\DecValTok{10}\NormalTok{)}

\CommentTok{\# Binomial: 10 trials, p=0.5}
\NormalTok{binom\_samples }\OperatorTok{=}\NormalTok{ binom.rvs(}\DecValTok{10}\NormalTok{, }\FloatTok{0.5}\NormalTok{, size}\OperatorTok{=}\DecValTok{5}\NormalTok{)}

\CommentTok{\# Gaussian: mu=0, sigma=1}
\NormalTok{gauss\_samples }\OperatorTok{=}\NormalTok{ norm.rvs(loc}\OperatorTok{=}\DecValTok{0}\NormalTok{, scale}\OperatorTok{=}\DecValTok{1}\NormalTok{, size}\OperatorTok{=}\DecValTok{5}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Bernoulli samples:"}\NormalTok{, sample)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Binomial samples:"}\NormalTok{, binom\_samples)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Gaussian samples:"}\NormalTok{, gauss\_samples)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-21}

Many machine learning algorithms assume specific distributions: logistic
regression assumes Bernoulli outputs, Naive Bayes uses
Binomial/Multinomial, and Gaussian assumptions appear in linear
regression, PCA, and generative models. Recognizing these distributions
connects statistical modeling to practical AI.

\subsubsection{Try It Yourself}\label{try-it-yourself-123}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What are the mean and variance of a Binomial(20, 0.4) distribution?
\item
  Simulate 1000 Gaussian samples with μ=5, σ=2 and compute their sample
  mean. How close is it to the true mean?
\item
  Explain why the Gaussian is often used to model noise in data.
\end{enumerate}

\subsection{125. Joint, Marginal, and Conditional
Probability}\label{joint-marginal-and-conditional-probability}

When dealing with multiple random variables, probabilities can be
combined (joint), reduced (marginal), or conditioned (conditional).
These operations form the grammar of probabilistic reasoning, allowing
us to express how variables interact and how knowledge of one affects
belief about another.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-124}

Think of two dice rolled together. The joint probability is the full
grid of all 36 outcomes. Marginal probability is like looking only at
one die's values, ignoring the other. Conditional probability is asking:
if the first die shows a 6, what is the probability that the sum is
greater than 10?

\subsubsection{Deep Dive}\label{deep-dive-124}

\begin{itemize}
\item
  Joint probability: probability of events happening together.

  \begin{itemize}
  \tightlist
  \item
    Discrete: P(X=x, Y=y).
  \item
    Continuous: joint density f(x,y).
  \end{itemize}
\item
  Marginal probability: probability of a subset of variables, obtained
  by summing/integrating over others.

  \begin{itemize}
  \tightlist
  \item
    Discrete: P(X=x) = Σ\_y P(X=x, Y=y).
  \item
    Continuous: f\_X(x) = ∫ f(x,y) dy.
  \end{itemize}
\item
  Conditional probability:

  \[
  P(X|Y) = \frac{P(X,Y)}{P(Y)}, \quad P(Y)>0.
  \]
\item
  Chain rule of probability:

  \[
  P(X_1, …, X_n) = \prod_{i=1}^n P(X_i | X_1, …, X_{i-1}).
  \]
\item
  In AI: joint models define distributions over data, marginals appear
  in feature distributions, and conditionals are central to Bayesian
  inference.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1134}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1959}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3402}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3505}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Joint & P(X,Y) & Image pixel + label distribution & \\
Marginal & P(X) = Σ\_y P(X,Y) & Distribution of one feature alone & \\
Conditional & P(X & Y) = P(X,Y)/P(Y) & Class probabilities given
features \\
Chain rule & P(X₁,\ldots,Xₙ) = Π P(Xᵢ & X₁\ldots Xᵢ₋₁) & Generative
sequence models \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-124}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Joint distribution for two binary variables X,Y}
\NormalTok{joint }\OperatorTok{=}\NormalTok{ np.array([[}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.2}\NormalTok{],}
\NormalTok{                  [}\FloatTok{0.3}\NormalTok{, }\FloatTok{0.4}\NormalTok{]])  }\CommentTok{\# rows=X, cols=Y}

\CommentTok{\# Marginals}
\NormalTok{P\_X }\OperatorTok{=}\NormalTok{ joint.}\BuiltInTok{sum}\NormalTok{(axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{P\_Y }\OperatorTok{=}\NormalTok{ joint.}\BuiltInTok{sum}\NormalTok{(axis}\OperatorTok{=}\DecValTok{0}\NormalTok{)}

\CommentTok{\# Conditional P(X|Y=1)}
\NormalTok{P\_X\_given\_Y1 }\OperatorTok{=}\NormalTok{ joint[:,}\DecValTok{1}\NormalTok{] }\OperatorTok{/}\NormalTok{ P\_Y[}\DecValTok{1}\NormalTok{]}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Joint:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, joint)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Marginal P(X):"}\NormalTok{, P\_X)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Marginal P(Y):"}\NormalTok{, P\_Y)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Conditional P(X|Y=1):"}\NormalTok{, P\_X\_given\_Y1)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-22}

Probabilistic models in AI---from Bayesian networks to hidden Markov
models---are built from joint, marginal, and conditional probabilities.
Classification is essentially conditional probability estimation
(P(label \textbar{} features)). Generative models learn joint
distributions, while inference often involves computing marginals.

\subsubsection{Try It Yourself}\label{try-it-yourself-124}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For a fair die and coin, what is the joint probability of rolling a 3
  and flipping heads?
\item
  From joint distribution P(X,Y), derive P(X) by marginalization.
\item
  Explain why P(A\textbar B) ≠ P(B\textbar A), with an example from
  medical diagnosis.
\end{enumerate}

\subsection{126. Independence and
Correlation}\label{independence-and-correlation}

Independence means two random variables do not influence each other:
knowing one tells you nothing about the other. Correlation measures the
strength and direction of linear dependence. Together, they help us
characterize whether features or events are related, redundant, or
informative.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-125}

Imagine rolling two dice. The result of one die does not affect the
other---this is independence. Now imagine height and weight: they are
not independent, because taller people tend to weigh more. The
correlation quantifies this relationship on a scale from −1 (perfect
negative) to +1 (perfect positive).

\subsubsection{Deep Dive}\label{deep-dive-125}

\begin{itemize}
\item
  Independence:

  \[
  P(X,Y) = P(X)P(Y), \quad \text{or equivalently } P(X|Y)=P(X).
  \]
\item
  Correlation coefficient (Pearson's ρ):

  \[
  \rho(X,Y) = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}.
  \]
\item
  Covariance:

  \[
  \text{Cov}(X,Y) = \mathbb{E}[(X-\mu_X)(Y-\mu_Y)].
  \]
\item
  Independence ⇒ zero correlation (for uncorrelated distributions), but
  zero correlation does not imply independence in general.
\item
  In AI: independence assumptions simplify models (Naive Bayes).
  Correlation analysis detects redundant features and spurious
  relationships.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1928}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1928}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.6145}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Role
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Independence & P(X,Y)=P(X)P(Y) & Feature independence in Naive Bayes \\
Covariance & E{[}(X−μX)(Y−μY){]} & Relationship strength \\
Correlation ρ & Cov(X,Y)/(σXσY) & Normalized measure (−1 to 1) \\
Zero correlation & ρ=0 & No linear relation, but not necessarily
independent \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-125}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Example data}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{])}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{10}\NormalTok{])  }\CommentTok{\# perfectly correlated}

\CommentTok{\# Covariance}
\NormalTok{cov }\OperatorTok{=}\NormalTok{ np.cov(X, Y, bias}\OperatorTok{=}\VariableTok{True}\NormalTok{)[}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{]}

\CommentTok{\# Correlation}
\NormalTok{corr }\OperatorTok{=}\NormalTok{ np.corrcoef(X, Y)[}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{]}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Covariance:"}\NormalTok{, cov)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Correlation:"}\NormalTok{, corr)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-23}

Understanding independence allows us to simplify joint distributions and
design tractable probabilistic models. Correlation helps in feature
engineering---removing redundant features or identifying signals.
Misinterpreting correlation as causation can lead to faulty AI
conclusions, so distinguishing the two is critical.

\subsubsection{Try It Yourself}\label{try-it-yourself-125}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  If X = coin toss, Y = die roll, are X and Y independent? Why?
\item
  Compute the correlation between X = {[}1,2,3{]} and Y = {[}3,2,1{]}.
  What does the sign indicate?
\item
  Give an example where two variables have zero correlation but are not
  independent.
\end{enumerate}

\subsection{127. Law of Large Numbers}\label{law-of-large-numbers}

The Law of Large Numbers (LLN) states that as the number of trials
grows, the average of observed outcomes converges to the expected value.
Randomness dominates in the short run, but averages stabilize in the
long run. This principle explains why empirical data approximates true
probabilities.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-126}

Imagine flipping a fair coin. In 10 flips, you might get 7 heads. In
1000 flips, you'll be close to 500 heads. The noise of chance evens out,
and the proportion of heads converges to 0.5. It's like blurry vision
becoming clearer as more data accumulates.

\subsubsection{Deep Dive}\label{deep-dive-126}

\begin{itemize}
\item
  Weak Law of Large Numbers (WLLN): For i.i.d. random variables
  X₁,\ldots,Xₙ with mean μ,

  \[
  \bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i \to μ \quad \text{in probability as } n→∞.
  \]
\item
  Strong Law of Large Numbers (SLLN):

  \[
  \bar{X}_n \to μ \quad \text{almost surely as } n→∞.
  \]
\item
  Conditions: finite expectation μ.
\item
  In AI: LLN underlies empirical risk minimization---training loss
  approximates expected loss as dataset size grows.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1351}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2162}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.6486}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Form
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Convergence Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning in AI
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Weak LLN & In probability & Training error ≈ expected error with enough
data \\
Strong LLN & Almost surely & Guarantees convergence on almost every
sequence \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-126}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Simulate coin flips (Bernoulli trials)}
\NormalTok{n\_trials }\OperatorTok{=} \DecValTok{10000}
\NormalTok{coin\_flips }\OperatorTok{=}\NormalTok{ np.random.binomial(}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{, n\_trials)}

\CommentTok{\# Running averages}
\NormalTok{running\_avg }\OperatorTok{=}\NormalTok{ np.cumsum(coin\_flips) }\OperatorTok{/}\NormalTok{ np.arange(}\DecValTok{1}\NormalTok{, n\_trials}\OperatorTok{+}\DecValTok{1}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Final running average:"}\NormalTok{, running\_avg[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-24}

LLN explains why training on larger datasets improves reliability. It
guarantees that averages of noisy observations approximate true
expectations, making probability-based models feasible. Without LLN,
empirical statistics like mean accuracy or loss would never stabilize.

\subsubsection{Try It Yourself}\label{try-it-yourself-126}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simulate 100 rolls of a fair die and compute the running average. Does
  it approach 3.5?
\item
  Explain how LLN justifies using validation accuracy to estimate
  generalization.
\item
  If a random variable has infinite variance, does the LLN still hold?
\end{enumerate}

\subsection{128. Central Limit Theorem}\label{central-limit-theorem}

The Central Limit Theorem (CLT) states that the distribution of the sum
(or average) of many independent, identically distributed random
variables tends toward a normal distribution, regardless of the original
distribution. This explains why the Gaussian distribution appears so
frequently in statistics and AI.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-127}

Imagine sampling numbers from any strange distribution---uniform,
skewed, even discrete. If you average enough samples, the histogram of
those averages begins to form the familiar bell curve. It's as if nature
smooths out irregularities when many random effects combine.

\subsubsection{Deep Dive}\label{deep-dive-127}

\begin{itemize}
\item
  Statement (simplified): Let X₁,\ldots,Xₙ be i.i.d. with mean μ and
  variance σ². Then

  \[
  \frac{\bar{X}_n - μ}{σ/\sqrt{n}} \to \mathcal{N}(0,1) \quad \text{as } n \to ∞.
  \]
\item
  Requirements: finite mean and variance.
\item
  Generalizations exist for weaker assumptions.
\item
  In AI: CLT justifies approximating distributions with Gaussians,
  motivates confidence intervals, and explains why stochastic gradients
  behave as noisy normal variables.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2264}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4245}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3491}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Application
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Sample mean distribution & (X̄ − μ)/(σ/√n) → N(0,1) & Confidence bounds
on model accuracy \\
Gaussian emergence & Sums/averages of random variables look normal &
Approximation in inference \& learning \\
Variance scaling & Std. error = σ/√n & More data = less uncertainty \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-127}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\CommentTok{\# Draw from uniform distribution}
\NormalTok{samples }\OperatorTok{=}\NormalTok{ np.random.uniform(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, (}\DecValTok{10000}\NormalTok{, }\DecValTok{50}\NormalTok{))  }\CommentTok{\# 50 samples each}
\NormalTok{averages }\OperatorTok{=}\NormalTok{ samples.mean(axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\CommentTok{\# Check mean and std}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Sample mean:"}\NormalTok{, np.mean(averages))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Sample std:"}\NormalTok{, np.std(averages))}

\CommentTok{\# Plot histogram}
\NormalTok{plt.hist(averages, bins}\OperatorTok{=}\DecValTok{30}\NormalTok{, density}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"CLT: Distribution of Averages (Uniform → Gaussian)"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-25}

The CLT explains why Gaussian assumptions are safe in many models, even
if underlying data is not Gaussian. It powers statistical testing,
confidence intervals, and uncertainty estimation. In machine learning,
it justifies treating stochastic gradient noise as Gaussian and
simplifies analysis of large models.

\subsubsection{Try It Yourself}\label{try-it-yourself-127}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simulate 1000 averages of 10 coin tosses (Bernoulli p=0.5). What does
  the histogram look like?
\item
  Explain why the CLT makes the Gaussian central to Bayesian inference.
\item
  How does increasing n (sample size) change the standard error of the
  sample mean?
\end{enumerate}

\subsection{129. Bayes' Theorem and Conditional
Inference}\label{bayes-theorem-and-conditional-inference}

Bayes' Theorem provides a way to update beliefs when new evidence
arrives. It relates prior knowledge, likelihood of data, and posterior
beliefs. This simple formula underpins probabilistic reasoning,
classification, and modern Bayesian machine learning.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-128}

Imagine a medical test for a rare disease. Before testing, you know the
disease is rare (prior). If the test comes back positive (evidence),
Bayes' Theorem updates your belief about whether the person is actually
sick (posterior). It's like recalculating odds every time you learn
something new.

\subsubsection{Deep Dive}\label{deep-dive-128}

\begin{itemize}
\item
  Bayes' Theorem:

  \[
  P(A|B) = \frac{P(B|A)P(A)}{P(B)}.
  \]

  \begin{itemize}
  \tightlist
  \item
    P(A): prior probability of event A.
  \item
    P(B\textbar A): likelihood of evidence given A.
  \item
    P(B): normalizing constant = Σ P(B\textbar Ai)P(Ai).
  \item
    P(A\textbar B): posterior probability after seeing B.
  \end{itemize}
\item
  Odds form:

  \[
  \text{Posterior odds} = \text{Prior odds} \times \text{Likelihood ratio}.
  \]
\item
  In AI:

  \begin{itemize}
  \tightlist
  \item
    Naive Bayes classifiers use conditional independence to simplify
    P(X\textbar Y).
  \item
    Bayesian inference updates model parameters.
  \item
    Probabilistic reasoning systems (e.g., spam filtering, diagnostics).
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0794}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2778}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3016}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3413}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Term
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Prior P(A) & Belief before seeing evidence & Spam rate before checking
email & \\
Likelihood & P(B & A): evidence given hypothesis & Probability email
contains ``free'' if spam \\
Posterior & P(A & B): updated belief after evidence & Probability email
is spam given ``free'' word \\
Normalizer & P(B) ensures probabilities sum to 1 & Adjust for total
frequency of evidence & \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-128}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example: Disease testing}
\NormalTok{P\_disease }\OperatorTok{=} \FloatTok{0.01}
\NormalTok{P\_pos\_given\_disease }\OperatorTok{=} \FloatTok{0.95}
\NormalTok{P\_pos\_given\_no }\OperatorTok{=} \FloatTok{0.05}

\CommentTok{\# Total probability of positive test}
\NormalTok{P\_pos }\OperatorTok{=}\NormalTok{ P\_pos\_given\_disease}\OperatorTok{*}\NormalTok{P\_disease }\OperatorTok{+}\NormalTok{ P\_pos\_given\_no}\OperatorTok{*}\NormalTok{(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{P\_disease)}

\CommentTok{\# Posterior}
\NormalTok{P\_disease\_given\_pos }\OperatorTok{=}\NormalTok{ (P\_pos\_given\_disease}\OperatorTok{*}\NormalTok{P\_disease) }\OperatorTok{/}\NormalTok{ P\_pos}
\BuiltInTok{print}\NormalTok{(}\StringTok{"P(disease | positive test):"}\NormalTok{, P\_disease\_given\_pos)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-26}

Bayes' Theorem is the foundation of probabilistic AI. It explains how
classifiers infer labels from features, how models incorporate
uncertainty, and how predictions adjust with new evidence. Without
Bayes, probabilistic reasoning in AI would be fragmented and incoherent.

\subsubsection{Try It Yourself}\label{try-it-yourself-128}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A spam filter assigns prior P(spam)=0.2. If
  P(``win''\textbar spam)=0.6 and P(``win''\textbar not spam)=0.05,
  compute P(spam\textbar{}``win'').
\item
  Why is P(A\textbar B) ≠ P(B\textbar A)? Give an everyday example.
\item
  Explain how Naive Bayes simplifies computing P(X\textbar Y) in high
  dimensions.
\end{enumerate}

\subsection{130. Probabilistic Models in
AI}\label{probabilistic-models-in-ai}

Probabilistic models describe data and uncertainty using distributions.
They provide structured ways to capture randomness, model dependencies,
and make predictions with confidence levels. These models are central to
AI, where uncertainty is the norm rather than the exception.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-129}

Think of predicting tomorrow's weather. Instead of saying ``It will
rain,'' a probabilistic model says, ``There's a 70\% chance of rain.''
This uncertainty-aware prediction is more realistic. Probabilistic
models act like maps with probabilities attached to each possible
future.

\subsubsection{Deep Dive}\label{deep-dive-129}

\begin{itemize}
\item
  Generative models: learn joint distributions P(X,Y). Example: Naive
  Bayes, Hidden Markov Models, Variational Autoencoders.
\item
  Discriminative models: focus on conditional probability
  P(Y\textbar X). Example: Logistic Regression, Conditional Random
  Fields.
\item
  Graphical models: represent dependencies with graphs. Example:
  Bayesian Networks, Markov Random Fields.
\item
  Probabilistic inference: computing marginals, posteriors, or MAP
  estimates.
\item
  In AI pipelines:

  \begin{itemize}
  \tightlist
  \item
    Uncertainty estimation in predictions.
  \item
    Decision-making under uncertainty.
  \item
    Data generation and simulation.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1628}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2791}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2674}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2907}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Model Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Focus
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Generative & Joint P(X,Y) & Naive Bayes, VAEs & \\
Discriminative & Conditional P(Y & X) & Logistic regression, CRFs \\
Graphical & Structure + dependencies & HMMs, Bayesian networks & \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-129}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.naive\_bayes }\ImportTok{import}\NormalTok{ GaussianNB}

\CommentTok{\# Example: simple Naive Bayes classifier}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\FloatTok{1.8}\NormalTok{, }\DecValTok{80}\NormalTok{], [}\FloatTok{1.6}\NormalTok{, }\DecValTok{60}\NormalTok{], [}\FloatTok{1.7}\NormalTok{, }\DecValTok{65}\NormalTok{], [}\FloatTok{1.5}\NormalTok{, }\DecValTok{50}\NormalTok{]])  }\CommentTok{\# features: height, weight}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{])  }\CommentTok{\# labels: 1=male, 0=female}

\NormalTok{model }\OperatorTok{=}\NormalTok{ GaussianNB()}
\NormalTok{model.fit(X, y)}

\CommentTok{\# Predict probabilities}
\NormalTok{probs }\OperatorTok{=}\NormalTok{ model.predict\_proba([[}\FloatTok{1.7}\NormalTok{, }\DecValTok{70}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Predicted probabilities:"}\NormalTok{, probs)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-27}

Probabilistic models let AI systems express confidence, combine prior
knowledge with new evidence, and reason about incomplete information.
From spam filters to speech recognition and modern generative AI,
probability provides the mathematical backbone for making reliable
predictions.

\subsubsection{Try It Yourself}\label{try-it-yourself-129}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Explain how Naive Bayes assumes independence among features.
\item
  What is the difference between modeling P(X,Y) vs P(Y\textbar X)?
\item
  Describe how a probabilistic model could handle missing data.
\end{enumerate}

\section{Chapter 14. Statistics and
Estimation}\label{chapter-14.-statistics-and-estimation}

\subsection{131. Descriptive Statistics and
Summaries}\label{descriptive-statistics-and-summaries}

Descriptive statistics condense raw data into interpretable summaries.
Instead of staring at thousands of numbers, we reduce them to measures
like mean, median, variance, and quantiles. These summaries highlight
central tendencies, variability, and patterns, making datasets
comprehensible.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-130}

Think of a classroom's exam scores. Instead of listing every score, you
might say, ``The average was 75, most students scored between 70 and 80,
and the highest was 95.'' These summaries give a clear picture without
overwhelming detail.

\subsubsection{Deep Dive}\label{deep-dive-130}

\begin{itemize}
\tightlist
\item
  Measures of central tendency: mean (average), median (middle), mode
  (most frequent).
\item
  Measures of dispersion: range, variance, standard deviation,
  interquartile range.
\item
  Shape descriptors: skewness (asymmetry), kurtosis (tail heaviness).
\item
  Visualization aids: histograms, box plots, summary tables.
\item
  In AI: descriptive stats guide feature engineering, outlier detection,
  and data preprocessing.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1566}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2892}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5542}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Statistic
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula / Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Use Case
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Mean (μ) & (1/n) Σ xi & Baseline average performance \\
Median & Middle value when sorted & Robust measure against outliers \\
Variance (σ²) & (1/n) Σ (xi−μ)² & Spread of feature distributions \\
IQR & Q3 − Q1 & Detecting outliers \\
Skewness & E{[}((X−μ)/σ)³{]} & Identifying asymmetry in feature
distributions \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-130}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ skew, kurtosis}

\NormalTok{data }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{])}

\NormalTok{mean }\OperatorTok{=}\NormalTok{ np.mean(data)}
\NormalTok{median }\OperatorTok{=}\NormalTok{ np.median(data)}
\NormalTok{var }\OperatorTok{=}\NormalTok{ np.var(data)}
\NormalTok{sk }\OperatorTok{=}\NormalTok{ skew(data)}
\NormalTok{kt }\OperatorTok{=}\NormalTok{ kurtosis(data)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Mean:"}\NormalTok{, mean)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Median:"}\NormalTok{, median)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Variance:"}\NormalTok{, var)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Skewness:"}\NormalTok{, sk)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Kurtosis:"}\NormalTok{, kt)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-28}

Before training a model, understanding your dataset is crucial.
Descriptive statistics reveal biases, anomalies, and trends. They are
the first checkpoint in exploratory data analysis (EDA), helping
practitioners avoid errors caused by misunderstood or skewed data.

\subsubsection{Try It Yourself}\label{try-it-yourself-130}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the mean, median, and variance of exam scores: {[}60, 65, 70,
  80, 85, 90, 100{]}.
\item
  Which is more robust to outliers: mean or median? Why?
\item
  Plot a histogram of 1000 random Gaussian samples and describe its
  shape.
\end{enumerate}

\subsection{132. Sampling Distributions}\label{sampling-distributions}

A sampling distribution is the probability distribution of a statistic
(like the mean or variance) computed from repeated random samples of the
same population. It explains how statistics vary from sample to sample
and provides the foundation for statistical inference.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-131}

Imagine repeatedly drawing small groups of students from a university
and calculating their average height. Each group will have a slightly
different average. If you plot all these averages, you'll see a new
distribution---the sampling distribution of the mean.

\subsubsection{Deep Dive}\label{deep-dive-131}

\begin{itemize}
\item
  Statistic vs parameter: parameter = fixed property of population,
  statistic = estimate from sample.
\item
  Sampling distribution: distribution of a statistic across repeated
  samples.
\item
  Key result: the sampling distribution of the sample mean has mean μ
  and variance σ²/n.
\item
  Central Limit Theorem: ensures the sampling distribution of the mean
  approaches normality for large n.
\item
  Standard error (SE): standard deviation of the sampling distribution:

  \[
  SE = \frac{\sigma}{\sqrt{n}}.
  \]
\item
  In AI: sampling distributions explain variability in validation
  accuracy, generalization gaps, and performance metrics.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2079}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3465}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4455}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula / Rule
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Connection
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Sampling distribution & Distribution of statistics & Variability of
model metrics \\
Standard error (SE) & σ/√n & Confidence in accuracy estimates \\
CLT link & Mean sampling distribution ≈ normal & Justifies Gaussian
assumptions in experiments \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-131}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Population: pretend test scores}
\NormalTok{population }\OperatorTok{=}\NormalTok{ np.random.normal(}\DecValTok{70}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{10000}\NormalTok{)}

\CommentTok{\# Draw repeated samples and compute means}
\NormalTok{sample\_means }\OperatorTok{=}\NormalTok{ [np.mean(np.random.choice(population, }\DecValTok{50}\NormalTok{)) }\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1000}\NormalTok{)]}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Mean of sample means:"}\NormalTok{, np.mean(sample\_means))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Std of sample means (SE):"}\NormalTok{, np.std(sample\_means))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-29}

Model evaluation relies on samples of data, not entire populations.
Sampling distributions quantify how much reported metrics (accuracy,
loss) can fluctuate by chance, guiding confidence intervals and
hypothesis tests. They help distinguish true improvements from random
variation.

\subsubsection{Try It Yourself}\label{try-it-yourself-131}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simulate rolling a die 30 times, compute the sample mean, and repeat
  500 times. Plot the distribution of means.
\item
  Explain why the standard error decreases as sample size increases.
\item
  How does the CLT connect sampling distributions to the normal
  distribution?
\end{enumerate}

\subsection{133. Point Estimation and
Properties}\label{point-estimation-and-properties}

Point estimation provides single-value guesses of population parameters
(like mean or variance) from data. Good estimators should be accurate,
stable, and efficient. Properties such as unbiasedness, consistency, and
efficiency define their quality.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-132}

Imagine trying to guess the average height of all students in a school.
You take a sample and compute the sample mean---it's your ``best
guess.'' Sometimes it's too high, sometimes too low, but with enough
data, it hovers around the true average.

\subsubsection{Deep Dive}\label{deep-dive-132}

\begin{itemize}
\item
  Estimator: a rule (function of data) to estimate a parameter θ.
\item
  Point estimate: realized value of the estimator.
\item
  Desirable properties:

  \begin{itemize}
  \tightlist
  \item
    Unbiasedness: E{[}θ̂{]} = θ.
  \item
    Consistency: θ̂ → θ as n→∞.
  \item
    Efficiency: estimator has the smallest variance among unbiased
    estimators.
  \item
    Sufficiency: θ̂ captures all information about θ in the data.
  \end{itemize}
\item
  Examples:

  \begin{itemize}
  \tightlist
  \item
    Sample mean for μ is unbiased and consistent.
  \item
    Sample variance (with denominator n−1) is unbiased for σ².
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1212}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4242}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4545}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Property
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Unbiasedness & E{[}θ̂{]} = θ & Sample mean as unbiased estimator of true
μ \\
Consistency & θ̂ → θ as n→∞ & Validation accuracy converging with data
size \\
Efficiency & Minimum variance among unbiased estimators & MLE often
efficient in large samples \\
Sufficiency & Captures all information about θ & Sufficient statistics
in probabilistic models \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-132}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# True population}
\NormalTok{population }\OperatorTok{=}\NormalTok{ np.random.normal(}\DecValTok{100}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{100000}\NormalTok{)}

\CommentTok{\# Draw sample}
\NormalTok{sample }\OperatorTok{=}\NormalTok{ np.random.choice(population, }\DecValTok{50}\NormalTok{)}

\CommentTok{\# Point estimators}
\NormalTok{mean\_est }\OperatorTok{=}\NormalTok{ np.mean(sample)}
\NormalTok{var\_est }\OperatorTok{=}\NormalTok{ np.var(sample, ddof}\OperatorTok{=}\DecValTok{1}\NormalTok{)  }\CommentTok{\# unbiased variance}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Sample mean (estimator of μ):"}\NormalTok{, mean\_est)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Sample variance (estimator of σ²):"}\NormalTok{, var\_est)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-30}

Point estimation underlies nearly all machine learning parameter
fitting. From estimating regression weights to learning probabilities in
Naive Bayes, we rely on estimators. Knowing their properties ensures our
models don't just fit data but provide reliable generalizations.

\subsubsection{Try It Yourself}\label{try-it-yourself-132}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Show that the sample mean is an unbiased estimator of the population
  mean.
\item
  Why do we divide by (n−1) instead of n when computing sample variance?
\item
  Explain how maximum likelihood estimation is a general framework for
  point estimation.
\end{enumerate}

\subsection{134. Maximum Likelihood Estimation
(MLE)}\label{maximum-likelihood-estimation-mle}

Maximum Likelihood Estimation is a method for finding parameter values
that make the observed data most probable. It transforms learning into
an optimization problem: choose parameters θ that maximize the
likelihood of data under a model.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-133}

Imagine tuning the parameters of a Gaussian curve to fit a histogram of
data. If the curve is too wide or shifted, the probability of observing
the actual data is low. Adjusting until the curve ``hugs'' the data
maximizes the likelihood---it's like aligning a mold to fit scattered
points.

\subsubsection{Deep Dive}\label{deep-dive-133}

\begin{itemize}
\item
  Likelihood function: For data x₁,\ldots,xₙ from distribution
  P(x\textbar θ):

  \[
  L(θ) = \prod_{i=1}^n P(x_i | θ).
  \]
\item
  Log-likelihood (easier to optimize):

  \[
  \ell(θ) = \sum_{i=1}^n \log P(x_i | θ).
  \]
\item
  MLE estimator:

  \[
  \hat{θ}_{MLE} = \arg\max_θ \ell(θ).
  \]
\item
  Properties:

  \begin{itemize}
  \tightlist
  \item
    Consistent: converges to true θ as n→∞.
  \item
    Asymptotically efficient: achieves minimum variance.
  \item
    Invariant: if θ̂ is MLE of θ, then g(θ̂) is MLE of g(θ).
  \end{itemize}
\item
  Example: For Gaussian(μ,σ²), MLE of μ is sample mean, and of σ² is
  (1/n) Σ(xᵢ−μ)².
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1400}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3600}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Connection
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Likelihood & L(θ)=Π P(xᵢ & θ) & Fit parameters to maximize data fit \\
Log-likelihood & ℓ(θ)=Σ log P(xᵢ & θ) & Used in optimization
algorithms \\
Estimator & θ̂=argmax ℓ(θ) & Logistic regression, HMMs, deep nets & \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-133}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ norm}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ minimize}

\CommentTok{\# Sample data}
\NormalTok{data }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{2.3}\NormalTok{, }\FloatTok{2.5}\NormalTok{, }\FloatTok{2.8}\NormalTok{, }\FloatTok{3.0}\NormalTok{, }\FloatTok{3.1}\NormalTok{])}

\CommentTok{\# Negative log{-}likelihood for Gaussian(μ,σ)}
\KeywordTok{def}\NormalTok{ nll(params):}
\NormalTok{    mu, sigma }\OperatorTok{=}\NormalTok{ params}
    \ControlFlowTok{return} \OperatorTok{{-}}\NormalTok{np.}\BuiltInTok{sum}\NormalTok{(norm.logpdf(data, mu, sigma))}

\CommentTok{\# Optimize}
\NormalTok{result }\OperatorTok{=}\NormalTok{ minimize(nll, x0}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{], bounds}\OperatorTok{=}\NormalTok{[(}\VariableTok{None}\NormalTok{,}\VariableTok{None}\NormalTok{),(}\FloatTok{1e{-}6}\NormalTok{,}\VariableTok{None}\NormalTok{)])}
\NormalTok{mu\_mle, sigma\_mle }\OperatorTok{=}\NormalTok{ result.x}

\BuiltInTok{print}\NormalTok{(}\StringTok{"MLE μ:"}\NormalTok{, mu\_mle)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"MLE σ:"}\NormalTok{, sigma\_mle)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-31}

MLE is the foundation of statistical learning. Logistic regression,
Gaussian Mixture Models, and Hidden Markov Models all rely on MLE. Even
deep learning loss functions (like cross-entropy) can be derived from
MLE principles, framing training as maximizing likelihood of observed
labels.

\subsubsection{Try It Yourself}\label{try-it-yourself-133}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Derive the MLE for the Bernoulli parameter p from n coin flips.
\item
  Show that the MLE for μ in a Gaussian is the sample mean.
\item
  Explain why taking the log of the likelihood simplifies optimization.
\end{enumerate}

\subsection{135. Confidence Intervals}\label{confidence-intervals}

A confidence interval (CI) gives a range of plausible values for a
population parameter, based on sample data. Instead of a single point
estimate, it quantifies uncertainty, reflecting how sample variability
affects inference.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-134}

Imagine shooting arrows at a target. A point estimate is one arrow at
the bullseye. A confidence interval is a band around the bullseye,
acknowledging that you might miss a little, but you're likely to land
within the band most of the time.

\subsubsection{Deep Dive}\label{deep-dive-134}

\begin{itemize}
\item
  Definition: A 95\% confidence interval for θ means that if we repeated
  the sampling process many times, about 95\% of such intervals would
  contain the true θ.
\item
  General form:

  \[
  \hat{θ} \pm z_{\alpha/2} \cdot SE(\hat{θ}),
  \]

  where SE = standard error, and z depends on confidence level.
\item
  For mean with known σ:

  \[
  CI = \bar{x} \pm z_{\alpha/2} \frac{σ}{\sqrt{n}}.
  \]
\item
  For mean with unknown σ: use t-distribution.
\item
  In AI: confidence intervals quantify reliability of reported metrics
  like accuracy, precision, or AUC.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2424}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2424}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5152}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Confidence Level
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
z-score (approx)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning in AI results
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
90\% & 1.64 & Narrower interval, less certain \\
95\% & 1.96 & Standard reporting level \\
99\% & 2.58 & Wider interval, stronger certainty \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-134}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ scipy.stats }\ImportTok{as}\NormalTok{ st}

\CommentTok{\# Sample data}
\NormalTok{data }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{2.3}\NormalTok{, }\FloatTok{2.5}\NormalTok{, }\FloatTok{2.8}\NormalTok{, }\FloatTok{3.0}\NormalTok{, }\FloatTok{3.1}\NormalTok{])}
\NormalTok{mean }\OperatorTok{=}\NormalTok{ np.mean(data)}
\NormalTok{sem }\OperatorTok{=}\NormalTok{ st.sem(data)  }\CommentTok{\# standard error}

\CommentTok{\# 95\% CI using t{-}distribution}
\NormalTok{ci }\OperatorTok{=}\NormalTok{ st.t.interval(}\FloatTok{0.95}\NormalTok{, }\BuiltInTok{len}\NormalTok{(data)}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, loc}\OperatorTok{=}\NormalTok{mean, scale}\OperatorTok{=}\NormalTok{sem)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Sample mean:"}\NormalTok{, mean)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"95}\SpecialCharTok{\% c}\StringTok{onfidence interval:"}\NormalTok{, ci)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-32}

Point estimates can be misleading if not accompanied by uncertainty.
Confidence intervals prevent overconfidence, enabling better decisions
in model evaluation and comparison. They ensure we know not just what
our estimate is, but how trustworthy it is.

\subsubsection{Try It Yourself}\label{try-it-yourself-134}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute a 95\% confidence interval for the mean of 100 coin tosses
  (with p=0.5).
\item
  Compare intervals at 90\% and 99\% confidence. Which is wider? Why?
\item
  Explain how confidence intervals help interpret differences between
  two classifiers' accuracies.
\end{enumerate}

\subsection{136. Hypothesis Testing}\label{hypothesis-testing}

Hypothesis testing is a formal procedure for deciding whether data
supports a claim about a population. It pits two competing statements
against each other: the null hypothesis (status quo) and the alternative
hypothesis (the effect or difference we are testing for). Statistical
evidence then determines whether to reject the null.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-135}

Imagine a courtroom. The null hypothesis is the presumption of
innocence. The alternative is the claim of guilt. The jury (our data)
doesn't have to prove guilt with certainty, only beyond a reasonable
doubt (statistical significance). Rejecting the null is like delivering
a guilty verdict.

\subsubsection{Deep Dive}\label{deep-dive-135}

\begin{itemize}
\item
  Null hypothesis (H₀): baseline claim, e.g., μ = μ₀.
\item
  Alternative hypothesis (H₁): competing claim, e.g., μ ≠ μ₀.
\item
  Test statistic: summarizes evidence from sample.
\item
  p-value: probability of seeing data as extreme as observed, if H₀ is
  true.
\item
  Decision rule: reject H₀ if p-value \textless{} α (significance level,
  often 0.05).
\item
  Errors:

  \begin{itemize}
  \tightlist
  \item
    Type I error: rejecting H₀ when true (false positive).
  \item
    Type II error: failing to reject H₀ when false (false negative).
  \end{itemize}
\item
  In AI: hypothesis tests validate model improvements, check feature
  effects, and compare algorithms.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1798}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3708}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4494}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Null (H₀) & Baseline assumption & ``Model A = Model B in accuracy'' \\
Alternative (H₁) & Competing claim & ``Model A \textgreater{} Model
B'' \\
Test statistic & Derived measure (t, z, χ²) & Difference in means
between models \\
p-value & Evidence strength & Probability improvement is due to
chance \\
Type I error & False positive (reject true H₀) & Claiming feature
matters when it doesn't \\
Type II error & False negative (miss true effect) & Overlooking a real
model improvement \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-135}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy }\ImportTok{import}\NormalTok{ stats}

\CommentTok{\# Accuracy of two models on 10 runs}
\NormalTok{model\_a }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.82}\NormalTok{, }\FloatTok{0.81}\NormalTok{, }\FloatTok{0.80}\NormalTok{, }\FloatTok{0.83}\NormalTok{, }\FloatTok{0.82}\NormalTok{, }\FloatTok{0.81}\NormalTok{, }\FloatTok{0.84}\NormalTok{, }\FloatTok{0.83}\NormalTok{, }\FloatTok{0.82}\NormalTok{, }\FloatTok{0.81}\NormalTok{])}
\NormalTok{model\_b }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.79}\NormalTok{, }\FloatTok{0.78}\NormalTok{, }\FloatTok{0.80}\NormalTok{, }\FloatTok{0.77}\NormalTok{, }\FloatTok{0.79}\NormalTok{, }\FloatTok{0.80}\NormalTok{, }\FloatTok{0.78}\NormalTok{, }\FloatTok{0.79}\NormalTok{, }\FloatTok{0.77}\NormalTok{, }\FloatTok{0.78}\NormalTok{])}

\CommentTok{\# Two{-}sample t{-}test}
\NormalTok{t\_stat, p\_val }\OperatorTok{=}\NormalTok{ stats.ttest\_ind(model\_a, model\_b)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"t{-}statistic:"}\NormalTok{, t\_stat, }\StringTok{"p{-}value:"}\NormalTok{, p\_val)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-33}

Hypothesis testing prevents AI practitioners from overclaiming results.
Improvements in accuracy may be due to randomness unless confirmed
statistically. Tests provide a disciplined framework for distinguishing
true effects from noise, ensuring reliable scientific progress.

\subsubsection{Try It Yourself}\label{try-it-yourself-135}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Toss a coin 100 times and test if it's fair (p=0.5).
\item
  Compare two classifiers with accuracies of 0.85 and 0.87 over 20 runs.
  Is the difference significant?
\item
  Explain the difference between Type I and Type II errors in model
  evaluation.
\end{enumerate}

\subsection{137. Bayesian Estimation}\label{bayesian-estimation}

Bayesian estimation updates beliefs about parameters by combining prior
knowledge with observed data. Instead of producing just a single point
estimate, it gives a full posterior distribution, reflecting both what
we assumed before and what the data tells us.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-136}

Imagine guessing the weight of an object. Before weighing, you already
have a prior belief (it's probably around 1 kg). After measuring, you
update that belief to account for the evidence. The result isn't one
number but a refined probability curve centered closer to the truth.

\subsubsection{Deep Dive}\label{deep-dive-136}

\begin{itemize}
\item
  Bayes' theorem for parameters θ:

  \[
  P(θ|D) = \frac{P(D|θ)P(θ)}{P(D)}.
  \]

  \begin{itemize}
  \tightlist
  \item
    Prior P(θ): belief before data.
  \item
    Likelihood P(D\textbar θ): probability of data given θ.
  \item
    Posterior P(θ\textbar D): updated belief after seeing data.
  \end{itemize}
\item
  Point estimates from posterior:

  \begin{itemize}
  \tightlist
  \item
    MAP (Maximum A Posteriori): θ̂ = argmax P(θ\textbar D).
  \item
    Posterior mean: E{[}θ\textbar D{]}.
  \end{itemize}
\item
  Conjugate priors: priors chosen to make posterior distribution same
  family as prior (e.g., Beta prior with Binomial likelihood).
\item
  In AI: Bayesian estimation appears in Naive Bayes, Bayesian neural
  networks, and hierarchical models.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1481}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4691}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3827}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Role
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Prior & Assumptions before data & Belief in feature importance \\
Likelihood & Data fit & Logistic regression likelihood \\
Posterior & Updated distribution & Updated model weights \\
MAP estimate & Most probable parameter after evidence & Regularized
parameter estimates \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-136}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ beta}

\CommentTok{\# Example: coin flips}
\CommentTok{\# Prior: Beta(2,2) \textasciitilde{} uniformish belief}
\NormalTok{prior\_a, prior\_b }\OperatorTok{=} \DecValTok{2}\NormalTok{, }\DecValTok{2}

\CommentTok{\# Data: 7 heads, 3 tails}
\NormalTok{heads, tails }\OperatorTok{=} \DecValTok{7}\NormalTok{, }\DecValTok{3}

\CommentTok{\# Posterior parameters}
\NormalTok{post\_a }\OperatorTok{=}\NormalTok{ prior\_a }\OperatorTok{+}\NormalTok{ heads}
\NormalTok{post\_b }\OperatorTok{=}\NormalTok{ prior\_b }\OperatorTok{+}\NormalTok{ tails}

\CommentTok{\# Posterior distribution}
\NormalTok{posterior }\OperatorTok{=}\NormalTok{ beta(post\_a, post\_b)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Posterior mean:"}\NormalTok{, posterior.mean())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"MAP estimate:"}\NormalTok{, (post\_a }\OperatorTok{{-}} \DecValTok{1}\NormalTok{) }\OperatorTok{/}\NormalTok{ (post\_a }\OperatorTok{+}\NormalTok{ post\_b }\OperatorTok{{-}} \DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-34}

Bayesian estimation provides a principled way to incorporate prior
knowledge, quantify uncertainty, and avoid overfitting. In machine
learning, it enables robust predictions even with small datasets, while
posterior distributions guide decisions under uncertainty.

\subsubsection{Try It Yourself}\label{try-it-yourself-136}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For 5 coin flips with 4 heads, use a Beta(1,1) prior to compute the
  posterior.
\item
  Compare MAP vs posterior mean estimates---when do they differ?
\item
  Explain how Bayesian estimation could help when training data is
  scarce.
\end{enumerate}

\subsection{138. Resampling Methods (Bootstrap,
Jackknife)}\label{resampling-methods-bootstrap-jackknife}

Resampling methods estimate the variability of a statistic by repeatedly
drawing new samples from the observed data. Instead of relying on strict
formulas, they use computation to approximate confidence intervals,
standard errors, and bias.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-137}

Imagine you only have one class of 30 students and their exam scores. To
estimate the variability of the average score, you can ``resample'' from
those 30 scores with replacement many times, creating many
pseudo-classes. The spread of these averages shows how uncertain your
estimate is.

\subsubsection{Deep Dive}\label{deep-dive-137}

\begin{itemize}
\item
  Bootstrap:

  \begin{itemize}
  \tightlist
  \item
    Resample with replacement from the dataset.
  \item
    Compute statistic for each resample.
  \item
    Approximate distribution of statistic across resamples.
  \end{itemize}
\item
  Jackknife:

  \begin{itemize}
  \tightlist
  \item
    Systematically leave one observation out at a time.
  \item
    Compute statistic for each reduced dataset.
  \item
    Useful for bias and variance estimation.
  \end{itemize}
\item
  Advantages: fewer assumptions, works with complex estimators.
\item
  Limitations: computationally expensive, less effective with very small
  datasets.
\item
  In AI: used for model evaluation, confidence intervals of performance
  metrics, and ensemble methods like bagging.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1071}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4167}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4762}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
How It Works
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Use Case
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Bootstrap & Sample with replacement, many times & Confidence intervals
for accuracy or AUC \\
Jackknife & Leave-one-out resampling & Variance estimation for small
datasets \\
Bagging & Bootstrap applied to ML models & Random forests, ensemble
learning \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-137}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{data }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{9}\NormalTok{])}

\CommentTok{\# Bootstrap mean estimates}
\NormalTok{bootstrap\_means }\OperatorTok{=}\NormalTok{ [np.mean(np.random.choice(data, size}\OperatorTok{=}\BuiltInTok{len}\NormalTok{(data), replace}\OperatorTok{=}\VariableTok{True}\NormalTok{))}
                   \ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1000}\NormalTok{)]}

\CommentTok{\# Jackknife mean estimates}
\NormalTok{jackknife\_means }\OperatorTok{=}\NormalTok{ [(np.mean(np.delete(data, i))) }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(data))]}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Bootstrap mean (approx):"}\NormalTok{, np.mean(bootstrap\_means))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Jackknife mean (approx):"}\NormalTok{, np.mean(jackknife\_means))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-35}

Resampling frees us from restrictive assumptions about distributions. In
AI, where data may not follow textbook distributions, resampling methods
provide reliable uncertainty estimates. Bootstrap underlies ensemble
learning, while jackknife gives insights into bias and stability of
estimators.

\subsubsection{Try It Yourself}\label{try-it-yourself-137}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute bootstrap confidence intervals for the median of a dataset.
\item
  Apply the jackknife to estimate the variance of the sample mean for a
  dataset of 20 numbers.
\item
  Explain how bagging in random forests is essentially bootstrap applied
  to decision trees.
\end{enumerate}

\subsection{139. Statistical Significance and
p-Values}\label{statistical-significance-and-p-values}

Statistical significance is a way to decide whether an observed effect
is likely real or just due to random chance. The p-value measures how
extreme the data is under the null hypothesis. A small p-value suggests
the null is unlikely, providing evidence for the alternative.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-138}

Imagine tossing a fair coin. If it lands heads 9 out of 10 times, you'd
be suspicious. The p-value answers: ``If the coin were truly fair, how
likely is it to see a result at least this extreme?'' A very small
probability means the fairness assumption (null) may not hold.

\subsubsection{Deep Dive}\label{deep-dive-138}

\begin{itemize}
\item
  p-value:

  \[
  p = P(\text{data or more extreme} | H_0).
  \]
\item
  Decision rule: Reject H₀ if p \textless{} α (commonly α=0.05).
\item
  Significance level (α): threshold chosen before the test.
\item
  Misinterpretations:

  \begin{itemize}
  \tightlist
  \item
    p ≠ probability that H₀ is true.
  \item
    p ≠ strength of effect size.
  \end{itemize}
\item
  In AI: used in A/B testing, comparing algorithms, and evaluating new
  features.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2330}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3495}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4175}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Term
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Null hypothesis & No effect or difference & ``Model A = Model B in
accuracy'' \\
p-value & Likelihood of observed data under H₀ & Probability new feature
effect is by chance \\
α = 0.05 & 5\% tolerance for false positives & Standard cutoff in ML
experiments \\
Statistical significance & Evidence strong enough to reject H₀ & Model
improvement deemed meaningful \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-138}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy }\ImportTok{import}\NormalTok{ stats}

\CommentTok{\# Two models\textquotesingle{} accuracies across 8 runs}
\NormalTok{model\_a }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.82}\NormalTok{, }\FloatTok{0.81}\NormalTok{, }\FloatTok{0.83}\NormalTok{, }\FloatTok{0.84}\NormalTok{, }\FloatTok{0.82}\NormalTok{, }\FloatTok{0.81}\NormalTok{, }\FloatTok{0.83}\NormalTok{, }\FloatTok{0.82}\NormalTok{])}
\NormalTok{model\_b }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.79}\NormalTok{, }\FloatTok{0.78}\NormalTok{, }\FloatTok{0.80}\NormalTok{, }\FloatTok{0.79}\NormalTok{, }\FloatTok{0.78}\NormalTok{, }\FloatTok{0.80}\NormalTok{, }\FloatTok{0.79}\NormalTok{, }\FloatTok{0.78}\NormalTok{])}

\CommentTok{\# Independent t{-}test}
\NormalTok{t\_stat, p\_val }\OperatorTok{=}\NormalTok{ stats.ttest\_ind(model\_a, model\_b)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"t{-}statistic:"}\NormalTok{, t\_stat)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"p{-}value:"}\NormalTok{, p\_val)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-36}

p-values and significance levels prevent us from overclaiming
improvements. In AI research and production, results must be
statistically significant before rollout. They provide a disciplined way
to guard against randomness being mistaken for progress.

\subsubsection{Try It Yourself}\label{try-it-yourself-138}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Flip a coin 20 times, observe 16 heads. Compute the p-value under H₀:
  fair coin.
\item
  Compare two classifiers with 0.80 vs 0.82 accuracy on 100 samples
  each. Is the difference significant?
\item
  Explain why a very small p-value does not always mean a large or
  important effect.
\end{enumerate}

\subsection{140. Applications in Data-Driven
AI}\label{applications-in-data-driven-ai}

Statistical methods turn raw data into actionable insights in AI. From
estimating parameters to testing hypotheses, they provide the tools for
making decisions under uncertainty. Statistics ensures that models are
not only trained but also validated, interpreted, and trusted.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-139}

Think of building a recommendation system. Descriptive stats summarize
user behavior, sampling distributions explain uncertainty, confidence
intervals quantify reliability, and hypothesis testing checks if a new
algorithm truly improves engagement. Each statistical tool plays a part
in the lifecycle.

\subsubsection{Deep Dive}\label{deep-dive-139}

\begin{itemize}
\tightlist
\item
  Exploratory Data Analysis (EDA): descriptive statistics and
  visualization to understand data.
\item
  Parameter Estimation: point and Bayesian estimators for model
  parameters.
\item
  Uncertainty Quantification: confidence intervals and Bayesian
  posteriors.
\item
  Model Evaluation: hypothesis testing and p-values to compare models.
\item
  Resampling: bootstrap methods to assess variability and support
  ensemble methods.
\item
  Decision-Making: statistical significance guides deployment choices.
\end{itemize}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Statistical Tool & AI Application \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Descriptive stats & Detecting skew, anomalies, data preprocessing \\
Estimation & Parameter fitting in regression, Naive Bayes \\
Confidence intervals & Reliable accuracy reports \\
Hypothesis testing & Validating improvements in A/B testing \\
Resampling & Random forests, bagging, model robustness \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-139}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.utils }\ImportTok{import}\NormalTok{ resample}

\CommentTok{\# Example: bootstrap confidence interval for accuracy}
\NormalTok{accuracies }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.81}\NormalTok{, }\FloatTok{0.82}\NormalTok{, }\FloatTok{0.80}\NormalTok{, }\FloatTok{0.83}\NormalTok{, }\FloatTok{0.81}\NormalTok{, }\FloatTok{0.82}\NormalTok{])}

\NormalTok{boot\_means }\OperatorTok{=}\NormalTok{ [np.mean(resample(accuracies)) }\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1000}\NormalTok{)]}
\NormalTok{ci\_low, ci\_high }\OperatorTok{=}\NormalTok{ np.percentile(boot\_means, [}\FloatTok{2.5}\NormalTok{, }\FloatTok{97.5}\NormalTok{])}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Mean accuracy:"}\NormalTok{, np.mean(accuracies))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"95\% CI:"}\NormalTok{, (ci\_low, ci\_high))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-37}

Without statistics, AI risks overfitting, overclaiming, or
misinterpreting results. Statistical thinking ensures that conclusions
drawn from data are robust, reproducible, and reliable. It turns machine
learning from heuristic curve-fitting into a scientific discipline.

\subsubsection{Try It Yourself}\label{try-it-yourself-139}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use bootstrap to estimate a 95\% confidence interval for model
  precision.
\item
  Explain how hypothesis testing prevents deploying a worse-performing
  model in A/B testing.
\item
  Give an example where descriptive statistics alone could mislead AI
  evaluation without deeper inference.
\end{enumerate}

\section{Chapter 15. Optimization and convex
analysis}\label{chapter-15.-optimization-and-convex-analysis}

\subsection{141. Optimization Problem
Formulation}\label{optimization-problem-formulation}

Optimization is the process of finding the best solution among many
possibilities, guided by an objective function. Formulating a problem in
optimization terms means defining variables to adjust, constraints to
respect, and an objective to minimize or maximize.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-140}

Imagine packing items into a suitcase. The goal is to maximize how much
value you carry while keeping within the weight limit. The items are
variables, the weight restriction is a constraint, and the total value
is the objective. Optimization frames this decision-making precisely.

\subsubsection{Deep Dive}\label{deep-dive-140}

\begin{itemize}
\item
  General form of optimization problem:

  \[
  \min_{x \in \mathbb{R}^n} f(x) \quad \text{subject to } g_i(x) \leq 0, \; h_j(x)=0.
  \]

  \begin{itemize}
  \item
    Objective function f(x): quantity to minimize or maximize.
  \item
    Decision variables x: parameters to choose.
  \item
    Constraints:

    \begin{itemize}
    \tightlist
    \item
      Inequalities gᵢ(x) ≤ 0.
    \item
      Equalities hⱼ(x) = 0.
    \end{itemize}
  \end{itemize}
\item
  Types of optimization problems:

  \begin{itemize}
  \tightlist
  \item
    Unconstrained: no restrictions, e.g.~minimizing f(x)=‖Ax−b‖².
  \item
    Constrained: restrictions present, e.g.~resource allocation.
  \item
    Convex vs non-convex: convex problems are easier, global solutions
    guaranteed.
  \end{itemize}
\item
  In AI: optimization underlies training (loss minimization),
  hyperparameter tuning, and resource scheduling.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1782}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3069}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5149}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Role
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Objective function & Defines what is being optimized & Loss function in
neural network training \\
Variables & Parameters to adjust & Model weights, feature weights \\
Constraints & Rules to satisfy & Fairness, resource limits \\
Convexity & Guarantees easier optimization & Logistic regression
(convex), deep nets (non-convex) \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-140}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ minimize}

\CommentTok{\# Example: unconstrained optimization}
\NormalTok{f }\OperatorTok{=} \KeywordTok{lambda}\NormalTok{ x: (x[}\DecValTok{0}\NormalTok{]}\OperatorTok{{-}}\DecValTok{2}\NormalTok{)}\DecValTok{2} \OperatorTok{+}\NormalTok{ (x[}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\DecValTok{3}\NormalTok{)}\DecValTok{2}  \CommentTok{\# objective function}

\NormalTok{result }\OperatorTok{=}\NormalTok{ minimize(f, x0}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{])  }\CommentTok{\# initial guess}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Optimal solution:"}\NormalTok{, result.x)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Minimum value:"}\NormalTok{, result.fun)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-38}

Every AI model is trained by solving an optimization problem: parameters
are tuned to minimize loss. Understanding how to frame objectives and
constraints transforms vague goals (``make accurate predictions'') into
solvable problems. Without proper formulation, optimization may fail or
produce meaningless results.

\subsubsection{Try It Yourself}\label{try-it-yourself-140}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write the optimization problem for training linear regression with
  squared error loss.
\item
  Formulate logistic regression as a constrained optimization problem.
\item
  Explain why convex optimization problems are more desirable than
  non-convex ones in AI.
\end{enumerate}

\subsection{142. Convex Sets and Convex
Functions}\label{convex-sets-and-convex-functions}

Convexity is the cornerstone of modern optimization. A set is convex if
any line segment between two points in it stays entirely inside. A
function is convex if its epigraph (region above its graph) is convex.
Convex problems are attractive because every local minimum is also a
global minimum.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-141}

Imagine a smooth bowl-shaped surface. Drop a marble anywhere, and it
will roll down to the bottom---the unique global minimum. Contrast this
with a rugged mountain range (non-convex), where marbles can get stuck
in local dips.

\subsubsection{Deep Dive}\label{deep-dive-141}

\begin{itemize}
\item
  Convex set: A set C ⊆ ℝⁿ is convex if ∀ x,y ∈ C and ∀ λ ∈ {[}0,1{]}:

  \[
  λx + (1−λ)y ∈ C.
  \]
\item
  Convex function: f is convex if its domain is convex and ∀ x,y and λ ∈
  {[}0,1{]}:

  \[
  f(λx + (1−λ)y) ≤ λf(x) + (1−λ)f(y).
  \]
\item
  Strict convexity: inequality is strict for x ≠ y.
\item
  Properties:

  \begin{itemize}
  \tightlist
  \item
    Sublevel sets of convex functions are convex.
  \item
    Convex functions have no ``false valleys.''
  \end{itemize}
\item
  In AI: many loss functions (squared error, logistic loss) are convex;
  guarantees on convergence exist for convex optimization.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1798}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4045}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4157}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Convex set & Line segment stays inside & Feasible region in linear
programming \\
Convex function & Weighted average lies above graph & Mean squared error
loss \\
Strict convexity & Unique minimum & Ridge regression objective \\
Non-convex & Many local minima, hard optimization & Deep neural
networks \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-141}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{f\_convex }\OperatorTok{=}\NormalTok{ x2        }\CommentTok{\# convex (bowl)}
\NormalTok{f\_nonconvex }\OperatorTok{=}\NormalTok{ np.sin(x)}\CommentTok{\# non{-}convex (wiggly)}

\NormalTok{plt.plot(x, f\_convex, label}\OperatorTok{=}\StringTok{"Convex: x\^{}2"}\NormalTok{)}
\NormalTok{plt.plot(x, f\_nonconvex, label}\OperatorTok{=}\StringTok{"Non{-}convex: sin(x)"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-39}

Convexity is what makes optimization reliable and efficient. Algorithms
like gradient descent and interior-point methods come with guarantees
for convex problems. Even though deep learning is non-convex, convex
analysis still provides intuition and local approximations that guide
practice.

\subsubsection{Try It Yourself}\label{try-it-yourself-141}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Prove that the set of solutions to Ax ≤ b is convex.
\item
  Show that f(x)=‖x‖² is convex using the definition.
\item
  Give an example of a convex loss function and explain why convexity
  helps optimization.
\end{enumerate}

\subsection{143. Gradient Descent and
Variants}\label{gradient-descent-and-variants}

Gradient descent is an iterative method for minimizing functions. By
following the negative gradient---the direction of steepest descent---we
approach a local (and sometimes global) minimum. Variants improve speed,
stability, and scalability in large-scale machine learning.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-142}

Imagine hiking down a foggy mountain with only a slope detector in your
hand. At each step, you move in the direction that goes downhill the
fastest. If your steps are too small, progress is slow; too big, and you
overshoot the valley. Variants of gradient descent adjust how you step.

\subsubsection{Deep Dive}\label{deep-dive-142}

\begin{itemize}
\item
  Basic gradient descent:

  \[
  x_{k+1} = x_k - η \nabla f(x_k),
  \]

  where η is the learning rate.
\item
  Variants:

  \begin{itemize}
  \tightlist
  \item
    Stochastic Gradient Descent (SGD): uses one sample at a time.
  \item
    Mini-batch GD: compromise between batch and SGD.
  \item
    Momentum: accelerates by remembering past gradients.
  \item
    Adaptive methods (AdaGrad, RMSProp, Adam): scale learning rate per
    parameter.
  \end{itemize}
\item
  Convergence: guaranteed for convex, smooth functions with proper η;
  trickier for non-convex.
\item
  In AI: the default optimizer for training neural networks and many
  statistical models.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1389}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3611}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Update Rule
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Application
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Batch GD & Uses full dataset per step & Small datasets, convex
optimization \\
SGD & One sample per step & Online learning, large-scale ML \\
Mini-batch & Subset of data per step & Neural network training \\
Momentum & Adds velocity term & Faster convergence, less oscillation \\
Adam & Adaptive learning rates & Standard in deep learning \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-142}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Function f(x) = (x{-}3)\^{}2}
\NormalTok{f }\OperatorTok{=} \KeywordTok{lambda}\NormalTok{ x: (x}\OperatorTok{{-}}\DecValTok{3}\NormalTok{)}\DecValTok{2}
\NormalTok{grad }\OperatorTok{=} \KeywordTok{lambda}\NormalTok{ x: }\DecValTok{2}\OperatorTok{*}\NormalTok{(x}\OperatorTok{{-}}\DecValTok{3}\NormalTok{)}

\NormalTok{x }\OperatorTok{=} \FloatTok{0.0}  \CommentTok{\# start point}
\NormalTok{eta }\OperatorTok{=} \FloatTok{0.1}
\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{10}\NormalTok{):}
\NormalTok{    x }\OperatorTok{{-}=}\NormalTok{ eta }\OperatorTok{*}\NormalTok{ grad(x)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"x=}\SpecialCharTok{\{}\NormalTok{x}\SpecialCharTok{:.4f\}}\SpecialStringTok{, f(x)=}\SpecialCharTok{\{}\NormalTok{f(x)}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-40}

Gradient descent is the workhorse of machine learning. Without it,
training models with millions of parameters would be impossible.
Variants like Adam make optimization robust to noisy gradients and poor
scaling, critical in deep learning.

\subsubsection{Try It Yourself}\label{try-it-yourself-142}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run gradient descent on f(x)=x² starting from x=10 with η=0.1. Does it
  converge to 0?
\item
  Compare SGD and batch GD for logistic regression. What are the
  trade-offs?
\item
  Explain why Adam is often chosen as the default optimizer in deep
  learning.
\end{enumerate}

\subsection{144. Constrained Optimization and Lagrange
Multipliers}\label{constrained-optimization-and-lagrange-multipliers}

Constrained optimization extends standard optimization by adding
conditions that the solution must satisfy. Lagrange multipliers
transform constrained problems into unconstrained ones by incorporating
the constraints into the objective, enabling powerful analytical and
computational methods.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-143}

Imagine trying to find the lowest point in a valley, but you're
restricted to walking along a fence. You can't just follow the valley
downward---you must stay on the fence. Lagrange multipliers act like
weights on the constraints, balancing the pull of the objective and the
restrictions.

\subsubsection{Deep Dive}\label{deep-dive-143}

\begin{itemize}
\item
  Problem form:

  \[
  \min f(x) \quad \text{s.t. } g_i(x)=0, \; h_j(x) \leq 0.
  \]
\item
  Lagrangian function:

  \[
  \mathcal{L}(x,λ,μ) = f(x) + \sum_i λ_i g_i(x) + \sum_j μ_j h_j(x),
  \]

  where λ, μ ≥ 0 are multipliers.
\item
  Karush-Kuhn-Tucker (KKT) conditions: generalization of first-order
  conditions for constrained problems.

  \begin{itemize}
  \tightlist
  \item
    Stationarity: ∇f(x*) + Σ λᵢ∇gᵢ(x*) + Σ μⱼ∇hⱼ(x*) = 0.
  \item
    Primal feasibility: constraints satisfied.
  \item
    Dual feasibility: μ ≥ 0.
  \item
    Complementary slackness: μⱼhⱼ(x*) = 0.
  \end{itemize}
\item
  In AI: constraints enforce fairness, resource limits, or structured
  predictions.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1978}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4286}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3736}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Element
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Application
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Lagrangian & Combines objective + constraints & Training with fairness
constraints \\
Multipliers (λ, μ) & Shadow prices: trade-off between goals & Resource
allocation in ML systems \\
KKT conditions & Optimality conditions under constraints & Support
Vector Machines (SVMs) \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-143}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ sympy }\ImportTok{as}\NormalTok{ sp}

\NormalTok{x, y, λ }\OperatorTok{=}\NormalTok{ sp.symbols(}\StringTok{\textquotesingle{}x y λ\textquotesingle{}}\NormalTok{)}
\NormalTok{f }\OperatorTok{=}\NormalTok{ x2 }\OperatorTok{+}\NormalTok{ y2  }\CommentTok{\# objective}
\NormalTok{g }\OperatorTok{=}\NormalTok{ x }\OperatorTok{+}\NormalTok{ y }\OperatorTok{{-}} \DecValTok{1}    \CommentTok{\# constraint}

\CommentTok{\# Lagrangian}
\NormalTok{L }\OperatorTok{=}\NormalTok{ f }\OperatorTok{+}\NormalTok{ λ}\OperatorTok{*}\NormalTok{g}

\CommentTok{\# Solve system: ∂L/∂x = 0, ∂L/∂y = 0, g=0}
\NormalTok{solutions }\OperatorTok{=}\NormalTok{ sp.solve([sp.diff(L, x), sp.diff(L, y), g], [x, y, λ])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Optimal solution:"}\NormalTok{, solutions)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-41}

Most real-world AI problems have constraints: fairness in predictions,
limited memory in deployment, or interpretability requirements. Lagrange
multipliers and KKT conditions give a systematic way to handle such
problems without brute force.

\subsubsection{Try It Yourself}\label{try-it-yourself-143}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Minimize f(x,y) = x² + y² subject to x+y=1. Solve using Lagrange
  multipliers.
\item
  Explain how SVMs use constrained optimization to separate data with a
  margin.
\item
  Give an AI example where inequality constraints are essential.
\end{enumerate}

\subsection{145. Duality in Optimization}\label{duality-in-optimization}

Duality provides an alternative perspective on optimization problems by
transforming them into related ``dual'' problems. The dual often offers
deeper insight, easier computation, or guarantees about the original
(primal) problem. In many cases, solving the dual is equivalent to
solving the primal.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-144}

Think of haggling in a marketplace. The seller wants to maximize profit
(primal problem), while the buyer wants to minimize cost (dual problem).
Their negotiations converge to a price where both objectives
meet---illustrating primal-dual optimality.

\subsubsection{Deep Dive}\label{deep-dive-144}

\begin{itemize}
\item
  Primal problem (general form):

  \[
  \min_x f(x) \quad \text{s.t. } g_i(x) \leq 0, \; h_j(x)=0.
  \]
\item
  Lagrangian:

  \[
  \mathcal{L}(x,λ,μ) = f(x) + \sum_i λ_i g_i(x) + \sum_j μ_j h_j(x).
  \]
\item
  Dual function:

  \[
  q(λ,μ) = \inf_x \mathcal{L}(x,λ,μ).
  \]
\item
  Dual problem:

  \[
  \max_{λ \geq 0, μ} q(λ,μ).
  \]
\item
  Weak duality: dual optimum ≤ primal optimum.
\item
  Strong duality: equality holds under convexity + regularity (Slater's
  condition).
\item
  In AI: duality is central to SVMs, resource allocation, and
  distributed optimization.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1628}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3837}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4535}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Role
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Primal problem & Original optimization goal & Training SVM in feature
space \\
Dual problem & Alternative view with multipliers & Kernel trick applied
in SVM dual form \\
Weak duality & Dual ≤ primal & Bound on objective value \\
Strong duality & Dual = primal (convex problems) & Guarantees optimal
solution equivalence \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-144}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ cvxpy }\ImportTok{as}\NormalTok{ cp}

\CommentTok{\# Primal: minimize x\^{}2 subject to x \textgreater{}= 1}
\NormalTok{x }\OperatorTok{=}\NormalTok{ cp.Variable()}
\NormalTok{objective }\OperatorTok{=}\NormalTok{ cp.Minimize(x2)}
\NormalTok{constraints }\OperatorTok{=}\NormalTok{ [x }\OperatorTok{\textgreater{}=} \DecValTok{1}\NormalTok{]}
\NormalTok{prob }\OperatorTok{=}\NormalTok{ cp.Problem(objective, constraints)}
\NormalTok{primal\_val }\OperatorTok{=}\NormalTok{ prob.solve()}

\CommentTok{\# Dual variables}
\NormalTok{dual\_val }\OperatorTok{=}\NormalTok{ constraints[}\DecValTok{0}\NormalTok{].dual\_value}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Primal optimum:"}\NormalTok{, primal\_val)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Dual variable (λ):"}\NormalTok{, dual\_val)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-42}

Duality gives bounds, simplifies complex problems, and enables
distributed computation. For example, SVM training is usually solved in
the dual because kernels appear naturally there. In large-scale AI, dual
formulations often reduce computational burden.

\subsubsection{Try It Yourself}\label{try-it-yourself-144}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write the dual of the problem: minimize x² subject to x ≥ 1.
\item
  Explain why the kernel trick works naturally in the SVM dual
  formulation.
\item
  Give an example where weak duality holds but strong duality fails.
\end{enumerate}

\subsection{146. Convex Optimization Algorithms (Interior Point,
etc.)}\label{convex-optimization-algorithms-interior-point-etc.}

Convex optimization problems can be solved efficiently with specialized
algorithms that exploit convexity. Unlike generic search, these methods
guarantee convergence to the global optimum. Interior point methods,
gradient-based algorithms, and barrier functions are among the most
powerful tools.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-145}

Imagine navigating a smooth valley bounded by steep cliffs. Instead of
walking along the edge (constraints), interior point methods guide you
smoothly through the interior, avoiding walls but still respecting the
boundaries. Each step moves closer to the lowest point without hitting
constraints head-on.

\subsubsection{Deep Dive}\label{deep-dive-145}

\begin{itemize}
\item
  First-order methods:

  \begin{itemize}
  \tightlist
  \item
    Gradient descent, projected gradient descent.
  \item
    Scalable but may converge slowly.
  \end{itemize}
\item
  Second-order methods:

  \begin{itemize}
  \item
    Newton's method: uses curvature (Hessian).
  \item
    Interior point methods: transform constraints into smooth barrier
    terms.

    \[
    \min f(x) - μ \sum \log(-g_i(x))
    \]

    with μ shrinking → enforces feasibility.
  \end{itemize}
\item
  Complexity: convex optimization can be solved in polynomial time;
  interior point methods are efficient for medium-scale problems.
\item
  Modern solvers: CVX, Gurobi, OSQP.
\item
  In AI: used in SVM training, logistic regression, optimal transport,
  and constrained learning.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1837}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3776}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4388}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Algorithm
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Idea
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Gradient methods & Follow slopes & Large-scale convex problems \\
Newton's method & Use curvature for fast convergence & Logistic
regression \\
Interior point & Barrier functions enforce constraints & Support Vector
Machines, linear programming \\
Projected gradient & Project steps back into feasible set & Constrained
parameter tuning \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-145}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ cvxpy }\ImportTok{as}\NormalTok{ cp}

\CommentTok{\# Example: minimize x\^{}2 + y\^{}2 subject to x+y \textgreater{}= 1}
\NormalTok{x, y }\OperatorTok{=}\NormalTok{ cp.Variable(), cp.Variable()}
\NormalTok{objective }\OperatorTok{=}\NormalTok{ cp.Minimize(x2 }\OperatorTok{+}\NormalTok{ y2)}
\NormalTok{constraints }\OperatorTok{=}\NormalTok{ [x }\OperatorTok{+}\NormalTok{ y }\OperatorTok{\textgreater{}=} \DecValTok{1}\NormalTok{]}
\NormalTok{prob }\OperatorTok{=}\NormalTok{ cp.Problem(objective, constraints)}
\NormalTok{result }\OperatorTok{=}\NormalTok{ prob.solve()}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Optimal x, y:"}\NormalTok{, x.value, y.value)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Optimal value:"}\NormalTok{, result)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-43}

Convex optimization algorithms provide the mathematical backbone of many
classical ML models. They make training provably efficient and
reliable---qualities often lost in non-convex deep learning. Even there,
convex methods appear in components like convex relaxations and
regularized losses.

\subsubsection{Try It Yourself}\label{try-it-yourself-145}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Solve min (x−2)²+(y−1)² subject to x+y=2 using CVX or by hand.
\item
  Explain how barrier functions prevent violating inequality
  constraints.
\item
  Compare gradient descent and interior point methods in terms of
  scalability and accuracy.
\end{enumerate}

\subsection{147. Non-Convex Optimization
Challenges}\label{non-convex-optimization-challenges}

Unlike convex problems, non-convex optimization involves rugged
landscapes with many local minima, saddle points, and flat regions.
Finding the global optimum is often intractable, but practical methods
aim for ``good enough'' solutions that generalize well.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-146}

Think of a hiker navigating a mountain range filled with peaks, valleys,
and plateaus. Unlike a simple bowl-shaped valley (convex), here the
hiker might get trapped in a small dip (local minimum) or wander
aimlessly on a flat ridge (saddle point).

\subsubsection{Deep Dive}\label{deep-dive-146}

\begin{itemize}
\item
  Local minima vs global minimum: Non-convex functions may have many
  local minima; algorithms risk getting stuck.
\item
  Saddle points: places where gradient = 0 but not optimal; common in
  high dimensions.
\item
  Plateaus and flat regions: slow convergence due to vanishing
  gradients.
\item
  No guarantees: non-convex optimization is generally NP-hard.
\item
  Heuristics \& strategies:

  \begin{itemize}
  \tightlist
  \item
    Random restarts, stochasticity (SGD helps escape saddles).
  \item
    Momentum-based methods.
  \item
    Regularization and good initialization.
  \item
    Relaxations to convex problems.
  \end{itemize}
\item
  In AI: deep learning is fundamentally non-convex, yet SGD finds
  solutions that generalize.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1566}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4337}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4096}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Challenge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Explanation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Local minima & Algorithm stuck in suboptimal valley & Training small
neural networks \\
Saddle points & Flat ridges, slow escape & High-dimensional deep nets \\
Flat plateaus & Gradients vanish, slow convergence & Vanishing gradient
problem in RNNs \\
Non-convexity & NP-hard in general & Training deep generative models \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-146}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{400}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{400}\NormalTok{)}
\NormalTok{X, Y }\OperatorTok{=}\NormalTok{ np.meshgrid(x, y)}
\NormalTok{Z }\OperatorTok{=}\NormalTok{ np.sin(X) }\OperatorTok{*}\NormalTok{ np.cos(Y)  }\CommentTok{\# non{-}convex surface}

\NormalTok{plt.contourf(X, Y, Z, levels}\OperatorTok{=}\DecValTok{20}\NormalTok{, cmap}\OperatorTok{=}\StringTok{"RdBu"}\NormalTok{)}
\NormalTok{plt.colorbar()}
\NormalTok{plt.title(}\StringTok{"Non{-}Convex Optimization Landscape"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-44}

Most modern AI models---from deep nets to reinforcement learning---are
trained by solving non-convex problems. Understanding the challenges
helps explain why training may be unstable, why initialization matters,
and why methods like SGD succeed despite theoretical hardness.

\subsubsection{Try It Yourself}\label{try-it-yourself-146}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Plot f(x)=sin(x) for x∈{[}−10,10{]}. Identify local minima and the
  global minimum.
\item
  Explain why SGD can escape saddle points more easily than batch
  gradient descent.
\item
  Give an example of a convex relaxation used to approximate a
  non-convex problem.
\end{enumerate}

\subsection{148. Stochastic Optimization}\label{stochastic-optimization}

Stochastic optimization uses randomness to handle large or uncertain
problems where exact computation is impractical. Instead of evaluating
the full objective, it samples parts of the data or uses noisy
approximations, making it scalable for modern machine learning.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-147}

Imagine trying to find the lowest point in a vast landscape. Checking
every inch is impossible. Instead, you take random walks, each giving a
rough sense of direction. With enough steps, the randomness averages
out, guiding you downhill efficiently.

\subsubsection{Deep Dive}\label{deep-dive-147}

\begin{itemize}
\item
  Stochastic Gradient Descent (SGD):

  \[
  x_{k+1} = x_k - η \nabla f_i(x_k),
  \]

  where gradient is estimated from a random sample i.
\item
  Mini-batch SGD: balances variance reduction and efficiency.
\item
  Variance reduction methods: SVRG, SAG, Adam adapt stochastic updates.
\item
  Monte Carlo optimization: approximates expectations with random
  samples.
\item
  Reinforcement learning: stochastic optimization used in policy
  gradient methods.
\item
  Advantages: scalable, handles noisy data.
\item
  Disadvantages: randomness may slow convergence, requires tuning.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2660}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3830}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3511}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Key Idea
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Application
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
SGD & Update using random sample & Neural network training \\
Mini-batch SGD & Small batch gradient estimate & Standard deep learning
practice \\
Variance reduction (SVRG) & Reduce noise in stochastic gradients &
Faster convergence in ML training \\
Monte Carlo optimization & Approximate expectation via sampling & RL,
generative models \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-147}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Function f(x) = (x{-}3)\^{}2}
\NormalTok{grad }\OperatorTok{=} \KeywordTok{lambda}\NormalTok{ x, i: }\DecValTok{2}\OperatorTok{*}\NormalTok{(x}\OperatorTok{{-}}\DecValTok{3}\NormalTok{) }\OperatorTok{+}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)  }\CommentTok{\# noisy gradient}

\NormalTok{x }\OperatorTok{=} \FloatTok{0.0}
\NormalTok{eta }\OperatorTok{=} \FloatTok{0.1}
\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{10}\NormalTok{):}
\NormalTok{    x }\OperatorTok{{-}=}\NormalTok{ eta }\OperatorTok{*}\NormalTok{ grad(x, \_)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"x=}\SpecialCharTok{\{}\NormalTok{x}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-45}

AI models are trained on massive datasets where exact optimization is
infeasible. Stochastic optimization makes learning tractable by trading
exactness for scalability. It powers deep learning, reinforcement
learning, and online algorithms.

\subsubsection{Try It Yourself}\label{try-it-yourself-147}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compare convergence of batch gradient descent and SGD on a quadratic
  function.
\item
  Explain why adding noise in optimization can help escape local minima.
\item
  Implement mini-batch SGD for logistic regression on a toy dataset.
\end{enumerate}

\subsection{149. Optimization in High
Dimensions}\label{optimization-in-high-dimensions}

High-dimensional optimization is challenging because the geometry of
space changes as dimensions grow. Distances concentrate, gradients may
vanish, and searching the landscape becomes exponentially harder. Yet,
most modern AI models, especially deep neural networks, live in very
high-dimensional spaces.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-148}

Imagine trying to search for a marble in a huge warehouse. In two
dimensions, you can scan rows and columns quickly. In a thousand
dimensions, nearly all points look equally far apart, and the marble
hides in an enormous volume that's impossible to search exhaustively.

\subsubsection{Deep Dive}\label{deep-dive-148}

\begin{itemize}
\item
  Curse of dimensionality: computational cost and data requirements grow
  exponentially with dimension.
\item
  Distance concentration: in high dimensions, distances between points
  become nearly identical, complicating nearest-neighbor methods.
\item
  Gradient issues: gradients can vanish or explode in deep networks.
\item
  Optimization challenges:

  \begin{itemize}
  \tightlist
  \item
    Saddle points become more common than local minima.
  \item
    Flat regions slow convergence.
  \item
    Regularization needed to control overfitting.
  \end{itemize}
\item
  Techniques:

  \begin{itemize}
  \tightlist
  \item
    Dimensionality reduction (PCA, autoencoders).
  \item
    Adaptive learning rates (Adam, RMSProp).
  \item
    Normalization layers (BatchNorm, LayerNorm).
  \item
    Random projections and low-rank approximations.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2421}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3053}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4526}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Challenge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Effect in High Dimensions
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Connection
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Curse of dimensionality & Requires exponential data & Feature
engineering, embeddings \\
Distance concentration & Points look equally far & Vector similarity
search, nearest neighbors \\
Saddle points dominance & Slows optimization & Deep network training \\
Gradient issues & Vanishing/exploding gradients & RNN training, weight
initialization \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-148}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Distance concentration demo}
\NormalTok{d }\OperatorTok{=} \DecValTok{1000}  \CommentTok{\# dimension}
\NormalTok{points }\OperatorTok{=}\NormalTok{ np.random.randn(}\DecValTok{1000}\NormalTok{, d)}

\CommentTok{\# Pairwise distances}
\ImportTok{from}\NormalTok{ scipy.spatial.distance }\ImportTok{import}\NormalTok{ pdist}
\NormalTok{distances }\OperatorTok{=}\NormalTok{ pdist(points, }\StringTok{\textquotesingle{}euclidean\textquotesingle{}}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Mean distance:"}\NormalTok{, np.mean(distances))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Std of distances:"}\NormalTok{, np.std(distances))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-46}

Most AI problems---from embeddings to deep nets---are inherently
high-dimensional. Understanding how optimization behaves in these spaces
explains why naive algorithms fail, why regularization is essential, and
why specialized techniques like normalization and adaptive methods
succeed.

\subsubsection{Try It Yourself}\label{try-it-yourself-148}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simulate distances in 10, 100, and 1000 dimensions. How does the
  variance change?
\item
  Explain why PCA can help optimization in high-dimensional feature
  spaces.
\item
  Give an example where high-dimensional embeddings improve AI
  performance despite optimization challenges.
\end{enumerate}

\subsection{150. Applications in ML
Training}\label{applications-in-ml-training}

Optimization is the engine behind machine learning. Training a model
means defining a loss function and using optimization algorithms to
minimize it with respect to the model's parameters. From linear
regression to deep neural networks, optimization turns data into
predictive power.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-149}

Think of sculpting a statue from a block of marble. The raw block is the
initial model with random parameters. Each optimization step chisels
away error, gradually shaping the model to fit the data.

\subsubsection{Deep Dive}\label{deep-dive-149}

\begin{itemize}
\tightlist
\item
  Linear models: closed-form solutions exist (e.g., least squares), but
  gradient descent is often used for scalability.
\item
  Logistic regression: convex optimization with log-loss.
\item
  Support Vector Machines: quadratic programming solved via dual
  optimization.
\item
  Neural networks: non-convex optimization with SGD and adaptive
  methods.
\item
  Regularization: adds penalties (L1, L2) to the objective, improving
  generalization.
\item
  Hyperparameter optimization: grid search, random search, Bayesian
  optimization.
\item
  Distributed optimization: data-parallel SGD, asynchronous updates for
  large-scale training.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2283}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4130}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3587}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Model/Task
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Optimization Formulation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Algorithm
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Linear regression & Minimize squared error & Gradient descent, closed
form \\
Logistic regression & Minimize log-loss & Newton's method, gradient
descent \\
SVM & Maximize margin, quadratic constraints & Interior point, dual
optimization \\
Neural networks & Minimize cross-entropy or MSE & SGD, Adam, RMSProp \\
Hyperparameter tuning & Black-box optimization & Bayesian
optimization \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-149}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LogisticRegression}

\CommentTok{\# Simple classification with logistic regression}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{],[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{],[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{],[}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{],[}\DecValTok{5}\NormalTok{,}\DecValTok{4}\NormalTok{],[}\DecValTok{6}\NormalTok{,}\DecValTok{5}\NormalTok{]])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{])}

\NormalTok{model }\OperatorTok{=}\NormalTok{ LogisticRegression()}
\NormalTok{model.fit(X, y)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Optimized coefficients:"}\NormalTok{, model.coef\_)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Intercept:"}\NormalTok{, model.intercept\_)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Accuracy:"}\NormalTok{, model.score(X, y))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-47}

Optimization is what makes learning feasible. Without it, models would
remain abstract definitions with no way to adjust parameters from data.
Every breakthrough in AI---from logistic regression to
transformers---relies on advances in optimization techniques.

\subsubsection{Try It Yourself}\label{try-it-yourself-149}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write the optimization objective for linear regression and solve for
  the closed-form solution.
\item
  Explain why SVM training is solved using a dual formulation.
\item
  Compare training with SGD vs Adam on a small neural network---what
  differences do you observe?
\end{enumerate}

\section{Chapter 16. Numerical methods and
stability}\label{chapter-16.-numerical-methods-and-stability}

\subsection{151. Numerical Representation and Rounding
Errors}\label{numerical-representation-and-rounding-errors}

Computers represent numbers with finite precision, which introduces
rounding errors. While small individually, these errors accumulate in
iterative algorithms, sometimes destabilizing optimization or inference.
Numerical analysis studies how to represent and control such errors.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-150}

Imagine pouring water into a cup but spilling a drop each time. One
spill seems negligible, but after thousands of pours, the missing water
adds up. Similarly, tiny rounding errors in floating-point arithmetic
can snowball into significant inaccuracies.

\subsubsection{Deep Dive}\label{deep-dive-150}

\begin{itemize}
\item
  Floating-point representation (IEEE 754): numbers stored with finite
  bits for sign, exponent, and mantissa.
\item
  Machine epsilon (ε): smallest number such that 1+ε \textgreater{} 1 in
  machine precision.
\item
  Types of errors:

  \begin{itemize}
  \tightlist
  \item
    Rounding error: due to truncation of digits.
  \item
    Cancellation: subtracting nearly equal numbers magnifies error.
  \item
    Overflow/underflow: exceeding representable range.
  \end{itemize}
\item
  Stability concerns: iterative methods (like gradient descent) can
  accumulate error.
\item
  Mitigations: scaling, normalization, higher precision, numerically
  stable algorithms.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1856}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4124}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4021}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Issue
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Rounding error & Truncation of decimals & Summing large feature
vectors \\
Cancellation & Loss of significance in subtraction & Variance
computation with large numbers \\
Overflow/underflow & Exceeding float limits & Softmax with very
large/small logits \\
Machine epsilon & Limit of precision (\textasciitilde1e-16 for float64)
& Convergence thresholds in optimization \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-150}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Machine epsilon}
\NormalTok{eps }\OperatorTok{=}\NormalTok{ np.finfo(}\BuiltInTok{float}\NormalTok{).eps}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Machine epsilon:"}\NormalTok{, eps)}

\CommentTok{\# Cancellation example}
\NormalTok{a, b }\OperatorTok{=} \FloatTok{1e16}\NormalTok{, }\FloatTok{1e16} \OperatorTok{+} \DecValTok{1}
\NormalTok{diff1 }\OperatorTok{=}\NormalTok{ b }\OperatorTok{{-}}\NormalTok{ a         }\CommentTok{\# exact difference should be 1}
\NormalTok{diff2 }\OperatorTok{=}\NormalTok{ (b }\OperatorTok{{-}}\NormalTok{ a) }\OperatorTok{+} \DecValTok{1}   \CommentTok{\# accumulation with error}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Cancellation error example:"}\NormalTok{, diff1, diff2)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-48}

AI systems rely on numerical computation at scale. Floating-point
limitations explain instabilities in training (exploding/vanishing
gradients) and motivate techniques like log-sum-exp for stable
probability calculations. Awareness of rounding errors prevents subtle
but serious bugs.

\subsubsection{Try It Yourself}\label{try-it-yourself-150}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute softmax(1000, 1001) directly and with log-sum-exp. Compare
  results.
\item
  Find machine epsilon for float32 and float64 in Python.
\item
  Explain why subtracting nearly equal probabilities can lead to
  unstable results.
\end{enumerate}

\subsection{152. Root-Finding Methods (Newton-Raphson,
Bisection)}\label{root-finding-methods-newton-raphson-bisection}

Root-finding algorithms locate solutions to equations of the form
f(x)=0. These methods are essential for optimization, solving nonlinear
equations, and iterative methods in AI. Different algorithms trade
speed, stability, and reliance on derivatives.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-151}

Imagine standing at a river, looking for the shallowest crossing. You
test different spots: if the water is too deep, move closer to the bank;
if it's shallow, you're near the crossing. Root-finding works the same
way---adjust guesses until the function value crosses zero.

\subsubsection{Deep Dive}\label{deep-dive-151}

\begin{itemize}
\item
  Bisection method:

  \begin{itemize}
  \tightlist
  \item
    Interval-based, guaranteed convergence if f is continuous and sign
    changes on {[}a,b{]}.
  \item
    Update: repeatedly halve the interval.
  \item
    Converges slowly (linear rate).
  \end{itemize}
\item
  Newton-Raphson method:

  \begin{itemize}
  \item
    Iterative update:

    \[
    x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}.
    \]
  \item
    Quadratic convergence if derivative is available and initial guess
    is good.
  \item
    Can diverge if poorly initialized.
  \end{itemize}
\item
  Secant method:

  \begin{itemize}
  \tightlist
  \item
    Approximates derivative numerically.
  \end{itemize}
\item
  In AI: solving logistic regression likelihood equations, computing
  eigenvalues, backpropagation steps.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1647}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1294}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.5059}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Convergence
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Needs derivative?
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Use Case
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Bisection & Linear & No & Robust threshold finding \\
Newton-Raphson & Quadratic & Yes & Logistic regression optimization \\
Secant & Superlinear & Approximate & Parameter estimation when
derivative costly \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-151}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Newton{-}Raphson for sqrt(2)}
\NormalTok{f }\OperatorTok{=} \KeywordTok{lambda}\NormalTok{ x: x2 }\OperatorTok{{-}} \DecValTok{2}
\NormalTok{f\_prime }\OperatorTok{=} \KeywordTok{lambda}\NormalTok{ x: }\DecValTok{2}\OperatorTok{*}\NormalTok{x}

\NormalTok{x }\OperatorTok{=} \FloatTok{1.0}
\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{5}\NormalTok{):}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ x }\OperatorTok{{-}}\NormalTok{ f(x)}\OperatorTok{/}\NormalTok{f\_prime(x)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"Approximation:"}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-49}

Root-finding is a building block for optimization and inference.
Newton's method accelerates convergence in training convex models, while
bisection provides safety when robustness is more important than speed.

\subsubsection{Try It Yourself}\label{try-it-yourself-151}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use bisection to find the root of f(x)=cos(x)−x.
\item
  Derive Newton's method for solving log-likelihood equations in
  logistic regression.
\item
  Compare convergence speed of bisection vs Newton on f(x)=x²−2.
\end{enumerate}

\subsection{153. Numerical Linear Algebra (LU, QR
Decomposition)}\label{numerical-linear-algebra-lu-qr-decomposition}

Numerical linear algebra develops stable and efficient ways to solve
systems of linear equations, factorize matrices, and compute
decompositions. These methods form the computational backbone of
optimization, statistics, and machine learning.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-152}

Imagine trying to solve a puzzle by breaking it into smaller, easier
sub-puzzles. Instead of directly inverting a giant matrix,
decompositions split it into triangular or orthogonal pieces that are
simpler to work with.

\subsubsection{Deep Dive}\label{deep-dive-152}

\begin{itemize}
\item
  LU decomposition:

  \begin{itemize}
  \tightlist
  \item
    Factorizes A into L (lower triangular) and U (upper triangular).
  \item
    Solves Ax=b efficiently by forward + backward substitution.
  \end{itemize}
\item
  QR decomposition:

  \begin{itemize}
  \tightlist
  \item
    Factorizes A into Q (orthogonal) and R (upper triangular).
  \item
    Useful for least-squares problems.
  \end{itemize}
\item
  Cholesky decomposition:

  \begin{itemize}
  \tightlist
  \item
    Special case for symmetric positive definite matrices: A=LLᵀ.
  \end{itemize}
\item
  SVD (Singular Value Decomposition): more general, stable but
  expensive.
\item
  Numerical concerns:

  \begin{itemize}
  \tightlist
  \item
    Pivoting improves stability.
  \item
    Condition number indicates sensitivity to perturbations.
  \end{itemize}
\item
  In AI: used in PCA, linear regression, matrix factorization, spectral
  methods.
\end{itemize}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Decomposition & Form & Use Case in AI \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
LU & A = LU & Solving linear systems \\
QR & A = QR & Least squares, orthogonalization \\
Cholesky & A = LLᵀ & Gaussian processes, covariance matrices \\
SVD & A = UΣVᵀ & Dimensionality reduction, embeddings \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-152}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.linalg }\ImportTok{import}\NormalTok{ lu, qr}

\NormalTok{A }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{], [}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{]])}

\CommentTok{\# LU decomposition}
\NormalTok{P, L, U }\OperatorTok{=}\NormalTok{ lu(A)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"L:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, L)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"U:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, U)}

\CommentTok{\# QR decomposition}
\NormalTok{Q, R }\OperatorTok{=}\NormalTok{ qr(A)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Q:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, Q)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"R:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, R)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-50}

Machine learning workflows rely on efficient linear algebra. From
solving regression equations to training large models, numerical
decompositions provide scalable, stable methods where naive matrix
inversion would fail.

\subsubsection{Try It Yourself}\label{try-it-yourself-152}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Solve Ax=b using LU decomposition for A={[}{[}4,2{]},{[}3,1{]}{]},
  b={[}1,2{]}.
\item
  Explain why QR decomposition is more stable than solving normal
  equations directly in least squares.
\item
  Compute the Cholesky decomposition of a covariance matrix and explain
  its role in Gaussian sampling.
\end{enumerate}

\subsection{154. Iterative Methods for Linear
Systems}\label{iterative-methods-for-linear-systems}

Iterative methods solve large systems of linear equations without
directly factorizing the matrix. Instead, they refine an approximate
solution step by step. These methods are essential when matrices are too
large or sparse for direct approaches like LU or QR.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-153}

Imagine adjusting the volume knob on a radio: you start with a guess,
then keep tuning slightly up or down until the signal comes in clearly.
Iterative solvers do the same---gradually refining estimates until the
solution is ``clear enough.''

\subsubsection{Deep Dive}\label{deep-dive-153}

\begin{itemize}
\item
  Problem: Solve Ax = b, where A is large and sparse.
\item
  Basic iterative methods:

  \begin{itemize}
  \tightlist
  \item
    Jacobi method: update each variable using the previous iteration.
  \item
    Gauss-Seidel method: uses latest updated values for faster
    convergence.
  \item
    Successive Over-Relaxation (SOR): accelerates Gauss-Seidel with
    relaxation factor.
  \end{itemize}
\item
  Krylov subspace methods:

  \begin{itemize}
  \tightlist
  \item
    Conjugate Gradient (CG): efficient for symmetric positive definite
    matrices.
  \item
    GMRES (Generalized Minimal Residual): for general nonsymmetric
    matrices.
  \end{itemize}
\item
  Convergence: depends on matrix properties (diagonal dominance,
  conditioning).
\item
  In AI: used in large-scale optimization, graph algorithms, Gaussian
  processes, and PDE-based models.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1978}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3516}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4505}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Requirement
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Jacobi & Diagonal dominance & Approximate inference in graphical
models \\
Gauss-Seidel & Stronger convergence than Jacobi & Sparse system solvers
in ML pipelines \\
Conjugate Gradient & Symmetric positive definite & Kernel methods,
Gaussian processes \\
GMRES & General sparse systems & Large-scale graph embeddings \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-153}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.sparse.linalg }\ImportTok{import}\NormalTok{ cg}

\CommentTok{\# Example system Ax = b}
\NormalTok{A }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{],[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{]])}
\NormalTok{b }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{])}

\CommentTok{\# Conjugate Gradient}
\NormalTok{x, info }\OperatorTok{=}\NormalTok{ cg(A, b)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Solution:"}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-51}

Iterative solvers scale where direct methods fail. In AI, datasets can
involve millions of variables and sparse matrices. Efficient iterative
algorithms enable training kernel machines, performing inference in
probabilistic models, and solving high-dimensional optimization
problems.

\subsubsection{Try It Yourself}\label{try-it-yourself-153}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Implement the Jacobi method for a 3×3 diagonally dominant system.
\item
  Compare convergence of Jacobi vs Gauss-Seidel on the same system.
\item
  Explain why Conjugate Gradient is preferred for symmetric positive
  definite matrices.
\end{enumerate}

\subsection{155. Numerical Differentiation and
Integration}\label{numerical-differentiation-and-integration}

When analytical solutions are unavailable, numerical methods approximate
derivatives and integrals. Differentiation estimates slopes using nearby
points, while integration approximates areas under curves. These methods
are essential for simulation, optimization, and probabilistic inference.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-154}

Think of measuring the slope of a hill without a formula. You check two
nearby altitudes and estimate the incline. Or, to measure land area, you
cut it into small strips and sum them up. Numerical differentiation and
integration work in the same way.

\subsubsection{Deep Dive}\label{deep-dive-154}

\begin{itemize}
\item
  Numerical differentiation:

  \begin{itemize}
  \item
    Forward difference:

    \[
    f'(x) \approx \frac{f(x+h)-f(x)}{h}.
    \]
  \item
    Central difference (more accurate):

    \[
    f'(x) \approx \frac{f(x+h)-f(x-h)}{2h}.
    \]
  \item
    Trade-off: small h reduces truncation error but increases round-off
    error.
  \end{itemize}
\item
  Numerical integration:

  \begin{itemize}
  \tightlist
  \item
    Rectangle/Trapezoidal rule: approximate area under curve.
  \item
    Simpson's rule: quadratic approximation, higher accuracy.
  \item
    Monte Carlo integration: estimate integral by random sampling,
    useful in high dimensions.
  \end{itemize}
\item
  In AI: used in gradient estimation, reinforcement learning (policy
  gradients), Bayesian inference, and sampling methods.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3152}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4348}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula / Idea
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Application
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Central difference & (f(x+h)-f(x-h))/(2h) & Gradient-free
optimization \\
Trapezoidal rule & Avg height × width & Numerical expectation in small
problems \\
Simpson's rule & Quadratic fit over intervals & Smooth density
integration \\
Monte Carlo integration & Random sampling approximation & Probabilistic
models, Bayesian inference \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-154}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Function}
\NormalTok{f }\OperatorTok{=} \KeywordTok{lambda}\NormalTok{ x: np.sin(x)}

\CommentTok{\# Numerical derivative at x=1}
\NormalTok{h }\OperatorTok{=} \FloatTok{1e{-}5}
\NormalTok{derivative }\OperatorTok{=}\NormalTok{ (f(}\DecValTok{1}\OperatorTok{+}\NormalTok{h) }\OperatorTok{{-}}\NormalTok{ f(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{h)) }\OperatorTok{/}\NormalTok{ (}\DecValTok{2}\OperatorTok{*}\NormalTok{h)}

\CommentTok{\# Numerical integration of sin(x) from 0 to pi}
\NormalTok{xs }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, np.pi, }\DecValTok{1000}\NormalTok{)}
\NormalTok{trapezoid }\OperatorTok{=}\NormalTok{ np.trapz(np.sin(xs), xs)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Derivative of sin at x=1 ≈"}\NormalTok{, derivative)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Integral of sin from 0 to pi ≈"}\NormalTok{, trapezoid)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-52}

Many AI models rely on gradients and expectations where closed forms
don't exist. Numerical differentiation provides approximate gradients,
while Monte Carlo integration handles high-dimensional expectations
central to probabilistic inference and generative modeling.

\subsubsection{Try It Yourself}\label{try-it-yourself-154}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estimate derivative of f(x)=exp(x) at x=0 using central difference.
\item
  Compute ∫₀¹ x² dx numerically with trapezoidal and Simpson's
  rule---compare accuracy.
\item
  Use Monte Carlo to approximate π by integrating the unit circle area.
\end{enumerate}

\subsection{156. Stability and Conditioning of
Problems}\label{stability-and-conditioning-of-problems}

Stability and conditioning describe how sensitive a numerical problem is
to small changes. Conditioning is a property of the problem itself,
while stability concerns the algorithm used to solve it. Together, they
determine whether numerical answers can be trusted.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-155}

Imagine balancing a pencil on its tip. The system (problem) is
ill-conditioned---tiny nudges cause big changes. Now imagine the floor
is also shaky (algorithm instability). Even with a well-posed problem,
an unstable method could still topple your pencil.

\subsubsection{Deep Dive}\label{deep-dive-155}

\begin{itemize}
\item
  Conditioning:

  \begin{itemize}
  \item
    A problem is well-conditioned if small input changes cause small
    output changes.
  \item
    Ill-conditioned if small errors in input cause large deviations in
    output.
  \item
    Condition number (κ):

    \[
    κ(A) = \|A\|\|A^{-1}\|.
    \]

    Large κ ⇒ ill-conditioned.
  \end{itemize}
\item
  Stability:

  \begin{itemize}
  \tightlist
  \item
    An algorithm is stable if it produces nearly correct results for
    nearly correct data.
  \item
    Example: Gaussian elimination with partial pivoting is more stable
    than without pivoting.
  \end{itemize}
\item
  Well-posedness (Hadamard): a problem must have existence, uniqueness,
  and continuous dependence on data.
\item
  In AI: conditioning affects gradient-based training, covariance
  estimation, and inversion of kernel matrices.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1633}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4082}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4286}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Well-conditioned & Small errors → small output change & PCA on
normalized data \\
Ill-conditioned & Small errors → large output change & Inverting
covariance in Gaussian processes \\
Stable algorithm & Doesn't magnify rounding errors & Pivoted LU for
regression problems \\
Unstable algo & Propagates or amplifies numerical errors & Naive
Gaussian elimination \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-155}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Ill{-}conditioned matrix}
\NormalTok{A }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\FloatTok{1.001}\NormalTok{], [}\FloatTok{1.001}\NormalTok{, }\FloatTok{1.002}\NormalTok{]])}
\NormalTok{cond }\OperatorTok{=}\NormalTok{ np.linalg.cond(A)}

\NormalTok{b }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{])}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linalg.solve(A, b)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Condition number:"}\NormalTok{, cond)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Solution:"}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-53}

AI systems often rely on solving large linear systems or optimizing
high-dimensional objectives. Poor conditioning leads to unstable
training (exploding/vanishing gradients). Stable algorithms and
preconditioning improve reliability.

\subsubsection{Try It Yourself}\label{try-it-yourself-155}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute condition numbers of random matrices of size 5×5. Which are
  ill-conditioned?
\item
  Explain why normalization improves conditioning in linear regression.
\item
  Give an AI example where unstable algorithms could cause misleading
  results.
\end{enumerate}

\subsection{157. Floating-Point Arithmetic and
Precision}\label{floating-point-arithmetic-and-precision}

Floating-point arithmetic allows computers to represent real numbers
approximately using a finite number of bits. While flexible, it
introduces rounding and precision issues that can accumulate, affecting
the reliability of numerical algorithms.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-156}

Think of measuring with a ruler that only has centimeter markings. If
you measure something 10 times and add the results, each small rounding
error adds up. Floating-point numbers work similarly---precise enough
for most tasks, but never exact.

\subsubsection{Deep Dive}\label{deep-dive-156}

\begin{itemize}
\item
  IEEE 754 format:

  \begin{itemize}
  \tightlist
  \item
    Single precision (float32): 1 sign bit, 8 exponent bits, 23 fraction
    bits (\textasciitilde7 decimal digits).
  \item
    Double precision (float64): 1 sign bit, 11 exponent bits, 52
    fraction bits (\textasciitilde16 decimal digits).
  \end{itemize}
\item
  Precision limits: machine epsilon ε ≈ 1.19×10⁻⁷ (float32), ≈
  2.22×10⁻¹⁶ (float64).
\item
  Common pitfalls:

  \begin{itemize}
  \tightlist
  \item
    Rounding error in sums/products.
  \item
    Cancellation when subtracting close numbers.
  \item
    Overflow/underflow for very large/small numbers.
  \end{itemize}
\item
  Workarounds:

  \begin{itemize}
  \tightlist
  \item
    Use higher precision if needed.
  \item
    Reorder operations for numerical stability.
  \item
    Apply log transformations for probabilities (log-sum-exp trick).
  \end{itemize}
\item
  In AI: float32 dominates training neural networks; float16 and
  bfloat16 reduce memory and speed up training with some precision
  trade-offs.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1972}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0845}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2113}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.5070}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Precision Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Digits
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Range Approx.
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Usage
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
float16 & \textasciitilde3-4 & 10⁻⁵ to 10⁵ & Mixed precision deep
learning \\
float32 & \textasciitilde7 & 10⁻³⁸ to 10³⁸ & Standard for training \\
float64 & \textasciitilde16 & 10⁻³⁰⁸ to 10³⁰⁸ & Scientific computing,
kernel methods \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-156}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Precision comparison}
\NormalTok{x32 }\OperatorTok{=}\NormalTok{ np.float32(}\FloatTok{1.0}\NormalTok{) }\OperatorTok{+}\NormalTok{ np.float32(}\FloatTok{1e{-}8}\NormalTok{)}
\NormalTok{x64 }\OperatorTok{=}\NormalTok{ np.float64(}\FloatTok{1.0}\NormalTok{) }\OperatorTok{+}\NormalTok{ np.float64(}\FloatTok{1e{-}8}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Float32 result:"}\NormalTok{, x32)  }\CommentTok{\# rounds away}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Float64 result:"}\NormalTok{, x64)  }\CommentTok{\# keeps precision}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-54}

Precision trade-offs influence speed, memory, and stability. Deep
learning thrives on float32/float16 for efficiency, but numerical
algorithms (like kernel methods or Gaussian processes) often require
float64 to avoid instability.

\subsubsection{Try It Yourself}\label{try-it-yourself-156}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add 1e-8 to 1.0 using float32 and float64. What happens?
\item
  Compute softmax({[}1000,1001{]}) with and without log-sum-exp. Compare
  results.
\item
  Explain why mixed precision training works despite reduced numerical
  accuracy.
\end{enumerate}

\subsection{158. Monte Carlo Methods}\label{monte-carlo-methods}

Monte Carlo methods use random sampling to approximate quantities that
are hard to compute exactly. By averaging many random trials, they
estimate integrals, expectations, or probabilities, making them
invaluable in high-dimensional and complex AI problems.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-157}

Imagine trying to measure the area of an irregular pond. Instead of
using formulas, you throw pebbles randomly in a bounding box. The
proportion that lands in the pond estimates its area. Monte Carlo
methods do the same with randomness and computation.

\subsubsection{Deep Dive}\label{deep-dive-157}

\begin{itemize}
\item
  Monte Carlo integration:

  \[
  \int f(x) dx \approx \frac{1}{N}\sum_{i=1}^N f(x_i), \quad x_i \sim p(x).
  \]
\item
  Law of Large Numbers: guarantees convergence as N→∞.
\item
  Variance reduction techniques: importance sampling, stratified
  sampling, control variates.
\item
  Markov Chain Monte Carlo (MCMC): generates samples from complex
  distributions (e.g., Metropolis-Hastings, Gibbs sampling).
\item
  Applications in AI:

  \begin{itemize}
  \tightlist
  \item
    Bayesian inference.
  \item
    Policy evaluation in reinforcement learning.
  \item
    Probabilistic graphical models.
  \item
    Simulation for uncertainty quantification.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1959}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4124}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3918}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Idea
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Plain Monte Carlo & Random uniform sampling & Estimating π or
integrals \\
Importance sampling & Bias sampling toward important regions & Rare
event probability in risk models \\
Stratified sampling & Divide space into strata for efficiency & Variance
reduction in simulation \\
MCMC & Construct Markov chain with target dist. & Bayesian neural
networks, topic models \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-157}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Monte Carlo estimate of pi}
\NormalTok{N }\OperatorTok{=} \DecValTok{100000}
\NormalTok{points }\OperatorTok{=}\NormalTok{ np.random.rand(N, }\DecValTok{2}\NormalTok{)}
\NormalTok{inside }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(points[:,}\DecValTok{0}\NormalTok{]}\DecValTok{2} \OperatorTok{+}\NormalTok{ points[:,}\DecValTok{1}\NormalTok{]}\DecValTok{2} \OperatorTok{\textless{}=} \DecValTok{1}\NormalTok{)}
\NormalTok{pi\_est }\OperatorTok{=} \DecValTok{4} \OperatorTok{*}\NormalTok{ inside }\OperatorTok{/}\NormalTok{ N}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Monte Carlo estimate of pi:"}\NormalTok{, pi\_est)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-55}

Monte Carlo makes the intractable tractable. High-dimensional integrals
appear in Bayesian models, reinforcement learning, and generative AI;
Monte Carlo is often the only feasible tool. It trades exactness for
scalability, a cornerstone of modern probabilistic AI.

\subsubsection{Try It Yourself}\label{try-it-yourself-157}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use Monte Carlo to estimate the integral of f(x)=exp(−x²) from −2 to
  2.
\item
  Implement importance sampling for rare-event probability estimation.
\item
  Run Gibbs sampling for a simple two-variable Gaussian distribution.
\end{enumerate}

\subsection{159. Error Propagation and
Analysis}\label{error-propagation-and-analysis}

Error propagation studies how small inaccuracies in inputs---whether
from measurement, rounding, or approximation---affect outputs of
computations. In numerical methods, understanding how errors accumulate
is essential for ensuring trustworthy results.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-158}

Imagine passing a message along a chain of people. Each person whispers
it slightly differently. By the time it reaches the end, the message may
have drifted far from the original. Computational pipelines behave the
same way---small errors compound through successive operations.

\subsubsection{Deep Dive}\label{deep-dive-158}

\begin{itemize}
\item
  Sources of error:

  \begin{itemize}
  \tightlist
  \item
    Input error: noisy data or imprecise measurements.
  \item
    Truncation error: approximating infinite processes (e.g., Taylor
    series).
  \item
    Rounding error: finite precision arithmetic.
  \end{itemize}
\item
  Error propagation formula (first-order): For y = f(x₁,\ldots,xₙ):

  \[
  \Delta y \approx \sum_{i=1}^n \frac{\partial f}{\partial x_i} \Delta x_i.
  \]
\item
  Condition number link: higher sensitivity ⇒ greater error
  amplification.
\item
  Monte Carlo error analysis: simulate error distributions via sampling.
\item
  In AI: affects stability of optimization, uncertainty in predictions,
  and reliability of simulations.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1739}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3696}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4565}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Error Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Input error & Noisy or approximate measurements & Sensor data for
robotics \\
Truncation error & Approximation cutoff & Numerical gradient
estimation \\
Rounding error & Finite precision representation & Softmax probabilities
in deep learning \\
Propagation & Errors amplify through computation & Long training
pipelines, iterative solvers \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-158}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Function sensitive to input errors}
\NormalTok{f }\OperatorTok{=} \KeywordTok{lambda}\NormalTok{ x: np.exp(x) }\OperatorTok{{-}}\NormalTok{ np.exp(x}\OperatorTok{{-}}\FloatTok{0.00001}\NormalTok{)}

\NormalTok{x\_true }\OperatorTok{=} \DecValTok{10}
\NormalTok{perturbations }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\FloatTok{1e{-}5}\NormalTok{, }\FloatTok{1e{-}5}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ dx }\KeywordTok{in}\NormalTok{ perturbations:}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ f(x\_true }\OperatorTok{+}\NormalTok{ dx)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"x=}\SpecialCharTok{\{}\NormalTok{x\_true}\OperatorTok{+}\NormalTok{dx}\SpecialCharTok{:.8f\}}\SpecialStringTok{, f(x)=}\SpecialCharTok{\{}\NormalTok{y}\SpecialCharTok{:.8e\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-56}

Error propagation explains why some algorithms are stable while others
collapse under noise. In AI, where models rely on massive computations,
unchecked error growth can lead to unreliable predictions, exploding
gradients, or divergence in training.

\subsubsection{Try It Yourself}\label{try-it-yourself-158}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use the propagation formula to estimate error in y = x² when x=1000
  with Δx=0.01.
\item
  Compare numerical and symbolic differentiation for small step
  sizes---observe truncation error.
\item
  Simulate how float32 rounding affects the cumulative sum of 1 million
  random numbers.
\end{enumerate}

\subsection{160. Numerical Methods in AI
Systems}\label{numerical-methods-in-ai-systems}

Numerical methods are the hidden engines inside AI systems, enabling
efficient optimization, stable learning, and scalable inference. From
solving linear systems to approximating integrals, they bridge the gap
between mathematical models and practical computation.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-159}

Think of AI as a skyscraper. The visible structure is the model---neural
networks, decision trees, probabilistic graphs. But the unseen
foundation is numerical methods: without solid algorithms for
computation, the skyscraper would collapse.

\subsubsection{Deep Dive}\label{deep-dive-159}

\begin{itemize}
\tightlist
\item
  Linear algebra methods: matrix factorizations (LU, QR, SVD) for
  regression, PCA, embeddings.
\item
  Optimization algorithms: gradient descent, interior point, stochastic
  optimization for model training.
\item
  Probability and statistics tools: Monte Carlo integration, resampling,
  numerical differentiation for uncertainty estimation.
\item
  Stability and conditioning: ensuring models remain reliable when data
  or computations are noisy.
\item
  Precision management: choosing float16, float32, or float64 depending
  on trade-offs between efficiency and accuracy.
\item
  Scalability: iterative solvers and distributed numerical methods allow
  AI to handle massive datasets.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.3529}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.6471}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Numerical Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Role in AI
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Linear solvers & Regression, covariance estimation \\
Optimization routines & Training neural networks, tuning hyperparams \\
Monte Carlo methods & Bayesian inference, RL simulations \\
Error/stability analysis & Reliable model evaluation \\
Mixed precision & Faster deep learning training \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-159}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.decomposition }\ImportTok{import}\NormalTok{ PCA}

\CommentTok{\# PCA using SVD under the hood (numerical linear algebra)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.random.randn(}\DecValTok{100}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{pca }\OperatorTok{=}\NormalTok{ PCA(n\_components}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{X\_reduced }\OperatorTok{=}\NormalTok{ pca.fit\_transform(X)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Original shape:"}\NormalTok{, X.shape)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Reduced shape:"}\NormalTok{, X\_reduced.shape)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-57}

Without robust numerical methods, AI would be brittle, slow, and
unreliable. Training transformers, running reinforcement learning
simulations, or doing large-scale probabilistic inference all depend on
efficient numerical algorithms that tame complexity.

\subsubsection{Try It Yourself}\label{try-it-yourself-159}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Implement PCA manually using SVD and compare with sklearn's PCA.
\item
  Train a small neural network using float16 and float32---compare speed
  and stability.
\item
  Explain how Monte Carlo integration enables probabilistic inference in
  Bayesian models.
\end{enumerate}

\section{Chapter 17. Information
Theory}\label{chapter-17.-information-theory}

\subsection{161. Entropy and Information
Content}\label{entropy-and-information-content}

Entropy measures the average uncertainty or surprise in a random
variable. Information content quantifies how much ``news'' an event
provides: rare events carry more information than common ones. Together,
they form the foundation of information theory.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-160}

Imagine guessing a number someone is thinking of. If they choose
uniformly between 1 and 1000, each answer feels surprising and
informative. If they always pick 7, there's no surprise---and no
information gained.

\subsubsection{Deep Dive}\label{deep-dive-160}

\begin{itemize}
\item
  Information content (self-information): For event \(x\) with
  probability \(p(x)\),

  \[
  I(x) = -\log p(x)
  \]

  Rare events (low \(p(x)\)) yield higher \(I(x)\).
\item
  Entropy (Shannon entropy): Average information of random variable
  \(X\):

  \[
  H(X) = -\sum_x p(x)\log p(x)
  \]

  \begin{itemize}
  \tightlist
  \item
    Maximum when all outcomes are equally likely.
  \item
    Minimum (0) when outcome is certain.
  \end{itemize}
\item
  Interpretations:

  \begin{itemize}
  \tightlist
  \item
    Average uncertainty.
  \item
    Expected code length in optimal compression.
  \item
    Measure of unpredictability in systems.
  \end{itemize}
\item
  Properties:

  \begin{itemize}
  \tightlist
  \item
    \(H(X) \geq 0\).
  \item
    \(H(X)\) is maximized for uniform distribution.
  \item
    Units: bits (log base 2), nats (log base \(e\)).
  \end{itemize}
\item
  In AI: used in decision trees (information gain), language modeling,
  reinforcement learning, and uncertainty quantification.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2468}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3896}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3636}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Distribution
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Entropy Value
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Interpretation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Certain outcome & \(H=0\) & No uncertainty \\
Fair coin toss & \(H=1\) bit & One bit needed per toss \\
Fair 6-sided die & \(H=\log_2 6 \approx 2.58\) bits & Average surprise
per roll \\
Biased coin (p=0.9) & \(H \approx 0.47\) bits & Less surprise than fair
coin \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-160}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ entropy(probs):}
    \ControlFlowTok{return} \OperatorTok{{-}}\NormalTok{np.}\BuiltInTok{sum}\NormalTok{([p}\OperatorTok{*}\NormalTok{np.log2(p) }\ControlFlowTok{for}\NormalTok{ p }\KeywordTok{in}\NormalTok{ probs }\ControlFlowTok{if}\NormalTok{ p }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{])}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Entropy fair coin:"}\NormalTok{, entropy([}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{]))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Entropy biased coin:"}\NormalTok{, entropy([}\FloatTok{0.9}\NormalTok{, }\FloatTok{0.1}\NormalTok{]))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Entropy fair die:"}\NormalTok{, entropy([}\DecValTok{1}\OperatorTok{/}\DecValTok{6}\NormalTok{]}\OperatorTok{*}\DecValTok{6}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-58}

Entropy provides a universal measure of uncertainty and compressibility.
In AI, it quantifies uncertainty in predictions, guides model training,
and connects probability with coding and decision-making. Without
entropy, concepts like information gain, cross-entropy loss, and
probabilistic learning would lack foundation.

\subsubsection{Try It Yourself}\label{try-it-yourself-160}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute entropy for a dataset where 80\% of labels are ``A'' and 20\%
  are ``B.''
\item
  Compare entropy of a uniform distribution vs a highly skewed one.
\item
  Explain why entropy measures the lower bound of lossless data
  compression.
\end{enumerate}

\subsection{162. Joint and Conditional
Entropy}\label{joint-and-conditional-entropy}

Joint entropy measures the uncertainty of two random variables
considered together. Conditional entropy refines this by asking: given
knowledge of one variable, how much uncertainty remains about the other?
These concepts extend entropy to relationships between variables.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-161}

Imagine rolling two dice. The joint entropy reflects the total
unpredictability of the pair. Now, suppose you already know the result
of the first die---how uncertain are you about the second? That
remaining uncertainty is the conditional entropy.

\subsubsection{Deep Dive}\label{deep-dive-161}

\begin{itemize}
\item
  Joint entropy: For random variables \(X, Y\):

  \[
  H(X, Y) = -\sum_{x,y} p(x,y) \log p(x,y)
  \]

  \begin{itemize}
  \tightlist
  \item
    Captures combined uncertainty of both variables.
  \end{itemize}
\item
  Conditional entropy: Uncertainty in \(Y\) given \(X\):

  \[
  H(Y \mid X) = -\sum_{x,y} p(x,y) \log p(y \mid x)
  \]

  \begin{itemize}
  \tightlist
  \item
    Measures average uncertainty left in \(Y\) once \(X\) is known.
  \end{itemize}
\item
  Relationships:

  \begin{itemize}
  \tightlist
  \item
    Chain rule: \(H(X, Y) = H(X) + H(Y \mid X)\).
  \item
    Symmetry: \(H(X, Y) = H(Y, X)\).
  \end{itemize}
\item
  Properties:

  \begin{itemize}
  \tightlist
  \item
    \(H(Y \mid X) \leq H(Y)\).
  \item
    Equality if \(X\) and \(Y\) are independent.
  \end{itemize}
\item
  In AI:

  \begin{itemize}
  \tightlist
  \item
    Joint entropy: modeling uncertainty across features.
  \item
    Conditional entropy: decision trees (information gain),
    communication efficiency, Bayesian networks.
  \end{itemize}
\end{itemize}

\subsubsection{Tiny Code}\label{tiny-code-161}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Example joint distribution for X,Y (binary variables)}
\NormalTok{p }\OperatorTok{=}\NormalTok{ np.array([[}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.25}\NormalTok{],}
\NormalTok{              [}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.25}\NormalTok{]])  }\CommentTok{\# independent uniform}

\KeywordTok{def}\NormalTok{ entropy(probs):}
    \ControlFlowTok{return} \OperatorTok{{-}}\NormalTok{np.}\BuiltInTok{sum}\NormalTok{([p}\OperatorTok{*}\NormalTok{np.log2(p) }\ControlFlowTok{for}\NormalTok{ p }\KeywordTok{in}\NormalTok{ probs.flatten() }\ControlFlowTok{if}\NormalTok{ p }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{])}

\KeywordTok{def}\NormalTok{ joint\_entropy(p):}
    \ControlFlowTok{return}\NormalTok{ entropy(p)}

\KeywordTok{def}\NormalTok{ conditional\_entropy(p):}
\NormalTok{    H }\OperatorTok{=} \DecValTok{0}
\NormalTok{    row\_sums }\OperatorTok{=}\NormalTok{ p.}\BuiltInTok{sum}\NormalTok{(axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(row\_sums)):}
        \ControlFlowTok{if}\NormalTok{ row\_sums[i] }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{:}
\NormalTok{            cond\_probs }\OperatorTok{=}\NormalTok{ p[i]}\OperatorTok{/}\NormalTok{row\_sums[i]}
\NormalTok{            H }\OperatorTok{+=}\NormalTok{ row\_sums[i] }\OperatorTok{*}\NormalTok{ entropy(cond\_probs)}
    \ControlFlowTok{return}\NormalTok{ H}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Joint entropy:"}\NormalTok{, joint\_entropy(p))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Conditional entropy H(Y|X):"}\NormalTok{, conditional\_entropy(p))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-59}

Joint and conditional entropy extend uncertainty beyond single
variables, capturing relationships and dependencies. They underpin
information gain in machine learning, compression schemes, and
probabilistic reasoning frameworks like Bayesian networks.

\subsubsection{Try It Yourself}\label{try-it-yourself-161}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calculate joint entropy for two independent coin tosses.
\item
  Compute conditional entropy for a biased coin where you're told
  whether the outcome is heads.
\item
  Explain why \(H(Y|X)=0\) when \(Y\) is a deterministic function of
  \(X\).
\end{enumerate}

\subsection{163. Mutual Information}\label{mutual-information}

Mutual information (MI) quantifies how much knowing one random variable
reduces uncertainty about another. It measures dependence: if two
variables are independent, their mutual information is zero; if
perfectly correlated, MI is maximized.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-162}

Think of two overlapping circles representing uncertainty about
variables \(X\) and \(Y\). The overlap region is the mutual
information---it's the shared knowledge between the two.

\subsubsection{Deep Dive}\label{deep-dive-162}

\begin{itemize}
\item
  Definition:

  \[
  I(X;Y) = \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}
  \]
\item
  Equivalent forms:

  \[
  I(X;Y) = H(X) + H(Y) - H(X,Y)
  \]

  \[
  I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
  \]
\item
  Properties:

  \begin{itemize}
  \tightlist
  \item
    Always nonnegative.
  \item
    Symmetric: \(I(X;Y) = I(Y;X)\).
  \item
    Zero iff \(X\) and \(Y\) are independent.
  \end{itemize}
\item
  Interpretation:

  \begin{itemize}
  \tightlist
  \item
    Reduction in uncertainty about one variable given the other.
  \item
    Shared information content.
  \end{itemize}
\item
  In AI:

  \begin{itemize}
  \tightlist
  \item
    Feature selection: pick features with high MI with labels.
  \item
    Clustering: measure similarity between variables.
  \item
    Representation learning: InfoNCE loss, variational bounds on MI.
  \item
    Communication: efficiency of transmitting signals.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Expression & Interpretation \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(I(X;Y)=0\) & X and Y are independent \\
Large \(I(X;Y)\) & Strong dependence between X and Y \\
\(I(X;Y)=H(X)\) & X completely determined by Y \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-162}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ mutual\_info\_score}

\CommentTok{\# Example joint distribution: correlated binary variables}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.random.binomial(}\DecValTok{1}\NormalTok{, }\FloatTok{0.7}\NormalTok{, size}\OperatorTok{=}\DecValTok{1000}\NormalTok{)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ X }\OperatorTok{\^{}}\NormalTok{ np.random.binomial(}\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{, size}\OperatorTok{=}\DecValTok{1000}\NormalTok{)  }\CommentTok{\# noisy copy of X}

\NormalTok{mi }\OperatorTok{=}\NormalTok{ mutual\_info\_score(X, Y)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Mutual Information:"}\NormalTok{, mi)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-60}

Mutual information generalizes correlation to capture both linear and
nonlinear dependencies. In AI, it guides feature selection, helps design
efficient encodings, and powers modern unsupervised and self-supervised
learning methods.

\subsubsection{Try It Yourself}\label{try-it-yourself-162}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute MI between two independent coin tosses---why is it zero?
\item
  Compute MI between a variable and its noisy copy---how does noise
  affect the value?
\item
  Explain how maximizing mutual information can improve learned
  representations.
\end{enumerate}

\subsection{164. Kullback--Leibler
Divergence}\label{kullbackleibler-divergence}

Kullback--Leibler (KL) divergence measures how one probability
distribution diverges from another. It quantifies the inefficiency of
assuming distribution \(Q\) when the true distribution is \(P\).

\subsubsection{Picture in Your Head}\label{picture-in-your-head-163}

Imagine packing luggage with the wrong-sized suitcases. If you assume
people pack small items (distribution \(Q\)), but in reality, they bring
bulky clothes (distribution \(P\)), you'll waste space or run out of
room. KL divergence measures that mismatch.

\subsubsection{Deep Dive}\label{deep-dive-163}

\begin{itemize}
\item
  Definition: For discrete distributions \(P\) and \(Q\):

  \[
  D_{KL}(P \parallel Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
  \]

  For continuous:

  \[
  D_{KL}(P \parallel Q) = \int p(x) \log \frac{p(x)}{q(x)} dx
  \]
\item
  Properties:

  \begin{itemize}
  \tightlist
  \item
    \(D_{KL}(P \parallel Q) \geq 0\) (Gibbs inequality).
  \item
    Asymmetric: \(D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P)\).
  \item
    Zero iff \(P=Q\) almost everywhere.
  \end{itemize}
\item
  Interpretations:

  \begin{itemize}
  \tightlist
  \item
    Extra bits required when coding samples from \(P\) using code
    optimized for \(Q\).
  \item
    Measure of distance (though not a true metric).
  \end{itemize}
\item
  In AI:

  \begin{itemize}
  \tightlist
  \item
    Variational inference (ELBO minimization).
  \item
    Regularizer in VAEs (match approximate posterior to prior).
  \item
    Policy optimization in RL (trust region methods).
  \item
    Comparing probability models.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.3718}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.6282}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Expression
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(D_{KL}(P \parallel Q)=0\) & Perfect match between P and Q \\
Large \(D_{KL}(P \parallel Q)\) & Q is a poor approximation of P \\
Asymmetry & Forward vs reverse KL lead to different behaviors \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-163}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ entropy}

\NormalTok{P }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{])       }\CommentTok{\# True distribution}
\NormalTok{Q }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.9}\NormalTok{, }\FloatTok{0.1}\NormalTok{])       }\CommentTok{\# Approximate distribution}

\NormalTok{kl }\OperatorTok{=}\NormalTok{ entropy(P, Q)  }\CommentTok{\# KL(P||Q)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"KL Divergence:"}\NormalTok{, kl)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-61}

KL divergence underpins much of probabilistic AI, from Bayesian
inference to deep generative models. It provides a bridge between
probability theory, coding theory, and optimization. Understanding it is
key to modern machine learning.

\subsubsection{Try It Yourself}\label{try-it-yourself-163}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute KL divergence between two biased coins (e.g., P={[}0.6,0.4{]},
  Q={[}0.5,0.5{]}).
\item
  Compare forward KL (P\textbar\textbar Q) and reverse KL
  (Q\textbar\textbar P). Which penalizes mode-covering vs mode-seeking?
\item
  Explain how KL divergence is used in training variational
  autoencoders.
\end{enumerate}

\subsection{165. Cross-Entropy and
Likelihood}\label{cross-entropy-and-likelihood}

Cross-entropy measures the average number of bits needed to encode
events from a true distribution \(P\) using a model distribution \(Q\).
It is directly related to likelihood: minimizing cross-entropy is
equivalent to maximizing the likelihood of the model given the data.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-164}

Imagine trying to compress text with a code designed for English, but
your text is actually in French. The mismatch wastes space.
Cross-entropy quantifies that inefficiency, and likelihood measures how
well your model explains the observed text.

\subsubsection{Deep Dive}\label{deep-dive-164}

\begin{itemize}
\item
  Cross-entropy definition:

  \[
  H(P, Q) = - \sum_x P(x) \log Q(x)
  \]

  \begin{itemize}
  \item
    Equals entropy \(H(P)\) plus KL divergence:

    \[
    H(P, Q) = H(P) + D_{KL}(P \parallel Q)
    \]
  \end{itemize}
\item
  Maximum likelihood connection:

  \begin{itemize}
  \item
    Given samples \(\{x_i\}\), maximizing likelihood

    \[
    \hat{\theta} = \arg\max_\theta \prod_i Q(x_i;\theta)
    \]

    is equivalent to minimizing cross-entropy between empirical
    distribution and model.
  \end{itemize}
\item
  Loss functions in AI:

  \begin{itemize}
  \item
    Binary cross-entropy:

    \[
    L = -[y \log \hat{y} + (1-y)\log(1-\hat{y})]
    \]
  \item
    Categorical cross-entropy:

    \[
    L = -\sum_{k} y_k \log \hat{y}_k
    \]
  \end{itemize}
\item
  Applications:

  \begin{itemize}
  \tightlist
  \item
    Classification tasks (logistic regression, neural networks).
  \item
    Language modeling (predicting next token).
  \item
    Probabilistic forecasting.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2178}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3762}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4059}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Use Case
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Cross-entropy \(H(P,Q)\) & \(-\sum P(x)\log Q(x)\) & Model evaluation
and training \\
Relation to KL & \(H(P,Q) = H(P) + D_{KL}(P\parallel Q)\) & Shows
inefficiency when using wrong model \\
Likelihood & Product of probabilities under model & Basis of parameter
estimation \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-164}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ log\_loss}

\CommentTok{\# True labels and predicted probabilities}
\NormalTok{y\_true }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{]}
\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ [}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.9}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{]}

\CommentTok{\# Binary cross{-}entropy}
\NormalTok{loss }\OperatorTok{=}\NormalTok{ log\_loss(y\_true, y\_pred)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Cross{-}Entropy Loss:"}\NormalTok{, loss)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-62}

Cross-entropy ties together coding theory and statistical learning. It
is the standard loss function for classification because minimizing it
maximizes likelihood, ensuring the model aligns as closely as possible
with the true data distribution.

\subsubsection{Try It Yourself}\label{try-it-yourself-164}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute cross-entropy for a biased coin with true p=0.7 but model
  q=0.5.
\item
  Show how minimizing cross-entropy improves a classifier's predictions.
\item
  Explain why cross-entropy is preferred over mean squared error for
  probability outputs.
\end{enumerate}

\subsection{166. Channel Capacity and Coding
Theorems}\label{channel-capacity-and-coding-theorems}

Channel capacity is the maximum rate at which information can be
reliably transmitted over a noisy communication channel. Coding theorems
guarantee that, with clever encoding, we can approach this limit while
keeping the error probability arbitrarily small.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-165}

Imagine trying to talk to a friend across a noisy café. If you speak too
fast, they'll miss words. But if you speak at or below a certain
pace---the channel capacity---they'll catch everything with the right
decoding strategy.

\subsubsection{Deep Dive}\label{deep-dive-165}

\begin{itemize}
\item
  Channel capacity:

  \begin{itemize}
  \item
    Defined as the maximum mutual information between input \(X\) and
    output \(Y\):

    \[
    C = \max_{p(x)} I(X;Y)
    \]
  \item
    Represents highest achievable communication rate (bits per channel
    use).
  \end{itemize}
\item
  Shannon's Channel Coding Theorem:

  \begin{itemize}
  \tightlist
  \item
    If rate \(R < C\), there exist coding schemes with error probability
    → 0 as block length grows.
  \item
    If \(R > C\), reliable communication is impossible.
  \end{itemize}
\item
  Types of channels:

  \begin{itemize}
  \tightlist
  \item
    Binary symmetric channel (BSC): flips bits with probability \(p\).
  \item
    Binary erasure channel (BEC): deletes bits with probability \(p\).
  \item
    Gaussian channel: continuous noise added to signal.
  \end{itemize}
\item
  Coding schemes:

  \begin{itemize}
  \tightlist
  \item
    Error-correcting codes: Hamming codes, Reed--Solomon, LDPC, Turbo,
    Polar codes.
  \item
    Trade-off between redundancy, efficiency, and error correction.
  \end{itemize}
\item
  In AI:

  \begin{itemize}
  \tightlist
  \item
    Inspiration for regularization (information bottleneck).
  \item
    Understanding data transmission in distributed learning.
  \item
    Analogies for generalization and noise robustness.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2895}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4079}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3026}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Channel Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Capacity Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Use
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Binary Symmetric (BSC) & \(C = 1 - H(p)\) & Noisy bit transmission \\
Binary Erasure (BEC) & \(C = 1 - p\) & Packet loss in networks \\
Gaussian & \(C = \tfrac{1}{2}\log_2(1+SNR)\) & Wireless
communications \\
\end{longtable}

Tiny Code Sample (Python, simulate BSC capacity)

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ log2}

\KeywordTok{def}\NormalTok{ binary\_entropy(p):}
    \ControlFlowTok{if}\NormalTok{ p }\OperatorTok{==} \DecValTok{0} \KeywordTok{or}\NormalTok{ p }\OperatorTok{==} \DecValTok{1}\NormalTok{: }\ControlFlowTok{return} \DecValTok{0}
    \ControlFlowTok{return} \OperatorTok{{-}}\NormalTok{p}\OperatorTok{*}\NormalTok{log2(p) }\OperatorTok{{-}}\NormalTok{ (}\DecValTok{1}\OperatorTok{{-}}\NormalTok{p)}\OperatorTok{*}\NormalTok{log2(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{p)}

\CommentTok{\# Capacity of Binary Symmetric Channel}
\NormalTok{p }\OperatorTok{=} \FloatTok{0.1}  \CommentTok{\# bit flip probability}
\NormalTok{C }\OperatorTok{=} \DecValTok{1} \OperatorTok{{-}}\NormalTok{ binary\_entropy(p)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"BSC Capacity:"}\NormalTok{, C, }\StringTok{"bits per channel use"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-63}

Channel capacity sets a fundamental limit: no algorithm can surpass it.
The coding theorems show how close we can get, forming the backbone of
digital communication. In AI, these ideas echo in information
bottlenecks, compression, and error-tolerant learning systems.

\subsubsection{Try It Yourself}\label{try-it-yourself-165}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute capacity of a BSC with error probability \(p=0.2\).
\item
  Compare capacity of a Gaussian channel with SNR = 10 dB and 20 dB.
\item
  Explain how redundancy in coding relates to regularization in machine
  learning.
\end{enumerate}

\subsection{167. Rate--Distortion Theory}\label{ratedistortion-theory}

Rate--distortion theory studies the trade-off between compression rate
(how many bits you use) and distortion (how much information is lost).
It answers: what is the minimum number of bits per symbol required to
represent data within a given tolerance of error?

\subsubsection{Picture in Your Head}\label{picture-in-your-head-166}

Imagine saving a photo. If you compress it heavily, the file is small
but blurry. If you save it losslessly, the file is large but perfect.
Rate--distortion theory formalizes this compromise between size and
quality.

\subsubsection{Deep Dive}\label{deep-dive-166}

\begin{itemize}
\item
  Distortion measure: Quantifies error between original \(x\) and
  reconstruction \(\hat{x}\). Example: mean squared error (MSE), Hamming
  distance.
\item
  Rate--distortion function: Minimum rate needed for distortion \(D\):

  \[
  R(D) = \min_{p(\hat{x}|x): E[d(x,\hat{x})] \leq D} I(X;\hat{X})
  \]
\item
  Interpretations:

  \begin{itemize}
  \tightlist
  \item
    At \(D=0\): \(R(D)=H(X)\) (lossless compression).
  \item
    As \(D\) increases, fewer bits are needed.
  \end{itemize}
\item
  Shannon's Rate--Distortion Theorem:

  \begin{itemize}
  \tightlist
  \item
    Provides theoretical lower bound on compression efficiency.
  \end{itemize}
\item
  Applications in AI:

  \begin{itemize}
  \tightlist
  \item
    Image/audio compression (JPEG, MP3).
  \item
    Variational autoencoders (ELBO resembles rate--distortion
    trade-off).
  \item
    Information bottleneck method (trade-off between relevance and
    compression).
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2286}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3143}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4571}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Distortion Level
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Bits per Symbol (Rate)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in Practice
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 (perfect) & \(H(X)\) & Lossless compression (PNG, FLAC) \\
Low & Slightly \textless{} \(H(X)\) & High-quality JPEG \\
High & Much smaller & Aggressive lossy compression \\
\end{longtable}

Tiny Code Sample (Python, toy rate--distortion curve)

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{D }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{50}\NormalTok{)  }\CommentTok{\# distortion}
\NormalTok{R }\OperatorTok{=}\NormalTok{ np.maximum(}\DecValTok{0}\NormalTok{, }\DecValTok{1} \OperatorTok{{-}}\NormalTok{ D)   }\CommentTok{\# toy linear approx for illustration}

\NormalTok{plt.plot(D, R)}
\NormalTok{plt.xlabel(}\StringTok{"Distortion"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Rate (bits/symbol)"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Toy Rate–Distortion Trade{-}off"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-64}

Rate--distortion theory reveals the limits of lossy compression: how
much data can be removed without exceeding a distortion threshold. In
AI, it inspires representation learning methods that balance
expressiveness with efficiency.

\subsubsection{Try It Yourself}\label{try-it-yourself-166}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the rate--distortion function for a binary source with Hamming
  distortion.
\item
  Compare distortion tolerance in JPEG vs PNG for the same image.
\item
  Explain how rate--distortion ideas appear in the variational
  autoencoder objective.
\end{enumerate}

\subsection{168. Information Bottleneck
Principle}\label{information-bottleneck-principle}

The Information Bottleneck (IB) principle describes how to extract the
most relevant information from an input while compressing away
irrelevant details. It formalizes learning as balancing two goals:
retain information about the target variable while discarding noise.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-167}

Imagine squeezing water through a filter. The wide stream of input data
passes through a narrow bottleneck that only lets essential drops
through---enough to reconstruct what matters, but not every detail.

\subsubsection{Deep Dive}\label{deep-dive-167}

\begin{itemize}
\item
  Formal objective: Given input \(X\) and target \(Y\), find compressed
  representation \(T\):

  \[
  \min I(X;T) - \beta I(T;Y)
  \]

  \begin{itemize}
  \tightlist
  \item
    \(I(X;T)\): how much input information is kept.
  \item
    \(I(T;Y)\): how useful the representation is for predicting \(Y\).
  \item
    \(\beta\): trade-off parameter between compression and relevance.
  \end{itemize}
\item
  Connections:

  \begin{itemize}
  \tightlist
  \item
    At \(\beta=0\): keep all information (\(T=X\)).
  \item
    Large \(\beta\): compress aggressively, retain only predictive
    parts.
  \item
    Related to rate--distortion theory with ``distortion'' defined by
    prediction error.
  \end{itemize}
\item
  In AI:

  \begin{itemize}
  \tightlist
  \item
    Neural networks: hidden layers act as information bottlenecks.
  \item
    Variational Information Bottleneck (VIB): practical approximation
    for deep learning.
  \item
    Regularization: prevents overfitting by discarding irrelevant
    detail.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1910}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3483}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4607}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Term
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(I(X;T)\) & Info retained from input & Latent representation
complexity \\
\(I(T;Y)\) & Info relevant for prediction & Accuracy of classifier \\
\(\beta\) trade-off & Compression vs predictive power & Tuning
representation learning objectives \\
\end{longtable}

Tiny Code Sample (Python, sketch of VIB loss)

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn.functional }\ImportTok{as}\NormalTok{ F}

\KeywordTok{def}\NormalTok{ vib\_loss(p\_y\_given\_t, q\_t\_given\_x, p\_t, y, beta}\OperatorTok{=}\FloatTok{1e{-}3}\NormalTok{):}
    \CommentTok{\# Prediction loss (cross{-}entropy)}
\NormalTok{    pred\_loss }\OperatorTok{=}\NormalTok{ F.nll\_loss(p\_y\_given\_t, y)}
    \CommentTok{\# KL divergence term for compression}
\NormalTok{    kl }\OperatorTok{=}\NormalTok{ torch.distributions.kl.kl\_divergence(q\_t\_given\_x, p\_t).mean()}
    \ControlFlowTok{return}\NormalTok{ pred\_loss }\OperatorTok{+}\NormalTok{ beta }\OperatorTok{*}\NormalTok{ kl}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-65}

The IB principle provides a unifying view of representation learning:
good models should compress inputs while preserving what matters for
outputs. It bridges coding theory, statistics, and deep learning, and
explains why deep networks generalize well despite huge capacity.

\subsubsection{Try It Yourself}\label{try-it-yourself-167}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Explain why the hidden representation of a neural net can be seen as a
  bottleneck.
\item
  Modify \(\beta\) in the VIB objective---what happens to compression vs
  accuracy?
\item
  Compare IB to rate--distortion theory: how do they differ in purpose?
\end{enumerate}

\subsection{169. Minimum Description Length
(MDL)}\label{minimum-description-length-mdl}

The Minimum Description Length principle views learning as compression:
the best model is the one that provides the shortest description of the
data plus the model itself. MDL formalizes Occam's razor---prefer
simpler models unless complexity is justified by better fit.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-168}

Imagine trying to explain a dataset to a friend. If you just read out
all the numbers, that's long. If you fit a simple pattern (``all numbers
are even up to 100''), your explanation is shorter. MDL says the best
explanation is the one that minimizes total description length.

\subsubsection{Deep Dive}\label{deep-dive-168}

\begin{itemize}
\item
  Formal principle: Total description length = model complexity + data
  encoding under model.

  \[
  L(M, D) = L(M) + L(D \mid M)
  \]

  \begin{itemize}
  \tightlist
  \item
    \(L(M)\): bits to describe the model.
  \item
    \(L(D|M)\): bits to encode the data given the model.
  \end{itemize}
\item
  Connections:

  \begin{itemize}
  \tightlist
  \item
    Equivalent to maximizing posterior probability in Bayesian
    inference.
  \item
    Related to Kolmogorov complexity (shortest program producing the
    data).
  \item
    Generalizes to stochastic models: choose the one with minimal
    codelength.
  \end{itemize}
\item
  Applications in AI:

  \begin{itemize}
  \tightlist
  \item
    Model selection (balancing bias--variance).
  \item
    Avoiding overfitting in machine learning.
  \item
    Feature selection via compressibility.
  \item
    Information-theoretic foundations of regularization.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1204}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3056}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2407}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Term
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(L(M)\) & Complexity cost of the model & Number of parameters in neural
net & \\
(L(D & M)) & Encoding cost of data given model & Log-likelihood under
model \\
MDL principle & Minimize total description length & Trade-off between
fit and simplicity & \\
\end{longtable}

Tiny Code Sample (Python, toy MDL for polynomial fit)

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ PolynomialFeatures}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LinearRegression}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ mean\_squared\_error}
\ImportTok{import}\NormalTok{ math}

\CommentTok{\# Generate noisy quadratic data}
\NormalTok{np.random.seed(}\DecValTok{0}\NormalTok{)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{20}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=} \DecValTok{2}\OperatorTok{*}\NormalTok{X[:,}\DecValTok{0}\NormalTok{]}\DecValTok{2} \OperatorTok{+} \FloatTok{0.1}\OperatorTok{*}\NormalTok{np.random.randn(}\DecValTok{20}\NormalTok{)}

\KeywordTok{def}\NormalTok{ mdl\_cost(degree):}
\NormalTok{    poly }\OperatorTok{=}\NormalTok{ PolynomialFeatures(degree)}
\NormalTok{    X\_poly }\OperatorTok{=}\NormalTok{ poly.fit\_transform(X)}
\NormalTok{    model }\OperatorTok{=}\NormalTok{ LinearRegression().fit(X\_poly, y)}
\NormalTok{    y\_pred }\OperatorTok{=}\NormalTok{ model.predict(X\_poly)}
\NormalTok{    mse }\OperatorTok{=}\NormalTok{ mean\_squared\_error(y, y\_pred)}
\NormalTok{    L\_D\_given\_M }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(y)}\OperatorTok{*}\NormalTok{math.log(mse}\OperatorTok{+}\FloatTok{1e{-}6}\NormalTok{)   }\CommentTok{\# data fit cost}
\NormalTok{    L\_M }\OperatorTok{=}\NormalTok{ degree                              }\CommentTok{\# model complexity proxy}
    \ControlFlowTok{return}\NormalTok{ L\_M }\OperatorTok{+}\NormalTok{ L\_D\_given\_M}

\ControlFlowTok{for}\NormalTok{ d }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{6}\NormalTok{):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Degree }\SpecialCharTok{\{}\NormalTok{d}\SpecialCharTok{\}}\SpecialStringTok{, MDL cost: }\SpecialCharTok{\{}\NormalTok{mdl\_cost(d)}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-66}

MDL offers a principled, universal way to balance model complexity with
data fit. It justifies why simpler models generalize better, and
underlies practical methods like AIC, BIC, and regularization penalties
in modern machine learning.

\subsubsection{Try It Yourself}\label{try-it-yourself-168}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compare MDL costs for fitting linear vs quadratic models to data.
\item
  Explain how MDL prevents overfitting in decision trees.
\item
  Relate MDL to deep learning regularization: how do weight penalties
  mimic description length?
\end{enumerate}

\subsection{170. Applications in Machine
Learning}\label{applications-in-machine-learning}

Information theory provides the language and tools to quantify
uncertainty, dependence, and efficiency. In machine learning, these
concepts directly translate into loss functions, regularization, and
representation learning.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-169}

Imagine teaching a child new words. You want to give them enough
examples to reduce uncertainty (entropy), focus on the most relevant
clues (mutual information), and avoid wasting effort on noise. Machine
learning systems operate under the same principles.

\subsubsection{Deep Dive}\label{deep-dive-169}

\begin{itemize}
\item
  Entropy \& Cross-Entropy:

  \begin{itemize}
  \tightlist
  \item
    Classification uses cross-entropy loss to align predicted and true
    distributions.
  \item
    Entropy measures model uncertainty, guiding exploration in
    reinforcement learning.
  \end{itemize}
\item
  Mutual Information:

  \begin{itemize}
  \tightlist
  \item
    Feature selection: choose variables with high MI with labels.
  \item
    Representation learning: InfoNCE and contrastive learning maximize
    MI between views.
  \end{itemize}
\item
  KL Divergence:

  \begin{itemize}
  \tightlist
  \item
    Core of variational inference and VAEs.
  \item
    Regularizes approximate posteriors toward priors.
  \end{itemize}
\item
  Channel Capacity:

  \begin{itemize}
  \tightlist
  \item
    Analogy for limits of model generalization.
  \item
    Bottleneck layers in deep nets function like constrained channels.
  \end{itemize}
\item
  Rate--Distortion \& Bottleneck:

  \begin{itemize}
  \tightlist
  \item
    Variational Information Bottleneck (VIB) balances compression and
    relevance.
  \item
    Applied in disentangled representation learning.
  \end{itemize}
\item
  MDL Principle:

  \begin{itemize}
  \tightlist
  \item
    Guides model selection by trading complexity for fit.
  \item
    Explains regularization penalties (L1, L2) as description length
    constraints.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2381}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3690}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3929}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Information Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Machine Learning Role
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Entropy & Quantify uncertainty & Exploration in RL \\
Cross-Entropy & Training objective & Classification, language
modeling \\
Mutual Information & Feature/repr. relevance & Contrastive learning,
clustering \\
KL Divergence & Approximate inference & VAEs, Bayesian deep learning \\
Channel Capacity & Limit of reliable info transfer & Neural bottlenecks,
compression \\
Rate--Distortion / IB & Compress yet preserve relevance & Representation
learning, VAEs \\
MDL & Model selection, generalization & Regularization, pruning \\
\end{longtable}

Tiny Code Sample (Python, InfoNCE Loss)

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn.functional }\ImportTok{as}\NormalTok{ F}

\KeywordTok{def}\NormalTok{ info\_nce\_loss(z\_i, z\_j, temperature}\OperatorTok{=}\FloatTok{0.1}\NormalTok{):}
    \CommentTok{\# z\_i, z\_j are embeddings from two augmented views}
\NormalTok{    batch\_size }\OperatorTok{=}\NormalTok{ z\_i.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    z }\OperatorTok{=}\NormalTok{ torch.cat([z\_i, z\_j], dim}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{    sim }\OperatorTok{=}\NormalTok{ F.cosine\_similarity(z.unsqueeze(}\DecValTok{1}\NormalTok{), z.unsqueeze(}\DecValTok{0}\NormalTok{), dim}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{    sim }\OperatorTok{/=}\NormalTok{ temperature}
\NormalTok{    labels }\OperatorTok{=}\NormalTok{ torch.arange(batch\_size, device}\OperatorTok{=}\NormalTok{z.device)}
\NormalTok{    labels }\OperatorTok{=}\NormalTok{ torch.cat([labels, labels], dim}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ F.cross\_entropy(sim, labels)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-67}

Information theory explains \emph{why} machine learning works. It
unifies compression, prediction, and generalization, showing that
learning is fundamentally about extracting, transmitting, and
representing information efficiently.

\subsubsection{Try It Yourself}\label{try-it-yourself-169}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train a classifier with cross-entropy loss and measure entropy of
  predictions on uncertain data.
\item
  Use mutual information to rank features in a dataset.
\item
  Relate the concept of channel capacity to overfitting in deep
  networks.
\end{enumerate}

\section{Chapter 18. Graphs, Matrices and Special
Methods}\label{chapter-18.-graphs-matrices-and-special-methods}

\subsection{171. Graphs: Nodes, Edges, and
Paths}\label{graphs-nodes-edges-and-paths}

Graphs are mathematical structures that capture relationships between
entities. A graph consists of nodes (vertices) and edges (links). They
can be directed or undirected, weighted or unweighted, and form the
foundation for reasoning about connectivity, flow, and structure.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-170}

Imagine a social network. Each person is a node, and each friendship is
an edge connecting two people. A path is just a chain of
friendships---how you get from one person to another through mutual
friends.

\subsubsection{Deep Dive}\label{deep-dive-170}

\begin{itemize}
\item
  Graph definition: \(G = (V, E)\) with vertex set \(V\) and edge set
  \(E\).
\item
  Nodes (vertices): fundamental units (people, cities, states).
\item
  Edges (links): represent relationships, can be:

  \begin{itemize}
  \tightlist
  \item
    Directed: (u,v) ≠ (v,u) → Twitter follow.
  \item
    Undirected: (u,v) = (v,u) → Facebook friendship.
  \end{itemize}
\item
  Weighted graphs: edges have values (distance, cost, similarity).
\item
  Paths and connectivity:

  \begin{itemize}
  \tightlist
  \item
    Path = sequence of edges between nodes.
  \item
    Cycle = path that starts and ends at same node.
  \item
    Connected graph = path exists between any two nodes.
  \end{itemize}
\item
  Special graphs: trees, bipartite graphs, complete graphs.
\item
  In AI: graphs model knowledge bases, molecules, neural nets,
  logistics, and interactions in multi-agent systems.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1548}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3452}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Element
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Node (vertex) & Entity & User in social network, word in NLP \\
Edge (link) & Relationship between entities & Friendship, co-occurrence,
road connection \\
Weighted edge & Strength or cost of relation & Distance between cities,
attention score \\
Path & Sequence of nodes/edges & Inference chain in knowledge graph \\
Cycle & Path that returns to start & Feedback loop in causal models \\
\end{longtable}

Tiny Code Sample (Python, using NetworkX)

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ networkx }\ImportTok{as}\NormalTok{ nx}

\CommentTok{\# Create graph}
\NormalTok{G }\OperatorTok{=}\NormalTok{ nx.Graph()}
\NormalTok{G.add\_edges\_from([(}\StringTok{"Alice"}\NormalTok{,}\StringTok{"Bob"}\NormalTok{), (}\StringTok{"Bob"}\NormalTok{,}\StringTok{"Carol"}\NormalTok{), (}\StringTok{"Alice"}\NormalTok{,}\StringTok{"Dan"}\NormalTok{)])}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Nodes:"}\NormalTok{, G.nodes())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Edges:"}\NormalTok{, G.edges())}

\CommentTok{\# Check paths}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Path Alice {-}\textgreater{} Carol:"}\NormalTok{, nx.shortest\_path(G, }\StringTok{"Alice"}\NormalTok{, }\StringTok{"Carol"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-68}

Graphs are the universal language of structure and relationships. In AI,
they support reasoning (knowledge graphs), learning (graph neural
networks), and optimization (routing, scheduling). Without graphs, many
AI systems would lack the ability to represent and reason about complex
connections.

\subsubsection{Try It Yourself}\label{try-it-yourself-170}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Construct a graph of five cities and connect them with distances as
  edge weights. Find the shortest path between two cities.
\item
  Build a bipartite graph of users and movies. What does a path from
  user A to user B mean?
\item
  Give an example where cycles in a graph model feedback in a real
  system (e.g., economy, ecology).
\end{enumerate}

\subsection{172. Adjacency and Incidence
Matrices}\label{adjacency-and-incidence-matrices}

Graphs can be represented algebraically using matrices. The adjacency
matrix encodes which nodes are connected, while the incidence matrix
captures relationships between nodes and edges. These matrix forms
enable powerful linear algebra techniques for analyzing graphs.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-171}

Think of a city map. You could describe it with a list of roads (edges)
connecting intersections (nodes), or you could build a big table. Each
row and column of the table represents intersections, and you mark a
``1'' whenever a road connects two intersections. That table is the
adjacency matrix.

\subsubsection{Deep Dive}\label{deep-dive-171}

\begin{itemize}
\item
  Adjacency matrix (A):

  \begin{itemize}
  \item
    For graph \(G=(V,E)\) with \(|V|=n\):

    \[
    A_{ij} = \begin{cases} 
      1 & \text{if edge } (i,j) \in E, \\
      0 & \text{otherwise.}
    \end{cases}
    \]
  \item
    For weighted graphs, entries contain weights instead of 1s.
  \item
    Properties: symmetric for undirected graphs; row sums give node
    degrees.
  \end{itemize}
\item
  Incidence matrix (B):

  \begin{itemize}
  \item
    Rows = nodes, columns = edges.
  \item
    For edge \(e=(i,j)\):

    \begin{itemize}
    \tightlist
    \item
      \(B_{i,e} = +1\), \(B_{j,e} = -1\), all others 0 (for directed
      graphs).
    \end{itemize}
  \item
    Captures how edges connect vertices.
  \end{itemize}
\item
  Linear algebra links:

  \begin{itemize}
  \tightlist
  \item
    Degree matrix: \(D_{ii} = \sum_j A_{ij}\).
  \item
    Graph Laplacian: \(L = D - A\).
  \end{itemize}
\item
  In AI: used in spectral clustering, graph convolutional networks,
  knowledge graph embeddings.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2069}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3563}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4368}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Matrix
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Use Case in AI
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Adjacency (A) & Node-to-node connectivity & Graph neural networks, node
embeddings \\
Weighted adjacency & Edge weights as entries & Shortest paths,
recommender systems \\
Incidence (B) & Node-to-edge mapping & Flow problems, electrical
circuits \\
Laplacian (L=D−A) & Derived from adjacency + degree & Spectral methods,
clustering, GNNs \\
\end{longtable}

Tiny Code Sample (Python, using NetworkX \& NumPy)

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ networkx }\ImportTok{as}\NormalTok{ nx}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Build graph}
\NormalTok{G }\OperatorTok{=}\NormalTok{ nx.Graph()}
\NormalTok{G.add\_edges\_from([(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{),(}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{),(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{)])}

\CommentTok{\# Adjacency matrix}
\NormalTok{A }\OperatorTok{=}\NormalTok{ nx.to\_numpy\_array(G)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Adjacency matrix:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, A)}

\CommentTok{\# Incidence matrix}
\NormalTok{B }\OperatorTok{=}\NormalTok{ nx.incidence\_matrix(G, oriented}\OperatorTok{=}\VariableTok{True}\NormalTok{).toarray()}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Incidence matrix:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, B)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-69}

Matrix representations let us apply linear algebra to graphs, unlocking
tools for clustering, spectral analysis, and graph neural networks. This
algebraic viewpoint turns structural problems into numerical ones,
making them solvable with efficient algorithms.

\subsubsection{Try It Yourself}\label{try-it-yourself-171}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Construct the adjacency matrix for a triangle graph (3 nodes, fully
  connected). What are its eigenvalues?
\item
  Build the incidence matrix for a 4-node chain graph. How do its
  columns reflect edge connections?
\item
  Use the Laplacian \(L=D-A\) of a small graph to compute its connected
  components.
\end{enumerate}

\subsection{173. Graph Traversals (DFS,
BFS)}\label{graph-traversals-dfs-bfs}

Graph traversal algorithms systematically explore nodes and edges.
Depth-First Search (DFS) goes as far as possible along one path before
backtracking, while Breadth-First Search (BFS) explores neighbors layer
by layer. These two strategies underpin many higher-level graph
algorithms.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-172}

Imagine searching a maze. DFS is like always taking the next hallway
until you hit a dead end, then backtracking. BFS is like exploring all
hallways one step at a time, ensuring you find the shortest way out.

\subsubsection{Deep Dive}\label{deep-dive-172}

\begin{itemize}
\item
  DFS (Depth-First Search):

  \begin{itemize}
  \tightlist
  \item
    Explores deep into a branch before backtracking.
  \item
    Implemented recursively or with a stack.
  \item
    Useful for detecting cycles, topological sorting, connected
    components.
  \end{itemize}
\item
  BFS (Breadth-First Search):

  \begin{itemize}
  \tightlist
  \item
    Explores all neighbors of current node before moving deeper.
  \item
    Uses a queue.
  \item
    Finds shortest paths in unweighted graphs.
  \end{itemize}
\item
  Complexity: \(O(|V| + |E|)\) for both.
\item
  In AI: used in search (state spaces, planning), social network
  analysis, knowledge graph queries.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3704}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3796}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Traversal
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mechanism
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
DFS & Stack/recursion & Memory-efficient, explores deeply & Topological
sort, constraint satisfaction \\
BFS & Queue, level-order & Finds shortest path in unweighted graphs &
Shortest queries in knowledge graphs \\
\end{longtable}

Tiny Code Sample (Python, DFS \& BFS with NetworkX)

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ networkx }\ImportTok{as}\NormalTok{ nx}
\ImportTok{from}\NormalTok{ collections }\ImportTok{import}\NormalTok{ deque}

\NormalTok{G }\OperatorTok{=}\NormalTok{ nx.Graph()}
\NormalTok{G.add\_edges\_from([(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),(}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{),(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{),(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{),(}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{)])}

\CommentTok{\# DFS}
\KeywordTok{def}\NormalTok{ dfs(graph, start, visited}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
    \ControlFlowTok{if}\NormalTok{ visited }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
\NormalTok{        visited }\OperatorTok{=} \BuiltInTok{set}\NormalTok{()}
\NormalTok{    visited.add(start)}
    \ControlFlowTok{for}\NormalTok{ neighbor }\KeywordTok{in}\NormalTok{ graph.neighbors(start):}
        \ControlFlowTok{if}\NormalTok{ neighbor }\KeywordTok{not} \KeywordTok{in}\NormalTok{ visited:}
\NormalTok{            dfs(graph, neighbor, visited)}
    \ControlFlowTok{return}\NormalTok{ visited}

\BuiltInTok{print}\NormalTok{(}\StringTok{"DFS from 0:"}\NormalTok{, dfs(G, }\DecValTok{0}\NormalTok{))}

\CommentTok{\# BFS}
\KeywordTok{def}\NormalTok{ bfs(graph, start):}
\NormalTok{    visited, queue }\OperatorTok{=} \BuiltInTok{set}\NormalTok{([start]), deque([start])}
\NormalTok{    order }\OperatorTok{=}\NormalTok{ []}
    \ControlFlowTok{while}\NormalTok{ queue:}
\NormalTok{        node }\OperatorTok{=}\NormalTok{ queue.popleft()}
\NormalTok{        order.append(node)}
        \ControlFlowTok{for}\NormalTok{ neighbor }\KeywordTok{in}\NormalTok{ graph.neighbors(node):}
            \ControlFlowTok{if}\NormalTok{ neighbor }\KeywordTok{not} \KeywordTok{in}\NormalTok{ visited:}
\NormalTok{                visited.add(neighbor)}
\NormalTok{                queue.append(neighbor)}
    \ControlFlowTok{return}\NormalTok{ order}

\BuiltInTok{print}\NormalTok{(}\StringTok{"BFS from 0:"}\NormalTok{, bfs(G, }\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-70}

Traversal is the backbone of graph algorithms. Whether navigating a
state space in AI search, analyzing social networks, or querying
knowledge graphs, DFS and BFS provide the exploration strategies on
which more complex reasoning is built.

\subsubsection{Try It Yourself}\label{try-it-yourself-172}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use BFS to find the shortest path between two nodes in an unweighted
  graph.
\item
  Modify DFS to detect cycles in a directed graph.
\item
  Compare the traversal order of BFS vs DFS on a binary tree---what
  insights do you gain?
\end{enumerate}

\subsection{174. Connectivity and
Components}\label{connectivity-and-components}

Connectivity describes whether nodes in a graph are reachable from one
another. A connected component is a maximal set of nodes where each pair
has a path between them. In directed graphs, we distinguish between
strongly and weakly connected components.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-173}

Think of islands connected by bridges. Each island cluster where you can
walk from any town to any other without leaving the cluster is a
connected component. If some islands are cut off, they form separate
components.

\subsubsection{Deep Dive}\label{deep-dive-173}

\begin{itemize}
\item
  Undirected graphs:

  \begin{itemize}
  \tightlist
  \item
    A graph is connected if every pair of nodes has a path.
  \item
    Otherwise, it splits into multiple connected components.
  \end{itemize}
\item
  Directed graphs:

  \begin{itemize}
  \tightlist
  \item
    Strongly connected component (SCC): every node reachable from every
    other node.
  \item
    Weakly connected component: connectivity holds if edge directions
    are ignored.
  \end{itemize}
\item
  Algorithms:

  \begin{itemize}
  \tightlist
  \item
    BFS/DFS to find connected components in undirected graphs.
  \item
    Kosaraju's, Tarjan's, or Gabow's algorithm for SCCs in directed
    graphs.
  \end{itemize}
\item
  Applications in AI:

  \begin{itemize}
  \tightlist
  \item
    Social network analysis (friendship clusters).
  \item
    Knowledge graphs (isolated subgraphs).
  \item
    Computer vision (connected pixel regions).
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2353}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4118}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3529}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Connected graph & All nodes reachable & Communication networks \\
Connected component & Maximal subset of mutually reachable nodes &
Community detection in social graphs \\
Strongly connected comp. & Directed paths in both directions exist & Web
graph link cycles \\
Weakly connected comp. & Paths exist if direction is ignored & Isolated
knowledge graph partitions \\
\end{longtable}

Tiny Code Sample (Python, NetworkX)

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ networkx }\ImportTok{as}\NormalTok{ nx}

\CommentTok{\# Undirected graph with two components}
\NormalTok{G }\OperatorTok{=}\NormalTok{ nx.Graph()}
\NormalTok{G.add\_edges\_from([(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{),(}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{)])}

\NormalTok{components }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(nx.connected\_components(G))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Connected components:"}\NormalTok{, components)}

\CommentTok{\# Directed graph SCCs}
\NormalTok{DG }\OperatorTok{=}\NormalTok{ nx.DiGraph()}
\NormalTok{DG.add\_edges\_from([(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{),(}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{),(}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{)])}
\NormalTok{sccs }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(nx.strongly\_connected\_components(DG))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Strongly connected components:"}\NormalTok{, sccs)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-71}

Understanding connectivity helps identify whether a system is unified or
fragmented. In AI, it reveals isolated data clusters, ensures graph
search completeness, and supports robustness analysis in networks and
multi-agent systems.

\subsubsection{Try It Yourself}\label{try-it-yourself-173}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Build a graph with three disconnected subgraphs and identify its
  connected components.
\item
  Create a directed cycle (A→B→C→A). Is it strongly connected? Weakly
  connected?
\item
  Explain how identifying SCCs might help in optimizing web crawlers or
  knowledge graph queries.
\end{enumerate}

\subsection{175. Graph Laplacians}\label{graph-laplacians}

The graph Laplacian is a matrix that encodes both connectivity and
structure of a graph. It is central to spectral graph theory, linking
graph properties with eigenvalues and eigenvectors. Laplacians underpin
clustering, graph embeddings, and diffusion processes in AI.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-174}

Imagine pouring dye on one node of a network of pipes. The way the dye
diffuses over time depends on how the pipes connect. The Laplacian
matrix mathematically describes that diffusion across the graph.

\subsubsection{Deep Dive}\label{deep-dive-174}

\begin{itemize}
\item
  Definition: For graph \(G=(V,E)\) with adjacency matrix \(A\) and
  degree matrix \(D\):

  \[
  L = D - A
  \]
\item
  Normalized forms:

  \begin{itemize}
  \tightlist
  \item
    Symmetric: \(L_{sym} = D^{-1/2} L D^{-1/2}\).
  \item
    Random-walk: \(L_{rw} = D^{-1} L\).
  \end{itemize}
\item
  Key properties:

  \begin{itemize}
  \tightlist
  \item
    \(L\) is symmetric and positive semi-definite.
  \item
    The smallest eigenvalue is always 0, with multiplicity equal to the
    number of connected components.
  \end{itemize}
\item
  Applications:

  \begin{itemize}
  \tightlist
  \item
    Spectral clustering: uses eigenvectors of Laplacian to partition
    graphs.
  \item
    Graph embeddings: Laplacian Eigenmaps for dimensionality reduction.
  \item
    Physics: models heat diffusion and random walks.
  \end{itemize}
\item
  In AI: community detection, semi-supervised learning, manifold
  learning, graph neural networks.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2740}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2603}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4658}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Variant
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Application in AI
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Unnormalized L & \(D - A\) & General graph analysis \\
Normalized \(L_{sym}\) & \(D^{-1/2}LD^{-1/2}\) & Spectral clustering \\
Random-walk \(L_{rw}\) & \(D^{-1}L\) & Markov processes, diffusion
models \\
\end{longtable}

Tiny Code Sample (Python, NumPy + NetworkX)

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ networkx }\ImportTok{as}\NormalTok{ nx}

\CommentTok{\# Build simple graph}
\NormalTok{G }\OperatorTok{=}\NormalTok{ nx.Graph()}
\NormalTok{G.add\_edges\_from([(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{),(}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{),(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{)])}

\CommentTok{\# Degree and adjacency matrices}
\NormalTok{A }\OperatorTok{=}\NormalTok{ nx.to\_numpy\_array(G)}
\NormalTok{D }\OperatorTok{=}\NormalTok{ np.diag(A.}\BuiltInTok{sum}\NormalTok{(axis}\OperatorTok{=}\DecValTok{1}\NormalTok{))}

\CommentTok{\# Laplacian}
\NormalTok{L }\OperatorTok{=}\NormalTok{ D }\OperatorTok{{-}}\NormalTok{ A}
\NormalTok{eigs, vecs }\OperatorTok{=}\NormalTok{ np.linalg.eigh(L)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Laplacian:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, L)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Eigenvalues:"}\NormalTok{, eigs)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-72}

The Laplacian turns graph problems into linear algebra problems. Its
spectral properties reveal clusters, connectivity, and diffusion
dynamics. This makes it indispensable in AI methods that rely on graph
structure, from GNNs to semi-supervised learning.

\subsubsection{Try It Yourself}\label{try-it-yourself-174}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Construct the Laplacian of a chain of 4 nodes and compute its
  eigenvalues.
\item
  Use the Fiedler vector (second-smallest eigenvector) to partition a
  graph into two clusters.
\item
  Explain how the Laplacian relates to random walks and Markov chains.
\end{enumerate}

\subsection{176. Spectral Decomposition of
Graphs}\label{spectral-decomposition-of-graphs}

Spectral graph theory studies the eigenvalues and eigenvectors of
matrices associated with graphs, especially the Laplacian and adjacency
matrices. These spectral properties reveal structure, connectivity, and
clustering in graphs.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-175}

Imagine plucking a guitar string. The vibration frequencies are
determined by the string's structure. Similarly, the ``frequencies''
(eigenvalues) of a graph come from its Laplacian, and the ``modes''
(eigenvectors) reveal how the graph naturally partitions.

\subsubsection{Deep Dive}\label{deep-dive-175}

\begin{itemize}
\item
  Adjacency spectrum: eigenvalues of adjacency matrix \(A\).

  \begin{itemize}
  \tightlist
  \item
    Capture connectivity patterns.
  \end{itemize}
\item
  Laplacian spectrum: eigenvalues of \(L=D-A\).

  \begin{itemize}
  \tightlist
  \item
    Smallest eigenvalue is always 0.
  \item
    Multiplicity of 0 equals number of connected components.
  \item
    Second-smallest eigenvalue (Fiedler value) measures graph
    connectivity.
  \end{itemize}
\item
  Eigenvectors:

  \begin{itemize}
  \tightlist
  \item
    Fiedler vector used to partition graphs (spectral clustering).
  \item
    Eigenvectors represent smooth variations across nodes.
  \end{itemize}
\item
  Applications:

  \begin{itemize}
  \tightlist
  \item
    Graph partitioning, community detection.
  \item
    Embeddings (Laplacian eigenmaps).
  \item
    Analyzing diffusion and random walks.
  \item
    Designing Graph Neural Networks with spectral filters.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3778}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3889}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Spectrum Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Information Provided
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Adjacency eigenvalues & Density, degree distribution & Social network
analysis \\
Laplacian eigenvalues & Connectivity, clustering structure & Spectral
clustering in ML \\
Eigenvectors & Node embeddings, smooth functions & Semi-supervised node
classification \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-165}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ networkx }\ImportTok{as}\NormalTok{ nx}

\CommentTok{\# Build simple graph}
\NormalTok{G }\OperatorTok{=}\NormalTok{ nx.path\_graph(}\DecValTok{5}\NormalTok{)  }\CommentTok{\# 5 nodes in a chain}

\CommentTok{\# Laplacian}
\NormalTok{L }\OperatorTok{=}\NormalTok{ nx.laplacian\_matrix(G).toarray()}

\CommentTok{\# Eigen{-}decomposition}
\NormalTok{eigs, vecs }\OperatorTok{=}\NormalTok{ np.linalg.eigh(L)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Eigenvalues:"}\NormalTok{, eigs)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Fiedler vector (2nd eigenvector):"}\NormalTok{, vecs[:,}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-73}

Spectral methods provide a bridge between graph theory and linear
algebra. In AI, they enable powerful techniques for clustering,
embeddings, and GNN architectures. Understanding the spectral view of
graphs is key to analyzing structure beyond simple connectivity.

\subsubsection{Try It Yourself}\label{try-it-yourself-175}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute Laplacian eigenvalues of a complete graph with 4 nodes. How
  many zeros appear?
\item
  Use the Fiedler vector to split a graph into two communities.
\item
  Explain how eigenvalues can indicate robustness of networks to
  node/edge removal.
\end{enumerate}

\subsection{177. Eigenvalues and Graph
Partitioning}\label{eigenvalues-and-graph-partitioning}

Graph partitioning divides a graph into groups of nodes while minimizing
connections between groups. Eigenvalues and eigenvectors of the
Laplacian provide a principled way to achieve this, forming the basis of
spectral clustering.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-176}

Imagine a city split by a river. People within each side interact more
with each other than across the river. The graph Laplacian's eigenvalues
reveal this ``natural cut,'' and the corresponding eigenvector helps
assign nodes to their side.

\subsubsection{Deep Dive}\label{deep-dive-176}

\begin{itemize}
\item
  Fiedler value (λ₂):

  \begin{itemize}
  \tightlist
  \item
    Second-smallest eigenvalue of Laplacian.
  \item
    Measures algebraic connectivity: small λ₂ means graph is loosely
    connected.
  \end{itemize}
\item
  Fiedler vector:

  \begin{itemize}
  \tightlist
  \item
    Corresponding eigenvector partitions nodes into two sets based on
    sign (or value threshold).
  \item
    Defines a ``spectral cut'' of the graph.
  \end{itemize}
\item
  Graph partitioning problem:

  \begin{itemize}
  \tightlist
  \item
    Minimize edge cuts between partitions while balancing group sizes.
  \item
    NP-hard in general, but spectral relaxation makes it tractable.
  \end{itemize}
\item
  Spectral clustering:

  \begin{itemize}
  \tightlist
  \item
    Use top k eigenvectors of normalized Laplacian as features.
  \item
    Apply k-means to cluster nodes.
  \end{itemize}
\item
  Applications in AI:

  \begin{itemize}
  \tightlist
  \item
    Community detection in social networks.
  \item
    Document clustering in NLP.
  \item
    Image segmentation (pixels as graph nodes).
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1881}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4455}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3663}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Role in Partitioning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Fiedler value λ₂ & Strength of connectivity & Detecting weakly linked
communities \\
Fiedler vector & Partition nodes into two sets & Splitting social
networks into groups \\
Spectral clustering & Uses eigenvectors of Laplacian for clustering &
Image segmentation, topic modeling \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-166}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ networkx }\ImportTok{as}\NormalTok{ nx}
\ImportTok{from}\NormalTok{ sklearn.cluster }\ImportTok{import}\NormalTok{ KMeans}

\CommentTok{\# Build graph}
\NormalTok{G }\OperatorTok{=}\NormalTok{ nx.karate\_club\_graph()}
\NormalTok{L }\OperatorTok{=}\NormalTok{ nx.normalized\_laplacian\_matrix(G).toarray()}

\CommentTok{\# Eigen{-}decomposition}
\NormalTok{eigs, vecs }\OperatorTok{=}\NormalTok{ np.linalg.eigh(L)}

\CommentTok{\# Use second eigenvector for 2{-}way partition}
\NormalTok{fiedler\_vector }\OperatorTok{=}\NormalTok{ vecs[:,}\DecValTok{1}\NormalTok{]}
\NormalTok{partition }\OperatorTok{=}\NormalTok{ fiedler\_vector }\OperatorTok{\textgreater{}} \DecValTok{0}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Partition groups:"}\NormalTok{, partition.astype(}\BuiltInTok{int}\NormalTok{))}

\CommentTok{\# k{-}means spectral clustering (k=2)}
\NormalTok{features }\OperatorTok{=}\NormalTok{ vecs[:,}\DecValTok{1}\NormalTok{:}\DecValTok{3}\NormalTok{]}
\NormalTok{labels }\OperatorTok{=}\NormalTok{ KMeans(n\_clusters}\OperatorTok{=}\DecValTok{2}\NormalTok{, n\_init}\OperatorTok{=}\DecValTok{10}\NormalTok{).fit\_predict(features)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Spectral clustering labels:"}\NormalTok{, labels)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-74}

Graph partitioning via eigenvalues is more robust than naive heuristics.
It reveals hidden communities and patterns, enabling AI systems to learn
structure in complex data. Without spectral methods, clustering
high-dimensional relational data would often be intractable.

\subsubsection{Try It Yourself}\label{try-it-yourself-176}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute λ₂ for a chain of 5 nodes and explain its meaning.
\item
  Use the Fiedler vector to partition a graph with two weakly connected
  clusters.
\item
  Apply spectral clustering to a pixel graph of an image---what
  structures emerge?
\end{enumerate}

\subsection{178. Random Walks and Markov Chains on
Graphs}\label{random-walks-and-markov-chains-on-graphs}

A random walk is a process of moving through a graph by randomly
choosing edges. When repeated indefinitely, it forms a Markov chain---a
stochastic process where the next state depends only on the current one.
Random walks connect graph structure with probability, enabling ranking,
clustering, and learning.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-177}

Imagine a tourist wandering a city. At every intersection (node), they
pick a random road (edge) to walk down. Over time, the frequency with
which they visit each place reflects the structure of the city.

\subsubsection{Deep Dive}\label{deep-dive-177}

\begin{itemize}
\item
  Random walk definition:

  \begin{itemize}
  \tightlist
  \item
    From node \(i\), move to neighbor \(j\) with probability
    \(1/\deg(i)\) (uniform case).
  \item
    Transition matrix: \(P = D^{-1}A\).
  \end{itemize}
\item
  Stationary distribution:

  \begin{itemize}
  \tightlist
  \item
    Probability distribution \(\pi\) where \(\pi = \pi P\).
  \item
    In undirected graphs, \(\pi_i \propto \deg(i)\).
  \end{itemize}
\item
  Markov chains:

  \begin{itemize}
  \tightlist
  \item
    Irreducible: all nodes reachable.
  \item
    Aperiodic: no fixed cycle.
  \item
    Converges to stationary distribution under these conditions.
  \end{itemize}
\item
  Applications in AI:

  \begin{itemize}
  \tightlist
  \item
    PageRank (random surfer model).
  \item
    Semi-supervised learning on graphs.
  \item
    Node embeddings (DeepWalk, node2vec).
  \item
    Sampling for large-scale graph analysis.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2300}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3800}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3900}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition/Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Transition matrix (P) & \(P=D^{-1}A\) & Defines step probabilities \\
Stationary distribution & \(\pi = \pi P\) & Long-run importance of nodes
(PageRank) \\
Mixing time & Steps to reach near-stationarity & Efficiency of
random-walk sampling \\
Biased random walk & Probabilities adjusted by weights/bias & node2vec
embeddings \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-167}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ networkx }\ImportTok{as}\NormalTok{ nx}

\CommentTok{\# Simple graph}
\NormalTok{G }\OperatorTok{=}\NormalTok{ nx.path\_graph(}\DecValTok{4}\NormalTok{)}
\NormalTok{A }\OperatorTok{=}\NormalTok{ nx.to\_numpy\_array(G)}
\NormalTok{D }\OperatorTok{=}\NormalTok{ np.diag(A.}\BuiltInTok{sum}\NormalTok{(axis}\OperatorTok{=}\DecValTok{1}\NormalTok{))}
\NormalTok{P }\OperatorTok{=}\NormalTok{ np.linalg.inv(D) }\OperatorTok{@}\NormalTok{ A}

\CommentTok{\# Random walk simulation}
\NormalTok{n\_steps }\OperatorTok{=} \DecValTok{10}
\NormalTok{state }\OperatorTok{=} \DecValTok{0}
\NormalTok{trajectory }\OperatorTok{=}\NormalTok{ [state]}
\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n\_steps):}
\NormalTok{    state }\OperatorTok{=}\NormalTok{ np.random.choice(}\BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(G)), p}\OperatorTok{=}\NormalTok{P[state])}
\NormalTok{    trajectory.append(state)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Transition matrix:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, P)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Random walk trajectory:"}\NormalTok{, trajectory)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-75}

Random walks connect probabilistic reasoning with graph structure. They
enable scalable algorithms for ranking, clustering, and representation
learning, powering search engines, recommendation systems, and
graph-based AI.

\subsubsection{Try It Yourself}\label{try-it-yourself-177}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simulate a random walk on a triangle graph. Does the stationary
  distribution match degree proportions?
\item
  Compute PageRank scores on a small directed graph using the random
  walk model.
\item
  Explain how biased random walks in node2vec capture both local and
  global graph structure.
\end{enumerate}

\subsection{179. Spectral Clustering}\label{spectral-clustering}

Spectral clustering partitions a graph using the eigenvalues and
eigenvectors of its Laplacian. Instead of clustering directly in the raw
feature space, it embeds nodes into a low-dimensional spectral space
where structure is easier to separate.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-178}

Think of shining light through a prism. The light splits into clear,
separated colors. Similarly, spectral clustering transforms graph data
into a space where groups become naturally separable.

\subsubsection{Deep Dive}\label{deep-dive-178}

\begin{itemize}
\item
  Steps of spectral clustering:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Construct similarity graph and adjacency matrix \(A\).
  \item
    Compute Laplacian \(L = D - A\) (or normalized versions).
  \item
    Find eigenvectors corresponding to the smallest nonzero eigenvalues.
  \item
    Use these eigenvectors as features in k-means clustering.
  \end{enumerate}
\item
  Why it works:

  \begin{itemize}
  \tightlist
  \item
    Eigenvectors encode smooth variations across the graph.
  \item
    Fiedler vector separates weakly connected groups.
  \end{itemize}
\item
  Normalized variants:

  \begin{itemize}
  \tightlist
  \item
    Shi--Malik (normalized cut): uses random-walk Laplacian.
  \item
    Ng--Jordan--Weiss: uses symmetric Laplacian.
  \end{itemize}
\item
  Applications in AI:

  \begin{itemize}
  \tightlist
  \item
    Image segmentation (pixels as graph nodes).
  \item
    Social/community detection.
  \item
    Document clustering.
  \item
    Semi-supervised learning.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2414}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4253}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Variant
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Laplacian Used
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Typical Use Case
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Unnormalized spectral & \(L = D - A\) & Small, balanced graphs \\
Shi--Malik (Ncut) & \(L_{rw} = D^{-1}L\) & Image segmentation,
partitioning \\
Ng--Jordan--Weiss & \(L_{sym} = D^{-1/2}LD^{-1/2}\) & General clustering
with normalization \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-168}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ networkx }\ImportTok{as}\NormalTok{ nx}
\ImportTok{from}\NormalTok{ sklearn.cluster }\ImportTok{import}\NormalTok{ KMeans}

\CommentTok{\# Build simple graph}
\NormalTok{G }\OperatorTok{=}\NormalTok{ nx.karate\_club\_graph()}
\NormalTok{L }\OperatorTok{=}\NormalTok{ nx.normalized\_laplacian\_matrix(G).toarray()}

\CommentTok{\# Eigen{-}decomposition}
\NormalTok{eigs, vecs }\OperatorTok{=}\NormalTok{ np.linalg.eigh(L)}

\CommentTok{\# Use k=2 smallest nonzero eigenvectors}
\NormalTok{X }\OperatorTok{=}\NormalTok{ vecs[:,}\DecValTok{1}\NormalTok{:}\DecValTok{3}\NormalTok{]}
\NormalTok{labels }\OperatorTok{=}\NormalTok{ KMeans(n\_clusters}\OperatorTok{=}\DecValTok{2}\NormalTok{, n\_init}\OperatorTok{=}\DecValTok{10}\NormalTok{).fit\_predict(X)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Spectral clustering labels:"}\NormalTok{, labels[:}\DecValTok{10}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-76}

Spectral clustering harnesses graph structure hidden in data,
outperforming traditional clustering in non-Euclidean or highly
structured datasets. It is a cornerstone method linking graph theory
with machine learning.

\subsubsection{Try It Yourself}\label{try-it-yourself-178}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Perform spectral clustering on a graph with two loosely connected
  clusters. Does the Fiedler vector split them?
\item
  Compare spectral clustering with k-means directly on raw
  coordinates---what differences emerge?
\item
  Apply spectral clustering to an image (treating pixels as nodes). How
  do the clusters map to regions?
\end{enumerate}

\subsection{180. Graph-Based AI
Applications}\label{graph-based-ai-applications}

Graphs naturally capture relationships, making them a central structure
for AI. From social networks to molecules, many domains are best modeled
as nodes and edges. Graph-based AI leverages algorithms and neural
architectures to reason, predict, and learn from such structured data.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-179}

Imagine a detective's board with people, places, and events connected by
strings. Graph-based AI is like training an assistant who not only
remembers all the connections but can also infer missing links and
predict what might happen next.

\subsubsection{Deep Dive}\label{deep-dive-179}

\begin{itemize}
\item
  Knowledge graphs: structured representations of entities and
  relations.

  \begin{itemize}
  \tightlist
  \item
    Used in search engines, question answering, and recommender systems.
  \end{itemize}
\item
  Graph Neural Networks (GNNs): extend deep learning to graphs.

  \begin{itemize}
  \tightlist
  \item
    Message-passing framework: nodes update embeddings based on
    neighbors.
  \item
    Variants: GCN, GAT, GraphSAGE.
  \end{itemize}
\item
  Graph embeddings: map nodes/edges/subgraphs into continuous space.

  \begin{itemize}
  \tightlist
  \item
    Enable link prediction, clustering, classification.
  \end{itemize}
\item
  Graph-based algorithms:

  \begin{itemize}
  \tightlist
  \item
    PageRank: ranking nodes by importance.
  \item
    Community detection: finding clusters of related nodes.
  \item
    Random walks: for node embeddings and sampling.
  \end{itemize}
\item
  Applications across AI:

  \begin{itemize}
  \tightlist
  \item
    NLP: semantic parsing, knowledge graphs.
  \item
    Vision: scene graphs, object relationships.
  \item
    Science: molecular property prediction, drug discovery.
  \item
    Robotics: planning with state-space graphs.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1702}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3830}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4468}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Domain
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Graph Representation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Application
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Social networks & Users as nodes, friendships as edges & Influence
prediction, community detection \\
Knowledge graphs & Entities + relations & Question answering, semantic
search \\
Molecules & Atoms as nodes, bonds as edges & Drug discovery, materials
science \\
Scenes & Objects and their relationships & Visual question answering,
scene reasoning \\
Planning & States as nodes, actions as edges & Robotics, reinforcement
learning \\
\end{longtable}

Tiny Code Sample (Python, Graph Neural Network with PyTorch Geometric)

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{from}\NormalTok{ torch\_geometric.data }\ImportTok{import}\NormalTok{ Data}
\ImportTok{from}\NormalTok{ torch\_geometric.nn }\ImportTok{import}\NormalTok{ GCNConv}

\CommentTok{\# Simple graph with 3 nodes and 2 edges}
\NormalTok{edge\_index }\OperatorTok{=}\NormalTok{ torch.tensor([[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{],}
\NormalTok{                           [}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{]], dtype}\OperatorTok{=}\NormalTok{torch.}\BuiltInTok{long}\NormalTok{)}
\NormalTok{x }\OperatorTok{=}\NormalTok{ torch.tensor([[}\DecValTok{1}\NormalTok{], [}\DecValTok{2}\NormalTok{], [}\DecValTok{3}\NormalTok{]], dtype}\OperatorTok{=}\NormalTok{torch.}\BuiltInTok{float}\NormalTok{)}

\NormalTok{data }\OperatorTok{=}\NormalTok{ Data(x}\OperatorTok{=}\NormalTok{x, edge\_index}\OperatorTok{=}\NormalTok{edge\_index)}

\KeywordTok{class}\NormalTok{ GCN(torch.nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.conv1 }\OperatorTok{=}\NormalTok{ GCNConv(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)}
    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, data):}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.conv1(data.x, data.edge\_index)}

\NormalTok{model }\OperatorTok{=}\NormalTok{ GCN()}
\NormalTok{out }\OperatorTok{=}\NormalTok{ model(data)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Node embeddings:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, out)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-77}

Graphs bridge symbolic reasoning and statistical learning, making them a
powerful tool for AI. They enable AI systems to capture structure,
context, and relationships---crucial for understanding language, vision,
and complex real-world systems.

\subsubsection{Try It Yourself}\label{try-it-yourself-179}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Build a small knowledge graph of three entities and use it to answer
  simple queries.
\item
  Train a GNN on a citation graph dataset and compare with logistic
  regression on node features.
\item
  Explain why graphs are a more natural representation than tables for
  molecules or social networks.
\end{enumerate}

\section{Chapter 19. Logic, Sets and Proof
Techniques}\label{chapter-19.-logic-sets-and-proof-techniques}

\subsection{181. Set Theory Fundamentals}\label{set-theory-fundamentals}

Set theory provides the foundation for modern mathematics, describing
collections of objects and the rules for manipulating them. In AI, sets
underlie probability, logic, databases, and knowledge representation.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-180}

Think of a basket of fruit. The basket is the set, and the fruits are
its elements. You can combine baskets (union), find fruits in both
baskets (intersection), or look at fruits missing from one basket
(difference).

\subsubsection{Deep Dive}\label{deep-dive-180}

\begin{itemize}
\item
  Basic definitions:

  \begin{itemize}
  \tightlist
  \item
    Set = collection of distinct elements.
  \item
    Notation: \(A = \{a, b, c\}\).
  \item
    Empty set: \(\varnothing\).
  \end{itemize}
\item
  Operations:

  \begin{itemize}
  \tightlist
  \item
    Union: \(A \cup B\).
  \item
    Intersection: \(A \cap B\).
  \item
    Difference: \(A \setminus B\).
  \item
    Complement: \(\overline{A}\).
  \end{itemize}
\item
  Special sets:

  \begin{itemize}
  \tightlist
  \item
    Universal set \(U\).
  \item
    Subsets: \(A \subseteq B\).
  \item
    Power set: set of all subsets of \(A\).
  \end{itemize}
\item
  Properties:

  \begin{itemize}
  \tightlist
  \item
    Commutativity, associativity, distributivity.
  \item
    De Morgan's laws:
    \(\overline{A \cup B} = \overline{A} \cap \overline{B}\).
  \end{itemize}
\item
  In AI: forming knowledge bases, defining probability events,
  representing state spaces.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1714}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2143}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.6143}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Operation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Union & \(A \cup B\) & Merging candidate features from two sources \\
Intersection & \(A \cap B\) & Common tokens in NLP vocabulary \\
Difference & \(A \setminus B\) & Features unique to one dataset \\
Power set & \(2^A\) & All possible feature subsets \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-169}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A }\OperatorTok{=}\NormalTok{ \{}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{\}}
\NormalTok{B }\OperatorTok{=}\NormalTok{ \{}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{\}}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Union:"}\NormalTok{, A }\OperatorTok{|}\NormalTok{ B)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Intersection:"}\NormalTok{, A }\OperatorTok{\&}\NormalTok{ B)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Difference:"}\NormalTok{, A }\OperatorTok{{-}}\NormalTok{ B)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Power set:"}\NormalTok{, [\{x }\ControlFlowTok{for}\NormalTok{ i,x }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(A) }\ControlFlowTok{if}\NormalTok{ (mask}\OperatorTok{\textgreater{}\textgreater{}}\NormalTok{i)}\OperatorTok{\&}\DecValTok{1}\NormalTok{\} }
                     \ControlFlowTok{for}\NormalTok{ mask }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\OperatorTok{\textless{}\textless{}}\BuiltInTok{len}\NormalTok{(A))])}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-78}

Set theory provides the language for probability, logic, and data
representation in AI. From defining event spaces in machine learning to
structuring knowledge graphs, sets offer a precise way to reason about
collections.

\subsubsection{Try It Yourself}\label{try-it-yourself-180}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write down two sets of words (e.g., \{cat, dog, fish\}, \{dog,
  bird\}). Compute their union and intersection.
\item
  List the power set of \{a, b\}.
\item
  Use De Morgan's law to simplify \(\overline{(A \cup B)}\) when
  \(A={1,2}\), \(B={2,3}\), \(U={1,2,3,4}\).
\end{enumerate}

\subsection{182. Relations and Functions}\label{relations-and-functions}

Relations describe connections between elements of sets, while functions
are special relations that assign exactly one output to each input.
These ideas underpin mappings, transformations, and dependencies across
mathematics and AI.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-181}

Imagine a school roster. A relation could pair each student with every
course they take. A function is stricter: each student gets exactly one
unique ID number.

\subsubsection{Deep Dive}\label{deep-dive-181}

\begin{itemize}
\item
  Relations:

  \begin{itemize}
  \tightlist
  \item
    A relation \(R\) between sets \(A\) and \(B\) is a subset of
    \(A \times B\).
  \item
    Examples: ``is a friend of,'' ``is greater than.''
  \item
    Properties: reflexive, symmetric, transitive, antisymmetric.
  \end{itemize}
\item
  Equivalence relations: reflexive, symmetric, transitive → partition
  set into equivalence classes.
\item
  Partial orders: reflexive, antisymmetric, transitive → define
  hierarchies.
\item
  Functions:

  \begin{itemize}
  \tightlist
  \item
    Special relation: \(f: A \to B\).
  \item
    Each \(a \in A\) has exactly one \(b \in B\).
  \item
    Surjective (onto), injective (one-to-one), bijective (both).
  \end{itemize}
\item
  In AI:

  \begin{itemize}
  \tightlist
  \item
    Relations: knowledge graphs (entities + relations).
  \item
    Functions: mappings from input features to predictions.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1980}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3564}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4455}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Relation & Subset of \(A \times B\) & User--item rating pairs in
recommender systems \\
Equivalence relation & Reflexive, symmetric, transitive & Grouping
synonyms in NLP \\
Partial order & Reflexive, antisymmetric, transitive & Task dependency
graph in scheduling \\
Function & Maps input to single output & Neural network mapping x → y \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-170}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Relation: list of pairs}
\NormalTok{students }\OperatorTok{=}\NormalTok{ \{}\StringTok{"Alice"}\NormalTok{, }\StringTok{"Bob"}\NormalTok{\}}
\NormalTok{courses }\OperatorTok{=}\NormalTok{ \{}\StringTok{"Math"}\NormalTok{, }\StringTok{"CS"}\NormalTok{\}}
\NormalTok{relation }\OperatorTok{=}\NormalTok{ \{(}\StringTok{"Alice"}\NormalTok{, }\StringTok{"Math"}\NormalTok{), (}\StringTok{"Bob"}\NormalTok{, }\StringTok{"CS"}\NormalTok{), (}\StringTok{"Alice"}\NormalTok{, }\StringTok{"CS"}\NormalTok{)\}}

\CommentTok{\# Function: mapping}
\NormalTok{f }\OperatorTok{=}\NormalTok{ \{}\StringTok{"Alice"}\NormalTok{: }\StringTok{"ID001"}\NormalTok{, }\StringTok{"Bob"}\NormalTok{: }\StringTok{"ID002"}\NormalTok{\}}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Relation:"}\NormalTok{, relation)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Function mapping:"}\NormalTok{, f)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-79}

Relations give AI systems the ability to represent structured
connections like ``works at'' or ``is similar to.'' Functions guarantee
consistent mappings, essential in deterministic prediction tasks. This
distinction underlies both symbolic and statistical approaches to AI.

\subsubsection{Try It Yourself}\label{try-it-yourself-181}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Give an example of a relation that is symmetric but not transitive.
\item
  Define a function \(f: \{1,2,3\} \to \{a,b\}\). Is it surjective?
  Injective?
\item
  Explain why equivalence relations are useful for clustering in AI.
\end{enumerate}

\subsection{183. Propositional Logic}\label{propositional-logic}

Propositional logic formalizes reasoning with statements that can be
true or false. It uses logical operators to build complex expressions
and determine truth systematically.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-182}

Imagine a set of switches that can be either ON (true) or OFF (false).
Combining them with rules like ``AND,'' ``OR,'' and ``NOT'' lets you
create more complex circuits. Propositional logic works like that:
simple truths combine into structured reasoning.

\subsubsection{Deep Dive}\label{deep-dive-182}

\begin{itemize}
\item
  Propositions: declarative statements with truth values (e.g., ``It is
  raining'').
\item
  Logical connectives:

  \begin{itemize}
  \tightlist
  \item
    NOT (¬p): true if p is false.
  \item
    AND (p ∧ q): true if both are true.
  \item
    OR (p ∨ q): true if at least one is true.
  \item
    IMPLIES (p → q): false only if p is true and q is false.
  \item
    IFF (p ↔ q): true if p and q have same truth value.
  \end{itemize}
\item
  Truth tables: define behavior of operators.
\item
  Normal forms:

  \begin{itemize}
  \tightlist
  \item
    CNF (conjunctive normal form): AND of ORs.
  \item
    DNF (disjunctive normal form): OR of ANDs.
  \end{itemize}
\item
  Inference: rules like modus ponens (p → q, p ⇒ q).
\item
  In AI: SAT solvers, planning, rule-based expert systems.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2097}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0968}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2742}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4194}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Operator
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Symbol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example (p=Rain, q=Cloudy)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Negation & ¬p & Opposite truth & ¬p = ``Not raining'' \\
Conjunction & p ∧ q & Both true & ``Raining AND Cloudy'' \\
Disjunction & p ∨ q & At least one true & ``Raining OR Cloudy'' \\
Implication & p → q & If p then q & ``If raining then cloudy'' \\
Biconditional & p ↔ q & Both same truth & ``Raining iff cloudy'' \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-171}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Truth table for implication}
\ImportTok{import}\NormalTok{ itertools}

\KeywordTok{def}\NormalTok{ implies(p, q):}
    \ControlFlowTok{return}\NormalTok{ (}\KeywordTok{not}\NormalTok{ p) }\KeywordTok{or}\NormalTok{ q}

\BuiltInTok{print}\NormalTok{(}\StringTok{"p q | p→q"}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ p, q }\KeywordTok{in}\NormalTok{ itertools.product([}\VariableTok{False}\NormalTok{, }\VariableTok{True}\NormalTok{], repeat}\OperatorTok{=}\DecValTok{2}\NormalTok{):}
    \BuiltInTok{print}\NormalTok{(p, q, }\StringTok{"|"}\NormalTok{, implies(p,q))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-80}

Propositional logic is the simplest formal system of reasoning and the
foundation for more expressive logics. In AI, it powers SAT solvers,
which in turn drive verification, planning, and optimization engines at
scale.

\subsubsection{Try It Yourself}\label{try-it-yourself-182}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Build a truth table for (p ∧ q) → r.
\item
  Convert (¬p ∨ q) into CNF and DNF.
\item
  Explain how propositional logic could represent constraints in a
  scheduling problem.
\end{enumerate}

\subsection{184. Predicate Logic and
Quantifiers}\label{predicate-logic-and-quantifiers}

Predicate logic (first-order logic) extends propositional logic by
allowing statements about objects and their properties, using
quantifiers to express generality. It can capture more complex
relationships and forms the backbone of formal reasoning in AI.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-183}

Think of propositional logic as reasoning with whole sentences: ``It is
raining.'' Predicate logic opens them up: ``For every city, if it is
cloudy, then it rains.'' Quantifiers let us say ``for all'' or ``there
exists,'' making reasoning far richer.

\subsubsection{Deep Dive}\label{deep-dive-183}

\begin{itemize}
\item
  Predicates: functions that return true/false depending on input.

  \begin{itemize}
  \tightlist
  \item
    Example: Likes(Alice, IceCream).
  \end{itemize}
\item
  Quantifiers:

  \begin{itemize}
  \tightlist
  \item
    Universal (∀x P(x)): P(x) holds for all x.
  \item
    Existential (∃x P(x)): P(x) holds for at least one x.
  \end{itemize}
\item
  Syntax examples:

  \begin{itemize}
  \tightlist
  \item
    ∀x (Human(x) → Mortal(x))
  \item
    ∃y (Student(y) ∧ Studies(y, AI))
  \end{itemize}
\item
  Semantics: defined over domains of discourse.
\item
  Inference rules:

  \begin{itemize}
  \tightlist
  \item
    Universal instantiation: from ∀x P(x), infer P(a).
  \item
    Existential generalization: from P(a), infer ∃x P(x).
  \end{itemize}
\item
  In AI: knowledge representation, natural language understanding,
  automated reasoning.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2278}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0759}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4051}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2911}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Element
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Symbol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Predicate & P(x) & Property or relation of object x & Human(Socrates) \\
Universal quant. & ∀x & For all x & ∀x Human(x) → Mortal(x) \\
Existential quant. & ∃x & There exists x & ∃x Loves(x, IceCream) \\
Nested quantifiers & ∀x∃y & For each x, there is a y & ∀x ∃y
Parent(y,x) \\
\end{longtable}

Tiny Code Sample (Python, simple predicate logic)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Domain of people and properties}
\NormalTok{people }\OperatorTok{=}\NormalTok{ [}\StringTok{"Alice"}\NormalTok{, }\StringTok{"Bob"}\NormalTok{, }\StringTok{"Charlie"}\NormalTok{]}
\NormalTok{likes\_icecream }\OperatorTok{=}\NormalTok{ \{}\StringTok{"Alice"}\NormalTok{, }\StringTok{"Charlie"}\NormalTok{\}}

\CommentTok{\# Predicate}
\KeywordTok{def}\NormalTok{ LikesIcecream(x):}
    \ControlFlowTok{return}\NormalTok{ x }\KeywordTok{in}\NormalTok{ likes\_icecream}

\CommentTok{\# Universal quantifier}
\NormalTok{all\_like }\OperatorTok{=} \BuiltInTok{all}\NormalTok{(LikesIcecream(p) }\ControlFlowTok{for}\NormalTok{ p }\KeywordTok{in}\NormalTok{ people)}

\CommentTok{\# Existential quantifier}
\NormalTok{exists\_like }\OperatorTok{=} \BuiltInTok{any}\NormalTok{(LikesIcecream(p) }\ControlFlowTok{for}\NormalTok{ p }\KeywordTok{in}\NormalTok{ people)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"∀x LikesIcecream(x):"}\NormalTok{, all\_like)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"∃x LikesIcecream(x):"}\NormalTok{, exists\_like)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-81}

Predicate logic allows AI systems to represent structured knowledge and
reason with it. Unlike propositional logic, it scales to domains with
many objects and relationships, making it essential for semantic
parsing, theorem proving, and symbolic AI.

\subsubsection{Try It Yourself}\label{try-it-yourself-183}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Express ``All cats are mammals, some mammals are pets'' in predicate
  logic.
\item
  Translate ``Every student studies some course'' into formal notation.
\item
  Explain why predicate logic is more powerful than propositional logic
  for knowledge graphs.
\end{enumerate}

\subsection{185. Logical Inference and
Deduction}\label{logical-inference-and-deduction}

Logical inference is the process of deriving new truths from known ones
using formal rules of deduction. Deduction ensures that if the premises
are true, the conclusion must also be true, providing a foundation for
automated reasoning in AI.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-184}

Think of a chain of dominoes. Each piece represents a logical statement.
If the first falls (premise is true), the rules ensure that the next
falls, and eventually the conclusion is reached without contradiction.

\subsubsection{Deep Dive}\label{deep-dive-184}

\begin{itemize}
\item
  Inference rules:

  \begin{itemize}
  \tightlist
  \item
    Modus Ponens: from \(p → q\) and \(p\), infer \(q\).
  \item
    Modus Tollens: from \(p → q\) and ¬q, infer ¬p.
  \item
    Hypothetical Syllogism: from \(p → q\), \(q → r\), infer \(p → r\).
  \item
    Universal Instantiation: from ∀x P(x), infer P(a).
  \end{itemize}
\item
  Deduction systems:

  \begin{itemize}
  \tightlist
  \item
    Natural deduction (step-by-step reasoning).
  \item
    Resolution (refutation-based).
  \item
    Sequent calculus.
  \end{itemize}
\item
  Soundness: if a conclusion can be derived, it must be true in all
  models.
\item
  Completeness: all truths in the system can, in principle, be derived.
\item
  In AI: SAT solvers, expert systems, theorem proving, program
  verification.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2340}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2553}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5106}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Rule
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formulation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Modus Ponens & \(p, p → q ⟹ q\) & If it rains, the ground gets wet. It
rains ⇒ wet \\
Modus Tollens & \(p → q, ¬q ⟹ ¬p\) & If rain ⇒ wet. Ground not wet ⇒ no
rain \\
Hypothetical Syllogism & \(p → q, q → r ⟹ p → r\) & If A is human ⇒
mortal, mortal ⇒ dies ⇒ A dies \\
Resolution & Eliminate contradictions & Used in SAT solving \\
\end{longtable}

Tiny Code Sample (Python: Modus Ponens)

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ modus\_ponens(p, implication):}
    \CommentTok{\# implication in form (p, q)}
\NormalTok{    antecedent, consequent }\OperatorTok{=}\NormalTok{ implication}
    \ControlFlowTok{if}\NormalTok{ p }\OperatorTok{==}\NormalTok{ antecedent:}
        \ControlFlowTok{return}\NormalTok{ consequent}
    \ControlFlowTok{return} \VariableTok{None}

\BuiltInTok{print}\NormalTok{(}\StringTok{"From (p → q) and p, infer q:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(modus\_ponens(}\StringTok{"It rains"}\NormalTok{, (}\StringTok{"It rains"}\NormalTok{, }\StringTok{"Ground is wet"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-82}

Inference and deduction provide the reasoning backbone for symbolic AI.
They allow systems not just to store knowledge but to derive
consequences, verify consistency, and explain their reasoning
steps---critical for trustworthy AI.

\subsubsection{Try It Yourself}\label{try-it-yourself-184}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use Modus Ponens to infer: ``If AI learns, it improves. AI learns.''
\item
  Show why resolution is powerful for proving contradictions in
  propositional logic.
\item
  Explain how completeness guarantees that no valid inference is left
  unreachable.
\end{enumerate}

\subsection{186. Proof Techniques: Direct, Contradiction,
Induction}\label{proof-techniques-direct-contradiction-induction}

Proof techniques provide structured methods for demonstrating that
statements are true. Direct proofs build step-by-step arguments, proof
by contradiction shows that denying the claim leads to impossibility,
and induction proves statements for all natural numbers by building on
simpler cases.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-185}

Imagine climbing a staircase. Direct proof is like walking up the steps
in order. Proof by contradiction is like assuming the staircase ends
suddenly and discovering that would make the entire building collapse.
Induction is like proving you can step onto the first stair, and if you
can move from one stair to the next, you can reach any stair.

\subsubsection{Deep Dive}\label{deep-dive-185}

\begin{itemize}
\item
  Direct proof:

  \begin{itemize}
  \tightlist
  \item
    Assume premises and apply logical rules until the conclusion is
    reached.
  \item
    Example: prove that the sum of two even numbers is even.
  \end{itemize}
\item
  Proof by contradiction:

  \begin{itemize}
  \tightlist
  \item
    Assume the negation of the statement.
  \item
    Show this assumption leads to inconsistency.
  \item
    Example: proof that √2 is irrational.
  \end{itemize}
\item
  Proof by induction:

  \begin{itemize}
  \tightlist
  \item
    Base case: show statement holds for n=1.
  \item
    Inductive step: assume it holds for n=k, prove it for n=k+1.
  \item
    Example: sum of first n integers = n(n+1)/2.
  \end{itemize}
\item
  Applications in AI: formal verification of algorithms, correctness
  proofs, mathematical foundations of learning theory.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1340}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3505}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5155}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI/Math
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Direct proof & Build argument step by step & Prove gradient descent
converges under assumptions \\
Contradiction & Assume false, derive impossibility & Show no smaller
counterexample exists \\
Induction & Base case + inductive step & Proof of recursive algorithm
correctness \\
\end{longtable}

Tiny Code Sample (Python: Induction Idea)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Verify induction hypothesis for sum of integers}
\KeywordTok{def}\NormalTok{ formula(n):}
    \ControlFlowTok{return}\NormalTok{ n}\OperatorTok{*}\NormalTok{(n}\OperatorTok{+}\DecValTok{1}\NormalTok{)}\OperatorTok{//}\DecValTok{2}

\CommentTok{\# Check base case and a few steps}
\ControlFlowTok{for}\NormalTok{ n }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{6}\NormalTok{):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"n=}\SpecialCharTok{\{}\NormalTok{n}\SpecialCharTok{\}}\SpecialStringTok{, sum=}\SpecialCharTok{\{}\BuiltInTok{sum}\NormalTok{(}\BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{,n}\OperatorTok{+}\DecValTok{1}\NormalTok{))}\SpecialCharTok{\}}\SpecialStringTok{, formula=}\SpecialCharTok{\{}\NormalTok{formula(n)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-83}

Proof techniques give rigor to reasoning in AI and computer science.
They ensure algorithms behave as expected, prevent hidden
contradictions, and provide guarantees---especially important in
safety-critical AI systems.

\subsubsection{Try It Yourself}\label{try-it-yourself-185}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a direct proof that the product of two odd numbers is odd.
\item
  Use contradiction to prove there is no largest prime number.
\item
  Apply induction to show that a binary tree with n nodes has exactly
  n−1 edges.
\end{enumerate}

\subsection{187. Mathematical Induction in
Depth}\label{mathematical-induction-in-depth}

Mathematical induction is a proof technique tailored to statements about
integers or recursively defined structures. It shows that if a property
holds for a base case and persists from \(n\) to \(n+1\), then it holds
universally. Strong induction and structural induction extend the idea
further.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-186}

Think of a row of dominoes. Knocking down the first (base case) and
proving each one pushes the next (inductive step) ensures the whole line
falls. Induction guarantees the truth of infinitely many cases with just
two steps.

\subsubsection{Deep Dive}\label{deep-dive-186}

\begin{itemize}
\item
  Ordinary induction:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Base case: prove statement for \(n=1\).
  \item
    Inductive hypothesis: assume statement holds for \(n=k\).
  \item
    Inductive step: prove statement for \(n=k+1\).
  \end{enumerate}
\item
  Strong induction:

  \begin{itemize}
  \tightlist
  \item
    Assume statement holds for all cases up to \(k\), then prove for
    \(k+1\).
  \item
    Useful when the \(k+1\) case depends on multiple earlier cases.
  \end{itemize}
\item
  Structural induction:

  \begin{itemize}
  \tightlist
  \item
    Extends induction to trees, graphs, or recursively defined data.
  \item
    Base case: prove for simplest structure.
  \item
    Inductive step: assume for substructures, prove for larger ones.
  \end{itemize}
\item
  Applications in AI:

  \begin{itemize}
  \tightlist
  \item
    Proving algorithm correctness (e.g., recursive sorting).
  \item
    Verifying properties of data structures.
  \item
    Formal reasoning about grammars and logical systems.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1800}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2100}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4100}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type of Induction
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Base Case
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Inductive Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI/CS
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Ordinary induction & \(n=1\) & From \(n=k\) ⇒ \(n=k+1\) & Proof of
arithmetic formulas \\
Strong induction & \(n=1\) & From all ≤k ⇒ \(n=k+1\) & Proving
correctness of divide-and-conquer \\
Structural induction & Smallest structure & From parts ⇒ whole & Proof
of correctness for syntax trees \\
\end{longtable}

Tiny Code Sample (Python, checking induction idea)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Verify sum of first n squares formula by brute force}
\KeywordTok{def}\NormalTok{ sum\_squares(n): }\ControlFlowTok{return} \BuiltInTok{sum}\NormalTok{(i}\OperatorTok{*}\NormalTok{i }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{,n}\OperatorTok{+}\DecValTok{1}\NormalTok{))}
\KeywordTok{def}\NormalTok{ formula(n): }\ControlFlowTok{return}\NormalTok{ n}\OperatorTok{*}\NormalTok{(n}\OperatorTok{+}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\NormalTok{(}\DecValTok{2}\OperatorTok{*}\NormalTok{n}\OperatorTok{+}\DecValTok{1}\NormalTok{)}\OperatorTok{//}\DecValTok{6}

\ControlFlowTok{for}\NormalTok{ n }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{6}\NormalTok{):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"n=}\SpecialCharTok{\{}\NormalTok{n}\SpecialCharTok{\}}\SpecialStringTok{, sum=}\SpecialCharTok{\{}\NormalTok{sum\_squares(n)}\SpecialCharTok{\}}\SpecialStringTok{, formula=}\SpecialCharTok{\{}\NormalTok{formula(n)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-84}

Induction provides a rigorous way to prove correctness of AI algorithms
and recursive models. It ensures trust in results across infinite cases,
making it essential in theory, programming, and verification.

\subsubsection{Try It Yourself}\label{try-it-yourself-186}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Prove by induction that \(1+2+...+n = n(n+1)/2\).
\item
  Use strong induction to prove that every integer ≥2 is a product of
  primes.
\item
  Apply structural induction to show that a binary tree with n nodes has
  n−1 edges.
\end{enumerate}

\subsection{188. Recursion and
Well-Foundedness}\label{recursion-and-well-foundedness}

Recursion defines objects or processes in terms of themselves, with a
base case anchoring the definition. Well-foundedness ensures recursion
doesn't loop forever: every recursive call must move closer to a base
case. Together, they guarantee termination and correctness.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-187}

Imagine Russian nesting dolls. Each doll contains a smaller one, until
you reach the smallest. Recursion works the same way---problems are
broken into smaller pieces until the simplest case is reached.

\subsubsection{Deep Dive}\label{deep-dive-187}

\begin{itemize}
\item
  Recursive definitions:

  \begin{itemize}
  \tightlist
  \item
    Factorial: \(n! = n \times (n-1)!\), with \(0! = 1\).
  \item
    Fibonacci: \(F(n) = F(n-1) + F(n-2)\), with \(F(0)=0, F(1)=1\).
  \end{itemize}
\item
  Well-foundedness:

  \begin{itemize}
  \tightlist
  \item
    Requires a measure (like size of n) that decreases at every step.
  \item
    Prevents infinite descent.
  \end{itemize}
\item
  Structural recursion:

  \begin{itemize}
  \tightlist
  \item
    Defined on data structures like lists or trees.
  \item
    Example: sum of list = head + sum(tail).
  \end{itemize}
\item
  Applications in AI:

  \begin{itemize}
  \tightlist
  \item
    Recursive search (DFS, minimax in games).
  \item
    Recursive neural networks for structured data.
  \item
    Inductive definitions in knowledge representation.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2353}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3882}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3765}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Base case & Anchor for recursion & \(F(0)=0\), \(F(1)=1\) in
Fibonacci \\
Recursive case & Define larger in terms of smaller & DFS visits
neighbors recursively \\
Well-foundedness & Guarantees termination & Depth decreases in search \\
Structural recursion & Recursion on data structures & Parsing trees in
NLP \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-172}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ factorial(n):}
    \ControlFlowTok{if}\NormalTok{ n }\OperatorTok{==} \DecValTok{0}\NormalTok{:   }\CommentTok{\# base case}
        \ControlFlowTok{return} \DecValTok{1}
    \ControlFlowTok{return}\NormalTok{ n }\OperatorTok{*}\NormalTok{ factorial(n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{)  }\CommentTok{\# recursive case}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Factorial 5:"}\NormalTok{, factorial(}\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-85}

Recursion is fundamental to algorithms, data structures, and AI
reasoning. Ensuring well-foundedness avoids infinite loops and
guarantees correctness---critical for search algorithms, symbolic
reasoning, and recursive neural models.

\subsubsection{Try It Yourself}\label{try-it-yourself-187}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a recursive function to compute the nth Fibonacci number. Prove
  it terminates.
\item
  Define a recursive function to count nodes in a binary tree.
\item
  Explain how minimax recursion in game AI relies on well-foundedness.
\end{enumerate}

\subsection{189. Formal Systems and
Completeness}\label{formal-systems-and-completeness}

A formal system is a framework consisting of symbols, rules for forming
expressions, and rules for deriving theorems. Completeness describes
whether the system can express and prove all truths within its intended
scope. Together, they define the boundaries of formal reasoning in
mathematics and AI.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-188}

Imagine a game with pieces (symbols), rules for valid moves (syntax),
and strategies to reach checkmate (proofs). A formal system is like such
a game---but instead of chess, it encodes mathematics or logic.
Completeness asks: ``Can every winning position be reached using the
rules?''

\subsubsection{Deep Dive}\label{deep-dive-188}

\begin{itemize}
\item
  Components of a formal system:

  \begin{itemize}
  \tightlist
  \item
    Alphabet: finite set of symbols.
  \item
    Grammar: rules to build well-formed formulas.
  \item
    Axioms: starting truths.
  \item
    Inference rules: how to derive theorems.
  \end{itemize}
\item
  Soundness: everything derivable is true.
\item
  Completeness: everything true is derivable.
\item
  Gödel's completeness theorem (first-order logic): every logically
  valid formula can be proven.
\item
  Gödel's incompleteness theorem: in arithmetic, no consistent formal
  system can be both complete and decidable.
\item
  In AI:

  \begin{itemize}
  \tightlist
  \item
    Used in theorem provers, logic programming (Prolog).
  \item
    Defines limits of symbolic reasoning.
  \item
    Influences design of verification tools and knowledge
    representation.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4388}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4184}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI/Logic
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Formal system & Symbols + rules for expressions + inference &
Propositional calculus, first-order logic \\
Soundness & Derivations ⊆ truths & No false theorem provable \\
Completeness & Truths ⊆ derivations & All valid statements can be
proved \\
Incompleteness & Some truths unprovable in system & Gödel's theorem for
arithmetic \\
\end{longtable}

Tiny Code Sample (Prolog Example)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\% Simple formal system in Prolog}
\NormalTok{parent(alice}\KeywordTok{,}\NormalTok{ bob)}\KeywordTok{.}
\NormalTok{parent(bob}\KeywordTok{,}\NormalTok{ carol)}\KeywordTok{.}

\NormalTok{ancestor(}\DataTypeTok{X}\KeywordTok{,}\DataTypeTok{Y}\NormalTok{) }\KeywordTok{:{-}}\NormalTok{ parent(}\DataTypeTok{X}\KeywordTok{,}\DataTypeTok{Y}\NormalTok{)}\KeywordTok{.}
\NormalTok{ancestor(}\DataTypeTok{X}\KeywordTok{,}\DataTypeTok{Y}\NormalTok{) }\KeywordTok{:{-}}\NormalTok{ parent(}\DataTypeTok{X}\KeywordTok{,}\DataTypeTok{Z}\NormalTok{)}\KeywordTok{,}\NormalTok{ ancestor(}\DataTypeTok{Z}\KeywordTok{,}\DataTypeTok{Y}\NormalTok{)}\KeywordTok{.}

\CommentTok{\% Query: ?{-} ancestor(alice, carol).}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-86}

Formal systems and completeness define the power and limits of
logic-based AI. They ensure reasoning is rigorous but also highlight
boundaries---no single system can capture all mathematical truths. This
awareness shapes how AI blends symbolic and statistical approaches.

\subsubsection{Try It Yourself}\label{try-it-yourself-188}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Define axioms and inference rules for propositional logic as a formal
  system.
\item
  Explain the difference between soundness and completeness using an
  example.
\item
  Reflect on why Gödel's incompleteness is important for AI safety and
  reasoning.
\end{enumerate}

\subsection{190. Logic in AI Reasoning
Systems}\label{logic-in-ai-reasoning-systems}

Logic provides a structured way for AI systems to represent knowledge
and reason with it. From rule-based systems to modern neuro-symbolic AI,
logical reasoning enables deduction, consistency checking, and
explanation.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-189}

Think of an AI as a detective. It gathers facts (``Alice is Bob's
parent''), applies rules (``All parents are ancestors''), and deduces
new conclusions (``Alice is Carol's ancestor''). Logic gives the
detective both the notebook (representation) and the reasoning rules
(inference).

\subsubsection{Deep Dive}\label{deep-dive-189}

\begin{itemize}
\item
  Rule-based reasoning:

  \begin{itemize}
  \tightlist
  \item
    Expert systems represent knowledge as IF--THEN rules.
  \item
    Inference engines apply forward or backward chaining.
  \end{itemize}
\item
  Knowledge representation:

  \begin{itemize}
  \tightlist
  \item
    Ontologies and semantic networks structure logical relationships.
  \item
    Description logics form the basis of the Semantic Web.
  \end{itemize}
\item
  Uncertainty in logic:

  \begin{itemize}
  \tightlist
  \item
    Probabilistic logics combine probability with deductive reasoning.
  \item
    Useful for noisy, real-world AI.
  \end{itemize}
\item
  Neuro-symbolic integration:

  \begin{itemize}
  \tightlist
  \item
    Combines neural networks with logical reasoning.
  \item
    Example: neural models extract facts, logic enforces consistency.
  \end{itemize}
\item
  Applications:

  \begin{itemize}
  \tightlist
  \item
    Automated planning and scheduling.
  \item
    Natural language understanding.
  \item
    Verification of AI models.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2381}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3714}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3905}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mechanism
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in AI
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Rule-based expert systems & Forward/backward chaining & Medical
diagnosis (MYCIN) \\
Description logics & Formal semantics for ontologies & Semantic Web,
knowledge graphs \\
Probabilistic logics & Add uncertainty to logical frameworks & AI for
robotics in uncertain environments \\
Neuro-symbolic AI & Neural + symbolic reasoning integration &
Knowledge-grounded NLP \\
\end{longtable}

Tiny Code Sample (Prolog)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\% Facts}
\NormalTok{parent(alice}\KeywordTok{,}\NormalTok{ bob)}\KeywordTok{.}
\NormalTok{parent(bob}\KeywordTok{,}\NormalTok{ carol)}\KeywordTok{.}

\CommentTok{\% Rule}
\NormalTok{ancestor(}\DataTypeTok{X}\KeywordTok{,}\DataTypeTok{Y}\NormalTok{) }\KeywordTok{:{-}}\NormalTok{ parent(}\DataTypeTok{X}\KeywordTok{,}\DataTypeTok{Y}\NormalTok{)}\KeywordTok{.}
\NormalTok{ancestor(}\DataTypeTok{X}\KeywordTok{,}\DataTypeTok{Y}\NormalTok{) }\KeywordTok{:{-}}\NormalTok{ parent(}\DataTypeTok{X}\KeywordTok{,}\DataTypeTok{Z}\NormalTok{)}\KeywordTok{,}\NormalTok{ ancestor(}\DataTypeTok{Z}\KeywordTok{,}\DataTypeTok{Y}\NormalTok{)}\KeywordTok{.}

\CommentTok{\% Query: ?{-} ancestor(alice, carol).}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-87}

Logic brings transparency, interpretability, and rigor to AI. While deep
learning excels at pattern recognition, logic ensures decisions are
consistent and explainable---critical for safety, fairness, and
accountability.

\subsubsection{Try It Yourself}\label{try-it-yourself-189}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write three facts about family relationships and a rule to infer
  grandparents.
\item
  Show how forward chaining can derive new knowledge from initial facts.
\item
  Explain how logic could complement deep learning in natural language
  question answering.
\end{enumerate}

\section{Chapter 20. Stochastic Process and Markov
chains}\label{chapter-20.-stochastic-process-and-markov-chains}

\subsection{191. Random Processes and
Sequences}\label{random-processes-and-sequences}

A random process is a collection of random variables indexed by time or
space, describing how uncertainty evolves. Sequences like coin tosses,
signals, or sensor readings can be modeled as realizations of such
processes, forming the basis for stochastic modeling in AI.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-190}

Think of flipping a coin repeatedly. Each toss is uncertain, but
together they form a sequence with a well-defined structure. Over time,
patterns emerge---like the proportion of heads approaching 0.5.

\subsubsection{Deep Dive}\label{deep-dive-190}

\begin{itemize}
\item
  Random sequences: ordered collections of random variables
  \(\{X_t\}_{t=1}^\infty\).
\item
  Random processes: map from index set (time, space) to outcomes.

  \begin{itemize}
  \tightlist
  \item
    Discrete-time vs continuous-time.
  \item
    Discrete-state vs continuous-state.
  \end{itemize}
\item
  Key properties:

  \begin{itemize}
  \tightlist
  \item
    Mean function: \(m(t) = E[X_t]\).
  \item
    Autocorrelation: \(R(s,t) = E[X_s X_t]\).
  \item
    Stationarity: statistical properties invariant over time.
  \end{itemize}
\item
  Examples:

  \begin{itemize}
  \tightlist
  \item
    IID sequence: independent identically distributed.
  \item
    Random walk: sum of IID noise terms.
  \item
    Gaussian process: every finite subset has multivariate normal
    distribution.
  \end{itemize}
\item
  Applications in AI:

  \begin{itemize}
  \tightlist
  \item
    Time-series prediction.
  \item
    Bayesian optimization (Gaussian processes).
  \item
    Modeling sensor noise in robotics.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1818}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3977}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4205}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Process Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
IID sequence & Independent, identical distribution & Shuffling training
data \\
Random walk & Incremental sum of noise & Stock price models \\
Gaussian process & Distribution over functions & Bayesian regression \\
Poisson process & Random events over time & Queueing systems, rare event
modeling \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-173}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\CommentTok{\# Simulate random walk}
\NormalTok{np.random.seed(}\DecValTok{0}\NormalTok{)}
\NormalTok{steps }\OperatorTok{=}\NormalTok{ np.random.choice([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{], size}\OperatorTok{=}\DecValTok{100}\NormalTok{)}
\NormalTok{random\_walk }\OperatorTok{=}\NormalTok{ np.cumsum(steps)}

\NormalTok{plt.plot(random\_walk)}
\NormalTok{plt.title(}\StringTok{"Random Walk"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-88}

Random processes provide the mathematical foundation for uncertainty
over time. In AI, they power predictive models, reinforcement learning,
Bayesian inference, and uncertainty quantification. Without them,
modeling dynamic, noisy environments would be impossible.

\subsubsection{Try It Yourself}\label{try-it-yourself-190}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simulate 100 coin tosses and compute the empirical frequency of heads.
\item
  Generate a Gaussian process with mean 0 and RBF kernel, and sample 3
  functions.
\item
  Explain how a random walk could model user behavior in recommendation
  systems.
\end{enumerate}

\subsection{192. Stationarity and
Ergodicity}\label{stationarity-and-ergodicity}

Stationarity describes when the statistical properties of a random
process do not change over time. Ergodicity ensures that long-run
averages from a single sequence equal expectations over the entire
process. Together, they provide the foundations for making reliable
inferences from time series.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-191}

Imagine watching waves at the beach. If the overall pattern of wave
height doesn't change day to day, the process is stationary. If one long
afternoon of observation gives you the same average as many afternoons
combined, the process is ergodic.

\subsubsection{Deep Dive}\label{deep-dive-191}

\begin{itemize}
\item
  Stationarity:

  \begin{itemize}
  \tightlist
  \item
    \emph{Strict-sense}: all joint distributions are time-invariant.
  \item
    \emph{Weak-sense}: mean and autocovariance depend only on lag, not
    absolute time.
  \item
    Examples: white noise (stationary), stock prices (non-stationary).
  \end{itemize}
\item
  Ergodicity:

  \begin{itemize}
  \tightlist
  \item
    Ensures time averages ≈ ensemble averages.
  \item
    Needed when we only have one sequence (common in practice).
  \end{itemize}
\item
  Testing stationarity:

  \begin{itemize}
  \tightlist
  \item
    Visual inspection (mean, variance drift).
  \item
    Unit root tests (ADF, KPSS).
  \end{itemize}
\item
  Applications in AI:

  \begin{itemize}
  \tightlist
  \item
    Reliable training on time-series data.
  \item
    Reinforcement learning policies assume ergodicity of environment
    states.
  \item
    Signal processing in robotics and speech.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2088}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4396}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3516}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Strict stationarity & Full distribution time-invariant & White noise
process \\
Weak stationarity & Mean, variance stable; covariance by lag & ARMA
models in forecasting \\
Ergodicity & Time average = expectation & Long-run reward estimation in
RL \\
\end{longtable}

Tiny Code Sample (Python, checking weak stationarity)

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ statsmodels.tsa.stattools }\ImportTok{import}\NormalTok{ adfuller}

\CommentTok{\# Generate AR(1) process: X\_t = 0.7 X\_\{t{-}1\} + noise}
\NormalTok{np.random.seed(}\DecValTok{0}\NormalTok{)}
\NormalTok{n }\OperatorTok{=} \DecValTok{200}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.zeros(n)}
\ControlFlowTok{for}\NormalTok{ t }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, n):}
\NormalTok{    x[t] }\OperatorTok{=} \FloatTok{0.7} \OperatorTok{*}\NormalTok{ x[t}\OperatorTok{{-}}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\NormalTok{ np.random.randn()}

\NormalTok{plt.plot(x)}
\NormalTok{plt.title(}\StringTok{"AR(1) Process"}\NormalTok{)}
\NormalTok{plt.show()}

\CommentTok{\# Augmented Dickey{-}Fuller test for stationarity}
\NormalTok{result }\OperatorTok{=}\NormalTok{ adfuller(x)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"ADF p{-}value:"}\NormalTok{, result[}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-89}

AI systems often rely on single observed sequences (like user logs or
sensor readings). Stationarity and ergodicity justify treating those
samples as representative of the whole process, enabling robust
forecasting, learning, and decision-making.

\subsubsection{Try It Yourself}\label{try-it-yourself-191}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simulate a random walk and test if it is stationary.
\item
  Compare the sample mean of one long trajectory to averages across many
  simulations.
\item
  Explain why non-stationarity (e.g., concept drift) is a major
  challenge for deployed AI models.
\end{enumerate}

\subsection{193. Discrete-Time Markov
Chains}\label{discrete-time-markov-chains}

A discrete-time Markov chain (DTMC) is a stochastic process where the
next state depends only on the current state, not the past history. This
memoryless property makes Markov chains a cornerstone of probabilistic
modeling in AI.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-192}

Think of a board game where each move depends only on the square you're
currently on and the dice roll---not on how you got there. That's how a
Markov chain works: the present fully determines the future.

\subsubsection{Deep Dive}\label{deep-dive-192}

\begin{itemize}
\item
  Definition:

  \begin{itemize}
  \item
    Sequence of random variables \(\{X_t\}\).
  \item
    Markov property:

    \[
    P(X_{t+1} \mid X_t, X_{t-1}, \dots, X_0) = P(X_{t+1} \mid X_t).
    \]
  \end{itemize}
\item
  Transition matrix \(P\):

  \begin{itemize}
  \tightlist
  \item
    \(P_{ij} = P(X_{t+1}=j \mid X_t=i)\).
  \item
    Rows sum to 1.
  \end{itemize}
\item
  Key properties:

  \begin{itemize}
  \tightlist
  \item
    Irreducibility: all states reachable.
  \item
    Periodicity: cycles of fixed length.
  \item
    Stationary distribution: \(\pi = \pi P\).
  \item
    Convergence: under mild conditions, DTMC converges to stationary
    distribution.
  \end{itemize}
\item
  Applications in AI:

  \begin{itemize}
  \tightlist
  \item
    Web search (PageRank).
  \item
    Hidden Markov Models (HMMs) in NLP.
  \item
    Reinforcement learning state transitions.
  \item
    Stochastic simulations.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2584}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4045}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3371}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Term
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Transition matrix & Probability of moving between states & PageRank
random surfer \\
Stationary distribution & Long-run probabilities & Importance ranking in
networks \\
Irreducible chain & Every state reachable & Exploration in RL
environments \\
Periodicity & Fixed cycles of states & Oscillatory processes \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-174}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Transition matrix for 3 states}
\NormalTok{P }\OperatorTok{=}\NormalTok{ np.array([[}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.6}\NormalTok{, }\FloatTok{0.3}\NormalTok{],}
\NormalTok{              [}\FloatTok{0.4}\NormalTok{, }\FloatTok{0.4}\NormalTok{, }\FloatTok{0.2}\NormalTok{],}
\NormalTok{              [}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.5}\NormalTok{]])}

\CommentTok{\# Simulate Markov chain}
\NormalTok{n\_steps }\OperatorTok{=} \DecValTok{10}
\NormalTok{state }\OperatorTok{=} \DecValTok{0}
\NormalTok{trajectory }\OperatorTok{=}\NormalTok{ [state]}
\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n\_steps):}
\NormalTok{    state }\OperatorTok{=}\NormalTok{ np.random.choice([}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], p}\OperatorTok{=}\NormalTok{P[state])}
\NormalTok{    trajectory.append(state)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Trajectory:"}\NormalTok{, trajectory)}

\CommentTok{\# Approximate stationary distribution}
\NormalTok{dist }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{]) }\OperatorTok{@}\NormalTok{ np.linalg.matrix\_power(P, }\DecValTok{50}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Stationary distribution:"}\NormalTok{, dist)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-90}

DTMCs strike a balance between simplicity and expressive power. They
model dynamic systems where history matters only through the current
state---perfect for many AI domains like sequence prediction, decision
processes, and probabilistic planning.

\subsubsection{Try It Yourself}\label{try-it-yourself-192}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Construct a 2-state weather model (sunny, rainy). Simulate 20 days.
\item
  Compute the stationary distribution of your model. What does it mean?
\item
  Explain why the Markov property simplifies reinforcement learning
  algorithms.
\end{enumerate}

\subsection{194. Continuous-Time Markov
Processes}\label{continuous-time-markov-processes}

Continuous-Time Markov Processes (CTMPs) extend the Markov property to
continuous time. Instead of stepping forward in discrete ticks, the
system evolves with random waiting times between transitions, often
modeled with exponential distributions.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-193}

Imagine customers arriving at a bank. The arrivals don't happen exactly
every 5 minutes, but randomly---sometimes quickly, sometimes after a
long gap. The ``clock'' is continuous, and the process is still
memoryless: the future depends only on the current state, not how long
you've been waiting.

\subsubsection{Deep Dive}\label{deep-dive-193}

\begin{itemize}
\item
  Definition:

  \begin{itemize}
  \item
    A stochastic process \(\{X(t)\}_{t \geq 0}\) with state space \(S\).
  \item
    Markov property:

    \[
    P(X(t+\Delta t)=j \mid X(t)=i, \text{history}) = P(X(t+\Delta t)=j \mid X(t)=i).
    \]
  \end{itemize}
\item
  Transition rates (generator matrix \(Q\)):

  \begin{itemize}
  \tightlist
  \item
    \(Q_{ij} \geq 0\) for \(i \neq j\).
  \item
    \(Q_{ii} = -\sum_{j \neq i} Q_{ij}\).
  \item
    Probability of leaving state \(i\) in small interval \(\Delta t\):
    \(-Q_{ii}\Delta t\).
  \end{itemize}
\item
  Waiting times:

  \begin{itemize}
  \tightlist
  \item
    Time spent in a state is exponentially distributed.
  \end{itemize}
\item
  Stationary distribution:

  \begin{itemize}
  \tightlist
  \item
    Solve \(\pi Q = 0\), with \(\sum_i \pi_i = 1\).
  \end{itemize}
\item
  Applications in AI:

  \begin{itemize}
  \tightlist
  \item
    Queueing models in computer systems.
  \item
    Continuous-time reinforcement learning.
  \item
    Reliability modeling for robotics and networks.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2421}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3579}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula / Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Generator matrix \(Q\) & Rates of transition between states & System
reliability analysis \\
Exponential waiting & \(P(T>t)=e^{-\lambda t}\) & Customer arrivals in
queueing models \\
Stationary distribution & \(\pi Q = 0\) & Long-run uptime vs downtime of
systems \\
\end{longtable}

Tiny Code Sample (Python, simulating CTMC)

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Generator matrix Q for 2{-}state system}
\NormalTok{Q }\OperatorTok{=}\NormalTok{ np.array([[}\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{],}
\NormalTok{              [}\FloatTok{0.2}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.2}\NormalTok{]])}

\NormalTok{n\_steps }\OperatorTok{=} \DecValTok{5}
\NormalTok{state }\OperatorTok{=} \DecValTok{0}
\NormalTok{times }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{]}
\NormalTok{trajectory }\OperatorTok{=}\NormalTok{ [state]}

\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n\_steps):}
\NormalTok{    rate }\OperatorTok{=} \OperatorTok{{-}}\NormalTok{Q[state,state]}
\NormalTok{    wait }\OperatorTok{=}\NormalTok{ np.random.exponential(}\DecValTok{1}\OperatorTok{/}\NormalTok{rate)  }\CommentTok{\# exponential waiting time}
\NormalTok{    next\_state }\OperatorTok{=}\NormalTok{ np.random.choice([}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{], p}\OperatorTok{=}\NormalTok{[}\FloatTok{0.0} \ControlFlowTok{if}\NormalTok{ i}\OperatorTok{==}\NormalTok{state }\ControlFlowTok{else}\NormalTok{ Q[state,i]}\OperatorTok{/}\NormalTok{rate }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in}\NormalTok{ [}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{]])}
\NormalTok{    times.append(times[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\NormalTok{wait)}
\NormalTok{    trajectory.append(next\_state)}
\NormalTok{    state }\OperatorTok{=}\NormalTok{ next\_state}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Times:"}\NormalTok{, times)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Trajectory:"}\NormalTok{, trajectory)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-91}

Many AI systems operate in real time where events occur
irregularly---like network failures, user interactions, or biological
processes. Continuous-time Markov processes capture these dynamics,
bridging probability theory and practical system modeling.

\subsubsection{Try It Yourself}\label{try-it-yourself-193}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Model a machine that alternates between \emph{working} and
  \emph{failed} with exponential waiting times.
\item
  Compute the stationary distribution for the machine's uptime.
\item
  Explain why CTMPs are better suited than DTMCs for modeling network
  traffic.
\end{enumerate}

\subsection{195. Transition Matrices and
Probabilities}\label{transition-matrices-and-probabilities}

Transition matrices describe how probabilities shift between states in a
Markov process. Each row encodes the probability distribution of moving
from one state to all others. They provide a compact and powerful way to
analyze dynamics and long-term behavior.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-194}

Think of a subway map where each station is a state. The transition
matrix is like the schedule: from each station, it lists the
probabilities of ending up at the others after one ride.

\subsubsection{Deep Dive}\label{deep-dive-194}

\begin{itemize}
\item
  Transition matrix (discrete-time Markov chain):

  \begin{itemize}
  \tightlist
  \item
    \(P_{ij} = P(X_{t+1}=j \mid X_t=i)\).
  \item
    Rows sum to 1.
  \end{itemize}
\item
  n-step transitions:

  \begin{itemize}
  \tightlist
  \item
    \(P^n\) gives probability of moving between states in n steps.
  \end{itemize}
\item
  Stationary distribution:

  \begin{itemize}
  \tightlist
  \item
    Vector \(\pi\) with \(\pi P = \pi\).
  \end{itemize}
\item
  Continuous-time case (generator matrix Q):

  \begin{itemize}
  \item
    Transition probabilities obtained via matrix exponential:

    \[
    P(t) = e^{Qt}.
    \]
  \end{itemize}
\item
  Applications in AI:

  \begin{itemize}
  \tightlist
  \item
    PageRank and ranking algorithms.
  \item
    Hidden Markov Models for NLP and speech.
  \item
    Modeling policies in reinforcement learning.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3108}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1757}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5135}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
One-step probability & \(P_{ij}\) & Next word prediction in HMM \\
n-step probability & \(P^n_{ij}\) & Multi-step planning in RL \\
Stationary distribution & \(\pi P = \pi\) & Long-run importance in
PageRank \\
Continuous-time & \(P(t)=e^{Qt}\) & Reliability modeling, queueing
systems \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-175}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Transition matrix for 3{-}state chain}
\NormalTok{P }\OperatorTok{=}\NormalTok{ np.array([[}\FloatTok{0.7}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.1}\NormalTok{],}
\NormalTok{              [}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.6}\NormalTok{, }\FloatTok{0.3}\NormalTok{],}
\NormalTok{              [}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.5}\NormalTok{]])}

\CommentTok{\# Two{-}step transition probabilities}
\NormalTok{P2 }\OperatorTok{=}\NormalTok{ np.linalg.matrix\_power(P, }\DecValTok{2}\NormalTok{)}

\CommentTok{\# Stationary distribution (approximate via power method)}
\NormalTok{pi }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{]) }\OperatorTok{@}\NormalTok{ np.linalg.matrix\_power(P, }\DecValTok{50}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"P\^{}2:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, P2)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Stationary distribution:"}\NormalTok{, pi)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-92}

Transition matrices turn probabilistic dynamics into linear algebra,
enabling efficient computation of future states, long-run distributions,
and stability analysis. This bridges stochastic processes with numerical
methods, making them core to AI reasoning under uncertainty.

\subsubsection{Try It Yourself}\label{try-it-yourself-194}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Construct a 2-state transition matrix for weather (sunny, rainy).
  Compute probabilities after 3 days.
\item
  Find the stationary distribution of a 3-state Markov chain by solving
  \(\pi P = \pi\).
\item
  Explain why transition matrices are key to reinforcement learning
  policy evaluation.
\end{enumerate}

\subsection{196. Markov Property and
Memorylessness}\label{markov-property-and-memorylessness}

The Markov property states that the future of a process depends only on
its present state, not its past history. This ``memorylessness''
simplifies modeling dynamic systems, allowing them to be described with
transition probabilities instead of full histories.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-195}

Imagine standing at a crossroads. To decide where you'll go next, you
only need to know where you are now---not the exact path you took to get
there.

\subsubsection{Deep Dive}\label{deep-dive-195}

\begin{itemize}
\item
  Formal definition: A stochastic process \(\{X_t\}\) has the Markov
  property if

  \[
  P(X_{t+1} \mid X_t, X_{t-1}, \ldots, X_0) = P(X_{t+1} \mid X_t).
  \]
\item
  Memorylessness:

  \begin{itemize}
  \tightlist
  \item
    In discrete-time Markov chains, the next state depends only on the
    current state.
  \item
    In continuous-time Markov processes, the waiting time in each state
    is exponentially distributed, which is also memoryless.
  \end{itemize}
\item
  Consequences:

  \begin{itemize}
  \tightlist
  \item
    Simplifies analysis of stochastic systems.
  \item
    Enables recursive computation of probabilities.
  \item
    Forms basis for dynamic programming.
  \end{itemize}
\item
  Limitations:

  \begin{itemize}
  \tightlist
  \item
    Not all processes are Markovian (e.g., stock markets with long-term
    dependencies).
  \item
    Extensions: higher-order Markov models, hidden Markov models.
  \end{itemize}
\item
  Applications in AI:

  \begin{itemize}
  \tightlist
  \item
    Reinforcement learning environments.
  \item
    Hidden Markov Models in NLP and speech recognition.
  \item
    State-space models for robotics and planning.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1613}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3978}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4409}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Markov property & Future depends only on present & Reinforcement
learning policies \\
Memorylessness & No dependency on elapsed time/history & Exponential
waiting times in CTMCs \\
Extension & Higher-order or hidden Markov models & Part-of-speech
tagging, sequence labeling \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-176}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Simple 2{-}state Markov chain: Sunny (0), Rainy (1)}
\NormalTok{P }\OperatorTok{=}\NormalTok{ np.array([[}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{],}
\NormalTok{              [}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{]])}

\NormalTok{state }\OperatorTok{=} \DecValTok{0}  \CommentTok{\# start Sunny}
\NormalTok{trajectory }\OperatorTok{=}\NormalTok{ [state]}
\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{10}\NormalTok{):}
\NormalTok{    state }\OperatorTok{=}\NormalTok{ np.random.choice([}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{], p}\OperatorTok{=}\NormalTok{P[state])}
\NormalTok{    trajectory.append(state)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Weather trajectory:"}\NormalTok{, trajectory)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-93}

The Markov property reduces complexity by removing dependence on the
full past, making dynamic systems tractable for analysis and learning.
Without it, reinforcement learning and probabilistic planning would be
computationally intractable.

\subsubsection{Try It Yourself}\label{try-it-yourself-195}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write down a simple 3-state Markov chain and verify the Markov
  property holds.
\item
  Explain how the exponential distribution's memorylessness supports
  continuous-time Markov processes.
\item
  Discuss a real-world process that violates the Markov
  property---what's missing?
\end{enumerate}

\subsection{197. Martingales and
Applications}\label{martingales-and-applications}

A martingale is a stochastic process where the conditional expectation
of the next value equals the current value, given all past information.
In other words, martingales are ``fair game'' processes with no
predictable trend up or down.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-196}

Think of repeatedly betting on a fair coin toss. Your expected fortune
after the next toss is exactly your current fortune, regardless of how
many wins or losses you've had before.

\subsubsection{Deep Dive}\label{deep-dive-196}

\begin{itemize}
\item
  Formal definition: A process \(\{X_t\}\) is a martingale with respect
  to a filtration \(\mathcal{F}_t\) if:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \(E[|X_t|] < \infty\).
  \item
    \(E[X_{t+1} \mid \mathcal{F}_t] = X_t\).
  \end{enumerate}
\item
  Submartingale: expectation increases
  (\(E[X_{t+1}\mid \mathcal{F}_t] \geq X_t\)).
\item
  Supermartingale: expectation decreases.
\item
  Key properties:

  \begin{itemize}
  \tightlist
  \item
    Martingale convergence theorem: under conditions, martingales
    converge almost surely.
  \item
    Optional stopping theorem: stopping a martingale at a fair time
    preserves expectation.
  \end{itemize}
\item
  Applications in AI:

  \begin{itemize}
  \tightlist
  \item
    Analysis of randomized algorithms.
  \item
    Reinforcement learning (value estimates as martingales).
  \item
    Finance models (asset prices under no-arbitrage).
  \item
    Bandit problems and regret analysis.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1848}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3696}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4457}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Martingale & Fair game, expected next = current & RL value updates under
unbiased estimates \\
Submartingale & Expected value grows & Regret bounds in online
learning \\
Supermartingale & Expected value shrinks & Discounted reward models \\
Optional stopping & Fairness persists under stopping & Termination in
stochastic simulations \\
\end{longtable}

\subsubsection{Tiny Code}\label{tiny-code-177}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{np.random.seed(}\DecValTok{0}\NormalTok{)}
\NormalTok{n }\OperatorTok{=} \DecValTok{20}
\NormalTok{steps }\OperatorTok{=}\NormalTok{ np.random.choice([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{], size}\OperatorTok{=}\NormalTok{n)  }\CommentTok{\# fair coin tosses}
\NormalTok{martingale }\OperatorTok{=}\NormalTok{ np.cumsum(steps)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Martingale sequence:"}\NormalTok{, martingale)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Expectation \textasciitilde{} 0:"}\NormalTok{, martingale.mean())}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-94}

Martingales provide the mathematical language for fairness, stability,
and unpredictability in stochastic systems. They allow AI researchers to
prove convergence guarantees, analyze uncertainty, and ensure robustness
in algorithms.

\subsubsection{Try It Yourself}\label{try-it-yourself-196}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simulate a random walk and check if it is a martingale.
\item
  Give an example of a process that is a submartingale but not a
  martingale.
\item
  Explain why martingale analysis is important in proving reinforcement
  learning convergence.
\end{enumerate}

\subsection{198. Hidden Markov Models}\label{hidden-markov-models}

A Hidden Markov Model (HMM) is a probabilistic model where the system
evolves through hidden states according to a Markov chain, but we only
observe outputs generated probabilistically from those states. HMMs
bridge unobservable dynamics and observable data.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-197}

Imagine trying to infer the weather based only on whether people carry
umbrellas. The actual weather (hidden state) follows a Markov chain,
while the umbrellas you see (observations) are noisy signals of it.

\subsubsection{Deep Dive}\label{deep-dive-197}

\begin{itemize}
\item
  Model structure:

  \begin{itemize}
  \tightlist
  \item
    Hidden states: \(S = \{s_1, s_2, \dots, s_N\}\).
  \item
    Transition probabilities: \(A = [a_{ij}]\).
  \item
    Emission probabilities: \(B = [b_j(o)]\), likelihood of observation
    given state.
  \item
    Initial distribution: \(\pi\).
  \end{itemize}
\item
  Key algorithms:

  \begin{itemize}
  \tightlist
  \item
    Forward algorithm: compute likelihood of observation sequence.
  \item
    Viterbi algorithm: most likely hidden state sequence.
  \item
    Baum-Welch (EM): learn parameters from data.
  \end{itemize}
\item
  Assumptions:

  \begin{itemize}
  \tightlist
  \item
    Markov property: next state depends only on current state.
  \item
    Observations independent given hidden states.
  \end{itemize}
\item
  Applications in AI:

  \begin{itemize}
  \tightlist
  \item
    Speech recognition (phonemes as states, audio as observations).
  \item
    NLP (part-of-speech tagging, named entity recognition).
  \item
    Bioinformatics (gene sequence modeling).
  \item
    Finance (regime-switching models).
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2200}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4100}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3700}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Hidden states & Latent variables evolving by Markov chain & Phonemes,
POS tags, weather \\
Emission probabilities & Distribution over observations & Acoustic
signals, words, user actions \\
Forward algorithm & Sequence likelihood & Speech recognition scoring \\
Viterbi algorithm & Most probable hidden sequence & Decoding phoneme or
tag sequences \\
\end{longtable}

Tiny Code Sample (Python, hmmlearn)

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ hmmlearn }\ImportTok{import}\NormalTok{ hmm}

\CommentTok{\# Define HMM with 2 hidden states}
\NormalTok{model }\OperatorTok{=}\NormalTok{ hmm.MultinomialHMM(n\_components}\OperatorTok{=}\DecValTok{2}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{model.startprob\_ }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.6}\NormalTok{, }\FloatTok{0.4}\NormalTok{])}
\NormalTok{model.transmat\_ }\OperatorTok{=}\NormalTok{ np.array([[}\FloatTok{0.7}\NormalTok{, }\FloatTok{0.3}\NormalTok{],}
\NormalTok{                            [}\FloatTok{0.4}\NormalTok{, }\FloatTok{0.6}\NormalTok{]])}
\NormalTok{model.emissionprob\_ }\OperatorTok{=}\NormalTok{ np.array([[}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{],}
\NormalTok{                                [}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.9}\NormalTok{]])}

\CommentTok{\# Observations: 0,1}
\NormalTok{obs }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{],[}\DecValTok{1}\NormalTok{],[}\DecValTok{0}\NormalTok{],[}\DecValTok{1}\NormalTok{]])}
\NormalTok{logprob, states }\OperatorTok{=}\NormalTok{ model.decode(obs, algorithm}\OperatorTok{=}\StringTok{"viterbi"}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Most likely states:"}\NormalTok{, states)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-95}

HMMs are a foundational model for reasoning under uncertainty with
sequential data. They remain essential in speech, language, and
biological sequence analysis, and their principles inspire more advanced
deep sequence models like RNNs and Transformers.

\subsubsection{Try It Yourself}\label{try-it-yourself-197}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Define a 2-state HMM for ``Rainy'' vs ``Sunny'' with umbrella
  observations. Simulate a sequence.
\item
  Use the Viterbi algorithm to decode the most likely weather given
  observations.
\item
  Compare HMMs to modern sequence models---what advantages remain for
  HMMs?
\end{enumerate}

\subsection{199. Stochastic Differential
Equations}\label{stochastic-differential-equations}

Stochastic Differential Equations (SDEs) extend ordinary differential
equations by adding random noise terms, typically modeled with Brownian
motion. They capture dynamics where systems evolve continuously but with
uncertainty at every step.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-198}

Imagine watching pollen floating in water. Its overall drift follows
physical laws, but random collisions with water molecules push it
unpredictably. An SDE models both the smooth drift and the jittery
randomness together.

\subsubsection{Deep Dive}\label{deep-dive-198}

\begin{itemize}
\item
  General form:

  \[
  dX_t = \mu(X_t, t)dt + \sigma(X_t, t)dW_t
  \]

  \begin{itemize}
  \tightlist
  \item
    Drift term \(\mu\): deterministic trend.
  \item
    Diffusion term \(\sigma\): random fluctuations.
  \item
    \(W_t\): Wiener process (Brownian motion).
  \end{itemize}
\item
  Solutions:

  \begin{itemize}
  \tightlist
  \item
    Interpreted via Itô or Stratonovich calculus.
  \item
    Numerical: Euler--Maruyama, Milstein methods.
  \end{itemize}
\item
  Examples:

  \begin{itemize}
  \tightlist
  \item
    Geometric Brownian motion: \(dS_t = \mu S_t dt + \sigma S_t dW_t\).
  \item
    Ornstein--Uhlenbeck process: mean-reverting dynamics.
  \end{itemize}
\item
  Applications in AI:

  \begin{itemize}
  \tightlist
  \item
    Stochastic gradient Langevin dynamics (SGLD) for Bayesian learning.
  \item
    Diffusion models in generative AI.
  \item
    Continuous-time reinforcement learning.
  \item
    Modeling uncertainty in robotics and finance.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2336}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3925}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3738}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Process Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Equation Form
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Geometric Brownian Motion & \(dS_t = \mu S_t dt + \sigma S_t dW_t\) &
Asset pricing, probabilistic forecasting \\
Ornstein--Uhlenbeck & \(dX_t = \theta(\mu - X_t)dt + \sigma dW_t\) &
Exploration in RL, noise in control \\
Langevin dynamics & Gradient + noise dynamics & Bayesian deep learning,
diffusion models \\
\end{longtable}

Tiny Code Sample (Python, Euler--Maruyama Simulation)

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{np.random.seed(}\DecValTok{0}\NormalTok{)}
\NormalTok{T, N }\OperatorTok{=} \FloatTok{1.0}\NormalTok{, }\DecValTok{1000}
\NormalTok{dt }\OperatorTok{=}\NormalTok{ T}\OperatorTok{/}\NormalTok{N}
\NormalTok{mu, sigma }\OperatorTok{=} \FloatTok{1.0}\NormalTok{, }\FloatTok{0.3}

\CommentTok{\# Simulate geometric Brownian motion}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.zeros(N)}
\NormalTok{X[}\DecValTok{0}\NormalTok{] }\OperatorTok{=} \DecValTok{1}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, N):}
\NormalTok{    dW }\OperatorTok{=}\NormalTok{ np.sqrt(dt) }\OperatorTok{*}\NormalTok{ np.random.randn()}
\NormalTok{    X[i] }\OperatorTok{=}\NormalTok{ X[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\NormalTok{ mu}\OperatorTok{*}\NormalTok{X[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{*}\NormalTok{dt }\OperatorTok{+}\NormalTok{ sigma}\OperatorTok{*}\NormalTok{X[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{*}\NormalTok{dW}

\NormalTok{plt.plot(np.linspace(}\DecValTok{0}\NormalTok{, T, N), X)}
\NormalTok{plt.title(}\StringTok{"Geometric Brownian Motion"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-96}

SDEs let AI systems model continuous uncertainty and randomness in
dynamic environments. They are the mathematical foundation of
diffusion-based generative models and stochastic optimization techniques
that dominate modern machine learning.

\subsubsection{Try It Yourself}\label{try-it-yourself-198}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simulate an Ornstein--Uhlenbeck process and observe its mean-reverting
  behavior.
\item
  Explain how SDEs relate to diffusion models for image generation.
\item
  Use SGLD to train a simple regression model with Bayesian uncertainty.
\end{enumerate}

\subsection{200. Monte Carlo Methods}\label{monte-carlo-methods-1}

Monte Carlo methods use randomness to approximate solutions to
mathematical and computational problems. By simulating many random
samples, they estimate expectations, probabilities, and integrals that
are otherwise intractable.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-199}

Imagine trying to measure the area of an irregularly shaped pond.
Instead of calculating exactly, you throw random pebbles into a square
containing the pond. The fraction that lands inside gives an estimate of
its area.

\subsubsection{Deep Dive}\label{deep-dive-199}

\begin{itemize}
\item
  Core idea: approximate \(\mathbb{E}[f(X)]\) by averaging over random
  draws of \(X\).

  \[
  \mathbb{E}[f(X)] \approx \frac{1}{N}\sum_{i=1}^N f(x_i), \quad x_i \sim p(x)
  \]
\item
  Variance reduction:

  \begin{itemize}
  \tightlist
  \item
    Importance sampling, control variates, stratified sampling.
  \end{itemize}
\item
  Monte Carlo integration:

  \begin{itemize}
  \tightlist
  \item
    Estimate integrals over high-dimensional spaces.
  \end{itemize}
\item
  Markov Chain Monte Carlo (MCMC):

  \begin{itemize}
  \tightlist
  \item
    Use dependent samples from a Markov chain to approximate
    distributions (Metropolis-Hastings, Gibbs sampling).
  \end{itemize}
\item
  Applications in AI:

  \begin{itemize}
  \tightlist
  \item
    Bayesian inference (posterior estimation).
  \item
    Reinforcement learning (policy evaluation with rollouts).
  \item
    Probabilistic programming.
  \item
    Simulation for planning under uncertainty.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2323}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4444}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3232}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Technique
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Basic Monte Carlo & Average over random samples & Estimating expected
reward in RL \\
Importance sampling & Reweight samples from different distribution &
Off-policy evaluation \\
MCMC & Generate dependent samples via Markov chain & Bayesian neural
networks \\
Variational Monte Carlo & Combine sampling with optimization &
Approximate posterior inference \\
\end{longtable}

Tiny Code Sample (Python, Monte Carlo for π)

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{N }\OperatorTok{=} \DecValTok{100000}
\NormalTok{points }\OperatorTok{=}\NormalTok{ np.random.rand(N,}\DecValTok{2}\NormalTok{)}
\NormalTok{inside\_circle }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(points[:,}\DecValTok{0}\NormalTok{]}\DecValTok{2} \OperatorTok{+}\NormalTok{ points[:,}\DecValTok{1}\NormalTok{]}\DecValTok{2} \OperatorTok{\textless{}=} \DecValTok{1}\NormalTok{)}
\NormalTok{pi\_estimate }\OperatorTok{=} \DecValTok{4} \OperatorTok{*}\NormalTok{ inside\_circle }\OperatorTok{/}\NormalTok{ N}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Monte Carlo estimate of π:"}\NormalTok{, pi\_estimate)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-97}

Monte Carlo methods make the intractable tractable. They allow AI
systems to approximate probabilities, expectations, and integrals in
high dimensions, powering Bayesian inference, probabilistic models, and
modern generative approaches.

\subsubsection{Try It Yourself}\label{try-it-yourself-199}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use Monte Carlo to estimate the integral of \(f(x)=e^{-x^2}\) over
  \([0,1]\).
\item
  Implement importance sampling for a skewed distribution.
\item
  Explain how MCMC can approximate the posterior of a Bayesian linear
  regression model.
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Volume 3. Data and
Representation}\label{volume-3.-data-and-representation}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{Bits}\NormalTok{ fall into place,}
\ExtensionTok{shapes}\NormalTok{ of meaning crystallize,}
\ExtensionTok{data}\NormalTok{ finds its form.}
\end{Highlighting}
\end{Shaded}

\section{Chapter 21. Data Lifecycle and
Governance}\label{chapter-21.-data-lifecycle-and-governance}

\subsection{201. Data Collection: Sources, Pipelines, and
APIs}\label{data-collection-sources-pipelines-and-apis}

Data collection defines the foundation of any intelligent system. It
determines what information is captured, how it flows into the system,
and what assurances exist about accuracy, timeliness, and ethical
compliance. If the inputs are poor, no amount of modeling can repair the
outcome.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-200}

Visualize a production line supplied by many vendors. If raw materials
are incomplete, delayed, or inconsistent, the final product suffers.
Data pipelines behave the same way: broken or unreliable inputs
propagate defects through the entire system.

\subsubsection{Deep Dive}\label{deep-dive-200}

Different origins of data:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1083}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3167}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2583}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3167}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Primary & Direct measurement or user interaction & High relevance,
tailored & Costly, limited scale \\
Secondary & Pre-existing collections or logs & Wide coverage, low cost &
Schema drift, uncertain quality \\
Synthetic & Generated or simulated data & Useful when real data is
scarce & May not match real-world distributions \\
\end{longtable}

Ways data enters a system:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1413}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4022}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4565}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Mode
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Common Uses
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Batch & Periodic collection in large chunks & Historical analysis,
scheduled updates \\
Streaming & Continuous flow of individual records & Real-time
monitoring, alerts \\
Hybrid & Combination of both & Systems needing both history and
immediacy \\
\end{longtable}

Pipelines provide the structured movement of data from origin to storage
and processing. They define when transformations occur, how errors are
handled, and how reliability is enforced. Interfaces allow external
systems to deliver or request data consistently, supporting structured
queries or real-time delivery depending on the design.

Challenges arise around:

\begin{itemize}
\tightlist
\item
  Reliability: missing, duplicated, or late arrivals affect stability.
\item
  Consistency: mismatched schemas, time zones, or measurement units
  create silent errors.
\item
  Ethics and legality: collecting without proper consent or safeguards
  undermines trust and compliance.
\end{itemize}

\subsubsection{Tiny Code}\label{tiny-code-178}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Step 1: Collect weather observation}
\NormalTok{weather }\OperatorTok{=}\NormalTok{ get(}\StringTok{"weather\_source"}\NormalTok{)}

\CommentTok{\# Step 2: Collect air quality observation}
\NormalTok{air }\OperatorTok{=}\NormalTok{ get(}\StringTok{"air\_source"}\NormalTok{)}

\CommentTok{\# Step 3: Normalize into unified schema}
\NormalTok{record }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"temperature"}\NormalTok{: weather[}\StringTok{"temp"}\NormalTok{],}
    \StringTok{"humidity"}\NormalTok{: weather[}\StringTok{"humidity"}\NormalTok{],}
    \StringTok{"pm25"}\NormalTok{: air[}\StringTok{"pm25"}\NormalTok{],}
    \StringTok{"timestamp"}\NormalTok{: weather[}\StringTok{"time"}\NormalTok{]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This merges heterogeneous observations into a consistent record for
later processing.

\subsubsection{Try It Yourself}\label{try-it-yourself-200}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Design a small workflow that records numerical data every hour and
  stores it in a simple file.
\item
  Extend the workflow to continue even if one collection step fails.
\item
  Add a derived feature such as relative change compared to the previous
  entry.
\end{enumerate}

\subsection{202. Data Ingestion: Streaming
vs.~Batch}\label{data-ingestion-streaming-vs.-batch}

Ingestion is the act of bringing collected data into a system for
storage and processing. Two dominant approaches exist: batch, which
transfers large amounts of data at once, and streaming, which delivers
records continuously. Each method comes with tradeoffs in latency,
complexity, and reliability.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-201}

Imagine two delivery models for supplies. In one, a truck arrives once a
day with everything needed for the next 24 hours. In the other, a
conveyor belt delivers items piece by piece as they are produced. Both
supply the factory, but they operate on different rhythms and demand
different infrastructure.

\subsubsection{Deep Dive}\label{deep-dive-201}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0710}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3115}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3388}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2787}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Advantages
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Batch & Data ingested periodically in large volumes & Efficient for
historical data, simpler to manage & Delayed updates, unsuitable for
real-time needs \\
Streaming & Continuous flow of events into the system & Low latency,
immediate availability & Higher system complexity, harder to guarantee
order \\
Hybrid & Combination of periodic bulk loads and continuous streams &
Balances historical completeness with real-time responsiveness &
Requires coordination across modes \\
\end{longtable}

Batch ingestion suits workloads like reporting, long-term analysis, or
training where slight delays are acceptable. Streaming ingestion is
essential for systems that react immediately to changes, such as anomaly
detection or online personalization. Hybrid ingestion acknowledges that
many applications need both---daily full refreshes for stability and
continuous feeds for responsiveness.

Critical concerns include ensuring that data is neither lost nor
duplicated, handling bursts or downtime gracefully, and preserving order
when sequence matters. Designing ingestion requires balancing
throughput, latency, and correctness guarantees according to the needs
of the task.

\subsubsection{Tiny Code}\label{tiny-code-179}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Batch ingestion: process all files from a directory}
\ControlFlowTok{for} \BuiltInTok{file} \KeywordTok{in}\NormalTok{ list\_files(}\StringTok{"daily\_dump"}\NormalTok{):}
\NormalTok{    records }\OperatorTok{=}\NormalTok{ read(}\BuiltInTok{file}\NormalTok{)}
\NormalTok{    store(records)}

\CommentTok{\# Streaming ingestion: handle one record at a time}
\ControlFlowTok{while} \VariableTok{True}\NormalTok{:}
\NormalTok{    event }\OperatorTok{=}\NormalTok{ get\_next\_event()}
\NormalTok{    store(event)}
\end{Highlighting}
\end{Shaded}

This contrast shows how batch processes accumulate and load data in
chunks, while streaming reacts to each new event as it arrives.

\subsubsection{Try It Yourself}\label{try-it-yourself-201}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Implement a batch ingestion workflow that reads daily logs and appends
  them to a master dataset.
\item
  Implement a streaming workflow that processes one event at a time,
  simulating sensor readings.
\item
  Compare latency and reliability between the two methods in a simple
  experiment.
\end{enumerate}

\subsection{203. Data Storage: Relational, NoSQL, Object
Stores}\label{data-storage-relational-nosql-object-stores}

Once data is ingested, it must be stored in a way that preserves
structure, enables retrieval, and supports downstream tasks. Different
storage paradigms exist, each optimized for particular shapes of data
and patterns of access. Choosing the right one impacts scalability,
consistency, and ease of analysis.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-202}

Think of three types of warehouses. One arranges items neatly in rows
and columns with precise labels. Another stacks them by category in
flexible bins, easy to expand when new types appear. A third simply
stores large sealed containers, each holding complex or irregular goods.
Each warehouse serves the same goal---keeping items safe---but with
different tradeoffs.

\subsubsection{Deep Dive}\label{deep-dive-202}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0983}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2428}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3121}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3468}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Storage Paradigm
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Structure
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Relational & Tables with rows and columns, fixed schema & Strong
consistency, well-suited for structured queries & Rigid schema, less
flexible for unstructured data \\
NoSQL & Key-value, document, or columnar stores & Flexible schema,
scales horizontally & Limited support for complex joins, weaker
guarantees \\
Object Stores & Files or blobs organized by identifiers & Handles large,
heterogeneous data efficiently & Slower for fine-grained queries, relies
on metadata indexing \\
\end{longtable}

Relational systems excel when data has predictable structure and strong
transactional needs. NoSQL approaches are preferred when data is
semi-structured or when scale-out and rapid schema evolution are
essential. Object stores dominate when dealing with images, videos,
logs, or mixed media that do not fit neatly into rows and columns.

Key concerns include balancing cost against performance, managing schema
evolution over time, and ensuring that metadata is robust enough to
support efficient discovery.

\subsubsection{Tiny Code}\label{tiny-code-180}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Relational{-}style record}
\NormalTok{row }\OperatorTok{=}\NormalTok{ \{}\StringTok{"id"}\NormalTok{: }\DecValTok{1}\NormalTok{, }\StringTok{"name"}\NormalTok{: }\StringTok{"Alice"}\NormalTok{, }\StringTok{"age"}\NormalTok{: }\DecValTok{30}\NormalTok{\}}

\CommentTok{\# NoSQL{-}style record}
\NormalTok{doc }\OperatorTok{=}\NormalTok{ \{}\StringTok{"user"}\NormalTok{: }\StringTok{"Bob"}\NormalTok{, }\StringTok{"preferences"}\NormalTok{: \{}\StringTok{"theme"}\NormalTok{: }\StringTok{"dark"}\NormalTok{, }\StringTok{"alerts"}\NormalTok{: }\VariableTok{True}\NormalTok{\}\}}

\CommentTok{\# Object store{-}style record}
\NormalTok{object\_id }\OperatorTok{=}\NormalTok{ save\_blob(}\StringTok{"profile\_picture.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Each snippet represents the same idea---storing information---but with
different abstractions.

\subsubsection{Try It Yourself}\label{try-it-yourself-202}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Represent the same dataset in table, document, and object form, and
  compare how querying might differ.
\item
  Add a new field to each storage type and examine how easily the system
  accommodates the change.
\item
  Simulate a workload where both structured queries and large file
  storage are needed, and discuss which combination of paradigms would
  be most efficient.
\end{enumerate}

\subsection{204. Data Cleaning and
Normalization}\label{data-cleaning-and-normalization}

Raw data often contains errors, inconsistencies, and irregular formats.
Cleaning and normalization ensure that the dataset is coherent,
consistent, and suitable for analysis or modeling. Without these steps,
biases and noise propagate into models, weakening their reliability.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-203}

Imagine collecting fruit from different orchards. Some baskets contain
apples labeled in kilograms, others in pounds. Some apples are bruised,
others duplicated across baskets. Before selling them at the market, you
must sort, remove damaged ones, convert all weights to the same unit,
and ensure that every apple has a clear label. Data cleaning works the
same way.

\subsubsection{Deep Dive}\label{deep-dive-203}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2158}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3094}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4748}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Task
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Examples
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Handling missing values & Prevent gaps from distorting analysis & Fill
with averages, interpolate over time, mark explicitly \\
Correcting inconsistencies & Align mismatched formats & Dates unified to
a standard format, names consistently capitalized \\
Removing duplicates & Avoid repeated influence of the same record &
Detect identical entries, merge partial overlaps \\
Standardizing units & Ensure comparability across sources & Kilograms
vs.~pounds, Celsius vs.~Fahrenheit \\
Scaling and normalization & Place values in comparable ranges & Min--max
scaling, z-score normalization \\
\end{longtable}

Cleaning focuses on removing or correcting flawed records. Normalization
ensures that numerical values can be compared fairly and that features
contribute proportionally to modeling. Both reduce noise and bias in
later stages.

Key challenges include deciding when to repair versus discard, handling
conflicting sources of truth, and documenting changes so that
transformations are transparent and reproducible.

\subsubsection{Tiny Code}\label{tiny-code-181}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{record }\OperatorTok{=}\NormalTok{ \{}\StringTok{"height"}\NormalTok{: }\StringTok{"72 in"}\NormalTok{, }\StringTok{"weight"}\NormalTok{: }\VariableTok{None}\NormalTok{, }\StringTok{"name"}\NormalTok{: }\StringTok{"alice"}\NormalTok{\}}

\CommentTok{\# Normalize units}
\NormalTok{record[}\StringTok{"height\_cm"}\NormalTok{] }\OperatorTok{=} \DecValTok{72} \OperatorTok{*} \FloatTok{2.54}

\CommentTok{\# Handle missing values}
\ControlFlowTok{if}\NormalTok{ record[}\StringTok{"weight"}\NormalTok{] }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
\NormalTok{    record[}\StringTok{"weight"}\NormalTok{] }\OperatorTok{=}\NormalTok{ average\_weight()}

\CommentTok{\# Standardize name format}
\NormalTok{record[}\StringTok{"name"}\NormalTok{] }\OperatorTok{=}\NormalTok{ record[}\StringTok{"name"}\NormalTok{].title()}
\end{Highlighting}
\end{Shaded}

The result is a consistent, usable record that aligns with others in the
dataset.

\subsubsection{Try It Yourself}\label{try-it-yourself-203}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take a small dataset with missing values and experiment with different
  strategies for filling them.
\item
  Convert measurements in mixed units to a common standard and compare
  results.
\item
  Simulate the impact of duplicate records on summary statistics before
  and after cleaning.
\end{enumerate}

\subsection{205. Metadata and Documentation
Practices}\label{metadata-and-documentation-practices}

Metadata is data about data. It records details such as origin,
structure, meaning, and quality. Documentation practices use metadata to
make datasets understandable, traceable, and reusable. Without them,
even high-quality data becomes opaque and difficult to maintain.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-204}

Imagine a library where books are stacked randomly without labels. Even
if the collection is vast and valuable, it becomes nearly useless
without catalogs, titles, or subject tags. Metadata acts as that catalog
for datasets, ensuring that others can find, interpret, and trust the
data.

\subsubsection{Deep Dive}\label{deep-dive-204}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1782}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3069}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5149}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Metadata Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Examples
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Descriptive & Helps humans understand content & Titles, keywords,
abstracts \\
Structural & Describes organization & Table schemas, relationships, file
formats \\
Administrative & Supports management and rights & Access permissions,
licensing, retention dates \\
Provenance & Tracks origin and history & Source systems, transformations
applied, versioning \\
Quality & Provides assurance & Missing value ratios, error rates,
validation checks \\
\end{longtable}

Strong documentation practices combine machine-readable metadata with
human-oriented explanations. Clear data dictionaries, schema diagrams,
and lineage records help teams understand what a dataset contains and
how it has changed over time.

Challenges include keeping metadata synchronized with evolving datasets,
avoiding excessive overhead, and balancing detail with usability. Good
metadata practices require continuous maintenance, not just one-time
annotation.

\subsubsection{Tiny Code}\label{tiny-code-182}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset\_metadata }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"name"}\NormalTok{: }\StringTok{"customer\_records"}\NormalTok{,}
    \StringTok{"description"}\NormalTok{: }\StringTok{"Basic demographics and purchase history"}\NormalTok{,}
    \StringTok{"schema"}\NormalTok{: \{}
        \StringTok{"id"}\NormalTok{: }\StringTok{"unique identifier"}\NormalTok{,}
        \StringTok{"age"}\NormalTok{: }\StringTok{"integer, years"}\NormalTok{,}
        \StringTok{"purchase\_total"}\NormalTok{: }\StringTok{"float, USD"}
\NormalTok{    \},}
    \StringTok{"provenance"}\NormalTok{: \{}
        \StringTok{"source"}\NormalTok{: }\StringTok{"transactional system"}\NormalTok{,}
        \StringTok{"last\_updated"}\NormalTok{: }\StringTok{"2025{-}09{-}17"}
\NormalTok{    \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This record makes the dataset understandable to both humans and
machines, improving reusability.

\subsubsection{Try It Yourself}\label{try-it-yourself-204}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a metadata record for a small dataset you use, including
  descriptive, structural, and provenance elements.
\item
  Compare two datasets without documentation and try to align their
  fields---then repeat the task with documented versions.
\item
  Design a minimal schema for capturing data quality indicators
  alongside the dataset itself.
\end{enumerate}

\subsection{206. Data Access Policies and
Permissions}\label{data-access-policies-and-permissions}

Data is valuable, but it can also be sensitive. Access policies and
permissions determine who can see, modify, or distribute datasets.
Proper controls protect privacy, ensure compliance, and reduce the risk
of misuse, while still enabling legitimate use.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-205}

Imagine a secure building with multiple rooms. Some people carry keys
that open only the lobby, others can enter restricted offices, and a
select few can access the vault. Data systems work the same way---access
levels must be carefully assigned to balance openness and security.

\subsubsection{Deep Dive}\label{deep-dive-205}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1682}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3645}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4673}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Policy Layer
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Examples
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Authentication & Verifies identity of users or systems & Login
credentials, tokens, biometric checks \\
Authorization & Defines what authenticated users can do & Read-only
vs.~edit vs.~admin rights \\
Granularity & Determines scope of access & Entire dataset, specific
tables, individual fields \\
Auditability & Records actions for accountability & Logs of who accessed
or changed data \\
Revocation & Removes access when conditions change & Employee
offboarding, expired contracts \\
\end{longtable}

Strong access control avoids the extremes of over-restriction (which
hampers collaboration) and over-exposure (which increases risk).
Policies must adapt to organizational roles, project needs, and evolving
legal frameworks.

Challenges include managing permissions at scale, preventing privilege
creep, and ensuring that sensitive attributes are protected even when
broader data is shared. Fine-grained controls---down to individual
fields or records---are often necessary in high-stakes environments.

\subsubsection{Tiny Code}\label{tiny-code-183}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example of role{-}based access rules}
\NormalTok{permissions }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"analyst"}\NormalTok{: [}\StringTok{"read\_dataset"}\NormalTok{],}
    \StringTok{"engineer"}\NormalTok{: [}\StringTok{"read\_dataset"}\NormalTok{, }\StringTok{"write\_dataset"}\NormalTok{],}
    \StringTok{"admin"}\NormalTok{: [}\StringTok{"read\_dataset"}\NormalTok{, }\StringTok{"write\_dataset"}\NormalTok{, }\StringTok{"manage\_permissions"}\NormalTok{]}
\NormalTok{\}}

\KeywordTok{def}\NormalTok{ can\_access(role, action):}
    \ControlFlowTok{return}\NormalTok{ action }\KeywordTok{in}\NormalTok{ permissions.get(role, [])}
\end{Highlighting}
\end{Shaded}

This simple rule structure shows how different roles can be restricted
or empowered based on responsibilities.

\subsubsection{Try It Yourself}\label{try-it-yourself-205}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Design a set of access rules for a dataset containing both public
  information and sensitive personal attributes.
\item
  Simulate an audit log showing who accessed the data, when, and what
  action they performed.
\item
  Discuss how permissions should evolve when a project shifts from
  experimentation to production deployment.
\end{enumerate}

\subsection{207. Version Control for
Datasets}\label{version-control-for-datasets}

Datasets evolve over time. Records are added, corrected, or removed, and
schemas may change. Version control ensures that each state of the data
is preserved, so experiments are reproducible and historical analyses
remain valid.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-206}

Imagine writing a book without saving drafts. If you make a mistake or
want to revisit an earlier chapter, the older version is gone forever.
Version control keeps every draft accessible, allowing comparison,
rollback, and traceability.

\subsubsection{Deep Dive}\label{deep-dive-206}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1870}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4390}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3740}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Examples
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Snapshots & Capture a full state of the dataset at a point in time &
Monthly archive of customer records \\
Incremental changes & Track additions, deletions, and updates & Daily
log of transactions \\
Schema versioning & Manage evolution of structure & Adding a new column,
changing data types \\
Lineage tracking & Preserve transformations across versions & From raw
logs → cleaned data → training set \\
Reproducibility & Ensure identical results can be obtained later &
Training a model on a specific dataset version \\
\end{longtable}

Version control allows branching for experimental pipelines and merging
when results are stable. It supports auditing by showing exactly what
data was available and how it looked at a given time.

Challenges include balancing storage cost with detail of history,
avoiding uncontrolled proliferation of versions, and aligning dataset
versions with code and model versions.

\subsubsection{Tiny Code}\label{tiny-code-184}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Store dataset with version tag}
\NormalTok{dataset\_v1 }\OperatorTok{=}\NormalTok{ \{}\StringTok{"version"}\NormalTok{: }\StringTok{"1.0"}\NormalTok{, }\StringTok{"records"}\NormalTok{: [...]\}}

\CommentTok{\# Update dataset and save as new version}
\NormalTok{dataset\_v2 }\OperatorTok{=}\NormalTok{ dataset\_v1.copy()}
\NormalTok{dataset\_v2[}\StringTok{"version"}\NormalTok{] }\OperatorTok{=} \StringTok{"2.0"}
\NormalTok{dataset\_v2[}\StringTok{"records"}\NormalTok{].append(new\_record)}
\end{Highlighting}
\end{Shaded}

This sketch highlights the idea of preserving old states while creating
new ones.

\subsubsection{Try It Yourself}\label{try-it-yourself-206}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take a dataset and create two distinct versions: one raw and one
  cleaned. Document the differences.
\item
  Simulate a schema change by adding a new field, then ensure older
  queries still work on past versions.
\item
  Design a naming or tagging scheme for dataset versions that aligns
  with experiments and models.
\end{enumerate}

\subsection{208. Data Governance
Frameworks}\label{data-governance-frameworks}

Data governance establishes the rules, responsibilities, and processes
that ensure data is managed properly throughout its lifecycle. It
provides the foundation for trust, compliance, and effective use of data
within organizations.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-207}

Think of a city with traffic laws, zoning rules, and public services.
Without governance, cars would collide, buildings would be unsafe, and
services would be chaotic. Data governance is the equivalent: a set of
structures that keep the ``city of data'' orderly and sustainable.

\subsubsection{Deep Dive}\label{deep-dive-207}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3619}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3714}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Governance Element
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Practices
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Policies & Define how data is used and protected & Usage guidelines,
retention rules \\
Roles \& Responsibilities & Assign accountability for data & Owners,
stewards, custodians \\
Standards & Ensure consistency across datasets & Naming conventions,
quality metrics \\
Compliance & Align with laws and regulations & Privacy safeguards,
retention schedules \\
Oversight & Monitor adherence and resolve disputes & Review boards,
audits \\
\end{longtable}

Governance frameworks aim to balance control with flexibility. They
enable innovation while reducing risks such as misuse, duplication, and
non-compliance. Without them, data practices become fragmented, leading
to inefficiency and mistrust.

Key challenges include ensuring participation across departments,
updating rules as technology evolves, and preventing governance from
becoming a bureaucratic bottleneck. The most effective frameworks are
living systems that adapt over time.

\subsubsection{Tiny Code}\label{tiny-code-185}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Governance rule example}
\NormalTok{rule }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"dataset"}\NormalTok{: }\StringTok{"customer\_records"}\NormalTok{,}
    \StringTok{"policy"}\NormalTok{: }\StringTok{"retain\_for\_years"}\NormalTok{,}
    \StringTok{"value"}\NormalTok{: }\DecValTok{7}\NormalTok{,}
    \StringTok{"responsible\_role"}\NormalTok{: }\StringTok{"data\_steward"}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This shows how a governance rule might define scope, requirement, and
accountability in structured form.

\subsubsection{Try It Yourself}\label{try-it-yourself-207}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a sample policy for how long sensitive data should be kept
  before deletion.
\item
  Define three roles (e.g., owner, steward, user) and describe their
  responsibilities for a dataset.
\item
  Propose a mechanism for reviewing and updating governance rules
  annually.
\end{enumerate}

\subsection{209. Stewardship, Ownership, and
Accountability}\label{stewardship-ownership-and-accountability}

Clear responsibility for data ensures it remains accurate, secure, and
useful. Stewardship, ownership, and accountability define who controls
data, who manages it day-to-day, and who is ultimately answerable for
its condition and use.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-208}

Imagine a community garden. One person legally owns the land, several
stewards take care of watering and weeding, and all members of the
community hold each other accountable for keeping the space healthy.
Data requires the same layered responsibility.

\subsubsection{Deep Dive}\label{deep-dive-208}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1057}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4715}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4228}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Role
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Responsibility
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Focus
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Owner & Holds legal or organizational authority over the data &
Strategic direction, compliance, ultimate decisions \\
Steward & Manages data quality and accessibility on a daily basis &
Standards, documentation, resolving issues \\
Custodian & Provides technical infrastructure for storage and security &
Availability, backups, permissions \\
User & Accesses and applies data for tasks & Correct usage, reporting
errors, respecting policies \\
\end{longtable}

Ownership clarifies who makes binding decisions. Stewardship ensures
data is maintained according to agreed standards. Custodianship provides
the tools and environments that keep data safe. Users complete the chain
by applying the data responsibly and giving feedback.

Challenges emerge when responsibilities are vague, duplicated, or
ignored. Without accountability, errors go uncorrected, permissions
drift, and compliance breaks down. Strong frameworks explicitly assign
roles and provide escalation paths for resolving disputes.

\subsubsection{Tiny Code}\label{tiny-code-186}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{roles }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"owner"}\NormalTok{: }\StringTok{"chief\_data\_officer"}\NormalTok{,}
    \StringTok{"steward"}\NormalTok{: }\StringTok{"quality\_team"}\NormalTok{,}
    \StringTok{"custodian"}\NormalTok{: }\StringTok{"infrastructure\_team"}\NormalTok{,}
    \StringTok{"user"}\NormalTok{: }\StringTok{"analyst\_group"}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This captures a simple mapping between dataset responsibilities and
organizational roles.

\subsubsection{Try It Yourself}\label{try-it-yourself-208}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Assign owner, steward, custodian, and user roles for a hypothetical
  dataset in healthcare or finance.
\item
  Write down how accountability would be enforced if errors in the
  dataset are discovered.
\item
  Discuss how responsibilities might shift when a dataset moves from
  experimental use to production-critical use.
\end{enumerate}

\subsection{210. End-of-Life: Archiving, Deletion, and
Sunsetting}\label{end-of-life-archiving-deletion-and-sunsetting}

Every dataset has a lifecycle. When it is no longer needed for active
use, it must be retired responsibly. End-of-life practices---archiving,
deletion, and sunsetting---ensure that data is preserved when valuable,
removed when risky, and always managed in compliance with policy and
law.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-209}

Think of a library that occasionally removes outdated books. Some are
placed in a historical archive, some are discarded to make room for new
material, and some collections are closed to the public but retained for
reference. Data requires the same careful handling at the end of its
useful life.

\subsubsection{Deep Dive}\label{deep-dive-209}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1217}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4783}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Practice
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Examples
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Archiving & Preserve data for long-term historical or legal reasons &
Old financial records, scientific observations \\
Deletion & Permanently remove data that is no longer needed & Removing
expired personal records \\
Sunsetting & Gradually phase out datasets or systems & Transition from
legacy datasets to new sources \\
\end{longtable}

Archiving safeguards information that may hold future value, but it must
be accompanied by metadata so that context is not lost. Deletion reduces
liability, especially for sensitive or regulated data, but requires
guarantees that removal is irreversible. Sunsetting allows smooth
transitions, ensuring users migrate to new systems before old ones
disappear.

Challenges include determining retention timelines, balancing storage
costs with potential value, and ensuring compliance with regulations.
Poor end-of-life management risks unnecessary expenses, legal exposure,
or loss of institutional knowledge.

\subsubsection{Tiny Code}\label{tiny-code-187}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ \{}\StringTok{"name"}\NormalTok{: }\StringTok{"transactions\_2015"}\NormalTok{, }\StringTok{"status"}\NormalTok{: }\StringTok{"active"}\NormalTok{\}}

\CommentTok{\# Archive}
\NormalTok{dataset[}\StringTok{"status"}\NormalTok{] }\OperatorTok{=} \StringTok{"archived"}

\CommentTok{\# Delete}
\KeywordTok{del}\NormalTok{ dataset}

\CommentTok{\# Sunset}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ \{}\StringTok{"name"}\NormalTok{: }\StringTok{"legacy\_system"}\NormalTok{, }\StringTok{"status"}\NormalTok{: }\StringTok{"deprecated"}\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

These states illustrate how datasets may shift between active use,
archived preservation, or eventual removal.

\subsubsection{Try It Yourself}\label{try-it-yourself-209}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Define a retention schedule for a dataset containing personal
  information, balancing usefulness and legal requirements.
\item
  Simulate the process of archiving a dataset, including how metadata
  should be preserved for future reference.
\item
  Design a sunset plan that transitions users from an old dataset to a
  newer, improved one without disruption.
\end{enumerate}

\section{Chapter 22. Data Models: Tensors, Tables and
Graphs}\label{chapter-22.-data-models-tensors-tables-and-graphs}

\subsection{211. Scalar, Vector, Matrix, and Tensor
Structures}\label{scalar-vector-matrix-and-tensor-structures}

At the heart of data representation are numerical structures of
increasing complexity. Scalars represent single values, vectors
represent ordered lists, matrices organize data into two dimensions, and
tensors generalize to higher dimensions. These structures form the
building blocks for most modern AI systems.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-210}

Imagine stacking objects. A scalar is a single brick. A vector is a line
of bricks placed end to end. A matrix is a full floor made of rows and
columns. A tensor is a multi-story building, where each floor is a
matrix and the whole structure extends into higher dimensions.

\subsubsection{Deep Dive}\label{deep-dive-210}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1176}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1176}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3294}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4353}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Structure
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Dimensions
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Common Uses
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Scalar & 0D & 7 & Single measurements, constants \\
Vector & 1D & {[}3, 5, 9{]} & Feature sets, embeddings \\
Matrix & 2D & {[}{[}1, 2{]}, {[}3, 4{]}{]} & Images, tabular data \\
Tensor & nD & 3D image stack, video frames & Multimodal data, deep
learning inputs \\
\end{longtable}

Scalars capture isolated quantities like temperature or price. Vectors
arrange values in a sequence, allowing operations such as dot products
or norms. Matrices extend to two-dimensional grids, useful for
representing images, tables, and transformations. Tensors generalize
further, enabling representation of structured collections like batches
of images or sequences with multiple channels.

Challenges involve handling memory efficiently, ensuring operations are
consistent across dimensions, and interpreting high-dimensional
structures in ways that remain meaningful.

\subsubsection{Tiny Code}\label{tiny-code-188}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scalar }\OperatorTok{=} \DecValTok{7}
\NormalTok{vector }\OperatorTok{=}\NormalTok{ [}\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{9}\NormalTok{]}
\NormalTok{matrix }\OperatorTok{=}\NormalTok{ [[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{], [}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{]]}
\NormalTok{tensor }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    [[}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{]],}
\NormalTok{    [[}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{], [}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{]]}
\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Each step adds dimensionality, providing richer structure for
representing data.

\subsubsection{Try It Yourself}\label{try-it-yourself-210}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Represent a grayscale image as a matrix and a color image as a tensor,
  then compare.
\item
  Implement addition and multiplication for scalars, vectors, and
  matrices, noting differences.
\item
  Create a 3D tensor representing weather readings (temperature,
  humidity, pressure) across multiple locations and times.
\end{enumerate}

\subsection{212. Tabular Data: Schema, Keys, and
Indexes}\label{tabular-data-schema-keys-and-indexes}

Tabular data organizes information into rows and columns under a fixed
schema. Each row represents a record, and each column captures an
attribute. Keys ensure uniqueness and integrity, while indexes
accelerate retrieval and filtering.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-211}

Imagine a spreadsheet. Each row is a student, each column is a property
like name, age, or grade. A unique student ID ensures no duplicates,
while the index at the side of the sheet lets you jump directly to the
right row without scanning everything.

\subsubsection{Deep Dive}\label{deep-dive-211}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1705}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3636}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4659}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Element
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Schema & Defines structure and data types & Name (string), Age
(integer), GPA (float) \\
Primary Key & Guarantees uniqueness & Student ID, Social Security
Number \\
Foreign Key & Connects related tables & Course ID linking enrollment to
courses \\
Index & Speeds up search and retrieval & Index on ``Last Name'' for
faster lookups \\
\end{longtable}

Schemas bring predictability, enabling validation and reducing
ambiguity. Keys enforce constraints that protect against duplicates and
ensure relational consistency. Indexes allow large tables to remain
efficient, transforming linear scans into fast lookups.

Challenges include schema drift (when fields change over time), ensuring
referential integrity across multiple tables, and balancing index
overhead against query speed.

\subsubsection{Tiny Code}\label{tiny-code-189}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Schema definition}
\NormalTok{student }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"id"}\NormalTok{: }\DecValTok{101}\NormalTok{,}
    \StringTok{"name"}\NormalTok{: }\StringTok{"Alice"}\NormalTok{,}
    \StringTok{"age"}\NormalTok{: }\DecValTok{20}\NormalTok{,}
    \StringTok{"gpa"}\NormalTok{: }\FloatTok{3.8}
\NormalTok{\}}

\CommentTok{\# Key enforcement}
\NormalTok{primary\_key }\OperatorTok{=} \StringTok{"id"}  \CommentTok{\# ensures uniqueness}
\NormalTok{foreign\_key }\OperatorTok{=}\NormalTok{ \{}\StringTok{"course\_id"}\NormalTok{: }\StringTok{"courses.id"}\NormalTok{\}  }\CommentTok{\# links to another table}
\end{Highlighting}
\end{Shaded}

This structure captures the essence of tabular organization: clarity,
integrity, and efficient retrieval.

\subsubsection{Try It Yourself}\label{try-it-yourself-211}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Define a schema for a table of books with fields for ISBN, title,
  author, and year.
\item
  Create a relationship between a table of students and a table of
  courses using keys.
\item
  Add an index to a large table and measure the difference in lookup
  speed compared to scanning all rows.
\end{enumerate}

\subsection{213. Graph Data: Nodes, Edges, and
Attributes}\label{graph-data-nodes-edges-and-attributes}

Graph data represents entities as nodes and the relationships between
them as edges. Each node or edge can carry attributes that describe
properties, enabling rich modeling of interconnected systems such as
social networks, knowledge bases, or transportation maps.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-212}

Think of a map of cities and roads. Each city is a node, each road is an
edge, and attributes like population or distance add detail. Together,
they form a structure where the meaning lies not just in the items
themselves but in how they connect.

\subsubsection{Deep Dive}\label{deep-dive-212}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2135}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4157}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3708}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Element
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Node & Represents an entity & Person, city, product \\
Edge & Connects two nodes & Friendship, road, purchase \\
Directed Edge & Has a direction from source to target & ``Follows'' on
social media \\
Undirected Edge & Represents mutual relation & Friendship,
siblinghood \\
Attributes & Properties of nodes or edges & Node: age, Edge: weight,
distance \\
\end{longtable}

Graphs excel where relationships are central. They capture many-to-many
connections naturally and allow queries such as ``shortest path,''
``most connected node,'' or ``communities.'' Attributes enrich graphs by
giving context beyond pure connectivity.

Challenges include handling very large graphs efficiently, ensuring
updates preserve consistency, and choosing storage formats that allow
fast traversal.

\subsubsection{Tiny Code}\label{tiny-code-190}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simple graph representation}
\NormalTok{graph }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"nodes"}\NormalTok{: \{}
        \DecValTok{1}\NormalTok{: \{}\StringTok{"name"}\NormalTok{: }\StringTok{"Alice"}\NormalTok{\},}
        \DecValTok{2}\NormalTok{: \{}\StringTok{"name"}\NormalTok{: }\StringTok{"Bob"}\NormalTok{\}}
\NormalTok{    \},}
    \StringTok{"edges"}\NormalTok{: [}
\NormalTok{        \{}\StringTok{"from"}\NormalTok{: }\DecValTok{1}\NormalTok{, }\StringTok{"to"}\NormalTok{: }\DecValTok{2}\NormalTok{, }\StringTok{"type"}\NormalTok{: }\StringTok{"friend"}\NormalTok{, }\StringTok{"strength"}\NormalTok{: }\FloatTok{0.9}\NormalTok{\}}
\NormalTok{    ]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This captures entities, their relationship, and an attribute describing
its strength.

\subsubsection{Try It Yourself}\label{try-it-yourself-212}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Build a small graph representing three people and their friendships.
\item
  Add attributes such as age for nodes and interaction frequency for
  edges.
\item
  Write a routine that finds the shortest path between two nodes in the
  graph.
\end{enumerate}

\subsection{214. Sparse vs.~Dense
Representations}\label{sparse-vs.-dense-representations}

Data can be represented as dense structures, where most elements are
filled, or as sparse structures, where most elements are empty or zero.
Choosing between them affects storage efficiency, computational speed,
and model performance.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-213}

Imagine a seating chart for a stadium. In a sold-out game, every seat is
filled---this is a dense representation. In a quiet practice session,
only a few spectators are scattered around; most seats are empty---this
is a sparse representation. Both charts describe the same stadium, but
one is full while the other is mostly empty.

\subsubsection{Deep Dive}\label{deep-dive-213}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0940}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2953}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3289}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2819}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Representation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Advantages
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Dense & Every element explicitly stored & Fast arithmetic, simple to
implement & Wastes memory when many values are zero \\
Sparse & Only non-zero elements stored with positions & Efficient memory
use, faster on highly empty data & More complex operations, indexing
overhead \\
\end{longtable}

Dense forms are best when data is compact and most values matter, such
as images or audio signals. Sparse forms are preferred for
high-dimensional data with few active features, such as text represented
by large vocabularies.

Key challenges include selecting thresholds for sparsity, designing
efficient data structures for storage, and ensuring algorithms remain
numerically stable when working with extremely sparse inputs.

\subsubsection{Tiny Code}\label{tiny-code-191}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Dense vector}
\NormalTok{dense }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{]}

\CommentTok{\# Sparse vector}
\NormalTok{sparse }\OperatorTok{=}\NormalTok{ \{}\DecValTok{2}\NormalTok{: }\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{: }\DecValTok{2}\NormalTok{\}  }\CommentTok{\# index: value}
\end{Highlighting}
\end{Shaded}

Both forms represent the same data, but the sparse version omits most
zeros and stores only what matters.

\subsubsection{Try It Yourself}\label{try-it-yourself-213}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Represent a document using a dense bag-of-words vector and a sparse
  dictionary; compare storage size.
\item
  Multiply two sparse vectors efficiently by iterating only over
  non-zero positions.
\item
  Simulate a dataset where sparsity increases with dimensionality and
  observe how storage needs change.
\end{enumerate}

\subsection{215. Structured vs.~Semi-Structured
vs.~Unstructured}\label{structured-vs.-semi-structured-vs.-unstructured}

Data varies in how strictly it follows predefined formats. Structured
data fits neatly into rows and columns, semi-structured data has
flexible organization with tags or hierarchies, and unstructured data
lacks consistent format altogether. Recognizing these categories helps
decide how to store, process, and analyze information.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-214}

Think of three types of storage rooms. One has shelves with labeled
boxes, each item in its proper place---that's structured. Another has
boxes with handwritten notes, some organized but others loosely
grouped---that's semi-structured. The last is a room filled with a pile
of papers, photos, and objects with no clear order---that's
unstructured.

\subsubsection{Deep Dive}\label{deep-dive-214}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1188}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2750}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1625}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2062}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2375}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Category
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Characteristics
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Examples
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Structured & Fixed schema, predictable fields & Tables, spreadsheets &
Easy querying, strong consistency & Inflexible for changing formats \\
Semi-Structured & Flexible tags or hierarchies, partial schema & Logs,
JSON, XML & Adaptable, self-describing & Can drift, harder to enforce
rules \\
Unstructured & No fixed schema, free form & Text, images, audio, video &
Rich information content & Hard to search, requires preprocessing \\
\end{longtable}

Structured data powers classical analytics and relational operations.
Semi-structured data is common in modern systems where schema evolves.
Unstructured data dominates in AI, where models extract patterns
directly from raw text, images, or speech.

Key challenges include integrating these types into unified pipelines,
ensuring searchability, and converting unstructured data into structured
features without losing nuance.

\subsubsection{Tiny Code}\label{tiny-code-192}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Structured}
\NormalTok{record }\OperatorTok{=}\NormalTok{ \{}\StringTok{"id"}\NormalTok{: }\DecValTok{1}\NormalTok{, }\StringTok{"name"}\NormalTok{: }\StringTok{"Alice"}\NormalTok{, }\StringTok{"age"}\NormalTok{: }\DecValTok{30}\NormalTok{\}}

\CommentTok{\# Semi{-}structured}
\NormalTok{log }\OperatorTok{=}\NormalTok{ \{}\StringTok{"event"}\NormalTok{: }\StringTok{"login"}\NormalTok{, }\StringTok{"details"}\NormalTok{: \{}\StringTok{"ip"}\NormalTok{: }\StringTok{"192.0.2.1"}\NormalTok{, }\StringTok{"device"}\NormalTok{: }\StringTok{"mobile"}\NormalTok{\}\}}

\CommentTok{\# Unstructured}
\NormalTok{text }\OperatorTok{=} \StringTok{"Alice logged in from her phone at 9 AM."}
\end{Highlighting}
\end{Shaded}

These examples represent the same fact in three different ways, each
with different strengths for analysis.

\subsubsection{Try It Yourself}\label{try-it-yourself-214}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take a short paragraph of text and represent it as structured
  keywords, semi-structured JSON, and raw unstructured text.
\item
  Compare how easy it is to query ``who logged in'' across each
  representation.
\item
  Design a simple pipeline that transforms unstructured text into
  structured fields suitable for analysis.
\end{enumerate}

\subsection{216. Encoding Relations: Adjacency Lists,
Matrices}\label{encoding-relations-adjacency-lists-matrices}

When data involves relationships between entities, those links need to
be encoded. Two common approaches are adjacency lists, which store
neighbors for each node, and adjacency matrices, which use a grid to
mark connections. Each balances memory use, efficiency, and clarity.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-215}

Imagine you're managing a group of friends. One approach is to keep a
list for each person, writing down who their friends are---that's an
adjacency list. Another approach is to draw a big square grid, writing
``1'' if two people are friends and ``0'' if not---that's an adjacency
matrix.

\subsubsection{Deep Dive}\label{deep-dive-215}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1183}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3077}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2663}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3077}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Representation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Structure
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Adjacency List & For each node, store a list of connected nodes &
Efficient for sparse graphs, easy to traverse & Slower to check if two
nodes are directly connected \\
Adjacency Matrix & Grid of size n × n marking presence/absence of edges
& Constant-time edge lookup, simple structure & Wastes space on sparse
graphs, expensive for large n \\
\end{longtable}

Adjacency lists are memory-efficient when graphs have few edges relative
to nodes. Adjacency matrices are straightforward and allow instant
connectivity checks, but scale poorly with graph size. Choosing between
them depends on graph density and the operations most important to the
task.

Hybrid approaches also exist, combining the strengths of both depending
on whether traversal or connectivity queries dominate.

\subsubsection{Tiny Code}\label{tiny-code-193}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Adjacency list}
\NormalTok{adj\_list }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"Alice"}\NormalTok{: [}\StringTok{"Bob"}\NormalTok{, }\StringTok{"Carol"}\NormalTok{],}
    \StringTok{"Bob"}\NormalTok{: [}\StringTok{"Alice"}\NormalTok{],}
    \StringTok{"Carol"}\NormalTok{: [}\StringTok{"Alice"}\NormalTok{]}
\NormalTok{\}}

\CommentTok{\# Adjacency matrix}
\NormalTok{nodes }\OperatorTok{=}\NormalTok{ [}\StringTok{"Alice"}\NormalTok{, }\StringTok{"Bob"}\NormalTok{, }\StringTok{"Carol"}\NormalTok{]}
\NormalTok{adj\_matrix }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{],}
\NormalTok{    [}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{],}
\NormalTok{    [}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{]}
\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Both structures represent the same small graph but in different ways.

\subsubsection{Try It Yourself}\label{try-it-yourself-215}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Represent a graph of five cities and their direct roads using both
  adjacency lists and matrices.
\item
  Compare the memory used when the graph is sparse (few roads) versus
  dense (many roads).
\item
  Implement a function that checks if two nodes are connected in both
  representations and measure which is faster.
\end{enumerate}

\subsection{217. Hybrid Data Models (Graph+Table,
Tensor+Graph)}\label{hybrid-data-models-graphtable-tensorgraph}

Some problems require combining multiple data representations. Hybrid
models merge structured formats like tables with relational formats like
graphs, or extend tensors with graph-like connectivity. These
combinations capture richer patterns that single models cannot.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-216}

Think of a school system. Student records sit neatly in tables with
names, IDs, and grades. But friendships and collaborations form a
network, better modeled as a graph. If you want to study both academic
performance and social influence, you need a hybrid model that links the
tabular and the relational.

\subsubsection{Deep Dive}\label{deep-dive-216}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2087}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4348}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3565}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Hybrid Form
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Use
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Graph + Table & Nodes and edges enriched with tabular attributes &
Social networks with demographic profiles \\
Tensor + Graph & Multidimensional arrays structured by connectivity &
Molecular structures, 3D meshes \\
Table + Unstructured & Rows linked to documents, images, or audio &
Medical records tied to scans and notes \\
\end{longtable}

Hybrid models enable more expressive queries: not only ``who knows
whom'' but also ``who knows whom and has similar attributes.'' They also
support learning systems that integrate different modalities, capturing
both structured regularities and unstructured context.

Challenges include designing schemas that bridge formats, managing
consistency across representations, and developing algorithms that can
operate effectively on combined structures.

\subsubsection{Tiny Code}\label{tiny-code-194}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Hybrid: table + graph}
\NormalTok{students }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    \{}\StringTok{"id"}\NormalTok{: }\DecValTok{1}\NormalTok{, }\StringTok{"name"}\NormalTok{: }\StringTok{"Alice"}\NormalTok{, }\StringTok{"grade"}\NormalTok{: }\DecValTok{90}\NormalTok{\},}
\NormalTok{    \{}\StringTok{"id"}\NormalTok{: }\DecValTok{2}\NormalTok{, }\StringTok{"name"}\NormalTok{: }\StringTok{"Bob"}\NormalTok{, }\StringTok{"grade"}\NormalTok{: }\DecValTok{85}\NormalTok{\}}
\NormalTok{]}

\NormalTok{friendships }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    \{}\StringTok{"from"}\NormalTok{: }\DecValTok{1}\NormalTok{, }\StringTok{"to"}\NormalTok{: }\DecValTok{2}\NormalTok{\}}
\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Here, the table captures attributes of students, while the graph encodes
their relationships.

\subsubsection{Try It Yourself}\label{try-it-yourself-216}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Build a dataset where each row describes a person and a separate graph
  encodes relationships. Link the two.
\item
  Represent a molecule both as a tensor of coordinates and as a graph of
  bonds.
\item
  Design a query that uses both formats, such as ``find students with
  above-average grades who are connected by friendships.''
\end{enumerate}

\subsection{218. Model Selection Criteria for
Tasks}\label{model-selection-criteria-for-tasks}

Different data models---tables, graphs, tensors, or hybrids---suit
different tasks. Choosing the right one depends on the structure of the
data, the queries or computations required, and the tradeoffs between
efficiency, expressiveness, and scalability.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-217}

Imagine choosing a vehicle. A bicycle is perfect for short, simple
trips. A truck is needed to haul heavy loads. A plane makes sense for
long distances. Each is a valid vehicle, but only the right one fits the
task at hand. Data models work the same way.

\subsubsection{Deep Dive}\label{deep-dive-217}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2727}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1414}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5859}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Task Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Suitable Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Why It Fits
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Tabular analytics & Tables & Fixed schema, strong support for
aggregation and filtering \\
Relational queries & Graphs & Natural representation of connections and
paths \\
High-dimensional arrays & Tensors & Efficient for linear algebra and
deep learning \\
Mixed modalities & Hybrid models & Capture both attributes and
relationships \\
\end{longtable}

Criteria for selection include:

\begin{itemize}
\tightlist
\item
  Structure of data: Is it relational, sequential, hierarchical, or
  grid-like?
\item
  Type of query: Does the system need joins, traversals, aggregations,
  or convolutions?
\item
  Scale and sparsity: Are there many empty values, dense features, or
  irregular patterns?
\item
  Evolution over time: How easily must the model adapt to schema drift
  or new data types?
\end{itemize}

The wrong choice leads to inefficiency or even intractability: a graph
stored as a dense table wastes space, while a tensor forced into a
tabular schema loses spatial coherence.

\subsubsection{Tiny Code}\label{tiny-code-195}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ choose\_model(task):}
    \ControlFlowTok{if}\NormalTok{ task }\OperatorTok{==} \StringTok{"aggregate\_sales"}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{"Table"}
    \ControlFlowTok{elif}\NormalTok{ task }\OperatorTok{==} \StringTok{"find\_shortest\_path"}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{"Graph"}
    \ControlFlowTok{elif}\NormalTok{ task }\OperatorTok{==} \StringTok{"train\_neural\_network"}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{"Tensor"}
    \ControlFlowTok{else}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{"Hybrid"}
\end{Highlighting}
\end{Shaded}

This sketch shows a simple mapping from task type to representation.

\subsubsection{Try It Yourself}\label{try-it-yourself-217}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take a dataset of airline flights and decide whether tables, graphs,
  or tensors fit best for different analyses.
\item
  Represent the same dataset in two models and compare efficiency of
  answering a specific query.
\item
  Propose a hybrid representation for a dataset that combines numerical
  measurements with network relationships.
\end{enumerate}

\subsection{219. Tradeoffs in Storage, Querying, and
Computation}\label{tradeoffs-in-storage-querying-and-computation}

Every data model balances competing goals. Some optimize for compact
storage, others for fast queries, others for efficient computation.
Understanding these tradeoffs helps in choosing representations that
match the real priorities of a system.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-218}

Think of three different kitchens. One is tiny but keeps everything
tightly packed---great for storage but hard to cook in. Another is
designed for speed, with tools within easy reach---perfect for quick
preparation but cluttered. A third is expansive, with space for complex
recipes but more effort to maintain. Data systems face the same
tradeoffs.

\subsubsection{Deep Dive}\label{deep-dive-218}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1190}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2381}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3492}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2937}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Focus
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Optimized For
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Costs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Situations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Storage & Minimize memory or disk space & Slower queries, compression
overhead & Archiving, rare access \\
Querying & Rapid lookups and aggregations & Higher index overhead, more
storage & Dashboards, reporting \\
Computation & Fast mathematical operations & Large memory footprint,
preprocessed formats & Training neural networks, simulations \\
\end{longtable}

Tradeoffs emerge in practical choices. A compressed representation saves
space but requires decompression for access. Index-heavy systems enable
instant queries but slow down writes. Dense tensors are efficient for
computation but wasteful when data is mostly zeros.

The key is alignment: systems should choose representations based on
whether their bottleneck is storage, retrieval, or processing. A
mismatch results in wasted resources or poor performance.

\subsubsection{Tiny Code}\label{tiny-code-196}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ optimize(goal):}
    \ControlFlowTok{if}\NormalTok{ goal }\OperatorTok{==} \StringTok{"storage"}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{"compressed\_format"}
    \ControlFlowTok{elif}\NormalTok{ goal }\OperatorTok{==} \StringTok{"query"}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{"indexed\_format"}
    \ControlFlowTok{elif}\NormalTok{ goal }\OperatorTok{==} \StringTok{"computation"}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{"dense\_format"}
\end{Highlighting}
\end{Shaded}

This pseudocode represents how a system might prioritize one factor over
the others.

\subsubsection{Try It Yourself}\label{try-it-yourself-218}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take a dataset and store it once in compressed form, once with heavy
  indexing, and once as a dense matrix. Compare storage size and query
  speed.
\item
  Identify whether storage, query speed, or computation efficiency is
  most important in three domains: finance, healthcare, and image
  recognition.
\item
  Design a hybrid system where archived data is stored compactly, but
  recent data is kept in a fast-query format.
\end{enumerate}

\subsection{220. Emerging Models: Hypergraphs, Multimodal
Objects}\label{emerging-models-hypergraphs-multimodal-objects}

Traditional models like tables, graphs, and tensors cover most needs,
but some applications demand richer structures. Hypergraphs generalize
graphs by allowing edges to connect more than two nodes. Multimodal
objects combine heterogeneous data---text, images, audio, or structured
attributes---into unified entities. These models expand the expressive
power of data representation.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-219}

Think of a study group. A simple graph shows pairwise friendships. A
hypergraph can represent an entire group session as a single connection
linking many students at once. Now imagine attaching not only names but
also notes, pictures, and audio from the meeting---this becomes a
multimodal object.

\subsubsection{Deep Dive}\label{deep-dive-219}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1280}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2683}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3354}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2683}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Hypergraph & Edges connect multiple nodes simultaneously & Captures
group relationships, higher-order interactions & Harder to visualize,
more complex algorithms \\
Multimodal Object & Combines multiple data types into one unit &
Preserves context across modalities & Integration and alignment are
challenging \\
Composite Models & Blend structured and unstructured components &
Flexible, expressive & Greater storage and processing complexity \\
\end{longtable}

Hypergraphs are useful for modeling collaborations, co-purchases, or
biochemical reactions where interactions naturally involve more than two
participants. Multimodal objects are increasingly central in AI, where
systems need to understand images with captions, videos with
transcripts, or records mixing structured attributes with unstructured
notes.

Challenges lie in standardization, ensuring consistency across
modalities, and designing algorithms that can exploit these structures
effectively.

\subsubsection{Tiny Code}\label{tiny-code-197}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Hypergraph: one edge connects multiple nodes}
\NormalTok{hyperedge }\OperatorTok{=}\NormalTok{ \{}\StringTok{"members"}\NormalTok{: [}\StringTok{"Alice"}\NormalTok{, }\StringTok{"Bob"}\NormalTok{, }\StringTok{"Carol"}\NormalTok{]\}}

\CommentTok{\# Multimodal object: text + image + numeric data}
\NormalTok{record }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"text"}\NormalTok{: }\StringTok{"Patient report"}\NormalTok{,}
    \StringTok{"image"}\NormalTok{: }\StringTok{"xray\_01.png"}\NormalTok{,}
    \StringTok{"age"}\NormalTok{: }\DecValTok{54}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

These sketches show richer representations beyond traditional pairs or
grids.

\subsubsection{Try It Yourself}\label{try-it-yourself-219}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Represent a classroom project group as a hypergraph instead of a
  simple graph.
\item
  Build a multimodal object combining a paragraph of text, a related
  image, and metadata like author and date.
\item
  Discuss a scenario (e.g., medical diagnosis, product recommendation)
  where combining modalities improves performance over single-type data.
\end{enumerate}

\section{Chapter 23. Feature Engineering and
Encodings}\label{chapter-23.-feature-engineering-and-encodings}

\subsection{221. Categorical Encoding: One-Hot, Label,
Target}\label{categorical-encoding-one-hot-label-target}

Categorical variables describe qualities---like color, country, or
product type---rather than continuous measurements. Models require
numerical representations, so encoding transforms categories into usable
forms. The choice of encoding affects interpretability, efficiency, and
predictive performance.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-220}

Imagine organizing a box of crayons. You can number them arbitrarily
(``red = 1, blue = 2''), which is simple but misleading---numbers imply
order. Or you can create a separate switch for each color (``red on/off,
blue on/off''), which avoids false order but takes more space. Encoding
is like deciding how to represent colors in a machine-friendly way.

\subsubsection{Deep Dive}\label{deep-dive-220}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1227}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3190}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2822}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2761}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Encoding Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Advantages
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Label Encoding & Assigns an integer to each category & Compact, simple &
Imposes artificial ordering \\
One-Hot Encoding & Creates a binary indicator for each category &
Preserves independence, widely used & Expands dimensionality, sparse \\
Target Encoding & Replaces category with statistics of target variable &
Captures predictive signal, reduces dimensions & Risk of leakage,
sensitive to rare categories \\
Hashing Encoding & Maps categories to fixed-size integers via hash &
Scales to very high-cardinality features & Collisions possible, less
interpretable \\
\end{longtable}

Choosing the method depends on the number of categories, the algorithm
in use, and the balance between interpretability and efficiency.

\subsubsection{Tiny Code}\label{tiny-code-198}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{colors }\OperatorTok{=}\NormalTok{ [}\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{, }\StringTok{"green"}\NormalTok{]}

\CommentTok{\# Label encoding}
\NormalTok{label }\OperatorTok{=}\NormalTok{ \{}\StringTok{"red"}\NormalTok{: }\DecValTok{0}\NormalTok{, }\StringTok{"blue"}\NormalTok{: }\DecValTok{1}\NormalTok{, }\StringTok{"green"}\NormalTok{: }\DecValTok{2}\NormalTok{\}}

\CommentTok{\# One{-}hot encoding}
\NormalTok{one\_hot }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"red"}\NormalTok{: [}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{],}
    \StringTok{"blue"}\NormalTok{: [}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{],}
    \StringTok{"green"}\NormalTok{: [}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\NormalTok{\}}

\CommentTok{\# Target encoding (example: average sales per color)}
\NormalTok{target }\OperatorTok{=}\NormalTok{ \{}\StringTok{"red"}\NormalTok{: }\FloatTok{10.2}\NormalTok{, }\StringTok{"blue"}\NormalTok{: }\FloatTok{8.5}\NormalTok{, }\StringTok{"green"}\NormalTok{: }\FloatTok{12.1}\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Each scheme represents the same categories differently, shaping how a
model interprets them.

\subsubsection{Try It Yourself}\label{try-it-yourself-220}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Encode a small dataset of fruit types using label encoding and one-hot
  encoding, then compare dimensionality.
\item
  Simulate target encoding with a regression variable and analyze the
  risk of overfitting.
\item
  For a dataset with 50,000 unique categories, discuss which encoding
  would be most practical and why.
\end{enumerate}

\subsection{222. Numerical Transformations: Scaling,
Normalization}\label{numerical-transformations-scaling-normalization}

Numerical features often vary in magnitude---some span thousands, others
are fractions. Scaling and normalization adjust these values so that
algorithms treat them consistently. Without these steps, models may
become biased toward features with larger ranges.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-221}

Imagine a recipe where one ingredient is measured in grams and another
in kilograms. If you treat them without adjustment, the heavier unit
dominates the mix. Scaling is like converting everything into the same
measurement system before cooking.

\subsubsection{Deep Dive}\label{deep-dive-221}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1543}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2716}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2716}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3025}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Transformation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Advantages
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Min--Max Scaling & Rescales values to a fixed range (e.g., 0--1) &
Preserves relative order, bounded values & Sensitive to outliers \\
Z-Score Normalization & Centers values at 0 with unit variance & Handles
differing means and scales well & Assumes roughly normal distribution \\
Log Transformation & Compresses large ranges via logarithms & Reduces
skewness, handles exponential growth & Cannot handle non-positive
values \\
Robust Scaling & Uses medians and interquartile ranges & Resistant to
outliers & Less interpretable when distributions are uniform \\
\end{longtable}

Scaling ensures comparability across features, while normalization
adjusts distributions for stability. The choice depends on distribution
shape, sensitivity to outliers, and algorithm requirements.

\subsubsection{Tiny Code}\label{tiny-code-199}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{values }\OperatorTok{=}\NormalTok{ [}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{10}\NormalTok{]}

\CommentTok{\# Min–Max scaling}
\NormalTok{min\_v, max\_v }\OperatorTok{=} \BuiltInTok{min}\NormalTok{(values), }\BuiltInTok{max}\NormalTok{(values)}
\NormalTok{scaled }\OperatorTok{=}\NormalTok{ [(v }\OperatorTok{{-}}\NormalTok{ min\_v) }\OperatorTok{/}\NormalTok{ (max\_v }\OperatorTok{{-}}\NormalTok{ min\_v) }\ControlFlowTok{for}\NormalTok{ v }\KeywordTok{in}\NormalTok{ values]}

\CommentTok{\# Z{-}score normalization}
\NormalTok{mean\_v }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(values) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(values)}
\NormalTok{std\_v }\OperatorTok{=}\NormalTok{ (}\BuiltInTok{sum}\NormalTok{((v}\OperatorTok{{-}}\NormalTok{mean\_v)}\DecValTok{2} \ControlFlowTok{for}\NormalTok{ v }\KeywordTok{in}\NormalTok{ values)}\OperatorTok{/}\BuiltInTok{len}\NormalTok{(values))}\FloatTok{0.5}
\NormalTok{normalized }\OperatorTok{=}\NormalTok{ [(v }\OperatorTok{{-}}\NormalTok{ mean\_v)}\OperatorTok{/}\NormalTok{std\_v }\ControlFlowTok{for}\NormalTok{ v }\KeywordTok{in}\NormalTok{ values]}
\end{Highlighting}
\end{Shaded}

Both methods transform the same data but yield different distributions
suited to different tasks.

\subsubsection{Try It Yourself}\label{try-it-yourself-221}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Apply min--max scaling and z-score normalization to the same dataset;
  compare results.
\item
  Take a skewed dataset and apply a log transformation; observe how the
  distribution changes.
\item
  Discuss which transformation would be most useful in anomaly detection
  where outliers matter.
\end{enumerate}

\subsection{223. Text Features: Bag-of-Words, TF-IDF,
Embeddings}\label{text-features-bag-of-words-tf-idf-embeddings}

Text is unstructured and must be converted into numbers before models
can use it. Bag-of-Words, TF-IDF, and embeddings are three major
approaches that capture different aspects of language: frequency,
importance, and meaning.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-222}

Think of analyzing a bookshelf. Counting how many times each word
appears across all books is like Bag-of-Words. Adjusting the count so
rare words stand out is like TF-IDF. Understanding that ``king'' and
``queen'' are related beyond spelling is like embeddings.

\subsubsection{Deep Dive}\label{deep-dive-222}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3828}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2188}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2734}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Bag-of-Words & Represents text as counts of each word & Simple,
interpretable & Ignores order and meaning \\
TF-IDF & Weights words by frequency and rarity & Highlights informative
terms & Still ignores semantics \\
Embeddings & Maps words into dense vectors in continuous space &
Captures semantic similarity & Requires training, less transparent \\
\end{longtable}

Bag-of-Words provides a baseline by treating each word independently.
TF-IDF emphasizes words that distinguish documents. Embeddings compress
language into vectors where similar words cluster, supporting semantic
reasoning.

Challenges include vocabulary size, handling out-of-vocabulary words,
and deciding how much context to preserve.

\subsubsection{Tiny Code}\label{tiny-code-200}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{doc }\OperatorTok{=} \StringTok{"AI transforms data into knowledge"}

\CommentTok{\# Bag{-}of{-}Words}
\NormalTok{bow }\OperatorTok{=}\NormalTok{ \{}\StringTok{"AI"}\NormalTok{: }\DecValTok{1}\NormalTok{, }\StringTok{"transforms"}\NormalTok{: }\DecValTok{1}\NormalTok{, }\StringTok{"data"}\NormalTok{: }\DecValTok{1}\NormalTok{, }\StringTok{"into"}\NormalTok{: }\DecValTok{1}\NormalTok{, }\StringTok{"knowledge"}\NormalTok{: }\DecValTok{1}\NormalTok{\}}

\CommentTok{\# TF{-}IDF (simplified example)}
\NormalTok{tfidf }\OperatorTok{=}\NormalTok{ \{}\StringTok{"AI"}\NormalTok{: }\FloatTok{0.7}\NormalTok{, }\StringTok{"transforms"}\NormalTok{: }\FloatTok{0.7}\NormalTok{, }\StringTok{"data"}\NormalTok{: }\FloatTok{0.3}\NormalTok{, }\StringTok{"into"}\NormalTok{: }\FloatTok{0.2}\NormalTok{, }\StringTok{"knowledge"}\NormalTok{: }\FloatTok{0.9}\NormalTok{\}}

\CommentTok{\# Embedding (conceptual)}
\NormalTok{embedding }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"AI"}\NormalTok{: [}\FloatTok{0.12}\NormalTok{, }\FloatTok{0.98}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.45}\NormalTok{],}
    \StringTok{"data"}\NormalTok{: [}\FloatTok{0.34}\NormalTok{, }\FloatTok{0.75}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.11}\NormalTok{]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Each representation captures different levels of information about the
same text.

\subsubsection{Try It Yourself}\label{try-it-yourself-222}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a Bag-of-Words representation for two short sentences and
  compare overlap.
\item
  Compute TF-IDF for a small set of documents and see which words stand
  out.
\item
  Use embeddings to find which words in a vocabulary are closest in
  meaning to ``science.''
\end{enumerate}

\subsection{224. Image Features: Histograms, CNN Feature
Maps}\label{image-features-histograms-cnn-feature-maps}

Images are arrays of pixels, but raw pixels are often too detailed and
noisy for learning directly. Feature extraction condenses images into
more informative representations, from simple histograms of pixel values
to high-level patterns captured by convolutional filters.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-223}

Imagine trying to describe a painting. You could count how many red,
green, and blue areas appear (a histogram). Or you could point out
shapes, textures, and objects recognized by your eye (feature maps).
Both summarize the same painting at different levels of abstraction.

\subsubsection{Deep Dive}\label{deep-dive-223}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1987}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3245}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2450}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2318}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Feature Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Color Histograms & Count distribution of pixel intensities & Simple,
interpretable & Ignores shape and spatial structure \\
Edge Detectors & Capture boundaries and gradients & Highlights contours
& Sensitive to noise \\
Texture Descriptors & Measure patterns like smoothness or repetition &
Useful for material recognition & Limited semantic information \\
Convolutional Feature Maps & Learned filters capture local and global
patterns & Scales to complex tasks, hierarchical & Harder to interpret
directly \\
\end{longtable}

Histograms provide global summaries, while convolutional maps
progressively build hierarchical representations: edges → textures →
shapes → objects. Both serve as compact alternatives to raw pixel
arrays.

Challenges include sensitivity to lighting or orientation, the curse of
dimensionality for handcrafted features, and balancing interpretability
with power.

\subsubsection{Tiny Code}\label{tiny-code-201}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{image }\OperatorTok{=}\NormalTok{ load\_image(}\StringTok{"cat.png"}\NormalTok{)}

\CommentTok{\# Color histogram (simplified)}
\NormalTok{histogram }\OperatorTok{=}\NormalTok{ count\_pixels\_by\_color(image)}

\CommentTok{\# Convolutional feature map (conceptual)}
\NormalTok{feature\_map }\OperatorTok{=}\NormalTok{ apply\_filters(image, filters}\OperatorTok{=}\NormalTok{[}\StringTok{"edge"}\NormalTok{, }\StringTok{"corner"}\NormalTok{, }\StringTok{"texture"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

This captures low-level distributions with histograms and higher-level
abstractions with feature maps.

\subsubsection{Try It Yourself}\label{try-it-yourself-223}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute a color histogram for two images of the same object under
  different lighting; compare results.
\item
  Apply edge detection to an image and observe how shapes become
  clearer.
\item
  Simulate a small filter bank and visualize how each filter highlights
  different image regions.
\end{enumerate}

\subsection{225. Audio Features: MFCCs, Spectrograms,
Wavelets}\label{audio-features-mfccs-spectrograms-wavelets}

Audio signals are continuous waveforms, but models need structured
features. Transformations such as spectrograms, MFCCs, and wavelets
convert raw sound into representations that highlight frequency, energy,
and perceptual cues.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-224}

Think of listening to music. You hear the rhythm (time), the pitch
(frequency), and the timbre (texture). A spectrogram is like a sheet of
music showing frequencies over time. MFCCs capture how humans perceive
sound. Wavelets zoom in and out, like listening closely to short riffs
or stepping back to hear the overall composition.

\subsubsection{Deep Dive}\label{deep-dive-224}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2527}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2912}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2143}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2418}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Feature Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Spectrogram & Time--frequency representation using Fourier transform &
Rich detail of frequency changes & High dimensionality, sensitive to
noise \\
MFCC (Mel-Frequency Cepstral Coefficients) & Compact features based on
human auditory scale & Effective for speech recognition & Loses
fine-grained detail \\
Wavelets & Decompose signal into multi-scale components & Captures both
local and global patterns & More complex to compute,
parameter-sensitive \\
\end{longtable}

Spectrograms reveal frequency energy across time slices. MFCCs reduce
this to features aligned with perception, widely used in speech and
speaker recognition. Wavelets provide flexible resolution, revealing
short bursts and long-term trends in the same signal.

Challenges include noise robustness, tradeoffs between resolution and
efficiency, and ensuring transformations preserve information relevant
to the task.

\subsubsection{Tiny Code}\label{tiny-code-202}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{audio }\OperatorTok{=}\NormalTok{ load\_audio(}\StringTok{"speech.wav"}\NormalTok{)}

\CommentTok{\# Spectrogram}
\NormalTok{spectrogram }\OperatorTok{=}\NormalTok{ fourier\_transform(audio)}

\CommentTok{\# MFCCs}
\NormalTok{mfccs }\OperatorTok{=}\NormalTok{ mel\_frequency\_cepstral(audio)}

\CommentTok{\# Wavelet transform}
\NormalTok{wavelet\_coeffs }\OperatorTok{=}\NormalTok{ wavelet\_decompose(audio)}
\end{Highlighting}
\end{Shaded}

Each transformation yields a different perspective on the same waveform.

\subsubsection{Try It Yourself}\label{try-it-yourself-224}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute spectrograms of two different sounds and compare their
  patterns.
\item
  Extract MFCCs from short speech samples and test whether they
  differentiate speakers.
\item
  Apply wavelet decomposition to a noisy signal and observe how
  denoising improves clarity.
\end{enumerate}

\subsection{226. Temporal Features: Lags, Windows, Fourier
Transforms}\label{temporal-features-lags-windows-fourier-transforms}

Temporal data captures events over time. To make it useful for models,
we derive features that represent history, periodicity, and trends. Lags
capture past values, windows summarize recent activity, and Fourier
transforms expose hidden cycles.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-225}

Think of tracking the weather. Looking at yesterday's temperature is a
lag. Calculating the average of the past week is a window. Recognizing
that seasons repeat yearly is like applying a Fourier transform. Each
reveals structure in time.

\subsubsection{Deep Dive}\label{deep-dive-225}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1325}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3179}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2450}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3046}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Feature Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Lag Features & Use past values as predictors & Simple, captures
short-term memory & Misses long-term patterns \\
Window Features & Summaries over fixed spans (mean, sum, variance) &
Smooths noise, captures recent trends & Choice of window size
critical \\
Fourier Features & Decompose signals into frequencies & Detects periodic
cycles & Assumes stationarity, can be hard to interpret \\
\end{longtable}

Lags and windows are most common in forecasting tasks, giving models a
memory of recent events. Fourier features uncover repeating patterns,
such as daily, weekly, or seasonal rhythms. Combined, they let systems
capture both immediate changes and deep cycles.

Challenges include selecting window sizes, handling irregular time
steps, and balancing interpretability with complexity.

\subsubsection{Tiny Code}\label{tiny-code-203}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{time\_series }\OperatorTok{=}\NormalTok{ [}\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{]}

\CommentTok{\# Lag feature: yesterday\textquotesingle{}s value}
\NormalTok{lag1 }\OperatorTok{=}\NormalTok{ time\_series[}\OperatorTok{{-}}\DecValTok{2}\NormalTok{]}

\CommentTok{\# Window feature: last 3{-}day average}
\NormalTok{window\_avg }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(time\_series[}\OperatorTok{{-}}\DecValTok{3}\NormalTok{:]) }\OperatorTok{/} \DecValTok{3}

\CommentTok{\# Fourier feature (conceptual)}
\NormalTok{frequencies }\OperatorTok{=}\NormalTok{ fourier\_decompose(time\_series)}
\end{Highlighting}
\end{Shaded}

Each method transforms raw sequences into features that highlight
different temporal aspects.

\subsubsection{Try It Yourself}\label{try-it-yourself-225}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute lag-1 and lag-2 features for a short temperature series and
  test their predictive value.
\item
  Try different window sizes (3-day, 7-day, 30-day) on sales data and
  compare stability.
\item
  Apply Fourier analysis to a seasonal dataset and identify dominant
  cycles.
\end{enumerate}

\subsection{227. Interaction Features and Polynomial
Expansion}\label{interaction-features-and-polynomial-expansion}

Single features capture individual effects, but real-world patterns
often arise from interactions between variables. Interaction features
combine multiple inputs, while polynomial expansions extend them into
higher-order terms, enabling models to capture nonlinear relationships.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-226}

Imagine predicting house prices. Square footage alone matters, as does
neighborhood. But the combination---large houses in expensive
areas---matters even more. That's an interaction. Polynomial expansion
is like considering not just size but also size squared, revealing
diminishing or accelerating effects.

\subsubsection{Deep Dive}\label{deep-dive-226}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1894}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3409}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2424}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2273}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Technique
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Pairwise Interactions & Multiply or combine two features & Captures
combined effects & Rapid feature growth \\
Polynomial Expansion & Add powers of features (squared, cubed, etc.) &
Models nonlinear curves & Can overfit, hard to interpret \\
Crossed Features & Encodes combinations of categorical values & Useful
in recommendation systems & High cardinality explosion \\
\end{longtable}

Interactions allow linear models to approximate complex relationships.
Polynomial expansions enable smooth curves without explicitly using
nonlinear models. Crossed features highlight patterns that exist only in
specific category combinations.

Challenges include managing dimensionality growth, preventing
overfitting, and keeping features interpretable. Feature selection or
regularization is often needed.

\subsubsection{Tiny Code}\label{tiny-code-204}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{size }\OperatorTok{=} \DecValTok{120}  \CommentTok{\# square meters}
\NormalTok{rooms }\OperatorTok{=} \DecValTok{3}

\CommentTok{\# Interaction feature}
\NormalTok{interaction }\OperatorTok{=}\NormalTok{ size }\OperatorTok{*}\NormalTok{ rooms}

\CommentTok{\# Polynomial expansion}
\NormalTok{poly\_size }\OperatorTok{=}\NormalTok{ [size, size2, size3]}
\end{Highlighting}
\end{Shaded}

These new features enrich the dataset, allowing models to capture more
nuanced patterns.

\subsubsection{Try It Yourself}\label{try-it-yourself-226}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create interaction features for a dataset of height and weight; test
  their usefulness in predicting BMI.
\item
  Apply polynomial expansion to a simple dataset and compare linear
  vs.~polynomial regression fits.
\item
  Discuss when interaction features are more appropriate than polynomial
  ones.
\end{enumerate}

\subsection{228. Hashing Tricks and Embedding
Tables}\label{hashing-tricks-and-embedding-tables}

High-cardinality categorical data, like user IDs or product codes,
creates challenges for representation. Hashing and embeddings offer
compact ways to handle these features without exploding dimensionality.
Hashing maps categories into fixed buckets, while embeddings learn dense
continuous vectors.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-227}

Imagine labeling mailboxes for an entire city. Creating one box per
resident is too many (like one-hot encoding). Instead, you could assign
people to a limited number of boxes by hashing their names---some will
share boxes. Or, better, you could assign each person a short code that
captures their neighborhood, preferences, and habits---like embeddings.

\subsubsection{Deep Dive}\label{deep-dive-227}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1274}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3694}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2548}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2484}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Hashing Trick & Apply a hash function to map categories into fixed
buckets & Scales well, no dictionary needed & Collisions may mix
unrelated categories \\
Embedding Tables & Learn dense vectors representing categories &
Captures semantic relationships, compact & Requires training, less
interpretable \\
\end{longtable}

Hashing is useful for real-time systems where memory is constrained and
categories are numerous or evolving. Embeddings shine when categories
have rich interactions and benefit from learned structure, such as words
in language or products in recommendations.

Challenges include handling collisions gracefully in hashing, deciding
embedding dimensions, and ensuring embeddings generalize beyond training
data.

\subsubsection{Tiny Code}\label{tiny-code-205}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Hashing trick}
\KeywordTok{def}\NormalTok{ hash\_category(cat, buckets}\OperatorTok{=}\DecValTok{1000}\NormalTok{):}
    \ControlFlowTok{return} \BuiltInTok{hash}\NormalTok{(cat) }\OperatorTok{\%}\NormalTok{ buckets}

\CommentTok{\# Embedding table (conceptual)}
\NormalTok{embedding\_table }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"user\_1"}\NormalTok{: [}\FloatTok{0.12}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.45}\NormalTok{, }\FloatTok{0.78}\NormalTok{],}
    \StringTok{"user\_2"}\NormalTok{: [}\FloatTok{0.34}\NormalTok{, }\FloatTok{0.10}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.22}\NormalTok{]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Both methods replace large sparse vectors with compact, manageable
forms.

\subsubsection{Try It Yourself}\label{try-it-yourself-227}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Hash a list of 100 unique categories into 10 buckets and observe
  collisions.
\item
  Train embeddings for a set of items and visualize them in 2D space to
  see clustering.
\item
  Compare model performance when using hashing vs.~embeddings on the
  same dataset.
\end{enumerate}

\subsection{229. Automated Feature Engineering (Feature
Stores)}\label{automated-feature-engineering-feature-stores}

Manually designing features is time-consuming and error-prone. Automated
feature engineering creates, manages, and reuses features
systematically. Central repositories, often called feature stores,
standardize definitions so teams can share and deploy features
consistently.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-228}

Imagine a restaurant kitchen. Instead of every chef preparing basic
ingredients from scratch, there's a pantry stocked with prepped
vegetables, sauces, and spices. Chefs assemble meals faster and more
consistently. Feature stores play the same role for machine
learning---ready-to-use ingredients for models.

\subsubsection{Deep Dive}\label{deep-dive-228}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1692}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5769}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2538}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Benefit
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Feature Generation & Automatically creates transformations (aggregates,
interactions, encodings) & Speeds up experimentation \\
Feature Registry & Central catalog of definitions and metadata & Ensures
consistency across teams \\
Feature Serving & Provides online and offline access to the same
features & Eliminates training--serving skew \\
Monitoring & Tracks freshness, drift, and quality of features & Prevents
silent model degradation \\
\end{longtable}

Automated feature engineering reduces duplication of work and enforces
consistent definitions of business logic. It also bridges
experimentation and production by ensuring that models use the same
features in both environments.

Challenges include handling data freshness requirements, preventing
feature bloat, and maintaining versioned definitions as business rules
evolve.

\subsubsection{Tiny Code}\label{tiny-code-206}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example of a registered feature}
\NormalTok{feature }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"name"}\NormalTok{: }\StringTok{"avg\_purchase\_last\_30d"}\NormalTok{,}
    \StringTok{"description"}\NormalTok{: }\StringTok{"Average customer spending over last 30 days"}\NormalTok{,}
    \StringTok{"data\_type"}\NormalTok{: }\StringTok{"float"}\NormalTok{,}
    \StringTok{"calculation"}\NormalTok{: }\StringTok{"sum(purchases)/30"}
\NormalTok{\}}

\CommentTok{\# Serving (conceptual)}
\NormalTok{value }\OperatorTok{=}\NormalTok{ get\_feature(}\StringTok{"avg\_purchase\_last\_30d"}\NormalTok{, customer\_id}\OperatorTok{=}\DecValTok{42}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This shows how a feature might be defined once and reused across
different models.

\subsubsection{Try It Yourself}\label{try-it-yourself-228}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Define three features for predicting customer churn and write down
  their definitions.
\item
  Simulate an online system where a feature value is updated daily and
  accessed in real time.
\item
  Compare the risk of inconsistency when features are hand-coded
  separately versus managed centrally.
\end{enumerate}

\subsection{230. Tradeoffs: Interpretability
vs.~Expressiveness}\label{tradeoffs-interpretability-vs.-expressiveness}

Feature engineering choices often balance between interpretability---how
easily humans can understand features---and expressiveness---how much
predictive power features give to models. Simple transformations are
transparent but may miss patterns; complex ones capture more nuance but
are harder to explain.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-229}

Think of a map. A simple sketch with landmarks is easy to read but lacks
detail. A satellite image is rich with information but overwhelming to
interpret. Features behave the same way: some are straightforward but
limited, others are powerful but opaque.

\subsubsection{Deep Dive}\label{deep-dive-229}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3191}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1702}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1489}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3617}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Interpretability
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Expressiveness
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Raw Features & High & Low & Age, income as-is \\
Simple Transformations & Medium & Medium & Ratios, log
transformations \\
Interactions/Polynomials & Lower & Higher & Size × location, squared
terms \\
Embeddings/Latent Features & Low & High & Word vectors, deep
representations \\
\end{longtable}

Interpretability helps with debugging, trust, and regulatory compliance.
Expressiveness improves accuracy and generalization. In practice, the
balance depends on context: healthcare may demand interpretability,
while recommendation systems prioritize expressiveness.

Challenges include avoiding overfitting with highly expressive features,
maintaining transparency for stakeholders, and combining both approaches
in hybrid systems.

\subsubsection{Tiny Code}\label{tiny-code-207}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Interpretable feature}
\NormalTok{income\_to\_age\_ratio }\OperatorTok{=}\NormalTok{ income }\OperatorTok{/}\NormalTok{ age}

\CommentTok{\# Expressive feature (embedding, conceptual)}
\NormalTok{user\_vector }\OperatorTok{=}\NormalTok{ [}\FloatTok{0.12}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.45}\NormalTok{, }\FloatTok{0.78}\NormalTok{, }\FloatTok{0.33}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

One feature is easily explained to stakeholders, while the other encodes
hidden patterns not directly interpretable.

\subsubsection{Try It Yourself}\label{try-it-yourself-229}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a dataset where both a simple interpretable feature and a
  complex embedding are available; compare model performance.
\item
  Explain to a non-technical audience what an interaction feature means
  in plain words.
\item
  Identify a domain where interpretability must dominate and another
  where expressiveness can take priority.
\end{enumerate}

\section{Chapter 24. Labelling, annotation, and weak
supervision}\label{chapter-24.-labelling-annotation-and-weak-supervision}

\subsection{231. Labeling Guidelines and
Taxonomies}\label{labeling-guidelines-and-taxonomies}

Labels give structure to raw data, defining what the model should learn.
Guidelines ensure that labeling is consistent, while taxonomies provide
hierarchical organization of categories. Together, they reduce ambiguity
and improve the reliability of supervised learning.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-230}

Imagine organizing a library. If one librarian files ``science fiction''
under ``fiction'' and another under ``fantasy,'' the collection becomes
inconsistent. Clear labeling rules and a shared taxonomy act like a
cataloging system that keeps everything aligned.

\subsubsection{Deep Dive}\label{deep-dive-230}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1145}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4046}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4809}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Element
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Guidelines & Instructions that define how labels should be applied &
``Mark tweets as positive only if sentiment is clearly positive'' \\
Taxonomy & Hierarchical structure of categories & Sentiment → Positive /
Negative / Neutral \\
Granularity & Defines level of detail & Species vs.~Genus vs.~Family in
biology \\
Consistency & Ensures reproducibility across annotators & Multiple
labelers agree on the same category \\
\end{longtable}

Guidelines prevent ambiguity, especially in subjective tasks like
sentiment analysis. Taxonomies keep categories coherent and scalable,
avoiding overlaps or gaps. Granularity determines how fine-grained the
labels should be, balancing simplicity and expressiveness.

Challenges arise when tasks are subjective, when taxonomies drift over
time, or when annotators interpret rules differently. Maintaining
clarity and updating taxonomies as domains evolve is critical.

\subsubsection{Tiny Code}\label{tiny-code-208}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{taxonomy }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"sentiment"}\NormalTok{: \{}
        \StringTok{"positive"}\NormalTok{: [],}
        \StringTok{"negative"}\NormalTok{: [],}
        \StringTok{"neutral"}\NormalTok{: []}
\NormalTok{    \}}
\NormalTok{\}}

\KeywordTok{def}\NormalTok{ apply\_label(text):}
    \ControlFlowTok{if} \StringTok{"love"} \KeywordTok{in}\NormalTok{ text:}
        \ControlFlowTok{return} \StringTok{"positive"}
    \ControlFlowTok{elif} \StringTok{"hate"} \KeywordTok{in}\NormalTok{ text:}
        \ControlFlowTok{return} \StringTok{"negative"}
    \ControlFlowTok{else}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{"neutral"}
\end{Highlighting}
\end{Shaded}

This sketch shows how rules map raw data into a structured taxonomy.

\subsubsection{Try It Yourself}\label{try-it-yourself-230}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Define a taxonomy for labeling customer support tickets (e.g.,
  billing, technical, general).
\item
  Write labeling guidelines for distinguishing between sarcasm and
  genuine sentiment.
\item
  Compare annotation results with and without detailed guidelines to
  measure consistency.
\end{enumerate}

\subsection{232. Human Annotation Workflows and
Tools}\label{human-annotation-workflows-and-tools}

Human annotation is the process of assigning labels or tags to data by
people. It is essential for supervised learning, where ground truth must
come from careful human judgment. Workflows and structured processes
ensure efficiency, quality, and reproducibility.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-231}

Imagine an assembly line where workers add labels to packages. If each
worker follows their own rules, chaos results. With clear instructions,
checkpoints, and quality checks, the assembly line produces consistent
results. Annotation workflows function the same way.

\subsubsection{Deep Dive}\label{deep-dive-231}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1959}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3505}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4536}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Activities
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Task Design & Define what annotators must do & Write clear instructions,
give examples \\
Training & Prepare annotators for consistency & Practice rounds,
feedback loops \\
Annotation & Actual labeling process & Highlighting text spans,
categorizing images \\
Quality Control & Detect errors or bias & Redundant labeling, spot
checks \\
Iteration & Refine guidelines and tasks & Update rules when
disagreements appear \\
\end{longtable}

Well-designed workflows avoid confusion and reduce noise in the labels.
Training ensures that annotators share the same understanding. Quality
control methods like redundancy (multiple annotators per item) or
consensus checks keep accuracy high. Iteration acknowledges that
labeling is rarely perfect on the first try.

Challenges include managing cost, preventing fatigue, handling
subjective judgments, and scaling to large datasets while maintaining
quality.

\subsubsection{Tiny Code}\label{tiny-code-209}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ annotate(item, guideline):}
    \CommentTok{\# Human reads item and applies guideline}
\NormalTok{    label }\OperatorTok{=}\NormalTok{ human\_label(item, guideline)}
    \ControlFlowTok{return}\NormalTok{ label}

\KeywordTok{def}\NormalTok{ consensus(labels):}
    \CommentTok{\# Majority vote for quality control}
    \ControlFlowTok{return} \BuiltInTok{max}\NormalTok{(}\BuiltInTok{set}\NormalTok{(labels), key}\OperatorTok{=}\NormalTok{labels.count)}
\end{Highlighting}
\end{Shaded}

This simple sketch shows annotation and consensus steps to improve
reliability.

\subsubsection{Try It Yourself}\label{try-it-yourself-231}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Design a small annotation task with three categories and write clear
  instructions.
\item
  Simulate having three annotators label the same data, then aggregate
  with majority voting.
\item
  Identify situations where consensus fails (e.g., subjective tasks) and
  propose solutions.
\end{enumerate}

\subsection{233. Active Learning for Efficient
Labeling}\label{active-learning-for-efficient-labeling}

Labeling data is expensive and time-consuming. Active learning reduces
effort by selecting the most informative examples for annotation.
Instead of labeling randomly, the system queries humans for cases where
the model is most uncertain or where labels add the most value.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-232}

Think of a teacher tutoring a student. Rather than practicing problems
the student already knows, the teacher focuses on the hardest
questions---where the student hesitates. Active learning works the same
way, directing human effort where it matters most.

\subsubsection{Deep Dive}\label{deep-dive-232}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1622}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3581}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2365}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2432}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Strategy
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Benefit
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Uncertainty Sampling & Pick examples where model confidence is lowest &
Maximizes learning per label & May focus on outliers \\
Query by Committee & Use multiple models and choose items they disagree
on & Captures diverse uncertainties & Requires maintaining multiple
models \\
Diversity Sampling & Select examples that represent varied data regions
& Prevents redundancy, broad coverage & May skip rare but important
cases \\
Hybrid Methods & Combine uncertainty and diversity & Balanced efficiency
& Higher implementation complexity \\
\end{longtable}

Active learning is most effective when unlabeled data is abundant and
labeling costs are high. It accelerates model improvement while
minimizing annotation effort.

Challenges include avoiding overfitting to uncertain noise, maintaining
fairness across categories, and deciding when to stop the process
(diminishing returns).

\subsubsection{Tiny Code}\label{tiny-code-210}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ active\_learning\_step(model, unlabeled\_pool):}
    \CommentTok{\# Rank examples by uncertainty}
\NormalTok{    ranked }\OperatorTok{=} \BuiltInTok{sorted}\NormalTok{(unlabeled\_pool, key}\OperatorTok{=}\KeywordTok{lambda}\NormalTok{ x: model.uncertainty(x), reverse}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
    \CommentTok{\# Select top{-}k for labeling}
    \ControlFlowTok{return}\NormalTok{ ranked[:}\DecValTok{10}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

This sketch shows how a system might prioritize uncertain samples for
annotation.

\subsubsection{Try It Yourself}\label{try-it-yourself-232}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train a simple classifier and implement uncertainty sampling on an
  unlabeled pool.
\item
  Compare model improvement using random sampling vs.~active learning.
\item
  Design a stopping criterion: when does active learning no longer add
  significant value?
\end{enumerate}

\subsection{234. Crowdsourcing and Quality
Control}\label{crowdsourcing-and-quality-control}

Crowdsourcing distributes labeling tasks to many people, often through
online platforms. It scales annotation efforts quickly but introduces
risks of inconsistency and noise. Quality control mechanisms ensure that
large, diverse groups still produce reliable labels.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-233}

Imagine assembling a giant jigsaw puzzle with hundreds of volunteers.
Some work carefully, others rush, and a few make mistakes. To complete
the puzzle correctly, you need checks---like comparing multiple answers
or assigning supervisors. Crowdsourced labeling requires the same
safeguards.

\subsubsection{Deep Dive}\label{deep-dive-233}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2054}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3661}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4286}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Redundancy & Have multiple workers label the same item & Majority voting
on sentiment labels \\
Gold Standard Tasks & Insert items with known labels & Detect careless
or low-quality workers \\
Consensus Measures & Evaluate agreement across workers & High
inter-rater agreement indicates reliability \\
Weighted Voting & Give more influence to skilled workers & Trust
annotators with consistent accuracy \\
Feedback Loops & Provide guidance to workers & Improve performance over
time \\
\end{longtable}

Crowdsourcing is powerful for scaling, especially in domains like image
tagging or sentiment analysis. But without controls, it risks
inconsistency and even malicious input. Quality measures strike a
balance between speed and reliability.

Challenges include designing tasks that are simple yet precise, managing
costs while ensuring redundancy, and filtering out unreliable annotators
without unfair bias.

\subsubsection{Tiny Code}\label{tiny-code-211}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ aggregate\_labels(labels):}
    \CommentTok{\# Majority vote for crowdsourced labels}
    \ControlFlowTok{return} \BuiltInTok{max}\NormalTok{(}\BuiltInTok{set}\NormalTok{(labels), key}\OperatorTok{=}\NormalTok{labels.count)}

\CommentTok{\# Example: three workers label "positive"}
\NormalTok{labels }\OperatorTok{=}\NormalTok{ [}\StringTok{"positive"}\NormalTok{, }\StringTok{"positive"}\NormalTok{, }\StringTok{"negative"}\NormalTok{]}
\NormalTok{final\_label }\OperatorTok{=}\NormalTok{ aggregate\_labels(labels)  }\CommentTok{\# {-}\textgreater{} "positive"}
\end{Highlighting}
\end{Shaded}

This shows how redundancy and aggregation can stabilize noisy inputs.

\subsubsection{Try It Yourself}\label{try-it-yourself-233}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Design a crowdsourcing task with clear instructions and minimal
  ambiguity.
\item
  Simulate redundancy by assigning the same items to three annotators
  and applying majority vote.
\item
  Insert a set of gold standard tasks into a labeling workflow and test
  whether annotators meet quality thresholds.
\end{enumerate}

\subsection{235. Semi-Supervised Label
Propagation}\label{semi-supervised-label-propagation}

Semi-supervised learning uses both labeled and unlabeled data. Label
propagation spreads information from labeled examples to nearby
unlabeled ones in a feature space or graph. This reduces manual labeling
effort by letting structure in the data guide the labeling process.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-234}

Imagine coloring a map where only a few cities are marked red or blue.
By looking at roads connecting them, you can guess that nearby towns
connected to red cities should also be red. Label propagation works the
same way, spreading labels through connections or similarity.

\subsubsection{Deep Dive}\label{deep-dive-234}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1554}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4974}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1813}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1658}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Graph-Based Propagation & Build a graph where nodes are data points and
edges reflect similarity; labels flow across edges & Captures local
structure, intuitive & Sensitive to graph construction \\
Nearest Neighbor Spreading & Assign unlabeled points based on closest
labeled examples & Simple, scalable & Can misclassify in noisy
regions \\
Iterative Propagation & Repeatedly update unlabeled points with weighted
averages of neighbors & Exploits smoothness assumptions & May reinforce
early mistakes \\
\end{longtable}

Label propagation works best when data has clusters where points of the
same class group together. It is especially effective in domains where
unlabeled data is abundant but labeled examples are costly.

Challenges include ensuring that similarity measures are meaningful,
avoiding propagation of errors, and handling overlapping or ambiguous
clusters.

\subsubsection{Tiny Code}\label{tiny-code-212}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ propagate\_labels(graph, labels, steps}\OperatorTok{=}\DecValTok{5}\NormalTok{):}
    \ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(steps):}
        \ControlFlowTok{for}\NormalTok{ node }\KeywordTok{in}\NormalTok{ graph.nodes:}
            \ControlFlowTok{if}\NormalTok{ node }\KeywordTok{not} \KeywordTok{in}\NormalTok{ labels:}
                \CommentTok{\# Assign label based on majority of neighbors}
\NormalTok{                neighbor\_labels }\OperatorTok{=}\NormalTok{ [labels[n] }\ControlFlowTok{for}\NormalTok{ n }\KeywordTok{in}\NormalTok{ graph.neighbors(node) }\ControlFlowTok{if}\NormalTok{ n }\KeywordTok{in}\NormalTok{ labels]}
                \ControlFlowTok{if}\NormalTok{ neighbor\_labels:}
\NormalTok{                    labels[node] }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(}\BuiltInTok{set}\NormalTok{(neighbor\_labels), key}\OperatorTok{=}\NormalTok{neighbor\_labels.count)}
    \ControlFlowTok{return}\NormalTok{ labels}
\end{Highlighting}
\end{Shaded}

This sketch shows how labels spread across a graph iteratively.

\subsubsection{Try It Yourself}\label{try-it-yourself-234}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a small graph with a few labeled nodes and propagate labels to
  the rest.
\item
  Compare accuracy when propagating labels versus random guessing.
\item
  Experiment with different similarity definitions (e.g., distance
  thresholds) and observe how results change.
\end{enumerate}

\subsection{236. Weak Labels: Distant Supervision,
Heuristics}\label{weak-labels-distant-supervision-heuristics}

Weak labeling assigns approximate or noisy labels instead of precise
human-verified ones. While imperfect, weak labels can train useful
models when clean data is scarce. Methods include distant supervision,
heuristics, and programmatic rules.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-235}

Imagine grading homework by scanning for keywords instead of reading
every answer carefully. It's faster but not always accurate. Weak
labeling works the same way: quick, scalable, but imperfect.

\subsubsection{Deep Dive}\label{deep-dive-235}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1462}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3626}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2339}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2573}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Distant Supervision & Use external resources (like knowledge bases) to
assign labels & Scales easily, leverages prior knowledge & Labels can be
noisy or inconsistent \\
Heuristic Rules & Apply patterns or keywords to infer labels & Fast,
domain-driven & Brittle, hard to generalize \\
Programmatic Labeling & Combine multiple weak sources algorithmically &
Scales across large datasets & Requires calibration and careful
combination \\
\end{longtable}

Weak labels are especially useful when unlabeled data is abundant but
human annotation is expensive. They serve as a starting point, often
refined later by human review or semi-supervised learning.

Challenges include controlling noise so models don't overfit incorrect
labels, handling class imbalance, and evaluating quality without
gold-standard data.

\subsubsection{Tiny Code}\label{tiny-code-213}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ weak\_label(text):}
    \ControlFlowTok{if} \StringTok{"great"} \KeywordTok{in}\NormalTok{ text }\KeywordTok{or} \StringTok{"excellent"} \KeywordTok{in}\NormalTok{ text:}
        \ControlFlowTok{return} \StringTok{"positive"}
    \ControlFlowTok{elif} \StringTok{"bad"} \KeywordTok{in}\NormalTok{ text }\KeywordTok{or} \StringTok{"terrible"} \KeywordTok{in}\NormalTok{ text:}
        \ControlFlowTok{return} \StringTok{"negative"}
    \ControlFlowTok{else}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{"neutral"}
\end{Highlighting}
\end{Shaded}

This heuristic labeling function assigns sentiment based on keywords, a
common weak supervision approach.

\subsubsection{Try It Yourself}\label{try-it-yourself-235}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write heuristic rules to weakly label a set of product reviews as
  positive or negative.
\item
  Combine multiple heuristic sources and resolve conflicts using
  majority voting.
\item
  Compare model performance trained on weak labels versus a small set of
  clean labels.
\end{enumerate}

\subsection{237. Programmatic Labeling}\label{programmatic-labeling}

Programmatic labeling uses code to generate labels at scale. Instead of
hand-labeling each example, rules, patterns, or weak supervision sources
are combined to assign labels automatically. The goal is to capture
domain knowledge in reusable labeling functions.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-236}

Imagine training a group of assistants by giving them clear if--then
rules: ``If a review contains `excellent,' mark it positive.'' Each
assistant applies the rules consistently. Programmatic labeling is like
encoding these assistants in code, letting them label vast datasets
quickly.

\subsubsection{Deep Dive}\label{deep-dive-236}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1760}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4240}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Labeling Functions & Small pieces of logic that assign tentative labels
& Keyword match: ``refund'' → complaint \\
Label Model & Combines multiple noisy sources into a consensus &
Resolves conflicts, weights reliable functions higher \\
Iteration & Refine rules based on errors and gaps & Add new patterns for
edge cases \\
\end{longtable}

Programmatic labeling allows rapid dataset creation while keeping human
input focused on designing and improving functions rather than labeling
every record. It's most effective in domains with strong heuristics or
structured signals.

Challenges include ensuring rules generalize, avoiding overfitting to
specific patterns, and balancing conflicting sources. Label models are
often needed to reconcile noisy or overlapping signals.

\subsubsection{Tiny Code}\label{tiny-code-214}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ label\_review(text):}
    \ControlFlowTok{if} \StringTok{"excellent"} \KeywordTok{in}\NormalTok{ text:}
        \ControlFlowTok{return} \StringTok{"positive"}
    \ControlFlowTok{if} \StringTok{"terrible"} \KeywordTok{in}\NormalTok{ text:}
        \ControlFlowTok{return} \StringTok{"negative"}
    \ControlFlowTok{return} \StringTok{"unknown"}

\NormalTok{reviews }\OperatorTok{=}\NormalTok{ [}\StringTok{"excellent service"}\NormalTok{, }\StringTok{"terrible food"}\NormalTok{, }\StringTok{"average experience"}\NormalTok{]}
\NormalTok{labels }\OperatorTok{=}\NormalTok{ [label\_review(r) }\ControlFlowTok{for}\NormalTok{ r }\KeywordTok{in}\NormalTok{ reviews]}
\end{Highlighting}
\end{Shaded}

This simple example shows labeling functions applied programmatically to
generate training data.

\subsubsection{Try It Yourself}\label{try-it-yourself-236}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write three labeling functions for classifying customer emails (e.g.,
  billing, technical, general).
\item
  Apply multiple functions to the same dataset and resolve conflicts
  using majority vote.
\item
  Evaluate how much model accuracy improves when adding more labeling
  functions.
\end{enumerate}

\subsection{238. Consensus, Adjudication, and
Agreement}\label{consensus-adjudication-and-agreement}

When multiple annotators label the same data, disagreements are
inevitable. Consensus, adjudication, and agreement metrics provide ways
to resolve conflicts and measure reliability, ensuring that final labels
are trustworthy.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-237}

Imagine three judges scoring a performance. If two give ``excellent''
and one gives ``good,'' majority vote determines consensus. If the
judges strongly disagree, a senior judge might make the final
call---that's adjudication. Agreement measures how often judges align,
showing whether the rules are clear.

\subsubsection{Deep Dive}\label{deep-dive-237}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1706}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2882}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2882}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2529}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Consensus (Majority Vote) & Label chosen by most annotators & Simple,
scalable & Can obscure minority but valid perspectives \\
Adjudication & Expert resolves disagreements manually & Ensures quality
in tough cases & Costly, slower \\
Agreement Metrics & Quantify consistency (e.g., Cohen's κ, Fleiss' κ) &
Identifies task clarity and annotator reliability & Requires statistical
interpretation \\
\end{longtable}

Consensus is efficient for large-scale crowdsourcing. Adjudication is
valuable for high-stakes datasets, such as medical or legal domains.
Agreement metrics highlight whether disagreements come from annotator
variability or from unclear guidelines.

Challenges include handling imbalanced label distributions, avoiding
bias toward majority classes, and deciding when to escalate to
adjudication.

\subsubsection{Tiny Code}\label{tiny-code-215}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{labels }\OperatorTok{=}\NormalTok{ [}\StringTok{"positive"}\NormalTok{, }\StringTok{"positive"}\NormalTok{, }\StringTok{"negative"}\NormalTok{]}

\CommentTok{\# Consensus}
\NormalTok{final\_label }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(}\BuiltInTok{set}\NormalTok{(labels), key}\OperatorTok{=}\NormalTok{labels.count)  }\CommentTok{\# {-}\textgreater{} "positive"}

\CommentTok{\# Agreement (simple percent)}
\NormalTok{agreement }\OperatorTok{=}\NormalTok{ labels.count(}\StringTok{"positive"}\NormalTok{) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(labels)  }\CommentTok{\# {-}\textgreater{} 0.67}
\end{Highlighting}
\end{Shaded}

This demonstrates both a consensus outcome and a basic measure of
agreement.

\subsubsection{Try It Yourself}\label{try-it-yourself-237}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simulate three annotators labeling 20 items and compute majority-vote
  consensus.
\item
  Apply an agreement metric to assess annotator reliability.
\item
  Discuss when manual adjudication should override automated consensus.
\end{enumerate}

\subsection{239. Annotation Biases and Cultural
Effects}\label{annotation-biases-and-cultural-effects}

Human annotators bring their own perspectives, experiences, and cultural
backgrounds. These can unintentionally introduce biases into labeled
datasets, shaping how models learn and behave. Recognizing and
mitigating annotation bias is critical for fairness and reliability.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-238}

Imagine asking people from different countries to label photos of food.
What one calls ``snack,'' another may call ``meal.'' The differences are
not errors but reflections of cultural norms. If models learn only from
one group, they may fail to generalize globally.

\subsubsection{Deep Dive}\label{deep-dive-238}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1615}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4385}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source of Bias
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Cultural Norms & Different societies interpret concepts differently &
Gesture labeled as polite in one culture, rude in another \\
Subjectivity & Ambiguous categories lead to personal interpretation &
Sentiment judged differently depending on annotator mood \\
Demographics & Annotator backgrounds shape labeling & Gendered
assumptions in occupation labels \\
Instruction Drift & Annotators apply rules inconsistently &
``Offensive'' interpreted more strictly by some than others \\
\end{longtable}

Bias in annotation can skew model predictions, reinforcing stereotypes
or excluding minority viewpoints. Mitigation strategies include
diversifying annotators, refining guidelines, measuring agreement across
groups, and explicitly auditing for cultural variance.

Challenges lie in balancing global consistency with local validity,
ensuring fairness without erasing context, and managing costs while
scaling annotation.

\subsubsection{Tiny Code}\label{tiny-code-216}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{annotations }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    \{}\StringTok{"annotator"}\NormalTok{: }\StringTok{"A"}\NormalTok{, }\StringTok{"label"}\NormalTok{: }\StringTok{"snack"}\NormalTok{\},}
\NormalTok{    \{}\StringTok{"annotator"}\NormalTok{: }\StringTok{"B"}\NormalTok{, }\StringTok{"label"}\NormalTok{: }\StringTok{"meal"}\NormalTok{\}}
\NormalTok{]}

\CommentTok{\# Detect disagreement as potential cultural bias}
\ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(}\BuiltInTok{set}\NormalTok{([a[}\StringTok{"label"}\NormalTok{] }\ControlFlowTok{for}\NormalTok{ a }\KeywordTok{in}\NormalTok{ annotations])) }\OperatorTok{\textgreater{}} \DecValTok{1}\NormalTok{:}
\NormalTok{    flag }\OperatorTok{=} \VariableTok{True}
\end{Highlighting}
\end{Shaded}

This shows how disagreements across annotators may reveal underlying
cultural differences.

\subsubsection{Try It Yourself}\label{try-it-yourself-238}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Collect annotations from two groups with different cultural
  backgrounds; compare label distributions.
\item
  Identify a dataset where subjective categories (e.g., sentiment,
  offensiveness) may show bias.
\item
  Propose methods for reducing cultural bias without losing diversity of
  interpretation.
\end{enumerate}

\subsection{240. Scaling Labeling for Foundation
Models}\label{scaling-labeling-for-foundation-models}

Foundation models require massive amounts of labeled or structured data,
but manual annotation at that scale is infeasible. Scaling labeling
relies on strategies like weak supervision, programmatic labeling,
synthetic data generation, and iterative feedback loops.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-239}

Imagine trying to label every grain of sand on a beach by hand---it's
impossible. Instead, you build machines that sort sand automatically,
check quality periodically, and correct only where errors matter most.
Scaled labeling systems work the same way for foundation models.

\subsubsection{Deep Dive}\label{deep-dive-239}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1645}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3487}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2368}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Weak Supervision & Apply noisy or approximate rules to generate labels &
Fast, low-cost & Labels may lack precision \\
Programmatic Labeling & Encode domain knowledge as reusable functions &
Scales flexibly & Requires expertise to design functions \\
Synthetic Data & Generate artificial labeled examples & Covers rare
cases, balances datasets & Risk of unrealistic distributions \\
Human-in-the-Loop & Use humans selectively for corrections and edge
cases & Improves quality where most needed & Slower than full
automation \\
\end{longtable}

Scaling requires combining these approaches into pipelines: automated
bulk labeling, targeted human review, and continuous refinement as
models improve.

Challenges include balancing label quality against scale, avoiding
propagation of systematic errors, and ensuring that synthetic or weak
labels don't bias the model unfairly.

\subsubsection{Tiny Code}\label{tiny-code-217}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ scaled\_labeling(data):}
    \CommentTok{\# Step 1: Programmatic rules}
\NormalTok{    weak\_labels }\OperatorTok{=}\NormalTok{ [rule\_based(d) }\ControlFlowTok{for}\NormalTok{ d }\KeywordTok{in}\NormalTok{ data]}
    
    \CommentTok{\# Step 2: Human correction on uncertain cases}
\NormalTok{    corrected }\OperatorTok{=}\NormalTok{ [human\_fix(d) }\ControlFlowTok{if}\NormalTok{ uncertain(d) }\ControlFlowTok{else}\NormalTok{ l }\ControlFlowTok{for}\NormalTok{ d, l }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{(data, weak\_labels)]}
    
    \ControlFlowTok{return}\NormalTok{ corrected}
\end{Highlighting}
\end{Shaded}

This sketch shows a hybrid pipeline combining automation with selective
human review.

\subsubsection{Try It Yourself}\label{try-it-yourself-239}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Design a pipeline that labels 1 million text samples using weak
  supervision and only 1\% human review.
\item
  Compare model performance on data labeled fully manually vs.~data
  labeled with a scaled pipeline.
\item
  Propose methods to validate quality when labeling at extreme scale
  without checking every instance.
\end{enumerate}

\section{Chapter 25. Sampling, splits, and experimental
design}\label{chapter-25.-sampling-splits-and-experimental-design}

\subsection{241. Random Sampling and
Stratification}\label{random-sampling-and-stratification}

Sampling selects a subset of data from a larger population. Random
sampling ensures each instance has an equal chance of selection,
reducing bias. Stratified sampling divides data into groups (strata) and
samples proportionally, preserving representation of key categories.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-240}

Imagine drawing marbles from a jar. With random sampling, you mix them
all and pick blindly. With stratified sampling, you first separate them
by color, then pick proportionally, ensuring no color is left out or
overrepresented.

\subsubsection{Deep Dive}\label{deep-dive-240}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1494}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3161}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2989}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2356}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Simple Random Sampling & Each record chosen independently with equal
probability & Easy, unbiased & May miss small but important groups \\
Stratified Sampling & Split data into subgroups and sample within each &
Preserves class balance, improves representativeness & Requires
knowledge of strata \\
Systematic Sampling & Select every k-th item after a random start &
Simple to implement & Risks bias if data has hidden periodicity \\
\end{longtable}

Random sampling works well for large, homogeneous datasets. Stratified
sampling is crucial when some groups are rare, as in imbalanced
classification problems. Systematic sampling provides efficiency in
ordered datasets but needs care to avoid periodic bias.

Challenges include defining strata correctly, handling overlapping
categories, and ensuring randomness when data pipelines are distributed.

\subsubsection{Tiny Code}\label{tiny-code-218}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ random}

\NormalTok{data }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(}\BuiltInTok{range}\NormalTok{(}\DecValTok{100}\NormalTok{))}

\CommentTok{\# Random sample of 10 items}
\NormalTok{sample\_random }\OperatorTok{=}\NormalTok{ random.sample(data, }\DecValTok{10}\NormalTok{)}

\CommentTok{\# Stratified sample (by even/odd)}
\NormalTok{even }\OperatorTok{=}\NormalTok{ [x }\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ data }\ControlFlowTok{if}\NormalTok{ x }\OperatorTok{\%} \DecValTok{2} \OperatorTok{==} \DecValTok{0}\NormalTok{]}
\NormalTok{odd }\OperatorTok{=}\NormalTok{ [x }\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ data }\ControlFlowTok{if}\NormalTok{ x }\OperatorTok{\%} \DecValTok{2} \OperatorTok{==} \DecValTok{1}\NormalTok{]}
\NormalTok{sample\_stratified }\OperatorTok{=}\NormalTok{ random.sample(even, }\DecValTok{5}\NormalTok{) }\OperatorTok{+}\NormalTok{ random.sample(odd, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Both methods select subsets, but stratification preserves subgroup
balance.

\subsubsection{Try It Yourself}\label{try-it-yourself-240}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take a dataset with 90\% class A and 10\% class B. Compare class
  distribution in random vs.~stratified samples of size 20.
\item
  Implement systematic sampling on a dataset of 1,000 items and analyze
  risks if the data has repeating patterns.
\item
  Discuss when random sampling alone may introduce hidden bias and how
  stratification mitigates it.
\end{enumerate}

\subsection{242. Train/Validation/Test
Splits}\label{trainvalidationtest-splits}

Machine learning models must be trained, tuned, and evaluated on
separate data to ensure fairness and generalization. Splitting data into
train, validation, and test sets enforces this separation, preventing
models from memorizing instead of learning.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-241}

Imagine studying for an exam. The textbook problems you practice on are
like the training set. The practice quiz you take to check your progress
is like the validation set. The final exam, unseen until test day, is
the test set.

\subsubsection{Deep Dive}\label{deep-dive-241}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1148}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3770}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0984}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4098}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Split
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Typical Size
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Notes
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Train & Used to fit model parameters & 60--80\% & Largest portion; model
``learns'' here \\
Validation & Tunes hyperparameters and prevents overfitting & 10--20\% &
Guides decisions like regularization, architecture \\
Test & Final evaluation of generalization & 10--20\% & Must remain
untouched until the end \\
\end{longtable}

Different strategies exist depending on dataset size:

\begin{itemize}
\tightlist
\item
  Holdout split: one-time partitioning, simple but may be noisy.
\item
  Cross-validation: repeated folds for robust estimation.
\item
  Nested validation: used when hyperparameter search itself risks
  overfitting.
\end{itemize}

Challenges include data leakage (information from validation/test
sneaking into training), ensuring distributions are consistent across
splits, and handling temporal or grouped data where random splits may
cause unrealistic overlap.

\subsubsection{Tiny Code}\label{tiny-code-219}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ train\_test\_split}

\NormalTok{X\_train, X\_temp, y\_train, y\_temp }\OperatorTok{=}\NormalTok{ train\_test\_split(X, y, test\_size}\OperatorTok{=}\FloatTok{0.3}\NormalTok{)}
\NormalTok{X\_val, X\_test, y\_val, y\_test }\OperatorTok{=}\NormalTok{ train\_test\_split(X\_temp, y\_temp, test\_size}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This creates 70\% train, 15\% validation, and 15\% test sets.

\subsubsection{Try It Yourself}\label{try-it-yourself-241}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Split a dataset into 70/15/15 and verify that class proportions remain
  similar across splits.
\item
  Compare performance estimates when using a single holdout set
  vs.~cross-validation.
\item
  Explain why touching the test set during model development invalidates
  evaluation.
\end{enumerate}

\subsection{243. Cross-Validation and
k-Folds}\label{cross-validation-and-k-folds}

Cross-validation estimates how well a model generalizes by splitting
data into multiple folds. The model trains on some folds and validates
on the remaining one, repeating until each fold has been tested. This
reduces variance compared to a single holdout split.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-242}

Imagine practicing for a debate. Instead of using just one set of
practice questions, you rotate through five different sets, each time
holding one back as the ``exam.'' By the end, every set has served as a
test, giving you a fairer picture of your readiness.

\subsubsection{Deep Dive}\label{deep-dive-242}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1698}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3962}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2138}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2201}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
k-Fold Cross-Validation & Split into k folds; train on k−1, test on 1,
repeat k times & Reliable, uses all data & Computationally expensive \\
Stratified k-Fold & Preserves class proportions in each fold & Essential
for imbalanced datasets & Slightly more complex \\
Leave-One-Out (LOO) & Each sample is its own test set & Maximal data
use, unbiased & Extremely costly for large datasets \\
Nested CV & Inner loop for hyperparameter tuning, outer loop for
evaluation & Prevents overfitting on validation & Doubles computation
effort \\
\end{longtable}

Cross-validation balances bias and variance, especially when data is
limited. It provides a more robust estimate of performance than a single
split, though at higher computational cost.

Challenges include ensuring folds are independent (e.g., no temporal
leakage), managing computation for large datasets, and interpreting
results across folds.

\subsubsection{Tiny Code}\label{tiny-code-220}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ KFold}

\NormalTok{kf }\OperatorTok{=}\NormalTok{ KFold(n\_splits}\OperatorTok{=}\DecValTok{5}\NormalTok{, shuffle}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ train\_idx, val\_idx }\KeywordTok{in}\NormalTok{ kf.split(X):}
\NormalTok{    X\_train, X\_val }\OperatorTok{=}\NormalTok{ X[train\_idx], X[val\_idx]}
\NormalTok{    y\_train, y\_val }\OperatorTok{=}\NormalTok{ y[train\_idx], y[val\_idx]}
    \CommentTok{\# train and evaluate model here}
\end{Highlighting}
\end{Shaded}

This example runs 5-fold cross-validation with shuffling.

\subsubsection{Try It Yourself}\label{try-it-yourself-242}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Implement 5-fold and 10-fold cross-validation on the same dataset;
  compare stability of results.
\item
  Apply stratified k-fold on an imbalanced classification task and
  compare with plain k-fold.
\item
  Discuss when leave-one-out cross-validation is preferable despite its
  cost.
\end{enumerate}

\subsection{244. Bootstrapping and
Resampling}\label{bootstrapping-and-resampling}

Bootstrapping is a resampling method that estimates variability by
repeatedly drawing samples with replacement from a dataset. It generates
multiple pseudo-datasets to approximate distributions, confidence
intervals, or error estimates without strong parametric assumptions.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-243}

Imagine you only have one basket of apples but want to understand the
variability in apple sizes. Instead of growing new apples, you
repeatedly scoop apples from the same basket, sometimes picking the same
apple more than once. Each scoop is a bootstrap sample, giving different
but related estimates.

\subsubsection{Deep Dive}\label{deep-dive-243}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1489}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3475}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2482}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2553}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Technique
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Bootstrapping & Sampling with replacement to create many datasets &
Simple, powerful, distribution-free & May misrepresent very small
datasets \\
Jackknife & Leave-one-out resampling & Easy variance estimation & Less
accurate for complex statistics \\
Permutation Tests & Shuffle labels to test hypotheses & Non-parametric,
robust & Computationally expensive \\
\end{longtable}

Bootstrapping is widely used to estimate confidence intervals for
statistics like mean, median, or regression coefficients. It avoids
assumptions of normality, making it flexible for real-world data.

Challenges include ensuring enough samples for stable estimates,
computational cost for large datasets, and handling dependence
structures like time series where naive resampling breaks correlations.

\subsubsection{Tiny Code}\label{tiny-code-221}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ random}

\NormalTok{data }\OperatorTok{=}\NormalTok{ [}\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{9}\NormalTok{]}

\KeywordTok{def}\NormalTok{ bootstrap(data, n}\OperatorTok{=}\DecValTok{1000}\NormalTok{):}
\NormalTok{    estimates }\OperatorTok{=}\NormalTok{ []}
    \ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
\NormalTok{        sample }\OperatorTok{=}\NormalTok{ [random.choice(data) }\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in}\NormalTok{ data]}
\NormalTok{        estimates.append(}\BuiltInTok{sum}\NormalTok{(sample) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(sample))  }\CommentTok{\# mean estimate}
    \ControlFlowTok{return}\NormalTok{ estimates}

\NormalTok{means }\OperatorTok{=}\NormalTok{ bootstrap(data)}
\end{Highlighting}
\end{Shaded}

This approximates the sampling distribution of the mean using bootstrap
resamples.

\subsubsection{Try It Yourself}\label{try-it-yourself-243}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use bootstrapping to estimate the 95\% confidence interval for the
  mean of a dataset.
\item
  Compare jackknife vs.~bootstrap estimates of variance on a small
  dataset.
\item
  Apply permutation tests to evaluate whether two groups differ
  significantly without assuming normality.
\end{enumerate}

\subsection{245. Balanced vs.~Imbalanced
Sampling}\label{balanced-vs.-imbalanced-sampling}

Real-world datasets often have unequal class distributions. For example,
fraud cases may be 1 in 1000 transactions. Balanced sampling techniques
adjust training data so that models don't ignore rare but important
classes.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-244}

Think of training a guard dog. If it only ever sees friendly neighbors,
it may never learn to bark at intruders. Showing it more intruder
examples---proportionally more than real life---helps it learn the
distinction.

\subsubsection{Deep Dive}\label{deep-dive-244}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2532}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2975}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2468}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2025}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Random Undersampling & Reduce majority class size & Simple, fast & Risk
of discarding useful data \\
Random Oversampling & Duplicate minority class samples & Balances
distribution & Can overfit rare cases \\
Synthetic Oversampling (SMOTE, etc.) & Create new synthetic samples for
minority class & Improves diversity, reduces overfitting & May generate
unrealistic samples \\
Cost-Sensitive Sampling & Adjust weights instead of data & Preserves
dataset, flexible & Needs careful tuning \\
\end{longtable}

Balanced sampling ensures models pay attention to rare but critical
events, such as disease detection or fraud identification. Imbalanced
sampling mimics real-world distributions but may yield biased models.

Challenges include deciding how much balancing is necessary, preventing
artificial inflation of rare cases, and evaluating models fairly with
respect to real distributions.

\subsubsection{Tiny Code}\label{tiny-code-222}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{majority }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{] }\OperatorTok{*} \DecValTok{1000}
\NormalTok{minority }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\NormalTok{] }\OperatorTok{*} \DecValTok{50}

\CommentTok{\# Oversample minority}
\NormalTok{balanced }\OperatorTok{=}\NormalTok{ majority }\OperatorTok{+}\NormalTok{ minority }\OperatorTok{*} \DecValTok{20}  \CommentTok{\# naive oversampling}

\CommentTok{\# Undersample majority}
\NormalTok{undersampled }\OperatorTok{=}\NormalTok{ majority[:}\DecValTok{50}\NormalTok{] }\OperatorTok{+}\NormalTok{ minority}
\end{Highlighting}
\end{Shaded}

Both methods rebalance classes, though in different ways.

\subsubsection{Try It Yourself}\label{try-it-yourself-244}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a dataset with 95\% negatives and 5\% positives. Apply
  undersampling and oversampling; compare class ratios.
\item
  Train a classifier on imbalanced vs.~balanced data and measure
  differences in recall.
\item
  Discuss when cost-sensitive approaches are better than altering the
  dataset itself.
\end{enumerate}

\subsection{246. Temporal Splits for Time
Series}\label{temporal-splits-for-time-series}

Time series data cannot be split randomly because order matters.
Temporal splits preserve chronology, training on past data and testing
on future data. This setup mirrors real-world forecasting, where
tomorrow must be predicted using only yesterday and earlier.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-245}

Think of watching a sports game. You can't use the final score to
predict what will happen at halftime. A fair split must only use earlier
plays to predict later outcomes.

\subsubsection{Deep Dive}\label{deep-dive-245}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1408}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3451}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2746}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2394}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Holdout by Time & Train on first portion, test on later portion &
Simple, respects chronology & Evaluation depends on single split \\
Rolling Window & Slide training window forward, test on next block &
Mimics deployment, multiple evaluations & Expensive for large
datasets \\
Expanding Window & Start small, keep adding data to training set & Uses
all available history & Older data may become irrelevant \\
\end{longtable}

Temporal splits ensure realistic evaluation, especially for domains like
finance, weather, or demand forecasting. They prevent leakage, where
future information accidentally informs the past.

Challenges include handling seasonality, deciding window sizes, and
ensuring enough data remains in each split. Non-stationarity complicates
evaluation, as past patterns may not hold in the future.

\subsubsection{Tiny Code}\label{tiny-code-223}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(}\BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{13}\NormalTok{))  }\CommentTok{\# months}

\CommentTok{\# Holdout split}
\NormalTok{train, test }\OperatorTok{=}\NormalTok{ data[:}\DecValTok{9}\NormalTok{], data[}\DecValTok{9}\NormalTok{:]}

\CommentTok{\# Rolling window (train 6, test 3)}
\NormalTok{splits }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    (data[i:i}\OperatorTok{+}\DecValTok{6}\NormalTok{], data[i}\OperatorTok{+}\DecValTok{6}\NormalTok{:i}\OperatorTok{+}\DecValTok{9}\NormalTok{])}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, }\BuiltInTok{len}\NormalTok{(data)}\OperatorTok{{-}}\DecValTok{9}\NormalTok{)}
\NormalTok{]}
\end{Highlighting}
\end{Shaded}

This shows both a simple holdout and a rolling evaluation.

\subsubsection{Try It Yourself}\label{try-it-yourself-245}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Split a sales dataset into 70\% past and 30\% future; train on past,
  evaluate on future.
\item
  Implement rolling windows for a dataset and compare stability of
  results across folds.
\item
  Discuss when older data should be excluded because it no longer
  reflects current patterns.
\end{enumerate}

\subsection{247. Domain Adaptation
Splits}\label{domain-adaptation-splits}

When training and deployment domains differ---such as medical images
from different hospitals or customer data from different
regions---evaluation must simulate this shift. Domain adaptation splits
divide data by source or domain, testing whether models generalize
beyond familiar distributions.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-246}

Imagine training a chef who practices only with Italian ingredients. If
tested with Japanese ingredients, performance may drop. A fair split
requires holding out whole cuisines, not just random dishes, to test
adaptability.

\subsubsection{Deep Dive}\label{deep-dive-246}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2621}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4466}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2913}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Split Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Use Case
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Source vs.~Target Split & Train on one domain, test on another &
Cross-hospital medical imaging \\
Leave-One-Domain-Out & Rotate, leaving one domain as test & Multi-region
customer data \\
Mixed Splits & Train on multiple domains, test on unseen ones &
Multilingual NLP tasks \\
\end{longtable}

Domain adaptation splits reveal vulnerabilities hidden by random
sampling, where train and test distributions look artificially similar.
They are crucial for robustness in real-world deployment, where data
shifts are common.

Challenges include severe performance drops when domains differ greatly,
deciding how to measure generalization, and ensuring that splits are
representative of real deployment conditions.

\subsubsection{Tiny Code}\label{tiny-code-224}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"hospital\_A"}\NormalTok{: [...],}
    \StringTok{"hospital\_B"}\NormalTok{: [...],}
    \StringTok{"hospital\_C"}\NormalTok{: [...]}
\NormalTok{\}}

\CommentTok{\# Leave{-}one{-}domain{-}out}
\NormalTok{train }\OperatorTok{=}\NormalTok{ data[}\StringTok{"hospital\_A"}\NormalTok{] }\OperatorTok{+}\NormalTok{ data[}\StringTok{"hospital\_B"}\NormalTok{]}
\NormalTok{test }\OperatorTok{=}\NormalTok{ data[}\StringTok{"hospital\_C"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

This setup tests whether a model trained on some domains works on a new
one.

\subsubsection{Try It Yourself}\label{try-it-yourself-246}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Split a dataset by geography (e.g., North vs.~South) and compare
  performance across domains.
\item
  Perform leave-one-domain-out validation on a multi-source dataset.
\item
  Discuss strategies to improve generalization when domain adaptation
  splits show large performance gaps.
\end{enumerate}

\subsection{248. Statistical Power and Sample
Size}\label{statistical-power-and-sample-size}

Statistical power measures the likelihood that an experiment will detect
a true effect. Power depends on effect size, sample size, significance
level, and variance. Determining the right sample size in advance
ensures reliable conclusions without wasting resources.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-247}

Imagine trying to hear a whisper in a noisy room. If only one person
listens, they might miss it. If 100 people listen, chances increase that
someone hears correctly. More samples increase the chance of detecting
real signals in noisy data.

\subsubsection{Deep Dive}\label{deep-dive-247}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2321}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4018}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3661}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Factor
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Role in Power
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Sample Size & Larger samples reduce noise, increasing power & Doubling
participants halves variance \\
Effect Size & Stronger effects are easier to detect & Large difference
in treatment vs.~control \\
Significance Level (α) & Lower thresholds make detection harder & α =
0.01 stricter than α = 0.05 \\
Variance & Higher variability reduces power & Noisy measurements obscure
effects \\
\end{longtable}

Balancing these factors is key. Too small a sample risks false
negatives. Too large wastes resources or finds trivial effects.

Challenges include estimating effect size in advance, handling multiple
hypothesis tests, and adapting when variance differs across subgroups.

\subsubsection{Tiny Code}\label{tiny-code-225}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ statsmodels.stats.power }\ImportTok{as}\NormalTok{ sp}

\CommentTok{\# Calculate sample size for 80\% power, alpha=0.05, effect size=0.5}
\NormalTok{analysis }\OperatorTok{=}\NormalTok{ sp.TTestIndPower()}
\NormalTok{n }\OperatorTok{=}\NormalTok{ analysis.solve\_power(effect\_size}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, power}\OperatorTok{=}\FloatTok{0.8}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.05}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This shows how to compute required sample size for a desired power
level.

\subsubsection{Try It Yourself}\label{try-it-yourself-247}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the sample size needed to detect a medium effect with 90\%
  power at α=0.05.
\item
  Simulate how increasing variance reduces the probability of detecting
  a true effect.
\item
  Discuss tradeoffs in setting stricter significance thresholds for
  high-stakes experiments.
\end{enumerate}

\subsection{249. Control Groups and Randomized
Experiments}\label{control-groups-and-randomized-experiments}

Control groups and randomized experiments establish causal validity. A
control group receives no treatment (or a baseline treatment), while the
experimental group receives the intervention. Random assignment ensures
differences in outcomes are due to the intervention, not hidden biases.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-248}

Think of testing a new fertilizer. One field is treated, another is left
untreated. If the treated field yields more crops, and fields were
chosen randomly, you can attribute the difference to the fertilizer
rather than soil quality or weather.

\subsubsection{Deep Dive}\label{deep-dive-248}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2043}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4624}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Element
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Control Group & Provides baseline comparison & Website with old
design \\
Treatment Group & Receives new intervention & Website with redesigned
layout \\
Randomization & Balances confounding factors & Assign users randomly to
old vs.~new design \\
Blinding & Prevents bias from expectations & Double-blind drug trial \\
\end{longtable}

Randomized controlled trials (RCTs) are the gold standard for measuring
causal effects in medicine, social science, and A/B testing in
technology. Without a proper control group and randomization, results
risk being confounded.

Challenges include ethical concerns (withholding treatment), ensuring
compliance, handling spillover effects between groups, and maintaining
statistical power.

\subsubsection{Tiny Code}\label{tiny-code-226}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ random}

\NormalTok{users }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(}\BuiltInTok{range}\NormalTok{(}\DecValTok{100}\NormalTok{))}
\NormalTok{random.shuffle(users)}

\NormalTok{control }\OperatorTok{=}\NormalTok{ users[:}\DecValTok{50}\NormalTok{]}
\NormalTok{treatment }\OperatorTok{=}\NormalTok{ users[}\DecValTok{50}\NormalTok{:]}

\CommentTok{\# Assign outcomes (simulated)}
\NormalTok{outcomes }\OperatorTok{=}\NormalTok{ \{u: }\StringTok{"baseline"} \ControlFlowTok{for}\NormalTok{ u }\KeywordTok{in}\NormalTok{ control\}}
\NormalTok{outcomes.update(\{u: }\StringTok{"intervention"} \ControlFlowTok{for}\NormalTok{ u }\KeywordTok{in}\NormalTok{ treatment\})}
\end{Highlighting}
\end{Shaded}

This assigns users randomly into control and treatment groups.

\subsubsection{Try It Yourself}\label{try-it-yourself-248}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Design an A/B test for a new app feature with a clear control and
  treatment group.
\item
  Simulate randomization and show how it balances demographics across
  groups.
\item
  Discuss when randomized experiments are impractical and what
  alternatives exist.
\end{enumerate}

\subsection{250. Pitfalls: Leakage, Overfitting,
Undercoverage}\label{pitfalls-leakage-overfitting-undercoverage}

Poor experimental design can produce misleading results. Three common
pitfalls are data leakage (using future or external information during
training), overfitting (memorizing noise instead of patterns), and
undercoverage (ignoring important parts of the population). Recognizing
these risks is key to trustworthy models.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-249}

Imagine a student cheating on an exam by peeking at the answer key
(leakage), memorizing past test questions without understanding concepts
(overfitting), or practicing only easy questions while ignoring harder
ones (undercoverage). Each leads to poor generalization.

\subsubsection{Deep Dive}\label{deep-dive-249}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1056}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4161}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1615}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3168}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Pitfall
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Consequence
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Leakage & Training data includes information not available at prediction
time & Artificially high accuracy & Using future stock prices to predict
current ones \\
Overfitting & Model fits noise instead of signal & Poor generalization &
Perfect accuracy on training set, bad on test \\
Undercoverage & Sampling misses key groups & Biased predictions &
Training only on urban data, failing in rural areas \\
\end{longtable}

Leakage gives an illusion of performance, often unnoticed until
deployment. Overfitting results from overly complex models relative to
data size. Undercoverage skews models by ignoring diversity, leading to
unfair or incomplete results.

Mitigation strategies include strict separation of train/test data,
regularization and validation for overfitting, and careful sampling to
ensure population coverage.

\subsubsection{Tiny Code}\label{tiny-code-227}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Leakage example}
\NormalTok{train\_features }\OperatorTok{=}\NormalTok{ [}\StringTok{"age"}\NormalTok{, }\StringTok{"income"}\NormalTok{, }\StringTok{"future\_purchase"}\NormalTok{]  }\CommentTok{\# invalid feature}
\CommentTok{\# Overfitting example}
\NormalTok{model.fit(X\_train, y\_train)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Train acc:"}\NormalTok{, model.score(X\_train, y\_train))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Test acc:"}\NormalTok{, model.score(X\_test, y\_test))  }\CommentTok{\# drops sharply}
\end{Highlighting}
\end{Shaded}

This shows how models can appear strong but fail in practice.

\subsubsection{Try It Yourself}\label{try-it-yourself-249}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Identify leakage in a dataset where target information is indirectly
  encoded in features.
\item
  Train an overly complex model on a small dataset and observe
  overfitting.
\item
  Design a sampling plan to avoid undercoverage in a national survey.
\end{enumerate}

\section{Chapter 26. Augmentation, synthesis, and
simulation}\label{chapter-26.-augmentation-synthesis-and-simulation}

\subsection{251. Image Augmentations}\label{image-augmentations}

Image augmentation artificially increases dataset size and diversity by
applying transformations to existing images. These transformations
preserve semantic meaning while introducing variation, helping models
generalize better.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-250}

Imagine showing a friend photos of the same cat. One photo is flipped,
another slightly rotated, another a bit darker. It's still the same cat,
but the variety helps your friend recognize it in different conditions.

\subsubsection{Deep Dive}\label{deep-dive-250}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1560}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3191}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2482}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2766}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Technique
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Benefit
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Risk
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Flips \& Rotations & Horizontal/vertical flips, small rotations & Adds
viewpoint diversity & May distort orientation-sensitive tasks \\
Cropping \& Scaling & Random crops, resizes & Improves robustness to
framing & Risk of cutting important objects \\
Color Jittering & Adjust brightness, contrast, saturation & Helps with
lighting variations & May reduce naturalness \\
Noise Injection & Add Gaussian or salt-and-pepper noise & Trains
robustness to sensor noise & Too much can obscure features \\
Cutout \& Mixup & Mask parts of images or blend multiple images &
Improves invariance, regularization & Less interpretable training
samples \\
\end{longtable}

Augmentation increases effective training data without new labeling.
It's especially important for small datasets or domains where collecting
new images is costly.

Challenges include choosing transformations that preserve labels,
ensuring augmented data matches deployment conditions, and avoiding
over-augmentation that confuses the model.

\subsubsection{Tiny Code}\label{tiny-code-228}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ torchvision }\ImportTok{import}\NormalTok{ transforms}

\NormalTok{augment }\OperatorTok{=}\NormalTok{ transforms.Compose([}
\NormalTok{    transforms.RandomHorizontalFlip(),}
\NormalTok{    transforms.RandomRotation(}\DecValTok{15}\NormalTok{),}
\NormalTok{    transforms.ColorJitter(brightness}\OperatorTok{=}\FloatTok{0.2}\NormalTok{, contrast}\OperatorTok{=}\FloatTok{0.2}\NormalTok{),}
\NormalTok{])}
\end{Highlighting}
\end{Shaded}

This pipeline randomly applies flips, rotations, and color adjustments
to images.

\subsubsection{Try It Yourself}\label{try-it-yourself-250}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Apply horizontal flips and random crops to a dataset of animals;
  compare model performance with and without augmentation.
\item
  Test how noise injection affects classification accuracy when images
  are corrupted at inference.
\item
  Design an augmentation pipeline for medical images where orientation
  and brightness must be preserved carefully.
\end{enumerate}

\subsection{252. Text Augmentations}\label{text-augmentations}

Text augmentation expands datasets by generating new variants of
existing text while keeping meaning intact. It reduces overfitting,
improves robustness, and helps models handle diverse phrasing.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-251}

Imagine explaining the same idea in different ways: ``The cat sat on the
mat,'' ``A mat was where the cat sat,'' ``On the mat, the cat rested.''
Each sentence carries the same idea, but the variety trains better
understanding.

\subsubsection{Deep Dive}\label{deep-dive-251}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2214}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3282}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2519}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1985}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Technique
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Benefit
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Risk
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Synonym Replacement & Swap words with synonyms & Simple, increases
lexical variety & May change nuance \\
Back-Translation & Translate to another language and back & Produces
natural paraphrases & Can introduce errors \\
Random Insertion/Deletion & Add or remove words & Encourages robustness
& May distort meaning \\
Contextual Augmentation & Use language models to suggest replacements &
More fluent, context-aware & Requires pretrained models \\
Template Generation & Fill predefined patterns with terms & Good for
domain-specific tasks & Limited diversity \\
\end{longtable}

These methods are widely used in sentiment analysis, intent recognition,
and low-resource NLP tasks.

Challenges include preserving label consistency (e.g., sentiment should
not flip), avoiding unnatural outputs, and balancing variety with
fidelity.

\subsubsection{Tiny Code}\label{tiny-code-229}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ random}

\NormalTok{sentence }\OperatorTok{=} \StringTok{"The cat sat on the mat"}
\NormalTok{synonyms }\OperatorTok{=}\NormalTok{ \{}\StringTok{"cat"}\NormalTok{: [}\StringTok{"feline"}\NormalTok{], }\StringTok{"sat"}\NormalTok{: [}\StringTok{"rested"}\NormalTok{], }\StringTok{"mat"}\NormalTok{: [}\StringTok{"rug"}\NormalTok{]\}}

\NormalTok{augmented }\OperatorTok{=} \StringTok{"The "} \OperatorTok{+}\NormalTok{ random.choice(synonyms[}\StringTok{"cat"}\NormalTok{]) }\OperatorTok{+} \StringTok{" "} \OperatorTok{\textbackslash{}}
           \OperatorTok{+}\NormalTok{ random.choice(synonyms[}\StringTok{"sat"}\NormalTok{]) }\OperatorTok{+} \StringTok{" on the "} \OperatorTok{\textbackslash{}}
           \OperatorTok{+}\NormalTok{ random.choice(synonyms[}\StringTok{"mat"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

This generates simple synonym-based variations of a sentence.

\subsubsection{Try It Yourself}\label{try-it-yourself-251}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Generate five augmented sentences using synonym replacement for a
  sentiment dataset.
\item
  Apply back-translation on a short paragraph and compare the meaning.
\item
  Use contextual augmentation to replace words in a sentence and
  evaluate label preservation.
\end{enumerate}

\subsection{253. Audio Augmentations}\label{audio-augmentations}

Audio augmentation creates variations of sound recordings to make models
robust against noise, distortions, and environmental changes. These
transformations preserve semantic meaning (e.g., speech content) while
challenging the model with realistic variability.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-252}

Imagine hearing the same song played on different speakers: loud, soft,
slightly distorted, or in a noisy café. It's still the same song, but
your ear learns to recognize it under many conditions.

\subsubsection{Deep Dive}\label{deep-dive-252}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1377}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2826}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2464}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Technique
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Benefit
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Risk
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Noise Injection & Add background sounds (static, crowd noise) &
Robustness to real-world noise & Too much may obscure speech \\
Time Stretching & Speed up or slow down without changing pitch & Models
handle varied speaking rates & Extreme values distort naturalness \\
Pitch Shifting & Raise or lower pitch & Captures speaker variability &
Excessive shifts may alter meaning \\
Time Masking & Drop short segments in time & Simulates dropouts,
improves resilience & Can remove important cues \\
SpecAugment & Apply masking to spectrograms (time/frequency) & Effective
in speech recognition & Requires careful parameter tuning \\
\end{longtable}

These methods are standard in speech recognition, music tagging, and
audio event detection.

Challenges include preserving intelligibility, balancing augmentation
strength, and ensuring synthetic transformations match deployment
environments.

\subsubsection{Tiny Code}\label{tiny-code-230}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ librosa}
\NormalTok{y, sr }\OperatorTok{=}\NormalTok{ librosa.load(}\StringTok{"speech.wav"}\NormalTok{)}

\CommentTok{\# Time stretch}
\NormalTok{y\_fast }\OperatorTok{=}\NormalTok{ librosa.effects.time\_stretch(y, rate}\OperatorTok{=}\FloatTok{1.2}\NormalTok{)}

\CommentTok{\# Pitch shift}
\NormalTok{y\_shifted }\OperatorTok{=}\NormalTok{ librosa.effects.pitch\_shift(y, sr, n\_steps}\OperatorTok{=}\DecValTok{2}\NormalTok{)}

\CommentTok{\# Add noise}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{noise }\OperatorTok{=}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\BuiltInTok{len}\NormalTok{(y))}
\NormalTok{y\_noisy }\OperatorTok{=}\NormalTok{ y }\OperatorTok{+}\NormalTok{ noise}
\end{Highlighting}
\end{Shaded}

This produces multiple augmented versions of the same audio clip.

\subsubsection{Try It Yourself}\label{try-it-yourself-252}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Apply time stretching to a speech sample and test recognition
  accuracy.
\item
  Add Gaussian noise to an audio dataset and measure how models adapt.
\item
  Compare performance of models trained with and without SpecAugment on
  noisy test sets.
\end{enumerate}

\subsection{254. Synthetic Data
Generation}\label{synthetic-data-generation}

Synthetic data is artificially generated rather than collected from
real-world observations. It expands datasets, balances rare classes, and
protects privacy while still providing useful training signals.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-253}

Imagine training pilots. You don't send them into storms right
away---you use a simulator. The simulator isn't real weather, but it's
close enough to prepare them. Synthetic data plays the same role for AI
models.

\subsubsection{Deep Dive}\label{deep-dive-253}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1806}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2917}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2222}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3056}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Rule-Based Simulation & Generate data from known formulas or rules &
Transparent, controllable & May oversimplify reality \\
Generative Models & Use GANs, VAEs, diffusion to create data & High
realism, flexible & Risk of artifacts, biases from training data \\
Agent-Based Simulation & Model interactions of multiple entities &
Captures dynamics and complexity & Computationally intensive \\
Data Balancing & Create rare cases to fix class imbalance & Improves
recall on rare events & Synthetic may not match real distribution \\
\end{longtable}

Synthetic data is widely used in robotics (simulated environments),
healthcare (privacy-preserving patient records), and finance (rare fraud
case generation).

Challenges include ensuring realism, avoiding systematic biases, and
validating that synthetic data improves rather than degrades
performance.

\subsubsection{Tiny Code}\label{tiny-code-231}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Generate synthetic 2D points in two classes}
\NormalTok{class0 }\OperatorTok{=}\NormalTok{ np.random.normal(loc}\OperatorTok{=}\FloatTok{0.0}\NormalTok{, scale}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, size}\OperatorTok{=}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\NormalTok{class1 }\OperatorTok{=}\NormalTok{ np.random.normal(loc}\OperatorTok{=}\FloatTok{3.0}\NormalTok{, scale}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, size}\OperatorTok{=}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

This creates a toy dataset mimicking two Gaussian-distributed classes.

\subsubsection{Try It Yourself}\label{try-it-yourself-253}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Generate synthetic minority-class examples for a fraud detection
  dataset.
\item
  Compare model performance trained on real data only vs.~real +
  synthetic.
\item
  Discuss risks when synthetic data is too ``clean'' compared to messy
  real-world data.
\end{enumerate}

\subsection{255. Data Simulation via Domain
Models}\label{data-simulation-via-domain-models}

Data simulation generates synthetic datasets by modeling the processes
that create real-world data. Instead of mimicking outputs directly,
simulation encodes domain knowledge---physical laws, social dynamics, or
system interactions---to produce realistic samples.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-254}

Imagine simulating traffic in a city. You don't record every car on
every road; instead, you model roads, signals, and driver behaviors. The
simulation produces traffic patterns that look like reality without
needing full observation.

\subsubsection{Deep Dive}\label{deep-dive-254}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1489}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3617}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2553}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2340}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Simulation Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Physics-Based & Encodes physical laws (e.g., Newtonian mechanics) &
Accurate for well-understood domains & Computationally heavy \\
Agent-Based & Simulates individual entities and interactions & Captures
emergent behavior & Requires careful parameter tuning \\
Stochastic Models & Uses probability distributions to model uncertainty
& Flexible, lightweight & May miss structural detail \\
Hybrid Models & Combine simulation with real-world data & Balances
realism and tractability & Integration complexity \\
\end{longtable}

Simulation is used in healthcare (epidemic spread), robotics (virtual
environments), and finance (market models). It is especially powerful
when real data is rare, sensitive, or expensive to collect.

Challenges include ensuring assumptions are valid, calibrating
parameters to real data, and balancing fidelity with efficiency. Overly
simplified simulations risk misleading models, while overly complex ones
may be impractical.

\subsubsection{Tiny Code}\label{tiny-code-232}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ random}

\KeywordTok{def}\NormalTok{ simulate\_queue(n\_customers, service\_rate}\OperatorTok{=}\FloatTok{0.8}\NormalTok{):}
\NormalTok{    times }\OperatorTok{=}\NormalTok{ []}
    \ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n\_customers):}
\NormalTok{        arrival }\OperatorTok{=}\NormalTok{ random.expovariate(}\FloatTok{1.0}\NormalTok{)}
\NormalTok{        service }\OperatorTok{=}\NormalTok{ random.expovariate(service\_rate)}
\NormalTok{        times.append((arrival, service))}
    \ControlFlowTok{return}\NormalTok{ times}

\NormalTok{simulated\_data }\OperatorTok{=}\NormalTok{ simulate\_queue(}\DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This toy example simulates arrival and service times in a queue.

\subsubsection{Try It Yourself}\label{try-it-yourself-254}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Build an agent-based simulation of people moving through a store and
  record purchase behavior.
\item
  Compare simulated epidemic curves from stochastic vs.~agent-based
  models.
\item
  Calibrate a simulation using partial real-world data and evaluate how
  closely it matches reality.
\end{enumerate}

\subsection{256. Oversampling and SMOTE}\label{oversampling-and-smote}

Oversampling techniques address class imbalance by creating more
examples of minority classes. The simplest method duplicates existing
samples, while SMOTE (Synthetic Minority Oversampling Technique)
generates new synthetic points by interpolating between real ones.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-255}

Imagine teaching a class where only two students ask rare but important
questions. To balance discussions, you either repeat their questions
(basic oversampling) or create variations of them with slightly
different wording (SMOTE). Both ensure their perspective is better
represented.

\subsubsection{Deep Dive}\label{deep-dive-255}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2203}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3277}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2260}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2260}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Random Oversampling & Duplicate minority examples & Simple, effective
for small imbalance & Risk of overfitting, no new information \\
SMOTE & Interpolate between neighbors to create synthetic examples &
Adds diversity, reduces overfitting risk & May generate unrealistic
samples \\
Variants (Borderline-SMOTE, ADASYN) & Focus on hard-to-classify or
sparse regions & Improves robustness & Complexity, possible noise
amplification \\
\end{longtable}

Oversampling improves recall on minority classes and stabilizes
training, especially for decision trees and linear models. SMOTE goes
further by enriching feature space, making classifiers less biased
toward majority classes.

Challenges include ensuring synthetic samples are realistic, avoiding
oversaturation of boundary regions, and handling high-dimensional data
where interpolation becomes less meaningful.

\subsubsection{Tiny Code}\label{tiny-code-233}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ imblearn.over\_sampling }\ImportTok{import}\NormalTok{ SMOTE}

\NormalTok{X\_res, y\_res }\OperatorTok{=}\NormalTok{ SMOTE().fit\_resample(X, y)}
\end{Highlighting}
\end{Shaded}

This balances class distributions by generating synthetic minority
samples.

\subsubsection{Try It Yourself}\label{try-it-yourself-255}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Apply random oversampling and SMOTE on an imbalanced dataset; compare
  class ratios.
\item
  Train a classifier before and after SMOTE; evaluate changes in recall
  and precision.
\item
  Discuss scenarios where SMOTE may hurt performance (e.g., overlapping
  classes).
\end{enumerate}

\subsection{257. Augmenting with External Knowledge
Sources}\label{augmenting-with-external-knowledge-sources}

Sometimes datasets lack enough diversity or context. External knowledge
sources---such as knowledge graphs, ontologies, lexicons, or pretrained
models---can enrich raw data with additional features or labels,
improving performance and robustness.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-256}

Think of a student studying a textbook. The textbook alone may leave
gaps, but consulting an encyclopedia or dictionary fills in missing
context. In the same way, external knowledge augments limited datasets
with broader background information.

\subsubsection{Deep Dive}\label{deep-dive-256}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1479}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3099}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2465}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2958}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Usage
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Knowledge Graphs & Add relational features between entities & Captures
structured world knowledge & Requires mapping raw data to graph nodes \\
Ontologies & Standardize categories and relationships & Ensures
consistency across datasets & May be rigid or domain-limited \\
Lexicons & Provide sentiment or semantic labels & Simple to integrate &
May miss nuance or domain-specific meaning \\
Pretrained Models & Supply embeddings or predictions as features &
Encodes rich representations & Risk of transferring bias \\
\end{longtable}

Augmenting with external sources is common in domains like NLP
(sentiment lexicons, pretrained embeddings), biology (ontologies), and
recommender systems (knowledge graphs).

Challenges include aligning external resources with internal data,
avoiding propagation of external biases, and ensuring updates stay
consistent with evolving datasets.

\subsubsection{Tiny Code}\label{tiny-code-234}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text }\OperatorTok{=} \StringTok{"The movie was fantastic"}

\CommentTok{\# Example: augment with sentiment lexicon}
\NormalTok{lexicon }\OperatorTok{=}\NormalTok{ \{}\StringTok{"fantastic"}\NormalTok{: }\StringTok{"positive"}\NormalTok{\}}
\NormalTok{features }\OperatorTok{=}\NormalTok{ \{}\StringTok{"sentiment\_hint"}\NormalTok{: lexicon.get(}\StringTok{"fantastic"}\NormalTok{, }\StringTok{"neutral"}\NormalTok{)\}}
\end{Highlighting}
\end{Shaded}

Here, the raw text gains an extra feature derived from external
knowledge.

\subsubsection{Try It Yourself}\label{try-it-yourself-256}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add features from a sentiment lexicon to a text classification
  dataset; compare accuracy.
\item
  Link entities in a dataset to a knowledge graph and extract relational
  features.
\item
  Discuss risks of importing bias when using pretrained models as
  feature generators.
\end{enumerate}

\subsection{258. Balancing Diversity and
Realism}\label{balancing-diversity-and-realism}

Data augmentation should increase diversity to improve generalization,
but excessive or unrealistic transformations can harm performance. The
goal is to balance variety with fidelity so that augmented samples
resemble what the model will face in deployment.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-257}

Think of training an athlete. Practicing under varied conditions---rain,
wind, different fields---improves adaptability. But if you make them
practice in absurd conditions, like underwater, the training no longer
transfers to real games.

\subsubsection{Deep Dive}\label{deep-dive-257}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0769}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3162}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2906}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3162}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Dimension
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Diversity
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Realism
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Tradeoff
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Image & Random rotations, noise, color shifts & Must still look like
valid objects & Too much distortion can confuse model \\
Text & Paraphrasing, synonym replacement & Meaning must remain
consistent & Aggressive edits may flip labels \\
Audio & Pitch shifts, background noise & Speech must stay intelligible &
Overly strong noise degrades content \\
\end{longtable}

Maintaining balance requires domain knowledge. For medical imaging, even
slight distortions can mislead. For consumer photos, aggressive color
changes may be acceptable. The right level of augmentation depends on
context, model robustness, and downstream tasks.

Challenges include quantifying realism, preventing label corruption, and
tuning augmentation pipelines without overfitting to synthetic variety.

\subsubsection{Tiny Code}\label{tiny-code-235}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ augment\_image(img, strength}\OperatorTok{=}\FloatTok{0.3}\NormalTok{):}
    \ControlFlowTok{if}\NormalTok{ strength }\OperatorTok{\textgreater{}} \FloatTok{0.5}\NormalTok{:}
        \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"Augmentation too strong, may harm realism"}\NormalTok{)}
    \CommentTok{\# Apply rotation and brightness jitter within safe limits}
    \ControlFlowTok{return}\NormalTok{ rotate(img, angle}\OperatorTok{=}\DecValTok{10}\OperatorTok{*}\NormalTok{strength), adjust\_brightness(img, factor}\OperatorTok{=}\DecValTok{1}\OperatorTok{+}\NormalTok{strength)}
\end{Highlighting}
\end{Shaded}

This sketch enforces a safeguard to keep transformations within
realistic bounds.

\subsubsection{Try It Yourself}\label{try-it-yourself-257}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Apply light, medium, and heavy augmentation to the same dataset;
  compare accuracy.
\item
  Identify a task where realism is critical (e.g., medical imaging) and
  discuss safe augmentations.
\item
  Design an augmentation pipeline that balances diversity and realism
  for speech recognition.
\end{enumerate}

\subsection{259. Augmentation Pipelines}\label{augmentation-pipelines}

An augmentation pipeline is a structured sequence of transformations
applied to data before training. Instead of using single augmentations
in isolation, pipelines combine multiple steps---randomized and
parameterized---to maximize diversity while maintaining realism.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-258}

Think of preparing ingredients for cooking. You don't always chop
vegetables the same way---sometimes smaller, sometimes larger, sometimes
stir-fried, sometimes steamed. A pipeline introduces controlled
variation, so the dish (dataset) remains recognizable but never
identical.

\subsubsection{Deep Dive}\label{deep-dive-258}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2019}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4423}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3558}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Role
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Randomization & Ensures no two augmented samples are identical & Random
rotation between -15° and +15° \\
Composition & Chains multiple transformations together & Flip → Crop →
Color Jitter \\
Parameter Ranges & Defines safe variability & Brightness factor between
0.8 and 1.2 \\
Conditional Logic & Applies certain augmentations only sometimes & 50\%
chance of noise injection \\
\end{longtable}

Augmentation pipelines are critical for deep learning, especially in
vision, speech, and text. They expand training sets manyfold while
simulating deployment variability.

Challenges include preventing unrealistic distortions, tuning pipeline
strength for different domains, and ensuring reproducibility across
experiments.

\subsubsection{Tiny Code}\label{tiny-code-236}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ torchvision }\ImportTok{import}\NormalTok{ transforms}

\NormalTok{pipeline }\OperatorTok{=}\NormalTok{ transforms.Compose([}
\NormalTok{    transforms.RandomHorizontalFlip(p}\OperatorTok{=}\FloatTok{0.5}\NormalTok{),}
\NormalTok{    transforms.RandomRotation(degrees}\OperatorTok{=}\DecValTok{15}\NormalTok{),}
\NormalTok{    transforms.ColorJitter(brightness}\OperatorTok{=}\FloatTok{0.2}\NormalTok{, contrast}\OperatorTok{=}\FloatTok{0.2}\NormalTok{),}
\NormalTok{    transforms.RandomResizedCrop(size}\OperatorTok{=}\DecValTok{224}\NormalTok{, scale}\OperatorTok{=}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{1.0}\NormalTok{))}
\NormalTok{])}
\end{Highlighting}
\end{Shaded}

This defines a vision augmentation pipeline that introduces controlled
randomness.

\subsubsection{Try It Yourself}\label{try-it-yourself-258}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Build a pipeline for text augmentation combining synonym replacement
  and back-translation.
\item
  Compare model performance using individual augmentations vs.~a full
  pipeline.
\item
  Experiment with different probabilities for applying augmentations;
  measure effects on robustness.
\end{enumerate}

\subsection{260. Evaluating Impact of
Augmentation}\label{evaluating-impact-of-augmentation}

Augmentation should not be used blindly---its effectiveness must be
tested. Evaluation compares model performance with and without
augmentation to determine whether transformations improve
generalization, robustness, and fairness.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-259}

Imagine training for a marathon with altitude masks, weighted vests, and
interval sprints. These techniques make training harder, but do they
actually improve race-day performance? You only know by testing under
real conditions.

\subsubsection{Deep Dive}\label{deep-dive-259}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2562}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4215}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3223}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Evaluation Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Accuracy Gains & Measure improvements on validation/test sets & Higher
F1 score with augmented training \\
Robustness & Test performance under noisy or shifted inputs & Evaluate
on corrupted images \\
Fairness & Check whether augmentation reduces bias & Compare error rates
across groups \\
Ablation Studies & Test augmentations individually and in combinations &
Rotation vs.~rotation+noise \\
Over-Augmentation Detection & Ensure augmentations don't degrade meaning
& Monitor label consistency \\
\end{longtable}

Proper evaluation requires controlled experiments. The same model should
be trained multiple times---with and without augmentation---to isolate
the effect. Cross-validation helps confirm stability.

Challenges include separating augmentation effects from randomness in
training, defining robustness metrics, and ensuring evaluation datasets
reflect real-world variability.

\subsubsection{Tiny Code}\label{tiny-code-237}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ evaluate\_with\_augmentation(model, data, augment}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
    \ControlFlowTok{if}\NormalTok{ augment:}
\NormalTok{        data }\OperatorTok{=}\NormalTok{ [augment(x) }\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ data]}
\NormalTok{    model.train(data)}
    \ControlFlowTok{return}\NormalTok{ model.evaluate(test\_set)}

\NormalTok{baseline }\OperatorTok{=}\NormalTok{ evaluate\_with\_augmentation(model, train\_set, augment}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\NormalTok{augmented }\OperatorTok{=}\NormalTok{ evaluate\_with\_augmentation(model, train\_set, augment}\OperatorTok{=}\NormalTok{pipeline)}
\end{Highlighting}
\end{Shaded}

This setup compares baseline training to augmented training.

\subsubsection{Try It Yourself}\label{try-it-yourself-259}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train a classifier with and without augmentation; compare accuracy and
  robustness to noise.
\item
  Run ablation studies to measure the effect of each augmentation
  individually.
\item
  Design metrics for detecting when augmentation introduces harmful
  distortions.
\end{enumerate}

\section{Chapter 27. Data Quality, Integrity, and
Bias}\label{chapter-27.-data-quality-integrity-and-bias}

\subsection{261. Definitions of Data Quality
Dimensions}\label{definitions-of-data-quality-dimensions}

Data quality refers to how well data serves its intended purpose.
High-quality data is accurate, complete, consistent, timely, valid, and
unique. Each dimension captures a different aspect of trustworthiness,
and together they form the foundation for reliable analysis and
modeling.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-260}

Imagine maintaining a library. If books are misprinted (inaccurate),
missing pages (incomplete), cataloged under two titles (inconsistent),
delivered years late (untimely), or stored in the wrong format
(invalid), the library fails its users. Data suffers the same
vulnerabilities.

\subsubsection{Deep Dive}\label{deep-dive-260}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1185}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3259}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2815}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2741}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Dimension
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example of Good
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example of Poor
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Accuracy & Data correctly reflects reality & Age recorded as 32 when
true age is 32 & Age recorded as 320 \\
Completeness & All necessary values are present & Every record has an
email address & Many records have empty email fields \\
Consistency & Values agree across systems & ``NY'' = ``New York''
everywhere & Some records show ``NY,'' others ``N.Y.'' \\
Timeliness & Data is up to date and available when needed & Inventory
updated hourly & Stock levels last updated months ago \\
Validity & Data follows defined rules and formats & Dates in YYYY-MM-DD
format & Dates like ``31/02/2023'' \\
Uniqueness & No duplicates exist unnecessarily & One row per customer &
Same customer appears multiple times \\
\end{longtable}

Each dimension targets a different failure mode. A dataset may be
accurate but incomplete, valid but inconsistent, or timely but not
unique. Quality requires considering all dimensions together.

Challenges include measuring quality at scale, resolving tradeoffs
(e.g., timeliness vs.~completeness), and aligning definitions with
business needs.

\subsubsection{Tiny Code}\label{tiny-code-238}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ check\_validity(record):}
    \CommentTok{\# Example: ensure age is within reasonable bounds}
    \ControlFlowTok{return} \DecValTok{0} \OperatorTok{\textless{}=}\NormalTok{ record[}\StringTok{"age"}\NormalTok{] }\OperatorTok{\textless{}=} \DecValTok{120}

\KeywordTok{def}\NormalTok{ check\_completeness(record, fields):}
    \ControlFlowTok{return} \BuiltInTok{all}\NormalTok{(record.get(f) }\KeywordTok{is} \KeywordTok{not} \VariableTok{None} \ControlFlowTok{for}\NormalTok{ f }\KeywordTok{in}\NormalTok{ fields)}
\end{Highlighting}
\end{Shaded}

Simple checks like these form the basis of automated data quality
audits.

\subsubsection{Try It Yourself}\label{try-it-yourself-260}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Audit a dataset for completeness, validity, and uniqueness; record
  failure rates.
\item
  Discuss which quality dimensions matter most in healthcare
  vs.~e-commerce.
\item
  Design rules to automatically detect inconsistencies across two linked
  databases.
\end{enumerate}

\subsection{262. Integrity Checks: Completeness,
Consistency}\label{integrity-checks-completeness-consistency}

Integrity checks verify whether data is whole and internally coherent.
Completeness ensures no required information is missing, while
consistency ensures that values align across records and systems.
Together, they act as safeguards against silent errors that can
undermine analysis.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-261}

Imagine filling out a passport form. If you leave the birthdate blank,
it's incomplete. If you write ``USA'' in one field and ``United States''
in another, it's inconsistent. Officials rely on both completeness and
consistency to trust the document.

\subsubsection{Deep Dive}\label{deep-dive-261}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1088}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2653}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2245}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4014}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Check Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example of Pass
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example of Fail
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Completeness & Ensures mandatory fields are filled & Every customer has
a phone number & Some records have null phone numbers \\
Consistency & Aligns values across fields and systems & Gender = ``M''
everywhere & Gender recorded as ``M,'' ``Male,'' and ``1'' in different
tables \\
\end{longtable}

These checks are fundamental in any data pipeline. Without them, missing
or conflicting values propagate downstream, leading to flawed models,
misleading dashboards, or compliance failures.

Why It Matters Completeness and consistency form the backbone of trust.
In healthcare, incomplete patient records can cause misdiagnosis. In
finance, inconsistent transaction logs can lead to reconciliation
errors. Even in recommendation systems, missing or conflicting user
preferences degrade personalization. Automated integrity checks reduce
manual cleaning costs and protect against silent data corruption.

\subsubsection{Tiny Code}\label{tiny-code-239}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ check\_completeness(record, fields):}
    \ControlFlowTok{return} \BuiltInTok{all}\NormalTok{(record.get(f) }\KeywordTok{not} \KeywordTok{in}\NormalTok{ [}\VariableTok{None}\NormalTok{, }\StringTok{""}\NormalTok{] }\ControlFlowTok{for}\NormalTok{ f }\KeywordTok{in}\NormalTok{ fields)}

\KeywordTok{def}\NormalTok{ check\_consistency(record):}
    \CommentTok{\# Example: state code and state name must match}
\NormalTok{    valid\_pairs }\OperatorTok{=}\NormalTok{ \{}\StringTok{"NY"}\NormalTok{: }\StringTok{"New York"}\NormalTok{, }\StringTok{"CA"}\NormalTok{: }\StringTok{"California"}\NormalTok{\}}
    \ControlFlowTok{return}\NormalTok{ valid\_pairs.get(record[}\StringTok{"state\_code"}\NormalTok{]) }\OperatorTok{==}\NormalTok{ record[}\StringTok{"state\_name"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

These simple rules prevent incomplete or contradictory entries from
entering the system.

\subsubsection{Try It Yourself}\label{try-it-yourself-261}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write integrity checks for a student database ensuring every record
  has a unique ID and non-empty name.
\item
  Identify inconsistencies in a dataset where country codes and country
  names don't align.
\item
  Compare the downstream effects of incomplete vs.~inconsistent data in
  a predictive model.
\end{enumerate}

\subsection{263. Error Detection and
Correction}\label{error-detection-and-correction}

Error detection identifies incorrect or corrupted data, while error
correction attempts to fix it automatically or flag it for review.
Errors arise from human entry mistakes, faulty sensors, system
migrations, or data integration issues. Detecting and correcting them
preserves dataset reliability.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-262}

Imagine transcribing a phone number. If you type one extra digit, that's
an error. If someone spots it and fixes it, correction restores trust.
In large datasets, these mistakes appear at scale, and automated checks
act like proofreaders.

\subsubsection{Deep Dive}\label{deep-dive-262}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1909}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2182}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2909}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Error Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Detection Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Correction Approach
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Typographical & ``Jhon'' instead of ``John'' & String similarity &
Replace with closest valid value \\
Format Violations & Date as ``31/02/2023'' & Regex or schema validation
& Coerce into valid nearest format \\
Outliers & Age = 999 & Range checks, statistical methods & Cap, impute,
or flag for review \\
Duplications & Two rows for same person & Entity resolution & Merge into
one record \\
\end{longtable}

Detection uses rules, patterns, or statistical models to spot anomalies.
Correction can be automatic (standardizing codes), heuristic (fuzzy
matching), or manual (flagging edge cases).

Why It Matters Uncorrected errors distort analysis, inflate variance,
and can lead to catastrophic real-world consequences. In logistics, a
wrong postal code delays shipments. In finance, a misplaced decimal can
alter reported revenue. Detecting and fixing errors early avoids
compounding problems as data flows downstream.

\subsubsection{Tiny Code}\label{tiny-code-240}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ detect\_outliers(values, low}\OperatorTok{=}\DecValTok{0}\NormalTok{, high}\OperatorTok{=}\DecValTok{120}\NormalTok{):}
    \ControlFlowTok{return}\NormalTok{ [v }\ControlFlowTok{for}\NormalTok{ v }\KeywordTok{in}\NormalTok{ values }\ControlFlowTok{if}\NormalTok{ v }\OperatorTok{\textless{}}\NormalTok{ low }\KeywordTok{or}\NormalTok{ v }\OperatorTok{\textgreater{}}\NormalTok{ high]}

\KeywordTok{def}\NormalTok{ correct\_typo(value, dictionary):}
    \CommentTok{\# Simple string similarity correction}
    \ControlFlowTok{return} \BuiltInTok{min}\NormalTok{(dictionary, key}\OperatorTok{=}\KeywordTok{lambda}\NormalTok{ w: levenshtein\_distance(value, w))}
\end{Highlighting}
\end{Shaded}

This example detects implausible ages and corrects typos using a
dictionary lookup.

\subsubsection{Try It Yourself}\label{try-it-yourself-262}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Detect and correct misspelled city names in a dataset using string
  similarity.
\item
  Implement a rule to flag transactions above \$1,000,000 as potential
  entry errors.
\item
  Discuss when automated correction is safe vs.~when human review is
  necessary.
\end{enumerate}

\subsection{264. Outlier and Anomaly
Identification}\label{outlier-and-anomaly-identification}

Outliers are extreme values that deviate sharply from the rest of the
data. Anomalies are unusual patterns that may signal errors, rare
events, or meaningful exceptions. Identifying them prevents distortion
of models and reveals hidden insights.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-263}

Think of measuring people's heights. Most fall between 150--200 cm, but
one record says 3,000 cm. That's an outlier. If a bank sees 100 small
daily transactions and suddenly one transfer of \$1 million, that's an
anomaly. Both stand out from the norm.

\subsubsection{Deep Dive}\label{deep-dive-263}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1463}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2846}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2439}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3252}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Best For
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Rule-Based & Thresholds, ranges, business rules & Simple,
domain-specific tasks & Misses subtle anomalies \\
Statistical & Z-scores, IQR, distributional tests & Continuous numeric
data & Sensitive to non-normal data \\
Distance-Based & k-NN, clustering residuals & Multidimensional data &
Expensive on large datasets \\
Model-Based & Autoencoders, isolation forests & Complex,
high-dimensional data & Requires tuning, interpretability issues \\
\end{longtable}

Outliers may represent data entry errors (age = 999), but anomalies may
signal critical events (credit card fraud). Proper handling depends on
context---removal for errors, retention for rare but valuable signals.

Why It Matters Ignoring anomalies can lead to misdiagnosis in
healthcare, overlooked fraud in finance, or undetected failures in
engineering systems. Conversely, mislabeling valid rare events as noise
discards useful information. Robust anomaly handling is therefore
essential for both safety and discovery.

\subsubsection{Tiny Code}\label{tiny-code-241}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{data }\OperatorTok{=}\NormalTok{ [}\DecValTok{10}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{100}\NormalTok{]  }\CommentTok{\# anomaly}

\NormalTok{mean, std }\OperatorTok{=}\NormalTok{ np.mean(data), np.std(data)}
\NormalTok{outliers }\OperatorTok{=}\NormalTok{ [x }\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ data }\ControlFlowTok{if} \BuiltInTok{abs}\NormalTok{(x }\OperatorTok{{-}}\NormalTok{ mean) }\OperatorTok{\textgreater{}} \DecValTok{3} \OperatorTok{*}\NormalTok{ std]}
\end{Highlighting}
\end{Shaded}

This detects values more than 3 standard deviations from the mean.

\subsubsection{Try It Yourself}\label{try-it-yourself-263}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use the IQR method to identify outliers in a salary dataset.
\item
  Train an anomaly detection model on credit card transactions and test
  with injected fraud cases.
\item
  Debate when anomalies should be corrected, removed, or preserved as
  meaningful signals.
\end{enumerate}

\subsection{265. Duplicate Detection and Entity
Resolution}\label{duplicate-detection-and-entity-resolution}

Duplicate detection identifies multiple records that refer to the same
entity. Entity resolution (ER) goes further by merging or linking them
into a single, consistent representation. These processes prevent
redundancy, confusion, and skewed analysis.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-264}

Imagine a contact list where ``Jon Smith,'' ``Jonathan Smith,'' and ``J.
Smith'' all refer to the same person. Without resolution, you might
think you know three people when in fact it's one.

\subsubsection{Deep Dive}\label{deep-dive-264}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4545}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3455}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Detection & Find records that may refer to the same entity & Duplicate
customer accounts \\
Comparison & Measure similarity across fields & Name: ``Jon Smith''
vs.~``Jonathan Smith'' \\
Resolution & Merge or link duplicates into one canonical record & Single
ID for all ``Smith'' variants \\
Survivorship Rules & Decide which values to keep & Prefer most recent
address \\
\end{longtable}

Techniques include exact matching, fuzzy matching (string distance,
phonetic encoding), and probabilistic models. Modern ER may also use
embeddings or graph-based approaches to capture relationships.

Why It Matters Duplicates inflate counts, bias statistics, and degrade
user experience. In healthcare, duplicate patient records can fragment
medical histories. In e-commerce, they can misrepresent sales figures or
inventory. Entity resolution ensures accurate analytics and safer
operations.

\subsubsection{Tiny Code}\label{tiny-code-242}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ difflib }\ImportTok{import}\NormalTok{ SequenceMatcher}

\KeywordTok{def}\NormalTok{ similar(a, b):}
    \ControlFlowTok{return}\NormalTok{ SequenceMatcher(}\VariableTok{None}\NormalTok{, a, b).ratio()}

\NormalTok{name1, name2 }\OperatorTok{=} \StringTok{"Jon Smith"}\NormalTok{, }\StringTok{"Jonathan Smith"}
\ControlFlowTok{if}\NormalTok{ similar(name1, name2) }\OperatorTok{\textgreater{}} \FloatTok{0.8}\NormalTok{:}
\NormalTok{    resolved }\OperatorTok{=} \VariableTok{True}
\end{Highlighting}
\end{Shaded}

This example uses string similarity to flag potential duplicates.

\subsubsection{Try It Yourself}\label{try-it-yourself-264}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Identify and merge duplicate customer records in a small dataset.
\item
  Compare exact matching vs.~fuzzy matching for detecting name
  duplicates.
\item
  Propose survivorship rules for resolving conflicting fields in merged
  entities.
\end{enumerate}

\subsection{266. Bias Sources: Sampling, Labeling,
Measurement}\label{bias-sources-sampling-labeling-measurement}

Bias arises when data does not accurately represent the reality it is
supposed to capture. Common sources include sampling bias (who or what
gets included), labeling bias (how outcomes are assigned), and
measurement bias (how features are recorded). Each introduces systematic
distortions that affect fairness and reliability.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-265}

Imagine surveying opinions by only asking people in one city (sampling
bias), misrecording their answers because of unclear questions (labeling
bias), or using a broken thermometer to measure temperature (measurement
bias). The dataset looks complete but tells a skewed story.

\subsubsection{Deep Dive}\label{deep-dive-265}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1307}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3268}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2680}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2745}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Bias Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Consequence
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Sampling Bias & Data collected from unrepresentative groups & Training
only on urban users & Poor performance on rural users \\
Labeling Bias & Labels reflect subjective or inconsistent judgment &
Annotators disagree on ``offensive'' tweets & Noisy targets, unfair
models \\
Measurement Bias & Systematic error in instruments or logging & Old
sensors under-report pollution & Misleading correlations, false
conclusions \\
\end{longtable}

Bias is often subtle, compounding across the pipeline. It may not be
obvious until deployment, when performance fails for underrepresented or
mismeasured groups.

Why It Matters Unchecked bias leads to unfair decisions, reputational
harm, and legal risks. In finance, biased credit models may discriminate
against minorities. In healthcare, biased datasets can worsen
disparities in diagnosis. Detecting and mitigating bias is not just
technical but also ethical.

\subsubsection{Tiny Code}\label{tiny-code-243}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ check\_sampling\_bias(dataset, group\_field):}
\NormalTok{    counts }\OperatorTok{=}\NormalTok{ dataset[group\_field].value\_counts(normalize}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ counts}

\CommentTok{\# Example: reveals underrepresented groups}
\end{Highlighting}
\end{Shaded}

This simple check highlights disproportionate representation across
groups.

\subsubsection{Try It Yourself}\label{try-it-yourself-265}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Audit a dataset for sampling bias by comparing its distribution
  against census data.
\item
  Examine annotation disagreements in a labeling task and identify
  labeling bias.
\item
  Propose a method to detect measurement bias in sensor readings
  collected over time.
\end{enumerate}

\subsection{267. Fairness Metrics and Bias
Audits}\label{fairness-metrics-and-bias-audits}

Fairness metrics quantify whether models treat groups equitably, while
bias audits systematically evaluate datasets and models for hidden
disparities. These methods move beyond intuition, providing measurable
indicators of fairness.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-266}

Imagine a hiring system. If it consistently favors one group of
applicants despite equal qualifications, something is wrong. Fairness
metrics are the measuring sticks that reveal such disparities.

\subsubsection{Deep Dive}\label{deep-dive-266}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1196}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3641}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2935}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2228}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Metric
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Use
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Demographic Parity & Equal positive prediction rates across groups &
Hiring rate equal for men and women & Ignores qualification
differences \\
Equal Opportunity & Equal true positive rates across groups & Same
recall for detecting disease in all ethnic groups & May conflict with
other fairness goals \\
Equalized Odds & Equal true and false positive rates & Balanced fairness
in credit scoring & Harder to satisfy in practice \\
Calibration & Predicted probabilities reflect true outcomes equally
across groups & 0.7 risk means 70\% chance for all groups & May trade
off with other fairness metrics \\
\end{longtable}

Bias audits combine these metrics with dataset checks: representation
balance, label distribution, and error breakdowns.

Why It Matters Without fairness metrics, hidden inequities persist. For
example, a medical AI may perform well overall but systematically
underdiagnose certain populations. Bias audits ensure trust, regulatory
compliance, and social responsibility.

\subsubsection{Tiny Code}\label{tiny-code-244}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ demographic\_parity(preds, labels, groups):}
\NormalTok{    rates }\OperatorTok{=}\NormalTok{ \{\}}
    \ControlFlowTok{for}\NormalTok{ g }\KeywordTok{in} \BuiltInTok{set}\NormalTok{(groups):}
\NormalTok{        rates[g] }\OperatorTok{=}\NormalTok{ preds[groups }\OperatorTok{==}\NormalTok{ g].mean()}
    \ControlFlowTok{return}\NormalTok{ rates}
\end{Highlighting}
\end{Shaded}

This function computes positive prediction rates across demographic
groups.

\subsubsection{Try It Yourself}\label{try-it-yourself-266}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calculate demographic parity for a loan approval dataset split by
  gender.
\item
  Compare equal opportunity vs.~equalized odds in a healthcare
  prediction task.
\item
  Design a bias audit checklist combining dataset inspection and
  fairness metrics.
\end{enumerate}

\subsection{268. Quality Monitoring in
Production}\label{quality-monitoring-in-production}

Data quality does not end at preprocessing---it must be continuously
monitored in production. As data pipelines evolve, new errors, shifts,
or corruptions can emerge. Monitoring tracks quality over time,
detecting issues before they damage models or decisions.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-267}

Imagine running a water treatment plant. Clean water at the source is
not enough---you must monitor pipes for leaks, contamination, or
pressure drops. Likewise, even high-quality training data can degrade
once systems are live.

\subsubsection{Deep Dive}\label{deep-dive-267}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2640}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3760}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3600}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Schema Validation & Ensure fields and formats remain consistent & Date
stays in YYYY-MM-DD \\
Range and Distribution Checks & Detect sudden shifts in values & Income
values suddenly all zero \\
Missing Data Alerts & Catch unexpected spikes in nulls & Address field
becomes 90\% empty \\
Drift Detection & Track changes in feature or label distributions &
Customer behavior shifts after product launch \\
Anomaly Alerts & Identify rare but impactful issues & Surge in duplicate
records \\
\end{longtable}

Monitoring integrates into pipelines, often with automated alerts and
dashboards. It provides early warning of data drift, pipeline failures,
or silent degradations that affect downstream models.

Why It Matters Models degrade not just from poor training but from
changing environments. Without monitoring, a recommendation system may
continue to suggest outdated items, or a risk model may ignore new fraud
patterns. Continuous monitoring ensures reliability and adaptability.

\subsubsection{Tiny Code}\label{tiny-code-245}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ monitor\_nulls(dataset, field, threshold}\OperatorTok{=}\FloatTok{0.1}\NormalTok{):}
\NormalTok{    null\_ratio }\OperatorTok{=}\NormalTok{ dataset[field].isnull().mean()}
    \ControlFlowTok{if}\NormalTok{ null\_ratio }\OperatorTok{\textgreater{}}\NormalTok{ threshold:}
\NormalTok{        alert(}\SpecialStringTok{f"High null ratio in }\SpecialCharTok{\{}\NormalTok{field}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{null\_ratio}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This simple check alerts when missing values exceed a set threshold.

\subsubsection{Try It Yourself}\label{try-it-yourself-267}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Implement a drift detection test by comparing training vs.~live
  feature distributions.
\item
  Create an alert for when categorical values in production deviate from
  the training schema.
\item
  Discuss what metrics are most critical for monitoring quality in
  healthcare vs.~e-commerce pipelines.
\end{enumerate}

\subsection{269. Tradeoffs: Quality vs.~Quantity
vs.~Freshness}\label{tradeoffs-quality-vs.-quantity-vs.-freshness}

Data projects often juggle three competing priorities: quality
(accuracy, consistency), quantity (size and coverage), and freshness
(timeliness). Optimizing one may degrade the others, and tradeoffs must
be explicitly managed depending on the application.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-268}

Think of preparing a meal. You can have it fast, cheap, or
delicious---but rarely all three at once. Data teams face the same
triangle: fresh streaming data may be noisy, high-quality curated data
may be slow, and massive datasets may sacrifice accuracy.

\subsubsection{Deep Dive}\label{deep-dive-268}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1130}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3217}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3391}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2261}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Priority
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Benefit
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Cost
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Quality & Reliable, trusted results & Slower, expensive to clean and
validate & Curated medical datasets \\
Quantity & Broader coverage, more training power & More noise,
redundancy & Web-scale language corpora \\
Freshness & Captures latest patterns & Limited checks, higher error risk
& Real-time fraud detection \\
\end{longtable}

Balancing depends on context:

\begin{itemize}
\tightlist
\item
  In finance, freshness may matter most (detecting fraud instantly).
\item
  In medicine, quality outweighs speed (accurate diagnosis is critical).
\item
  In search engines, quantity and freshness dominate, even if noise
  remains.
\end{itemize}

Why It Matters Mismanaging tradeoffs can cripple performance. A fraud
model trained only on high-quality but outdated data misses new attack
vectors. A recommendation system trained on vast but noisy clicks may
degrade personalization. Teams must decide deliberately where to
compromise.

\subsubsection{Tiny Code}\label{tiny-code-246}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ prioritize(goal):}
    \ControlFlowTok{if}\NormalTok{ goal }\OperatorTok{==} \StringTok{"quality"}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{"Run strict validation, slower updates"}
    \ControlFlowTok{elif}\NormalTok{ goal }\OperatorTok{==} \StringTok{"quantity"}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{"Ingest everything, minimal filtering"}
    \ControlFlowTok{elif}\NormalTok{ goal }\OperatorTok{==} \StringTok{"freshness"}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{"Stream live data, relax checks"}
\end{Highlighting}
\end{Shaded}

A simplistic sketch of how priorities influence data pipeline design.

\subsubsection{Try It Yourself}\label{try-it-yourself-268}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Identify which priority (quality, quantity, freshness) dominates in
  self-driving cars, and justify why.
\item
  Simulate tradeoffs by training a model on (a) small curated data, (b)
  massive noisy data, (c) fresh but partially unvalidated data.
\item
  Debate whether balancing all three is possible in large-scale systems,
  or if explicit sacrifice is always required.
\end{enumerate}

\subsection{270. Case Studies of Data
Bias}\label{case-studies-of-data-bias}

Data bias is not abstract---it has shaped real-world failures across
domains. Case studies reveal how biased sampling, labeling, or
measurement created unfair or unsafe outcomes, and how organizations
responded. These examples illustrate the stakes of responsible data
practices.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-269}

Imagine an airport security system trained mostly on images of
light-skinned passengers. It works well in lab tests but struggles badly
with darker skin tones. The bias was baked in at the data level, not in
the algorithm itself.

\subsubsection{Deep Dive}\label{deep-dive-269}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3100}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3400}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2250}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Case
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Bias Source
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Consequence
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Lesson
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Facial Recognition & Sampling bias: underrepresentation of darker skin &
Misidentification rates disproportionately high & Ensure demographic
diversity in training data \\
Medical Risk Scores & Labeling bias: used healthcare spending as a proxy
for health & Black patients labeled as ``lower risk'' despite worse
health outcomes & Align labels with true outcomes, not proxies \\
Loan Approval Systems & Measurement bias: income proxies encoded
historical inequities & Higher rejection rates for minority applicants &
Audit features for hidden correlations \\
Language Models & Data collection bias: scraped toxic or imbalanced text
& Reinforcement of stereotypes, harmful outputs & Filter, balance, and
monitor training corpora \\
\end{longtable}

These cases show that bias often comes not from malicious design but
from shortcuts in data collection or labeling.

Why It Matters Bias is not just technical---it affects fairness,
legality, and human lives. Case studies make clear that biased data
leads to real harm: wrongful arrests, denied healthcare, financial
exclusion, and perpetuation of stereotypes. Learning from past failures
is essential to prevent repetition.

\subsubsection{Tiny Code}\label{tiny-code-247}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ audit\_balance(dataset, group\_field):}
\NormalTok{    distribution }\OperatorTok{=}\NormalTok{ dataset[group\_field].value\_counts(normalize}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ distribution}

\CommentTok{\# Example: reveals imbalance in demographic representation}
\end{Highlighting}
\end{Shaded}

This highlights skew in dataset composition, a common bias source.

\subsubsection{Try It Yourself}\label{try-it-yourself-269}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Analyze a well-known dataset (e.g., ImageNet, COMPAS) and identify
  potential biases.
\item
  Propose alternative labeling strategies that reduce bias in risk
  prediction tasks.
\item
  Debate: is completely unbiased data possible, or is the goal to make
  bias transparent and manageable?
\end{enumerate}

\section{Chapter 28. Privacy, security and
anonymization}\label{chapter-28.-privacy-security-and-anonymization}

\subsection{271. Principles of Data
Privacy}\label{principles-of-data-privacy}

Data privacy ensures that personal or sensitive information is
collected, stored, and used responsibly. Core principles include
minimizing data collection, restricting access, protecting
confidentiality, and giving individuals control over their information.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-270}

Imagine lending someone your diary. You might allow them to read a
single entry but not photocopy the whole book or share it with
strangers. Data privacy works the same way: controlled, limited, and
respectful access.

\subsubsection{Deep Dive}\label{deep-dive-270}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1803}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3607}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4590}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Principle
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Data Minimization & Collect only what is necessary & Storing email but
not home address for newsletter signup \\
Purpose Limitation & Use data only for the purpose stated & Health data
collected for care, not for marketing \\
Access Control & Restrict who can see sensitive data & Role-based
permissions in databases \\
Transparency & Inform users about data use & Privacy notices, consent
forms \\
Accountability & Organizations are responsible for compliance & Audit
logs and privacy officers \\
\end{longtable}

These principles underpin legal frameworks worldwide and guide technical
implementations like anonymization, encryption, and secure access
protocols.

Why It Matters Privacy breaches erode trust, invite regulatory
penalties, and cause real harm to individuals. For example, leaked
health records can damage reputations and careers. Respecting privacy
ensures compliance, protects users, and sustains long-term data
ecosystems.

\subsubsection{Tiny Code}\label{tiny-code-248}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ minimize\_data(record):}
    \CommentTok{\# Retain only necessary fields}
    \ControlFlowTok{return}\NormalTok{ \{}\StringTok{"email"}\NormalTok{: record[}\StringTok{"email"}\NormalTok{]\}}

\KeywordTok{def}\NormalTok{ access\_allowed(user\_role, resource):}
\NormalTok{    permissions }\OperatorTok{=}\NormalTok{ \{}\StringTok{"doctor"}\NormalTok{: [}\StringTok{"medical"}\NormalTok{], }\StringTok{"admin"}\NormalTok{: [}\StringTok{"logs"}\NormalTok{]\}}
    \ControlFlowTok{return}\NormalTok{ resource }\KeywordTok{in}\NormalTok{ permissions.get(user\_role, [])}
\end{Highlighting}
\end{Shaded}

This sketch enforces minimization and role-based access.

\subsubsection{Try It Yourself}\label{try-it-yourself-270}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Review a dataset and identify which fields could be removed under data
  minimization.
\item
  Draft a privacy notice explaining how data is collected and used in a
  small project.
\item
  Compare how purpose limitation applies differently in healthcare
  vs.~advertising.
\end{enumerate}

\subsection{272. Differential Privacy}\label{differential-privacy}

Differential privacy provides a mathematical guarantee that individual
records in a dataset cannot be identified, even when aggregate
statistics are shared. It works by injecting carefully calibrated noise
so that outputs look nearly the same whether or not any single person's
data is included.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-271}

Imagine whispering the results of a poll in a crowded room. If you speak
softly enough, no one can tell whether one particular person's vote
influenced what you said, but the overall trend is still audible.

\subsubsection{Deep Dive}\label{deep-dive-271}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1905}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4095}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Element
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ε (Epsilon) & Privacy budget controlling noise strength & Smaller ε =
stronger privacy \\
Noise Injection & Add random variation to results & Report average
salary ± random noise \\
Global vs.~Local & Noise applied at system-level vs.~per user &
Centralized release vs.~local app telemetry \\
\end{longtable}

Differential privacy is widely used for publishing statistics, training
machine learning models, and collecting telemetry without exposing
individuals. It balances privacy (protection of individuals) with
utility (accuracy of aggregates).

Why It Matters Traditional anonymization (removing names, masking IDs)
is often insufficient---individuals can still be re-identified by
combining datasets. Differential privacy provides provable protection,
enabling safe data sharing and analysis without betraying individual
confidentiality.

\subsubsection{Tiny Code}\label{tiny-code-249}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ dp\_average(data, epsilon}\OperatorTok{=}\FloatTok{1.0}\NormalTok{):}
\NormalTok{    true\_avg }\OperatorTok{=}\NormalTok{ np.mean(data)}
\NormalTok{    noise }\OperatorTok{=}\NormalTok{ np.random.laplace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\NormalTok{epsilon)}
    \ControlFlowTok{return}\NormalTok{ true\_avg }\OperatorTok{+}\NormalTok{ noise}
\end{Highlighting}
\end{Shaded}

This example adds Laplace noise to obscure the contribution of any one
individual.

\subsubsection{Try It Yourself}\label{try-it-yourself-271}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Implement a differentially private count of users in a dataset.
\item
  Experiment with different ε values and observe the tradeoff between
  privacy and accuracy.
\item
  Debate: should organizations be required by law to apply differential
  privacy when publishing statistics?
\end{enumerate}

\subsection{273. Federated Learning and Privacy-Preserving
Computation}\label{federated-learning-and-privacy-preserving-computation}

Federated learning allows models to be trained collaboratively across
many devices or organizations without centralizing raw data. Instead of
sharing personal data, only model updates are exchanged.
Privacy-preserving computation techniques, such as secure aggregation,
ensure that no individual's contribution can be reconstructed.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-272}

Think of a classroom where each student solves math problems privately.
Instead of handing in their notebooks, they only submit the final
answers to the teacher, who combines them to see how well the class is
doing. The teacher learns patterns without ever seeing individual work.

\subsubsection{Deep Dive}\label{deep-dive-272}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2553}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4113}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Technique
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Federated Averaging & Aggregate model updates across devices &
Smartphones train local models on typing habits \\
Secure Aggregation & Mask updates so server cannot see individual
contributions & Encrypted updates combined into one \\
Personalization Layers & Allow local fine-tuning on devices & Speech
recognition adapting to a user's accent \\
Hybrid with Differential Privacy & Add noise before sharing updates &
Prevents leakage from gradients \\
\end{longtable}

Federated learning enables collaboration across hospitals, banks, or
mobile devices without exposing raw data. It shifts the paradigm from
``data to the model'' to ``model to the data.''

Why It Matters Centralizing sensitive data creates risks of breaches and
regulatory non-compliance. Federated approaches let organizations and
individuals benefit from shared intelligence while keeping private data
decentralized. In healthcare, this means learning across hospitals
without exposing patient records; in consumer apps, improving
personalization without sending keystrokes to servers.

\subsubsection{Tiny Code}\label{tiny-code-250}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ federated\_average(updates):}
    \CommentTok{\# updates: list of weight vectors from clients}
\NormalTok{    total }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(updates)}
    \ControlFlowTok{return}\NormalTok{ total }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(updates)}

\CommentTok{\# Each client trains locally, only shares updates}
\end{Highlighting}
\end{Shaded}

This sketch shows how client contributions are averaged into a global
model.

\subsubsection{Try It Yourself}\label{try-it-yourself-272}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simulate federated learning with three clients training local models
  on different subsets of data.
\item
  Discuss how secure aggregation protects against server-side attacks.
\item
  Compare benefits and tradeoffs of federated learning vs.~central
  training on anonymized data.
\end{enumerate}

\subsection{274. Homomorphic Encryption}\label{homomorphic-encryption}

Homomorphic encryption allows computations to be performed directly on
encrypted data without decrypting it. The results, once decrypted, match
what would have been obtained if the computation were done on the raw
data. This enables secure processing while preserving confidentiality.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-273}

Imagine putting ingredients inside a locked, transparent box. A chef can
chop, stir, and cook them through built-in tools without ever opening
the box. When unlocked later, the meal is ready---yet the chef never saw
the raw ingredients.

\subsubsection{Deep Dive}\label{deep-dive-273}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1875}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3542}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2431}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2153}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Use
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Partially Homomorphic & Supports one operation (addition or
multiplication) & Securely sum encrypted salaries & Limited
flexibility \\
Somewhat Homomorphic & Supports limited operations of both types & Basic
statistical computations & Depth of operations constrained \\
Fully Homomorphic (FHE) & Supports arbitrary computations &
Privacy-preserving machine learning & Very computationally expensive \\
\end{longtable}

Homomorphic encryption is applied in healthcare (outsourcing encrypted
medical analysis), finance (secure auditing of transactions), and cloud
computing (delegating computation without revealing data).

Why It Matters Normally, data must be decrypted before processing,
exposing it to risks. With homomorphic encryption, organizations can
outsource computation securely, preserving confidentiality even if
servers are untrusted. It bridges the gap between utility and security
in sensitive domains.

\subsubsection{Tiny Code}\label{tiny-code-251}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Pseudocode: encrypted addition}
\NormalTok{enc\_a }\OperatorTok{=}\NormalTok{ encrypt(}\DecValTok{5}\NormalTok{)}
\NormalTok{enc\_b }\OperatorTok{=}\NormalTok{ encrypt(}\DecValTok{3}\NormalTok{)}

\NormalTok{enc\_sum }\OperatorTok{=}\NormalTok{ enc\_a }\OperatorTok{+}\NormalTok{ enc\_b  }\CommentTok{\# computed while still encrypted}
\NormalTok{result }\OperatorTok{=}\NormalTok{ decrypt(enc\_sum)  }\CommentTok{\# {-}\textgreater{} 8}
\end{Highlighting}
\end{Shaded}

The addition is valid even though the system never saw the raw values.

\subsubsection{Try It Yourself}\label{try-it-yourself-273}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Explain how homomorphic encryption differs from traditional encryption
  during computation.
\item
  Identify a real-world use case where FHE is worth the computational
  cost.
\item
  Debate: is homomorphic encryption practical for large-scale machine
  learning today, or still mostly theoretical?
\end{enumerate}

\subsection{275. Secure Multi-Party
Computation}\label{secure-multi-party-computation}

Secure multi-party computation (SMPC) allows multiple parties to jointly
compute a function over their inputs without revealing those inputs to
one another. Each participant only learns the agreed-upon output, never
the private data of others.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-274}

Imagine three friends want to know who earns the highest salary, but
none wants to reveal their exact income. They use a protocol where each
contributes coded pieces of their number, and together they compute the
maximum. The answer is known, but individual salaries remain secret.

\subsubsection{Deep Dive}\label{deep-dive-274}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3810}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2109}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2653}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Technique
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Use
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Secret Sharing & Split data into random shares distributed across
parties & Computing sum of private values & Requires multiple
non-colluding parties \\
Garbled Circuits & Encode computation as encrypted circuit & Secure
auctions, comparisons & High communication overhead \\
Hybrid Approaches & Combine SMPC with homomorphic encryption & Private
ML training & Complexity and latency \\
\end{longtable}

SMPC is used in domains where collaboration is essential but data
sharing is sensitive: banks estimating joint fraud risk, hospitals
aggregating patient outcomes, or researchers pooling genomic data.

Why It Matters Traditional collaboration requires trusting a central
party. SMPC removes that need, ensuring data confidentiality even among
competitors. It unlocks insights that no participant could gain alone
while keeping individual data safe.

\subsubsection{Tiny Code}\label{tiny-code-252}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example: secret sharing for sum}
\KeywordTok{def}\NormalTok{ share\_secret(value, n}\OperatorTok{=}\DecValTok{3}\NormalTok{):}
    \ImportTok{import}\NormalTok{ random}
\NormalTok{    shares }\OperatorTok{=}\NormalTok{ [random.randint(}\DecValTok{0}\NormalTok{, }\DecValTok{100}\NormalTok{) }\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{)]}
\NormalTok{    final }\OperatorTok{=}\NormalTok{ value }\OperatorTok{{-}} \BuiltInTok{sum}\NormalTok{(shares)}
    \ControlFlowTok{return}\NormalTok{ shares }\OperatorTok{+}\NormalTok{ [final]}

\CommentTok{\# Each party gets one share; only all together can recover the value}
\end{Highlighting}
\end{Shaded}

Each participant holds meaningless fragments until combined.

\subsubsection{Try It Yourself}\label{try-it-yourself-274}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simulate secure summation among three organizations using secret
  sharing.
\item
  Discuss tradeoffs between SMPC and homomorphic encryption.
\item
  Propose a scenario in healthcare where SMPC enables collaboration
  without breaching privacy.
\end{enumerate}

\subsection{276. Access Control and
Security}\label{access-control-and-security}

Access control defines who is allowed to see, modify, or delete data.
Security mechanisms enforce these rules to prevent unauthorized use.
Together, they ensure that sensitive data is only handled by trusted
parties under the right conditions.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-275}

Think of a museum. Some rooms are open to everyone, others only to
staff, and some only to the curator. Keys and guards enforce these
boundaries. Data systems use authentication, authorization, and
encryption as their keys and guards.

\subsubsection{Deep Dive}\label{deep-dive-275}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2745}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3725}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3529}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Layer
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Authentication & Verify identity & Login with password or biometric \\
Authorization & Decide what authenticated users can do & Admin can
delete, user can only view \\
Encryption & Protect data in storage and transit & Encrypted databases
and HTTPS \\
Auditing & Record who accessed what and when & Access logs in a hospital
system \\
Role-Based Access (RBAC) & Assign permissions by role & Doctor vs.~nurse
privileges \\
\end{longtable}

Access control can be fine-grained (field-level, row-level) or coarse
(dataset-level). Security also covers patching vulnerabilities,
monitoring intrusions, and enforcing least-privilege principles.

Why It Matters Without strict access controls, even high-quality data
becomes a liability. A single unauthorized access can lead to breaches,
financial loss, and erosion of trust. In regulated domains like finance
or healthcare, access control is both a technical necessity and a legal
requirement.

\subsubsection{Tiny Code}\label{tiny-code-253}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ can\_access(user\_role, resource, action):}
\NormalTok{    permissions }\OperatorTok{=}\NormalTok{ \{}
        \StringTok{"admin"}\NormalTok{: \{}\StringTok{"dataset"}\NormalTok{: [}\StringTok{"read"}\NormalTok{, }\StringTok{"write"}\NormalTok{, }\StringTok{"delete"}\NormalTok{]\},}
        \StringTok{"analyst"}\NormalTok{: \{}\StringTok{"dataset"}\NormalTok{: [}\StringTok{"read"}\NormalTok{]\},}
\NormalTok{    \}}
    \ControlFlowTok{return}\NormalTok{ action }\KeywordTok{in}\NormalTok{ permissions.get(user\_role, \{\}).get(resource, [])}
\end{Highlighting}
\end{Shaded}

This function enforces role-based permissions for different users.

\subsubsection{Try It Yourself}\label{try-it-yourself-275}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Design a role-based access control (RBAC) scheme for a hospital's
  patient database.
\item
  Implement a simple audit log that records who accessed data and when.
\item
  Discuss the risks of giving ``superuser'' access too broadly in an
  organization.
\end{enumerate}

\subsection{277. Data Breaches and Threat
Modeling}\label{data-breaches-and-threat-modeling}

Data breaches occur when unauthorized actors gain access to sensitive
information. Threat modeling is the process of identifying potential
attack vectors, assessing vulnerabilities, and planning defenses before
breaches happen. Together, they frame both the risks and proactive
strategies for securing data.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-276}

Imagine a castle with treasures inside. Attackers may scale the walls,
sneak through tunnels, or bribe guards. Threat modeling maps out every
possible entry point, while breach response plans prepare for the worst
if someone gets in.

\subsubsection{Deep Dive}\label{deep-dive-276}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2449}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3776}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3776}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Threat Vector
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mitigation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
External Attacks & Hackers exploiting unpatched software & Regular
updates, firewalls \\
Insider Threats & Employee misuse of access rights & Least-privilege,
auditing \\
Social Engineering & Phishing emails stealing credentials & User
training, MFA \\
Physical Theft & Stolen laptops or drives & Encryption at rest \\
Supply Chain Attacks & Malicious code in dependencies & Dependency
scanning, integrity checks \\
\end{longtable}

Threat modeling frameworks break down systems into assets, threats, and
countermeasures. By anticipating attacker behavior, organizations can
prioritize defenses and reduce breach likelihood.

Why It Matters Breaches compromise trust, trigger regulatory fines, and
cause financial and reputational damage. Proactive threat modeling
ensures defenses are built into systems rather than patched reactively.
A single overlooked vector---like weak API security---can expose
millions of records.

\subsubsection{Tiny Code}\label{tiny-code-254}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ threat\_model(assets, threats):}
\NormalTok{    model }\OperatorTok{=}\NormalTok{ \{\}}
    \ControlFlowTok{for}\NormalTok{ asset }\KeywordTok{in}\NormalTok{ assets:}
\NormalTok{        model[asset] }\OperatorTok{=}\NormalTok{ [t }\ControlFlowTok{for}\NormalTok{ t }\KeywordTok{in}\NormalTok{ threats }\ControlFlowTok{if}\NormalTok{ t[}\StringTok{"target"}\NormalTok{] }\OperatorTok{==}\NormalTok{ asset]}
    \ControlFlowTok{return}\NormalTok{ model}

\NormalTok{assets }\OperatorTok{=}\NormalTok{ [}\StringTok{"database"}\NormalTok{, }\StringTok{"API"}\NormalTok{, }\StringTok{"user\_credentials"}\NormalTok{]}
\NormalTok{threats }\OperatorTok{=}\NormalTok{ [\{}\StringTok{"target"}\NormalTok{: }\StringTok{"database"}\NormalTok{, }\StringTok{"type"}\NormalTok{: }\StringTok{"SQL injection"}\NormalTok{\}]}
\end{Highlighting}
\end{Shaded}

This sketch links assets to their possible threats for structured
analysis.

\subsubsection{Try It Yourself}\label{try-it-yourself-276}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Identify three potential threat vectors for a cloud-hosted dataset.
\item
  Build a simple threat model for an e-commerce platform handling
  payments.
\item
  Discuss how insider threats differ from external threats in both
  detection and mitigation.
\end{enumerate}

\subsection{278. Privacy--Utility
Tradeoffs}\label{privacyutility-tradeoffs}

Stronger privacy protections often reduce the usefulness of data. The
challenge is balancing privacy (protecting individuals) and utility
(retaining analytical value). Every privacy-enhancing
method---anonymization, noise injection, aggregation---carries the risk
of weakening data insights.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-277}

Imagine looking at a city map blurred for privacy. The blur protects
residents' exact addresses but also makes it harder to plan bus routes.
The more blur you add, the safer the individuals, but the less useful
the map.

\subsubsection{Deep Dive}\label{deep-dive-277}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Privacy Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Effect on Data
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Utility Loss Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Anonymization & Removes identifiers & Harder to link patient history
across hospitals \\
Aggregation & Groups data into buckets & City-level stats hide
neighborhood patterns \\
Noise Injection & Adds randomness & Salary analysis less precise at
individual level \\
Differential Privacy & Formal privacy guarantee & Tradeoff controlled by
privacy budget (ε) \\
\end{longtable}

No single solution fits all contexts. High-stakes domains like
healthcare may prioritize privacy even at the cost of reduced precision,
while real-time systems like fraud detection may tolerate weaker privacy
to preserve accuracy.

Why It Matters If privacy is neglected, individuals are exposed to
re-identification risks. If utility is neglected, organizations cannot
make informed decisions. The balance must be guided by domain,
regulation, and ethical standards.

\subsubsection{Tiny Code}\label{tiny-code-255}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ add\_noise(value, epsilon}\OperatorTok{=}\FloatTok{1.0}\NormalTok{):}
    \ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{    noise }\OperatorTok{=}\NormalTok{ np.random.laplace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\NormalTok{epsilon)}
    \ControlFlowTok{return}\NormalTok{ value }\OperatorTok{+}\NormalTok{ noise}

\CommentTok{\# Higher epsilon = less noise, more utility, weaker privacy}
\end{Highlighting}
\end{Shaded}

This demonstrates the adjustable tradeoff between privacy and utility.

\subsubsection{Try It Yourself}\label{try-it-yourself-277}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Apply aggregation to location data and analyze what insights are lost
  compared to raw coordinates.
\item
  Add varying levels of noise to a dataset and measure how prediction
  accuracy changes.
\item
  Debate whether privacy or utility should take precedence in government
  census data.
\end{enumerate}

\subsection{279. Legal Frameworks}\label{legal-frameworks}

Legal frameworks establish the rules for how personal and sensitive data
must be collected, stored, and shared. They define obligations for
organizations, rights for individuals, and penalties for violations.
Compliance is not optional---it is enforced by governments worldwide.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-278}

Think of traffic laws. Drivers must follow speed limits, signals, and
safety rules, not just for efficiency but for protection of everyone on
the road. Data laws function the same way: clear rules to ensure safety,
fairness, and accountability in the digital world.

\subsubsection{Deep Dive}\label{deep-dive-278}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0985}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1212}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4167}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3636}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Framework
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Region
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Key Principles
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Requirement
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
GDPR & European Union & Consent, right to be forgotten, data
minimization & Explicit consent before processing personal data \\
CCPA/CPRA & California, USA & Transparency, opt-out rights & Consumers
can opt out of data sales \\
HIPAA & USA (healthcare) & Confidentiality, integrity, availability of
health info & Secure transmission of patient records \\
PIPEDA & Canada & Accountability, limiting use, openness & Organizations
must obtain meaningful consent \\
LGPD & Brazil & Lawfulness, purpose limitation, user rights & Clear
disclosure of processing activities \\
\end{longtable}

These frameworks often overlap but differ in scope and enforcement.
Multinational organizations must comply with all relevant laws, which
may impose stricter standards than internal policies.

Why It Matters Ignoring legal frameworks risks lawsuits, regulatory
fines, and reputational harm. More importantly, these laws codify
societal expectations of privacy and fairness. Compliance is both a
legal duty and a trust-building measure with customers and stakeholders.

\subsubsection{Tiny Code}\label{tiny-code-256}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ check\_gdpr\_consent(user):}
    \ControlFlowTok{if} \KeywordTok{not}\NormalTok{ user.get(}\StringTok{"consent"}\NormalTok{):}
        \ControlFlowTok{raise} \PreprocessorTok{PermissionError}\NormalTok{(}\StringTok{"No consent: processing not allowed"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This enforces a GDPR-style rule requiring explicit consent.

\subsubsection{Try It Yourself}\label{try-it-yourself-278}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compare GDPR's ``right to be forgotten'' with CCPA's opt-out
  mechanism.
\item
  Identify which frameworks would apply to a healthcare startup
  operating in both the US and EU.
\item
  Debate whether current laws adequately address AI training data
  collected from the web.
\end{enumerate}

\subsection{280. Auditing and Compliance}\label{auditing-and-compliance}

Auditing and compliance ensure that data practices follow internal
policies, industry standards, and legal regulations. Audits check
whether systems meet requirements, while compliance establishes
processes to prevent violations before they occur.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-279}

Imagine a factory producing medicine. Inspectors periodically check the
process to confirm it meets safety standards. The medicine may work, but
without audits and compliance, no one can be sure it's safe. Data
pipelines require the same oversight.

\subsubsection{Deep Dive}\label{deep-dive-279}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2130}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3796}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4074}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Internal Audits & Verify adherence to company policies & Review of who
accessed sensitive datasets \\
External Audits & Independent verification for regulators & Third-party
certification of GDPR compliance \\
Compliance Programs & Continuous processes to enforce standards &
Employee training, automated monitoring \\
Audit Trails & Logs of all data access and changes & Immutable logs in
healthcare records \\
Remediation & Corrective actions after findings & Patching
vulnerabilities, retraining staff \\
\end{longtable}

Auditing requires both technical and organizational controls---logs,
encryption, access policies, and governance procedures. Compliance
transforms these from one-off checks into ongoing practice.

Why It Matters Without audits, data misuse can go undetected for years.
Without compliance, organizations may meet requirements once but quickly
drift into non-conformance. Both protect against fines, strengthen
trust, and ensure ethical use of data in sensitive applications.

\subsubsection{Tiny Code}\label{tiny-code-257}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ datetime}

\KeywordTok{def}\NormalTok{ log\_access(user, resource):}
    \ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\StringTok{"audit.log"}\NormalTok{, }\StringTok{"a"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{        f.write(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{datetime}\SpecialCharTok{.}\NormalTok{datetime}\SpecialCharTok{.}\NormalTok{now()}\SpecialCharTok{\}}\SpecialStringTok{ {-} }\SpecialCharTok{\{}\NormalTok{user}\SpecialCharTok{\}}\SpecialStringTok{ accessed }\SpecialCharTok{\{}\NormalTok{resource}\SpecialCharTok{\}}\CharTok{\textbackslash{}n}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This sketch keeps a simple audit trail of data access events.

\subsubsection{Try It Yourself}\label{try-it-yourself-279}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Design an audit trail system for a financial transactions database.
\item
  Compare internal vs.~external audits: what risks does each mitigate?
\item
  Propose a compliance checklist for a startup handling personal health
  data.
\end{enumerate}

\section{Chapter 29. Datasets, Benchmarks and Data
Cards}\label{chapter-29.-datasets-benchmarks-and-data-cards}

\subsection{281. Iconic Benchmarks in AI
Research}\label{iconic-benchmarks-in-ai-research}

Benchmarks serve as standardized tests to measure and compare progress
in AI. Iconic benchmarks---those widely adopted across decades---become
milestones that shape the direction of research. They provide a common
ground for evaluating models, exposing limitations, and motivating
innovation.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-280}

Think of school exams shared nationwide. Students from different schools
are measured by the same questions, making results comparable.
Benchmarks like MNIST or ImageNet serve the same role in AI: common
tests that reveal who's ahead and where gaps remain.

\subsubsection{Deep Dive}\label{deep-dive-280}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1361}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2177}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3810}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2653}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Benchmark
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Domain
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Contribution
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
MNIST & Handwritten digit recognition & Popularized deep learning,
simple entry point & Too easy today; models achieve \textgreater99\% \\
ImageNet & Large-scale image classification & Sparked deep CNN
revolution (AlexNet, 2012) & Static dataset, biased categories \\
GLUE / SuperGLUE & Natural language understanding & Unified NLP
evaluation; accelerated transformer progress & Narrow,
benchmark-specific optimization \\
COCO & Object detection, segmentation & Complex scenes, multiple tasks &
Labels costly and limited \\
Atari / ALE & Reinforcement learning & Standardized game environments &
Limited diversity, not real-world \\
WMT & Machine translation & Annual shared tasks, multilingual scope &
Focuses on narrow domains \\
\end{longtable}

These iconic datasets and competitions created inflection points in AI.
They highlight how shared challenges can catalyze breakthroughs but also
illustrate the risks of ``benchmark chasing,'' where models overfit to
leaderboards rather than generalizing.

Why It Matters Without benchmarks, progress would be anecdotal,
fragmented, and hard to compare. Iconic benchmarks have guided funding,
research agendas, and industrial adoption. But reliance on a few tests
risks tunnel vision---real-world complexity often far exceeds benchmark
scope.

\subsubsection{Tiny Code}\label{tiny-code-258}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.datasets }\ImportTok{import}\NormalTok{ fetch\_openml}
\NormalTok{mnist }\OperatorTok{=}\NormalTok{ fetch\_openml(}\StringTok{\textquotesingle{}mnist\_784\textquotesingle{}}\NormalTok{)}
\NormalTok{X, y }\OperatorTok{=}\NormalTok{ mnist.data, mnist.target}
\BuiltInTok{print}\NormalTok{(}\StringTok{"MNIST size:"}\NormalTok{, X.shape)}
\end{Highlighting}
\end{Shaded}

This loads MNIST, one of the simplest but most historically influential
benchmarks.

\subsubsection{Try It Yourself}\label{try-it-yourself-280}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compare error rates of classical ML vs.~deep learning on MNIST.
\item
  Analyze ImageNet's role in popularizing convolutional networks.
\item
  Debate whether leaderboards accelerate progress or encourage narrow
  optimization.
\end{enumerate}

\subsection{282. Domain-Specific
Datasets}\label{domain-specific-datasets}

While general-purpose benchmarks push broad progress, domain-specific
datasets focus on specialized applications. They capture the nuances,
constraints, and goals of a particular field---healthcare, finance, law,
education, or scientific research. These datasets often require expert
knowledge to create and interpret.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-281}

Imagine training chefs. General cooking exams measure basic skills like
chopping or boiling. But a pastry competition tests precision in
desserts, while a sushi exam tests knife skills and fish preparation.
Each domain-specific test reveals expertise beyond general training.

\subsubsection{Deep Dive}\label{deep-dive-281}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1197}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2393}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3504}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2906}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Domain
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Dataset
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Focus
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Challenge
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Healthcare & MIMIC-III (clinical records) & Patient monitoring,
mortality prediction & Privacy concerns, annotation cost \\
Finance & LOBSTER (limit order book) & Market microstructure, trading
strategies & High-frequency, noisy data \\
Law & CaseHOLD, LexGLUE & Legal reasoning, precedent retrieval & Complex
language, domain expertise \\
Education & ASSISTments & Student knowledge tracing & Long-term,
longitudinal data \\
Science & ProteinNet, MoleculeNet & Protein folding, molecular
prediction & High dimensionality, data scarcity \\
\end{longtable}

Domain datasets often require costly annotation by experts (e.g.,
radiologists, lawyers). They may also involve strict compliance with
privacy or licensing restrictions, making access more limited than open
benchmarks.

Why It Matters Domain-specific datasets drive applied AI. Breakthroughs
in healthcare, law, or finance depend not on generic datasets but on
high-quality, domain-tailored ones. They ensure models are trained on
data that matches deployment conditions, bridging the gap from lab to
practice.

\subsubsection{Tiny Code}\label{tiny-code-259}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}

\CommentTok{\# Example: simplified clinical dataset}
\NormalTok{data }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}
    \StringTok{"patient\_id"}\NormalTok{: [}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{],}
    \StringTok{"heart\_rate"}\NormalTok{: [}\DecValTok{88}\NormalTok{, }\DecValTok{110}\NormalTok{, }\DecValTok{72}\NormalTok{],}
    \StringTok{"outcome"}\NormalTok{: [}\StringTok{"stable"}\NormalTok{, }\StringTok{"critical"}\NormalTok{, }\StringTok{"stable"}\NormalTok{]}
\NormalTok{\})}
\BuiltInTok{print}\NormalTok{(data.head())}
\end{Highlighting}
\end{Shaded}

This sketch mimics a small domain dataset, capturing structured signals
tied to real-world tasks.

\subsubsection{Try It Yourself}\label{try-it-yourself-281}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compare the challenges of annotating medical vs.~financial datasets.
\item
  Propose a domain where no benchmark currently exists but would be
  valuable.
\item
  Debate whether domain-specific datasets should prioritize openness or
  strict access control.
\end{enumerate}

\subsection{283. Dataset Documentation
Standards}\label{dataset-documentation-standards}

Datasets require documentation to ensure they are understood, trusted,
and responsibly reused. Standards like \emph{datasheets for datasets},
\emph{data cards}, and \emph{model cards} define structured ways to
describe how data was collected, annotated, processed, and intended to
be used.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-282}

Think of buying food at a grocery store. Labels list ingredients,
nutritional values, and expiration dates. Without them, you wouldn't
know if something is safe to eat. Dataset documentation serves as the
``nutrition label'' for data.

\subsubsection{Deep Dive}\label{deep-dive-282}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2093}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3178}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4729}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Standard
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Content
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Datasheets for Datasets & Provide detailed dataset ``spec sheet'' &
Collection process, annotator demographics, known limitations \\
Data Cards & User-friendly summaries for practitioners & Intended uses,
risks, evaluation metrics \\
Model Cards (related) & Document trained models on datasets &
Performance by subgroup, ethical considerations \\
\end{longtable}

Documentation should cover:

\begin{itemize}
\tightlist
\item
  Provenance: where the data came from
\item
  Composition: what it contains, including distributions
\item
  Collection process: who collected it, how, under what conditions
\item
  Preprocessing: cleaning, filtering, augmentation
\item
  Intended uses and misuses: guidance for responsible application
\end{itemize}

Why It Matters Without documentation, datasets become black boxes. Users
may unknowingly replicate biases, violate privacy, or misuse data
outside its intended scope. Clear standards increase reproducibility,
accountability, and fairness in AI systems.

\subsubsection{Tiny Code}\label{tiny-code-260}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset\_card }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"name"}\NormalTok{: }\StringTok{"Example Dataset"}\NormalTok{,}
    \StringTok{"source"}\NormalTok{: }\StringTok{"Survey responses, 2023"}\NormalTok{,}
    \StringTok{"intended\_use"}\NormalTok{: }\StringTok{"Sentiment analysis research"}\NormalTok{,}
    \StringTok{"limitations"}\NormalTok{: }\StringTok{"Not representative across regions"}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This mimics a lightweight data card with essential details.

\subsubsection{Try It Yourself}\label{try-it-yourself-282}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Draft a mini data card for a dataset you've used, including
  provenance, intended use, and limitations.
\item
  Compare the goals of datasheets vs.~data cards: which fits better for
  open datasets?
\item
  Debate whether dataset documentation should be mandatory for
  publication in research conferences.
\end{enumerate}

\subsection{284. Benchmarking Practices and
Leaderboards}\label{benchmarking-practices-and-leaderboards}

Benchmarking practices establish how models are evaluated on datasets,
while leaderboards track performance across methods. They provide
structured comparisons, motivate progress, and highlight
state-of-the-art techniques. However, they can also lead to narrow
optimization when progress is measured only by rankings.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-283}

Think of a race track. Different runners compete on the same course, and
results are recorded on a scoreboard. This allows fair comparison---but
if runners train only for that one track, they may fail elsewhere.

\subsubsection{Deep Dive}\label{deep-dive-283}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1871}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3094}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1727}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3309}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Practice
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Risk
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Standardized Splits & Ensure models train/test on same partitions & GLUE
train/dev/test & Leakage or unfair comparisons if splits differ \\
Shared Metrics & Enable apples-to-apples evaluation & Accuracy, F1,
BLEU, mAP & Overfitting to metric quirks \\
Leaderboards & Public rankings of models & Kaggle, Papers with Code &
Incentive to ``game'' benchmarks \\
Reproducibility Checks & Verify reported results & Code and seed sharing
& Often neglected in practice \\
Dynamic Benchmarks & Update tasks over time & Dynabench & Better
robustness but less comparability \\
\end{longtable}

Leaderboards can accelerate research but risk creating a ``race to the
top'' where small gains are overemphasized and generalization is
ignored. Responsible benchmarking requires context, multiple metrics,
and periodic refresh.

Why It Matters Benchmarks and leaderboards shape entire research
agendas. Progress in NLP and vision has often been benchmark-driven. But
blind optimization leads to diminishing returns and brittle systems.
Balanced practices maintain comparability without sacrificing
generality.

\subsubsection{Tiny Code}\label{tiny-code-261}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ evaluate(model, test\_set, metric):}
\NormalTok{    predictions }\OperatorTok{=}\NormalTok{ model.predict(test\_set.features)}
    \ControlFlowTok{return}\NormalTok{ metric(test\_set.labels, predictions)}

\NormalTok{score }\OperatorTok{=}\NormalTok{ evaluate(model, test\_set, f1\_score)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Model F1:"}\NormalTok{, score)}
\end{Highlighting}
\end{Shaded}

This example shows a consistent evaluation function that enforces
fairness across submissions.

\subsubsection{Try It Yourself}\label{try-it-yourself-283}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compare strengths and weaknesses of accuracy vs.~F1 on imbalanced
  datasets.
\item
  Propose a benchmarking protocol that reduces leaderboard overfitting.
\item
  Debate: do leaderboards accelerate science, or do they distort it by
  rewarding small, benchmark-specific tricks?
\end{enumerate}

\subsection{285. Dataset Shift and
Obsolescence}\label{dataset-shift-and-obsolescence}

Dataset shift occurs when the distribution of training data differs from
the distribution seen in deployment. Obsolescence happens when datasets
age and no longer reflect current realities. Both reduce model
reliability, even if models perform well during initial evaluation.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-284}

Imagine training a weather model on patterns from the 1980s. Climate
change has altered conditions, so the model struggles today. The data
itself hasn't changed, but the world has.

\subsubsection{Deep Dive}\label{deep-dive-284}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1138}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3353}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2934}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2575}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type of Shift
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Impact
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Covariate Shift & Input distribution changes, but label relationship
stays & Different demographics in deployment vs.~training & Reduced
accuracy, especially on edge groups \\
Label Shift & Label distribution changes & Fraud becomes rarer after new
regulations & Model miscalibrates predictions \\
Concept Drift & Label relationship changes & Spam tactics evolve, old
signals no longer valid & Model fails to detect new patterns \\
Obsolescence & Dataset no longer reflects reality & Old product catalogs
in recommender systems & Outdated predictions, poor user experience \\
\end{longtable}

Detecting shift requires monitoring input distributions, error rates,
and calibration over time. Mitigation includes retraining, domain
adaptation, and continual learning.

Why It Matters Even high-quality datasets degrade in value as the world
evolves. Medical datasets may omit new diseases, financial data may miss
novel market instruments, and language datasets may fail to capture
emerging slang. Ignoring shift risks silent model decay.

\subsubsection{Tiny Code}\label{tiny-code-262}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ detect\_shift(train\_dist, live\_dist, threshold}\OperatorTok{=}\FloatTok{0.1}\NormalTok{):}
\NormalTok{    diff }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{abs}\NormalTok{(train\_dist }\OperatorTok{{-}}\NormalTok{ live\_dist).}\BuiltInTok{sum}\NormalTok{()}
    \ControlFlowTok{return}\NormalTok{ diff }\OperatorTok{\textgreater{}}\NormalTok{ threshold}

\CommentTok{\# Example: compare feature distributions between training and production}
\end{Highlighting}
\end{Shaded}

This sketch flags significant divergence in feature distributions.

\subsubsection{Try It Yourself}\label{try-it-yourself-284}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Identify a real-world domain where dataset shift is frequent (e.g.,
  cybersecurity, social media).
\item
  Simulate concept drift by modifying label rules over time; observe
  model degradation.
\item
  Propose strategies for keeping benchmark datasets relevant over
  decades.
\end{enumerate}

\subsection{286. Creating Custom
Benchmarks}\label{creating-custom-benchmarks}

Custom benchmarks are designed when existing datasets fail to capture
the challenges of a particular task or domain. They define evaluation
standards tailored to specific goals, ensuring models are tested under
conditions that matter most for real-world performance.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-285}

Think of building a driving test for autonomous cars. General exams
(like vision recognition) aren't enough---you need tasks like merging in
traffic, handling rain, and reacting to pedestrians. A custom benchmark
reflects those unique requirements.

\subsubsection{Deep Dive}\label{deep-dive-285}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2650}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4017}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Define Task Scope & Clarify what should be measured & Detecting rare
diseases in medical scans \\
Collect Representative Data & Capture relevant scenarios & Images from
diverse hospitals, devices \\
Design Evaluation Metrics & Choose fairness and robustness measures &
Sensitivity, specificity, subgroup breakdowns \\
Create Splits & Ensure generalization tests & Hospital A for training,
Hospital B for testing \\
Publish with Documentation & Enable reproducibility and trust & Data
card detailing biases and limitations \\
\end{longtable}

Custom benchmarks may combine synthetic, real, or simulated data. They
often require domain experts to define tasks and interpret results.

Why It Matters Generic benchmarks can mislead---models may excel on
ImageNet but fail in radiology. Custom benchmarks align evaluation with
actual deployment conditions, ensuring research progress translates into
practical impact. They also surface failure modes that standard
benchmarks overlook.

\subsubsection{Tiny Code}\label{tiny-code-263}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{benchmark }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"task"}\NormalTok{: }\StringTok{"disease\_detection"}\NormalTok{,}
    \StringTok{"metric"}\NormalTok{: }\StringTok{"sensitivity"}\NormalTok{,}
    \StringTok{"train\_split"}\NormalTok{: }\StringTok{"hospital\_A"}\NormalTok{,}
    \StringTok{"test\_split"}\NormalTok{: }\StringTok{"hospital\_B"}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This sketch encodes a simple benchmark definition, separating task,
metric, and data sources.

\subsubsection{Try It Yourself}\label{try-it-yourself-285}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Propose a benchmark for autonomous drones, including data sources and
  metrics.
\item
  Compare risks of overfitting to a custom benchmark vs.~using a
  general-purpose dataset.
\item
  Draft a checklist for releasing a benchmark dataset responsibly.
\end{enumerate}

\subsection{287. Bias and Ethics in Benchmark
Design}\label{bias-and-ethics-in-benchmark-design}

Benchmarks are not neutral. Decisions about what data to include, how to
label it, and which metrics to prioritize embed values and biases.
Ethical benchmark design requires awareness of representation, fairness,
and downstream consequences.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-286}

Imagine a spelling bee that only includes English words of Latin origin.
Contestants may appear skilled, but the test unfairly excludes knowledge
of other linguistic roots. Similarly, benchmarks can unintentionally
reward narrow abilities while penalizing others.

\subsubsection{Deep Dive}\label{deep-dive-286}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1119}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2657}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3427}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2797}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Design Choice
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Potential Bias
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Impact
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Sampling & Over- or underrepresentation of groups & Benchmark with
mostly Western news articles & Models generalize poorly to global
data \\
Labeling & Subjective or inconsistent judgments & Offensive speech
labeled without cultural context & Misclassification, unfair
moderation \\
Metrics & Optimizing for narrow criteria & Accuracy as sole metric in
imbalanced data & Ignores fairness, robustness \\
Task Framing & What is measured defines progress & Focusing only on
short text QA in NLP & Neglects reasoning or long context tasks \\
\end{longtable}

Ethical benchmark design requires diverse representation, transparent
documentation, and ongoing audits to detect misuse or obsolescence.

Why It Matters A biased benchmark can mislead entire research fields.
For instance, biased facial recognition datasets have contributed to
harmful systems with disproportionate error rates. Ethics in benchmark
design is not only about fairness but also about scientific validity and
social responsibility.

\subsubsection{Tiny Code}\label{tiny-code-264}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ audit\_representation(dataset, group\_field):}
\NormalTok{    counts }\OperatorTok{=}\NormalTok{ dataset[group\_field].value\_counts(normalize}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ counts}

\CommentTok{\# Reveals imbalances across demographic groups in a benchmark}
\end{Highlighting}
\end{Shaded}

This highlights hidden skew in benchmark composition.

\subsubsection{Try It Yourself}\label{try-it-yourself-286}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Audit an existing benchmark for representation gaps across
  demographics or domains.
\item
  Propose fairness-aware metrics to supplement accuracy in imbalanced
  benchmarks.
\item
  Debate whether benchmarks should expire after a certain time to
  prevent overfitting and ethical drift.
\end{enumerate}

\subsection{288. Open Data Initiatives}\label{open-data-initiatives}

Open data initiatives aim to make datasets freely available for
research, innovation, and public benefit. They encourage transparency,
reproducibility, and collaboration by lowering barriers to access.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-287}

Think of a public library. Anyone can walk in, borrow books, and build
knowledge without needing special permission. Open datasets function as
libraries for AI and science, enabling anyone to experiment and
contribute.

\subsubsection{Deep Dive}\label{deep-dive-287}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2917}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Initiative
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Domain
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Contribution
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
UCI Machine Learning Repository & General ML & Early standard source for
small datasets & Limited scale today \\
Kaggle Datasets & Multidomain & Community sharing, competitions &
Variable quality \\
Open Images & Computer Vision & Large-scale, annotated image set &
Biased toward Western contexts \\
OpenStreetMap & Geospatial & Global, crowdsourced maps & Inconsistent
coverage \\
Human Genome Project & Biology & Free access to genetic data & Ethical
and privacy concerns \\
\end{longtable}

Open data democratizes access but raises challenges around privacy,
governance, and sustainability. Quality control and maintenance are
often left to communities or volunteer groups.

Why It Matters Without open datasets, progress would remain siloed
within corporations or elite institutions. Open initiatives enable
reproducibility, accelerate learning, and foster innovation globally. At
the same time, openness must be balanced with privacy, consent, and
responsible usage.

\subsubsection{Tiny Code}\label{tiny-code-265}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}

\CommentTok{\# Example: loading an open dataset}
\NormalTok{url }\OperatorTok{=} \StringTok{"https://archive.ics.uci.edu/ml/machine{-}learning{-}databases/iris/iris.data"}
\NormalTok{iris }\OperatorTok{=}\NormalTok{ pd.read\_csv(url, header}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(iris.head())}
\end{Highlighting}
\end{Shaded}

This demonstrates easy access to open datasets that have shaped decades
of ML research.

\subsubsection{Try It Yourself}\label{try-it-yourself-287}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Identify benefits and risks of releasing medical datasets as open
  data.
\item
  Compare community-driven initiatives (like OpenStreetMap) with
  institutional ones (like Human Genome Project).
\item
  Debate whether all government-funded research datasets should be
  mandated as open by law.
\end{enumerate}

\subsection{289. Dataset Licensing and Access
Restrictions}\label{dataset-licensing-and-access-restrictions}

Licensing defines how datasets can be used, shared, and modified. Access
restrictions determine who may obtain the data and under what
conditions. These mechanisms balance openness with protection of
privacy, intellectual property, and ethical use.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-288}

Imagine a library with different sections. Some books are public domain
and free to copy. Others can be read only in the reading room. Rare
manuscripts require special permission. Datasets are governed the same
way---some open, some restricted, some closed entirely.

\subsubsection{Deep Dive}\label{deep-dive-288}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4167}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
License Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Characteristics
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Open Licenses & Free to use, often with attribution & Creative Commons
(CC-BY) \\
Copyleft Licenses & Derivatives must also remain open & GNU GPL for data
derivatives \\
Non-Commercial & Prohibits commercial use & CC-BY-NC \\
Custom Licenses & Domain-specific terms & Kaggle competition rules \\
\end{longtable}

Access restrictions include:

\begin{itemize}
\tightlist
\item
  Tiered Access: Public, registered, or vetted users
\item
  Data Use Agreements: Contracts limiting use cases
\item
  Sensitive Data Controls: HIPAA, GDPR constraints on health and
  personal data
\end{itemize}

Why It Matters Without clear licenses, datasets exist in legal gray
zones. Users risk violations by redistributing or commercializing them.
Restrictions protect privacy and respect ownership but may slow
innovation. Responsible licensing fosters clarity, fairness, and
compliance.

\subsubsection{Tiny Code}\label{tiny-code-266}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset\_license }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"name"}\NormalTok{: }\StringTok{"Example Dataset"}\NormalTok{,}
    \StringTok{"license"}\NormalTok{: }\StringTok{"CC{-}BY{-}NC"}\NormalTok{,}
    \StringTok{"access"}\NormalTok{: }\StringTok{"registered users only"}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This sketch encodes terms for dataset use and access.

\subsubsection{Try It Yourself}\label{try-it-yourself-288}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compare implications of CC-BY vs.~CC-BY-NC licenses for a dataset.
\item
  Draft a data use agreement for a clinical dataset requiring IRB
  approval.
\item
  Debate: should all academic datasets be open by default, or should
  restrictions be the norm?
\end{enumerate}

\subsection{290. Sustainability and Long-Term
Curation}\label{sustainability-and-long-term-curation}

Datasets, like software, require maintenance. Sustainability involves
ensuring that datasets remain usable, relevant, and accessible over
decades. Long-term curation means preserving not only the raw data but
also metadata, documentation, and context so that future researchers can
trust and interpret it.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-289}

Think of a museum preserving ancient manuscripts. Without climate
control, translation notes, and careful archiving, the manuscripts
degrade into unreadable fragments. Datasets need the same care to avoid
becoming digital fossils.

\subsubsection{Deep Dive}\label{deep-dive-289}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2114}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4472}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3415}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Challenge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Data Rot & Links, formats, or storage systems become obsolete & Broken
URLs to classic ML datasets \\
Context Loss & Metadata and documentation disappear & Dataset without
info on collection methods \\
Funding Sustainability & Hosting and curation need long-term support &
Public repositories losing grants \\
Evolving Standards & Old formats may not match new tools & CSV datasets
without schema definitions \\
Ethical Drift & Data collected under outdated norms becomes problematic
& Social media data reused without consent \\
\end{longtable}

Sustainable datasets require redundant storage, clear licensing,
versioning, and continuous stewardship. Initiatives like institutional
repositories and national archives help, but sustainability often
remains an afterthought.

Why It Matters Without long-term curation, future researchers may be
unable to reproduce today's results or understand historical progress.
Benchmark datasets risk obsolescence, and domain-specific data may be
lost entirely. Sustainability ensures that knowledge survives beyond
immediate use cases.

\subsubsection{Tiny Code}\label{tiny-code-267}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset\_metadata }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"name"}\NormalTok{: }\StringTok{"Climate Observations"}\NormalTok{,}
    \StringTok{"version"}\NormalTok{: }\StringTok{"1.2"}\NormalTok{,}
    \StringTok{"last\_updated"}\NormalTok{: }\StringTok{"2025{-}01{-}01"}\NormalTok{,}
    \StringTok{"archived\_at"}\NormalTok{: }\StringTok{"https://doi.org/10.xxxx/archive"}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Metadata like this helps preserve context for future use.

\subsubsection{Try It Yourself}\label{try-it-yourself-289}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Propose a sustainability plan for an open dataset, including storage,
  funding, and stewardship.
\item
  Identify risks of ``data rot'' in ML benchmarks and suggest preventive
  measures.
\item
  Debate whether long-term curation is a responsibility of dataset
  creators, institutions, or the broader community.
\end{enumerate}

\section{Chapter 30. Data Verisioning and
Lineage}\label{chapter-30.-data-verisioning-and-lineage}

\subsection{291. Concepts of Data
Versioning}\label{concepts-of-data-versioning}

Data versioning is the practice of tracking, labeling, and managing
different states of a dataset over time. Just as software evolves
through versions, datasets evolve through corrections, additions, and
reprocessing. Versioning ensures reproducibility, accountability, and
clarity in collaborative projects.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-290}

Think of writing a book. Draft 1 is messy, Draft 2 fixes typos, Draft 3
adds new chapters. Without clear versioning, collaborators won't know
which draft is final. Datasets behave the same way---constantly updated,
and risky without explicit versions.

\subsubsection{Deep Dive}\label{deep-dive-290}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4261}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3739}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Versioning Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Snapshots & Immutable captures of data at a point in time & Census 2020
vs.~Census 2021 \\
Incremental Updates & Track only changes between versions & Daily log
additions \\
Branching \& Merging & Support parallel modifications and reconciliation
& Different teams labeling the same dataset \\
Semantic Versioning & Encode meaning into version numbers & v1.2 =
bugfix, v2.0 = schema change \\
Lineage Links & Connect derived datasets to their sources & Aggregated
sales data from raw transactions \\
\end{longtable}

Good versioning allows experiments to be replicated years later, ensures
fairness in benchmarking, and prevents confusion in regulated domains
where auditability is required.

Why It Matters Without versioning, two teams may train on slightly
different datasets without realizing it, leading to irreproducible
results. In healthcare or finance, untracked changes could even
invalidate compliance. Versioning is not only technical hygiene but also
scientific integrity.

\subsubsection{Tiny Code}\label{tiny-code-268}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset\_v1 }\OperatorTok{=}\NormalTok{ load\_dataset(}\StringTok{"sales\_data"}\NormalTok{, version}\OperatorTok{=}\StringTok{"1.0"}\NormalTok{)}
\NormalTok{dataset\_v2 }\OperatorTok{=}\NormalTok{ load\_dataset(}\StringTok{"sales\_data"}\NormalTok{, version}\OperatorTok{=}\StringTok{"2.0"}\NormalTok{)}

\CommentTok{\# Explicit versioning avoids silent mismatches}
\end{Highlighting}
\end{Shaded}

This ensures consistency by referencing dataset versions explicitly.

\subsubsection{Try It Yourself}\label{try-it-yourself-290}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Design a versioning scheme (semantic or date-based) for a streaming
  dataset.
\item
  Compare risks of unversioned data in research vs.~production.
\item
  Propose how versioning could integrate with model reproducibility in
  ML pipelines.
\end{enumerate}

\subsection{292. Git-like Systems for
Data}\label{git-like-systems-for-data}

Git-like systems for data bring version control concepts from software
engineering into dataset management. Instead of treating data as static
files, these systems allow branching, merging, and commit history,
making collaboration and experimentation reproducible.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-291}

Imagine a team of authors co-writing a novel. Each works on different
chapters, later merging them into a unified draft. Conflicts are
resolved, and every change is tracked. Git does this for code, and
Git-like systems extend the same discipline to data.

\subsubsection{Deep Dive}\label{deep-dive-291}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2417}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3417}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4167}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Feature
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example in Data Context
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Commits & Record each change with metadata & Adding 1,000 new rows \\
Branches & Parallel workstreams for experimentation & Creating a branch
to test new labels \\
Merges & Combine branches with conflict resolution & Reconciling two
different data-cleaning strategies \\
Diffs & Identify changes between versions & Comparing schema
modifications \\
Distributed Collaboration & Allow teams to contribute independently &
Multiple labs curating shared benchmark \\
\end{longtable}

Systems like these enable collaborative dataset development,
reproducible pipelines, and audit trails of changes.

Why It Matters Traditional file storage hides data evolution. Without
history, teams risk overwriting each other's work or losing the ability
to reproduce experiments. Git-like systems enforce structure,
accountability, and trust---critical for research, regulated industries,
and shared benchmarks.

\subsubsection{Tiny Code}\label{tiny-code-269}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example commit workflow for data}
\NormalTok{repo.init(}\StringTok{"customer\_data"}\NormalTok{)}
\NormalTok{repo.commit(}\StringTok{"Initial load of Q1 data"}\NormalTok{)}
\NormalTok{repo.branch(}\StringTok{"cleaning\_experiment"}\NormalTok{)}
\NormalTok{repo.commit(}\StringTok{"Removed null values from address field"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This shows data tracked like source code, with commits and branches.

\subsubsection{Try It Yourself}\label{try-it-yourself-291}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Propose how branching could be used for experimenting with different
  preprocessing strategies.
\item
  Compare diffs of two dataset versions and identify potential
  conflicts.
\item
  Debate challenges of scaling Git-like systems to terabyte-scale
  datasets.
\end{enumerate}

\subsection{293. Lineage Tracking: Provenance
Graphs}\label{lineage-tracking-provenance-graphs}

Lineage tracking records the origin and transformation history of data,
creating a ``provenance graph'' that shows how each dataset version was
derived. This ensures transparency, reproducibility, and accountability
in complex pipelines.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-292}

Imagine a family tree. Each person is connected to parents and
grandparents, showing ancestry. Provenance graphs work the same way,
tracing every dataset back to its raw sources and the transformations
applied along the way.

\subsubsection{Deep Dive}\label{deep-dive-292}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2400}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3900}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3700}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Element
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Role
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Source Nodes & Original data inputs & Raw transaction logs \\
Transformation Nodes & Processing steps applied & Aggregation,
filtering, normalization \\
Derived Datasets & Outputs of transformations & Monthly sales
summaries \\
Edges & Relationships linking inputs to outputs & ``Cleaned data derived
from raw logs'' \\
\end{longtable}

Lineage tracking can be visualized as a directed acyclic graph (DAG)
that maps dependencies across datasets. It helps with debugging,
auditing, and understanding how errors or biases propagate through
pipelines.

Why It Matters Without lineage, it is difficult to answer: \emph{Where
did this number come from?} In regulated industries, being unable to
prove provenance can invalidate results. Lineage graphs also make
collaboration easier, as teams see exactly which steps led to a dataset.

\subsubsection{Tiny Code}\label{tiny-code-270}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lineage }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"raw\_logs"}\NormalTok{: [],}
    \StringTok{"cleaned\_logs"}\NormalTok{: [}\StringTok{"raw\_logs"}\NormalTok{],}
    \StringTok{"monthly\_summary"}\NormalTok{: [}\StringTok{"cleaned\_logs"}\NormalTok{]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This simple structure encodes dependencies between dataset versions.

\subsubsection{Try It Yourself}\label{try-it-yourself-292}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Draw a provenance graph for a machine learning pipeline from raw data
  to model predictions.
\item
  Propose how lineage tracking could detect error propagation in
  financial reporting.
\item
  Debate whether lineage tracking should be mandatory for all datasets
  in healthcare research.
\end{enumerate}

\subsection{294. Reproducibility with Data
Snapshots}\label{reproducibility-with-data-snapshots}

Data snapshots are immutable captures of a dataset at a given point in
time. They allow experiments, analyses, or models to be reproduced
exactly, even years later, regardless of ongoing changes to the original
data source.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-293}

Think of taking a photograph of a landscape. The scenery may change with
seasons, but the photo preserves the exact state forever. A data
snapshot does the same, freezing the dataset in its original form for
reliable future reference.

\subsubsection{Deep Dive}\label{deep-dive-293}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1538}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4327}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4135}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Immutability & Prevents accidental or intentional edits & Archived
snapshot of 2023 census data \\
Timestamping & Captures exact point in time & Financial transactions as
of March 31, 2025 \\
Storage & Preserves frozen copy, often in object stores & Parquet files
versioned by date \\
Linking & Associated with experiments or publications & Paper cites
dataset snapshot DOI \\
\end{longtable}

Snapshots complement versioning by ensuring reproducibility of
experiments. Even if the ``live'' dataset evolves, researchers can
always go back to the frozen version.

Why It Matters Without snapshots, claims cannot be verified, and
experiments cannot be reproduced. A small change in training data can
alter results, breaking trust in science and industry. Snapshots provide
a stable ground truth for auditing, validation, and regulatory
compliance.

\subsubsection{Tiny Code}\label{tiny-code-271}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ create\_snapshot(dataset, version, storage):}
\NormalTok{    path }\OperatorTok{=} \SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{storage}\SpecialCharTok{\}}\SpecialStringTok{/}\SpecialCharTok{\{}\NormalTok{dataset}\SpecialCharTok{\}}\SpecialStringTok{\_v}\SpecialCharTok{\{}\NormalTok{version}\SpecialCharTok{\}}\SpecialStringTok{.parquet"}
\NormalTok{    save(dataset, path)}
    \ControlFlowTok{return}\NormalTok{ path}

\NormalTok{snapshot }\OperatorTok{=}\NormalTok{ create\_snapshot(}\StringTok{"customer\_data"}\NormalTok{, }\StringTok{"2025{-}03{-}01"}\NormalTok{, }\StringTok{"/archive"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This sketch shows how a dataset snapshot could be stored with explicit
versioning.

\subsubsection{Try It Yourself}\label{try-it-yourself-293}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a snapshot of a dataset and use it to reproduce an experiment
  six months later.
\item
  Debate the storage and cost tradeoffs of snapshotting large-scale
  datasets.
\item
  Propose a system for citing dataset snapshots in academic
  publications.
\end{enumerate}

\subsection{295. Immutable vs.~Mutable
Storage}\label{immutable-vs.-mutable-storage}

Data can be stored in immutable or mutable forms. Immutable storage
preserves every version without alteration, while mutable storage allows
edits and overwrites. The choice affects reproducibility, auditability,
and efficiency.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-294}

Think of a diary vs.~a whiteboard. A diary records entries permanently,
each page capturing a moment in time. A whiteboard can be erased and
rewritten, showing only the latest version. Immutable and mutable
storage mirror these two approaches.

\subsubsection{Deep Dive}\label{deep-dive-294}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1161}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2054}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3571}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3214}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Storage Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Characteristics
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Benefits
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Drawbacks
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Immutable & Write-once, append-only & Guarantees reproducibility, full
history & Higher storage costs, slower updates \\
Mutable & Overwrites allowed & Saves space, efficient for corrections &
Loses history, harder to audit \\
Hybrid & Combines both & Mutable staging, immutable archival & Added
system complexity \\
\end{longtable}

Immutable storage is common in regulatory settings, where tamper-proof
audit logs are required. Mutable storage suits fast-changing systems,
like transactional databases. Hybrids are often used: mutable for
working datasets, immutable for compliance snapshots.

Why It Matters If history is lost through mutable updates, experiments
and audits cannot be reliably reproduced. Conversely, keeping everything
immutable can be expensive and inefficient. Choosing the right balance
ensures both integrity and practicality.

\subsubsection{Tiny Code}\label{tiny-code-272}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ ImmutableStore:}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{):}
        \VariableTok{self}\NormalTok{.store }\OperatorTok{=}\NormalTok{ \{\}}
    \KeywordTok{def}\NormalTok{ write(}\VariableTok{self}\NormalTok{, key, value):}
\NormalTok{        version }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.store.get(key, [])) }\OperatorTok{+} \DecValTok{1}
        \VariableTok{self}\NormalTok{.store.setdefault(key, []).append((version, value))}
        \ControlFlowTok{return}\NormalTok{ version}
\end{Highlighting}
\end{Shaded}

This sketch shows an append-only design where each write creates a new
version.

\subsubsection{Try It Yourself}\label{try-it-yourself-294}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compare immutable vs.~mutable storage for a financial ledger. Which is
  safer, and why?
\item
  Propose a hybrid strategy for managing machine learning training data.
\item
  Debate whether cloud providers should offer immutable storage by
  default.
\end{enumerate}

\subsection{296. Lineage in Streaming
vs.~Batch}\label{lineage-in-streaming-vs.-batch}

Lineage in batch processing tracks how datasets are created through
discrete jobs, while in streaming systems it must capture
transformations in real time. Both ensure transparency, but streaming
adds challenges of scale, latency, and continuous updates.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-295}

Imagine cooking. In batch mode, you prepare all ingredients, cook them
at once, and serve a finished dish---you can trace every step. In
streaming, ingredients arrive continuously, and you must cook on the fly
while keeping track of where each piece came from.

\subsubsection{Deep Dive}\label{deep-dive-295}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0747}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3046}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3563}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2644}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Mode
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Lineage Tracking Style
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Challenge
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Batch & Logs transformations per job & ETL pipeline producing monthly
sales reports & Easy to snapshot but less frequent updates \\
Streaming & Records lineage per event/message & Real-time fraud
detection with Kafka streams & High throughput, requires low-latency
metadata \\
Hybrid & Combines streaming ingestion with batch consolidation &
Clickstream logs processed in real time and summarized nightly &
Synchronization across modes \\
\end{longtable}

Batch lineage often uses job metadata, while streaming requires
fine-grained tracking---event IDs, timestamps, and transformation
chains. Provenance may be maintained with lightweight logs or DAGs
updated continuously.

Why It Matters Inaccurate lineage breaks trust. In batch pipelines,
errors can usually be traced back after the fact. In streaming, errors
propagate instantly, making real-time lineage critical for debugging,
auditing, and compliance in domains like finance and healthcare.

\subsubsection{Tiny Code}\label{tiny-code-273}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ track\_lineage(event\_id, source, transformation):}
    \ControlFlowTok{return}\NormalTok{ \{}
        \StringTok{"event\_id"}\NormalTok{: event\_id,}
        \StringTok{"source"}\NormalTok{: source,}
        \StringTok{"transformation"}\NormalTok{: transformation}
\NormalTok{    \}}

\NormalTok{lineage\_record }\OperatorTok{=}\NormalTok{ track\_lineage(}\StringTok{"txn123"}\NormalTok{, }\StringTok{"raw\_stream"}\NormalTok{, }\StringTok{"filter\_high\_value"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This sketch records provenance for a single streaming event.

\subsubsection{Try It Yourself}\label{try-it-yourself-295}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compare error tracing in a batch ETL pipeline vs.~a real-time fraud
  detection system.
\item
  Propose metadata that should be logged for each streaming event to
  ensure lineage.
\item
  Debate whether fine-grained lineage in streaming is worth the
  performance cost.
\end{enumerate}

\subsection{297. DataOps for Lifecycle
Management}\label{dataops-for-lifecycle-management}

DataOps applies DevOps principles to data pipelines, focusing on
automation, collaboration, and continuous delivery of reliable data. For
lifecycle management, it ensures that data moves smoothly from ingestion
to consumption while maintaining quality, security, and traceability.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-296}

Think of a factory assembly line. Raw materials enter one side, undergo
processing at each station, and emerge as finished goods. DataOps turns
data pipelines into well-managed assembly lines, with checks,
monitoring, and automation at every step.

\subsubsection{Deep Dive}\label{deep-dive-296}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2261}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4435}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3304}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Principle
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Application in Data Lifecycle
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Continuous Integration & Automated validation when data changes & Schema
checks on new batches \\
Continuous Delivery & Deploy updated data to consumers quickly &
Real-time dashboards refreshed hourly \\
Monitoring \& Feedback & Detect drift, errors, and failures & Alert on
missing records in daily load \\
Collaboration & Break silos between data engineers, scientists, ops &
Shared data catalogs and versioning \\
Automation & Orchestrate ingestion, cleaning, transformation & CI/CD
pipelines for data workflows \\
\end{longtable}

DataOps combines process discipline with technical tooling, making
pipelines robust and auditable. It embeds governance and lineage
tracking as integral parts of data delivery.

Why It Matters Without DataOps, pipelines become brittle---errors slip
through, fixes are manual, and collaboration slows. With DataOps, data
becomes a reliable product: versioned, monitored, and continuously
improved. This is essential for scaling AI and analytics in production.

\subsubsection{Tiny Code}\label{tiny-code-274}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ data\_pipeline():}
\NormalTok{    validate\_schema()}
\NormalTok{    clean\_data()}
\NormalTok{    transform()}
\NormalTok{    load\_to\_warehouse()}
\NormalTok{    monitor\_quality()}
\end{Highlighting}
\end{Shaded}

A simplified pipeline sketch reflecting automated stages in DataOps.

\subsubsection{Try It Yourself}\label{try-it-yourself-296}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Map how DevOps concepts (CI/CD, monitoring) translate into DataOps
  practices.
\item
  Propose automation steps that reduce human error in data cleaning.
\item
  Debate whether DataOps should be a cultural shift (people + process)
  or primarily a tooling problem.
\end{enumerate}

\subsection{298. Governance and Audit of
Changes}\label{governance-and-audit-of-changes}

Governance ensures that all modifications to datasets are controlled,
documented, and aligned with organizational policies. Auditability
provides a trail of who changed what, when, and why. Together, they
bring accountability and trust to data management.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-297}

Imagine a financial ledger where every transaction is signed and
timestamped. Even if money moves through many accounts, each step is
traceable. Dataset governance works the same way---every update is
logged to prevent silent changes.

\subsubsection{Deep Dive}\label{deep-dive-297}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2131}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4016}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3852}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Change Control & Formal approval before altering critical datasets &
Manager approval before schema modification \\
Audit Trails & Record history of edits and access & Immutable logs of
patient record updates \\
Policy Enforcement & Align changes with compliance standards & Rejecting
uploads without consent documentation \\
Role-Based Permissions & Restrict who can make certain changes & Only
admins can delete records \\
Review \& Remediation & Periodic audits to detect anomalies & Quarterly
checks for unauthorized changes \\
\end{longtable}

Governance and auditing often rely on metadata systems, access controls,
and automated policy checks. They also require cultural practices:
change reviews, approvals, and accountability across teams.

Why It Matters Untracked or unauthorized changes can lead to broken
pipelines, compliance violations, or biased models. In regulated
industries, lacking audit logs can result in legal penalties. Governance
ensures reliability, while auditing enforces trust and transparency.

\subsubsection{Tiny Code}\label{tiny-code-275}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ log\_change(user, action, dataset, timestamp):}
\NormalTok{    entry }\OperatorTok{=} \SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{timestamp}\SpecialCharTok{\}}\SpecialStringTok{ | }\SpecialCharTok{\{}\NormalTok{user}\SpecialCharTok{\}}\SpecialStringTok{ | }\SpecialCharTok{\{}\NormalTok{action}\SpecialCharTok{\}}\SpecialStringTok{ | }\SpecialCharTok{\{}\NormalTok{dataset}\SpecialCharTok{\}}\CharTok{\textbackslash{}n}\SpecialStringTok{"}
    \ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\StringTok{"audit\_log.txt"}\NormalTok{, }\StringTok{"a"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{        f.write(entry)}
\end{Highlighting}
\end{Shaded}

This sketch captures a simple change log for dataset governance.

\subsubsection{Try It Yourself}\label{try-it-yourself-297}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Propose an audit trail design for tracking schema changes in a data
  warehouse.
\item
  Compare manual governance boards vs.~automated policy enforcement.
\item
  Debate whether audit logs should be immutable by default, even if
  storage costs rise.
\end{enumerate}

\subsection{299. Integration with ML
Pipelines}\label{integration-with-ml-pipelines}

Data versioning and lineage must integrate seamlessly into machine
learning (ML) pipelines. Each experiment should link models to the exact
data snapshot, transformations, and parameters used, ensuring that
results can be traced and reproduced.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-298}

Think of baking a cake. To reproduce it, you need not only the recipe
but also the exact ingredients from a specific batch. If the flour or
sugar changes, the outcome may differ. ML pipelines require the same
precision in tracking datasets.

\subsubsection{Deep Dive}\label{deep-dive-298}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2190}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4095}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3714}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Integration Point
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Data Ingestion & Capture version of input dataset & Model trained on
sales\_data v1.2 \\
Feature Engineering & Record transformations & Normalized age, one-hot
encoded country \\
Training & Link dataset snapshot to model artifacts & Model X trained on
March 2025 snapshot \\
Evaluation & Use consistent test dataset version & Test always on
benchmark v3.0 \\
Deployment & Monitor live data vs.~training distribution & Alert if
drift from v3.0 baseline \\
\end{longtable}

Tight integration avoids silent mismatches between model code and data.
Tools like pipelines, metadata stores, and experiment trackers can
enforce this automatically.

Why It Matters Without integration, it's impossible to know which
dataset produced which model. This breaks reproducibility, complicates
debugging, and risks compliance failures. By embedding data versioning
into pipelines, organizations ensure models remain trustworthy and
auditable.

\subsubsection{Tiny Code}\label{tiny-code-276}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{experiment }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"model\_id"}\NormalTok{: }\StringTok{"XGBoost\_v5"}\NormalTok{,}
    \StringTok{"train\_data"}\NormalTok{: }\StringTok{"sales\_data\_v1.2"}\NormalTok{,}
    \StringTok{"test\_data"}\NormalTok{: }\StringTok{"sales\_data\_v1.3"}\NormalTok{,}
    \StringTok{"features"}\NormalTok{: [}\StringTok{"age\_norm"}\NormalTok{, }\StringTok{"country\_onehot"}\NormalTok{]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This sketch records dataset versions and transformations tied to a model
experiment.

\subsubsection{Try It Yourself}\label{try-it-yourself-298}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Design a metadata schema linking dataset versions to trained models.
\item
  Propose a pipeline mechanism that prevents deploying models trained on
  outdated data.
\item
  Debate whether data versioning should be mandatory for publishing ML
  research.
\end{enumerate}

\subsection{300. Open Challenges in Data
Versioning}\label{open-challenges-in-data-versioning}

Despite progress in tools and practices, data versioning remains
difficult at scale. Challenges include handling massive datasets,
integrating with diverse pipelines, and balancing immutability with
efficiency. Open questions drive research into better systems for
tracking, storing, and governing evolving data.

\subsubsection{Picture in Your Head}\label{picture-in-your-head-299}

Imagine trying to keep every edition of every newspaper ever printed,
complete with corrections, supplements, and regional variations.
Managing dataset versions across organizations feels just as
overwhelming.

\subsubsection{Deep Dive}\label{deep-dive-299}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1417}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Challenge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Scale & Storing petabytes of versions is costly & Genomics datasets with
millions of samples \\
Granularity & Versioning entire datasets vs.~subsets or rows & Only 1\%
of records changed, but full snapshot stored \\
Integration & Linking versioning with ML, BI, and analytics tools &
Training pipelines unaware of version IDs \\
Collaboration & Managing concurrent edits by multiple teams & Conflicts
in feature engineering pipelines \\
Usability & Complexity of tools hinders adoption & Engineers default to
ad-hoc copies \\
Longevity & Ensuring decades-long reproducibility & Climate models
requiring multi-decade archives \\
\end{longtable}

Current approaches---Git-like systems, snapshots, and lineage
graphs---partially solve the problem but face tradeoffs between cost,
usability, and completeness.

\subsubsection{Why It Matters}\label{why-it-matters-98}

As AI grows data-hungry, versioning becomes a cornerstone of
reproducibility, governance, and trust. Without robust solutions,
research risks irreproducibility, and production systems risk silent
errors from mismatched data. Future innovation must tackle scalability,
automation, and standardization.

\subsubsection{Tiny Code}\label{tiny-code-277}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ version\_data(dataset, changes):}
    \CommentTok{\# naive approach: full copy per version}
\NormalTok{    version\_id }\OperatorTok{=} \BuiltInTok{hash}\NormalTok{(dataset }\OperatorTok{+} \BuiltInTok{str}\NormalTok{(changes))}
\NormalTok{    store[version\_id] }\OperatorTok{=}\NormalTok{ apply\_changes(dataset, changes)}
    \ControlFlowTok{return}\NormalTok{ version\_id}
\end{Highlighting}
\end{Shaded}

This simplistic approach highlights inefficiency---copying entire
datasets for minor updates.

\subsubsection{Try It Yourself}\label{try-it-yourself-299}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Propose storage-efficient strategies for versioning large datasets
  with minimal changes.
\item
  Debate whether global standards for dataset versioning should exist,
  like semantic versioning in software.
\item
  Identify domains (e.g., healthcare, climate science) where versioning
  challenges are most urgent and why.
\end{enumerate}




\end{document}
