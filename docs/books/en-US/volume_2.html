<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.23">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Volume 2. Mathematicial Foundations – The Little Book of Artificial Intelligence</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../books/en-US/volume_3.html" rel="next">
<link href="../../books/en-US/volume_1.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-1fe81d0376b2c50856e68e651e390326.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-27c261d06b905028a18691de25d09dde.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../books/en-US/volume_2.html"><span class="chapter-title">Volume 2. Mathematicial Foundations</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../../index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">The Little Book of Artificial Intelligence</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Contents</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-US/volume_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Volume 1. First principles of Artificial Intelligence</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-US/volume_2.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Volume 2. Mathematicial Foundations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-US/volume_3.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Volume 3. Data and Representation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-US/volume_4.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Volume 4. Search and Planning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-US/volume_5.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Volume 5. Logic and Knowledge</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-US/volume_6.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Volume 6. Probabilistic Modeling and Inference</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-US/volume_7.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Volume 7. Machine Learning Theory and Practice</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-US/volume_8.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Volume 8. Supervised Learning Systems</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-US/volume_9.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Volume 9. Unsupervised, self-supervised and representation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-US/volume_10.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Volume 10. Deep Learning Core</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-US/volume_11.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Volume 11. Large Language Models</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#chapter-11.-linear-algebra-for-representations" id="toc-chapter-11.-linear-algebra-for-representations" class="nav-link active" data-scroll-target="#chapter-11.-linear-algebra-for-representations">Chapter 11. Linear Algebra for Representations</a>
  <ul class="collapse">
  <li><a href="#scalars-vectors-and-matrices" id="toc-scalars-vectors-and-matrices" class="nav-link" data-scroll-target="#scalars-vectors-and-matrices">101. Scalars, Vectors, and Matrices</a></li>
  <li><a href="#vector-operations-and-norms" id="toc-vector-operations-and-norms" class="nav-link" data-scroll-target="#vector-operations-and-norms">102. Vector Operations and Norms</a></li>
  <li><a href="#matrix-multiplication-and-properties" id="toc-matrix-multiplication-and-properties" class="nav-link" data-scroll-target="#matrix-multiplication-and-properties">103. Matrix Multiplication and Properties</a></li>
  <li><a href="#linear-independence-and-span" id="toc-linear-independence-and-span" class="nav-link" data-scroll-target="#linear-independence-and-span">104. Linear Independence and Span</a></li>
  <li><a href="#rank-null-space-and-solutions-of-ax-b" id="toc-rank-null-space-and-solutions-of-ax-b" class="nav-link" data-scroll-target="#rank-null-space-and-solutions-of-ax-b">105. Rank, Null Space, and Solutions of Ax = b</a></li>
  <li><a href="#orthogonality-and-projections" id="toc-orthogonality-and-projections" class="nav-link" data-scroll-target="#orthogonality-and-projections">106. Orthogonality and Projections</a></li>
  <li><a href="#eigenvalues-and-eigenvectors" id="toc-eigenvalues-and-eigenvectors" class="nav-link" data-scroll-target="#eigenvalues-and-eigenvectors">107. Eigenvalues and Eigenvectors</a></li>
  <li><a href="#singular-value-decomposition-svd" id="toc-singular-value-decomposition-svd" class="nav-link" data-scroll-target="#singular-value-decomposition-svd">108. Singular Value Decomposition (SVD)</a></li>
  <li><a href="#tensors-and-higher-order-structures" id="toc-tensors-and-higher-order-structures" class="nav-link" data-scroll-target="#tensors-and-higher-order-structures">109. Tensors and Higher-Order Structures</a></li>
  <li><a href="#applications-in-ai-representations" id="toc-applications-in-ai-representations" class="nav-link" data-scroll-target="#applications-in-ai-representations">110. Applications in AI Representations</a></li>
  </ul></li>
  <li><a href="#chapter-12.-differential-and-integral-calculus" id="toc-chapter-12.-differential-and-integral-calculus" class="nav-link" data-scroll-target="#chapter-12.-differential-and-integral-calculus">Chapter 12. Differential and Integral Calculus</a>
  <ul class="collapse">
  <li><a href="#functions-limits-and-continuity" id="toc-functions-limits-and-continuity" class="nav-link" data-scroll-target="#functions-limits-and-continuity">111. Functions, Limits, and Continuity</a></li>
  <li><a href="#derivatives-and-gradients" id="toc-derivatives-and-gradients" class="nav-link" data-scroll-target="#derivatives-and-gradients">112. Derivatives and Gradients</a></li>
  <li><a href="#partial-derivatives-and-multivariable-calculus" id="toc-partial-derivatives-and-multivariable-calculus" class="nav-link" data-scroll-target="#partial-derivatives-and-multivariable-calculus">113. Partial Derivatives and Multivariable Calculus</a></li>
  <li><a href="#gradient-vectors-and-directional-derivatives" id="toc-gradient-vectors-and-directional-derivatives" class="nav-link" data-scroll-target="#gradient-vectors-and-directional-derivatives">114. Gradient Vectors and Directional Derivatives</a></li>
  <li><a href="#jacobians-and-hessians" id="toc-jacobians-and-hessians" class="nav-link" data-scroll-target="#jacobians-and-hessians">115. Jacobians and Hessians</a></li>
  <li><a href="#optimization-and-critical-points" id="toc-optimization-and-critical-points" class="nav-link" data-scroll-target="#optimization-and-critical-points">116. Optimization and Critical Points</a></li>
  <li><a href="#integrals-and-areas-under-curves" id="toc-integrals-and-areas-under-curves" class="nav-link" data-scroll-target="#integrals-and-areas-under-curves">117. Integrals and Areas under Curves</a></li>
  <li><a href="#multiple-integrals-and-volumes" id="toc-multiple-integrals-and-volumes" class="nav-link" data-scroll-target="#multiple-integrals-and-volumes">118. Multiple Integrals and Volumes</a></li>
  <li><a href="#differential-equations-basics" id="toc-differential-equations-basics" class="nav-link" data-scroll-target="#differential-equations-basics">119. Differential Equations Basics</a></li>
  <li><a href="#calculus-in-machine-learning-applications" id="toc-calculus-in-machine-learning-applications" class="nav-link" data-scroll-target="#calculus-in-machine-learning-applications">120. Calculus in Machine Learning Applications</a></li>
  </ul></li>
  <li><a href="#chapter-13.-probability-theory-fundamentals" id="toc-chapter-13.-probability-theory-fundamentals" class="nav-link" data-scroll-target="#chapter-13.-probability-theory-fundamentals">Chapter 13. Probability Theory Fundamentals</a>
  <ul class="collapse">
  <li><a href="#probability-axioms-and-sample-spaces" id="toc-probability-axioms-and-sample-spaces" class="nav-link" data-scroll-target="#probability-axioms-and-sample-spaces">121. Probability Axioms and Sample Spaces</a></li>
  <li><a href="#random-variables-and-distributions" id="toc-random-variables-and-distributions" class="nav-link" data-scroll-target="#random-variables-and-distributions">122. Random Variables and Distributions</a></li>
  <li><a href="#expectation-variance-and-moments" id="toc-expectation-variance-and-moments" class="nav-link" data-scroll-target="#expectation-variance-and-moments">123. Expectation, Variance, and Moments</a></li>
  <li><a href="#common-distributions-bernoulli-binomial-gaussian" id="toc-common-distributions-bernoulli-binomial-gaussian" class="nav-link" data-scroll-target="#common-distributions-bernoulli-binomial-gaussian">124. Common Distributions (Bernoulli, Binomial, Gaussian)</a></li>
  <li><a href="#joint-marginal-and-conditional-probability" id="toc-joint-marginal-and-conditional-probability" class="nav-link" data-scroll-target="#joint-marginal-and-conditional-probability">125. Joint, Marginal, and Conditional Probability</a></li>
  <li><a href="#independence-and-correlation" id="toc-independence-and-correlation" class="nav-link" data-scroll-target="#independence-and-correlation">126. Independence and Correlation</a></li>
  <li><a href="#law-of-large-numbers" id="toc-law-of-large-numbers" class="nav-link" data-scroll-target="#law-of-large-numbers">127. Law of Large Numbers</a></li>
  <li><a href="#central-limit-theorem" id="toc-central-limit-theorem" class="nav-link" data-scroll-target="#central-limit-theorem">128. Central Limit Theorem</a></li>
  <li><a href="#bayes-theorem-and-conditional-inference" id="toc-bayes-theorem-and-conditional-inference" class="nav-link" data-scroll-target="#bayes-theorem-and-conditional-inference">129. Bayes’ Theorem and Conditional Inference</a></li>
  <li><a href="#probabilistic-models-in-ai" id="toc-probabilistic-models-in-ai" class="nav-link" data-scroll-target="#probabilistic-models-in-ai">130. Probabilistic Models in AI</a></li>
  </ul></li>
  <li><a href="#chapter-14.-statistics-and-estimation" id="toc-chapter-14.-statistics-and-estimation" class="nav-link" data-scroll-target="#chapter-14.-statistics-and-estimation">Chapter 14. Statistics and Estimation</a>
  <ul class="collapse">
  <li><a href="#descriptive-statistics-and-summaries" id="toc-descriptive-statistics-and-summaries" class="nav-link" data-scroll-target="#descriptive-statistics-and-summaries">131. Descriptive Statistics and Summaries</a></li>
  <li><a href="#sampling-distributions" id="toc-sampling-distributions" class="nav-link" data-scroll-target="#sampling-distributions">132. Sampling Distributions</a></li>
  <li><a href="#point-estimation-and-properties" id="toc-point-estimation-and-properties" class="nav-link" data-scroll-target="#point-estimation-and-properties">133. Point Estimation and Properties</a></li>
  <li><a href="#maximum-likelihood-estimation-mle" id="toc-maximum-likelihood-estimation-mle" class="nav-link" data-scroll-target="#maximum-likelihood-estimation-mle">134. Maximum Likelihood Estimation (MLE)</a></li>
  <li><a href="#confidence-intervals" id="toc-confidence-intervals" class="nav-link" data-scroll-target="#confidence-intervals">135. Confidence Intervals</a></li>
  <li><a href="#hypothesis-testing" id="toc-hypothesis-testing" class="nav-link" data-scroll-target="#hypothesis-testing">136. Hypothesis Testing</a></li>
  <li><a href="#bayesian-estimation" id="toc-bayesian-estimation" class="nav-link" data-scroll-target="#bayesian-estimation">137. Bayesian Estimation</a></li>
  <li><a href="#resampling-methods-bootstrap-jackknife" id="toc-resampling-methods-bootstrap-jackknife" class="nav-link" data-scroll-target="#resampling-methods-bootstrap-jackknife">138. Resampling Methods (Bootstrap, Jackknife)</a></li>
  <li><a href="#statistical-significance-and-p-values" id="toc-statistical-significance-and-p-values" class="nav-link" data-scroll-target="#statistical-significance-and-p-values">139. Statistical Significance and p-Values</a></li>
  <li><a href="#applications-in-data-driven-ai" id="toc-applications-in-data-driven-ai" class="nav-link" data-scroll-target="#applications-in-data-driven-ai">140. Applications in Data-Driven AI</a></li>
  </ul></li>
  <li><a href="#chapter-15.-optimization-and-convex-analysis" id="toc-chapter-15.-optimization-and-convex-analysis" class="nav-link" data-scroll-target="#chapter-15.-optimization-and-convex-analysis">Chapter 15. Optimization and convex analysis</a>
  <ul class="collapse">
  <li><a href="#optimization-problem-formulation" id="toc-optimization-problem-formulation" class="nav-link" data-scroll-target="#optimization-problem-formulation">141. Optimization Problem Formulation</a></li>
  <li><a href="#convex-sets-and-convex-functions" id="toc-convex-sets-and-convex-functions" class="nav-link" data-scroll-target="#convex-sets-and-convex-functions">142. Convex Sets and Convex Functions</a></li>
  <li><a href="#gradient-descent-and-variants" id="toc-gradient-descent-and-variants" class="nav-link" data-scroll-target="#gradient-descent-and-variants">143. Gradient Descent and Variants</a></li>
  <li><a href="#constrained-optimization-and-lagrange-multipliers" id="toc-constrained-optimization-and-lagrange-multipliers" class="nav-link" data-scroll-target="#constrained-optimization-and-lagrange-multipliers">144. Constrained Optimization and Lagrange Multipliers</a></li>
  <li><a href="#duality-in-optimization" id="toc-duality-in-optimization" class="nav-link" data-scroll-target="#duality-in-optimization">145. Duality in Optimization</a></li>
  <li><a href="#convex-optimization-algorithms-interior-point-etc." id="toc-convex-optimization-algorithms-interior-point-etc." class="nav-link" data-scroll-target="#convex-optimization-algorithms-interior-point-etc.">146. Convex Optimization Algorithms (Interior Point, etc.)</a></li>
  <li><a href="#non-convex-optimization-challenges" id="toc-non-convex-optimization-challenges" class="nav-link" data-scroll-target="#non-convex-optimization-challenges">147. Non-Convex Optimization Challenges</a></li>
  <li><a href="#stochastic-optimization" id="toc-stochastic-optimization" class="nav-link" data-scroll-target="#stochastic-optimization">148. Stochastic Optimization</a></li>
  <li><a href="#optimization-in-high-dimensions" id="toc-optimization-in-high-dimensions" class="nav-link" data-scroll-target="#optimization-in-high-dimensions">149. Optimization in High Dimensions</a></li>
  <li><a href="#applications-in-ml-training" id="toc-applications-in-ml-training" class="nav-link" data-scroll-target="#applications-in-ml-training">150. Applications in ML Training</a></li>
  </ul></li>
  <li><a href="#chapter-16.-numerical-methods-and-stability" id="toc-chapter-16.-numerical-methods-and-stability" class="nav-link" data-scroll-target="#chapter-16.-numerical-methods-and-stability">Chapter 16. Numerical methods and stability</a>
  <ul class="collapse">
  <li><a href="#numerical-representation-and-rounding-errors" id="toc-numerical-representation-and-rounding-errors" class="nav-link" data-scroll-target="#numerical-representation-and-rounding-errors">151. Numerical Representation and Rounding Errors</a></li>
  <li><a href="#root-finding-methods-newton-raphson-bisection" id="toc-root-finding-methods-newton-raphson-bisection" class="nav-link" data-scroll-target="#root-finding-methods-newton-raphson-bisection">152. Root-Finding Methods (Newton-Raphson, Bisection)</a></li>
  <li><a href="#numerical-linear-algebra-lu-qr-decomposition" id="toc-numerical-linear-algebra-lu-qr-decomposition" class="nav-link" data-scroll-target="#numerical-linear-algebra-lu-qr-decomposition">153. Numerical Linear Algebra (LU, QR Decomposition)</a></li>
  <li><a href="#iterative-methods-for-linear-systems" id="toc-iterative-methods-for-linear-systems" class="nav-link" data-scroll-target="#iterative-methods-for-linear-systems">154. Iterative Methods for Linear Systems</a></li>
  <li><a href="#numerical-differentiation-and-integration" id="toc-numerical-differentiation-and-integration" class="nav-link" data-scroll-target="#numerical-differentiation-and-integration">155. Numerical Differentiation and Integration</a></li>
  <li><a href="#stability-and-conditioning-of-problems" id="toc-stability-and-conditioning-of-problems" class="nav-link" data-scroll-target="#stability-and-conditioning-of-problems">156. Stability and Conditioning of Problems</a></li>
  <li><a href="#floating-point-arithmetic-and-precision" id="toc-floating-point-arithmetic-and-precision" class="nav-link" data-scroll-target="#floating-point-arithmetic-and-precision">157. Floating-Point Arithmetic and Precision</a></li>
  <li><a href="#monte-carlo-methods" id="toc-monte-carlo-methods" class="nav-link" data-scroll-target="#monte-carlo-methods">158. Monte Carlo Methods</a></li>
  <li><a href="#error-propagation-and-analysis" id="toc-error-propagation-and-analysis" class="nav-link" data-scroll-target="#error-propagation-and-analysis">159. Error Propagation and Analysis</a></li>
  <li><a href="#numerical-methods-in-ai-systems" id="toc-numerical-methods-in-ai-systems" class="nav-link" data-scroll-target="#numerical-methods-in-ai-systems">160. Numerical Methods in AI Systems</a></li>
  </ul></li>
  <li><a href="#chapter-17.-information-theory" id="toc-chapter-17.-information-theory" class="nav-link" data-scroll-target="#chapter-17.-information-theory">Chapter 17. Information Theory</a>
  <ul class="collapse">
  <li><a href="#entropy-and-information-content" id="toc-entropy-and-information-content" class="nav-link" data-scroll-target="#entropy-and-information-content">161. Entropy and Information Content</a></li>
  <li><a href="#joint-and-conditional-entropy" id="toc-joint-and-conditional-entropy" class="nav-link" data-scroll-target="#joint-and-conditional-entropy">162. Joint and Conditional Entropy</a></li>
  <li><a href="#mutual-information" id="toc-mutual-information" class="nav-link" data-scroll-target="#mutual-information">163. Mutual Information</a></li>
  <li><a href="#kullbackleibler-divergence" id="toc-kullbackleibler-divergence" class="nav-link" data-scroll-target="#kullbackleibler-divergence">164. Kullback–Leibler Divergence</a></li>
  <li><a href="#cross-entropy-and-likelihood" id="toc-cross-entropy-and-likelihood" class="nav-link" data-scroll-target="#cross-entropy-and-likelihood">165. Cross-Entropy and Likelihood</a></li>
  <li><a href="#channel-capacity-and-coding-theorems" id="toc-channel-capacity-and-coding-theorems" class="nav-link" data-scroll-target="#channel-capacity-and-coding-theorems">166. Channel Capacity and Coding Theorems</a></li>
  <li><a href="#ratedistortion-theory" id="toc-ratedistortion-theory" class="nav-link" data-scroll-target="#ratedistortion-theory">167. Rate–Distortion Theory</a></li>
  <li><a href="#information-bottleneck-principle" id="toc-information-bottleneck-principle" class="nav-link" data-scroll-target="#information-bottleneck-principle">168. Information Bottleneck Principle</a></li>
  <li><a href="#minimum-description-length-mdl" id="toc-minimum-description-length-mdl" class="nav-link" data-scroll-target="#minimum-description-length-mdl">169. Minimum Description Length (MDL)</a></li>
  <li><a href="#applications-in-machine-learning" id="toc-applications-in-machine-learning" class="nav-link" data-scroll-target="#applications-in-machine-learning">170. Applications in Machine Learning</a></li>
  </ul></li>
  <li><a href="#chapter-18.-graphs-matrices-and-special-methods" id="toc-chapter-18.-graphs-matrices-and-special-methods" class="nav-link" data-scroll-target="#chapter-18.-graphs-matrices-and-special-methods">Chapter 18. Graphs, Matrices and Special Methods</a>
  <ul class="collapse">
  <li><a href="#graphs-nodes-edges-and-paths" id="toc-graphs-nodes-edges-and-paths" class="nav-link" data-scroll-target="#graphs-nodes-edges-and-paths">171. Graphs: Nodes, Edges, and Paths</a></li>
  <li><a href="#adjacency-and-incidence-matrices" id="toc-adjacency-and-incidence-matrices" class="nav-link" data-scroll-target="#adjacency-and-incidence-matrices">172. Adjacency and Incidence Matrices</a></li>
  <li><a href="#graph-traversals-dfs-bfs" id="toc-graph-traversals-dfs-bfs" class="nav-link" data-scroll-target="#graph-traversals-dfs-bfs">173. Graph Traversals (DFS, BFS)</a></li>
  <li><a href="#connectivity-and-components" id="toc-connectivity-and-components" class="nav-link" data-scroll-target="#connectivity-and-components">174. Connectivity and Components</a></li>
  <li><a href="#graph-laplacians" id="toc-graph-laplacians" class="nav-link" data-scroll-target="#graph-laplacians">175. Graph Laplacians</a></li>
  <li><a href="#spectral-decomposition-of-graphs" id="toc-spectral-decomposition-of-graphs" class="nav-link" data-scroll-target="#spectral-decomposition-of-graphs">176. Spectral Decomposition of Graphs</a></li>
  <li><a href="#eigenvalues-and-graph-partitioning" id="toc-eigenvalues-and-graph-partitioning" class="nav-link" data-scroll-target="#eigenvalues-and-graph-partitioning">177. Eigenvalues and Graph Partitioning</a></li>
  <li><a href="#random-walks-and-markov-chains-on-graphs" id="toc-random-walks-and-markov-chains-on-graphs" class="nav-link" data-scroll-target="#random-walks-and-markov-chains-on-graphs">178. Random Walks and Markov Chains on Graphs</a></li>
  <li><a href="#spectral-clustering" id="toc-spectral-clustering" class="nav-link" data-scroll-target="#spectral-clustering">179. Spectral Clustering</a></li>
  <li><a href="#graph-based-ai-applications" id="toc-graph-based-ai-applications" class="nav-link" data-scroll-target="#graph-based-ai-applications">180. Graph-Based AI Applications</a></li>
  </ul></li>
  <li><a href="#chapter-19.-logic-sets-and-proof-techniques" id="toc-chapter-19.-logic-sets-and-proof-techniques" class="nav-link" data-scroll-target="#chapter-19.-logic-sets-and-proof-techniques">Chapter 19. Logic, Sets and Proof Techniques</a>
  <ul class="collapse">
  <li><a href="#set-theory-fundamentals" id="toc-set-theory-fundamentals" class="nav-link" data-scroll-target="#set-theory-fundamentals">181. Set Theory Fundamentals</a></li>
  <li><a href="#relations-and-functions" id="toc-relations-and-functions" class="nav-link" data-scroll-target="#relations-and-functions">182. Relations and Functions</a></li>
  <li><a href="#propositional-logic" id="toc-propositional-logic" class="nav-link" data-scroll-target="#propositional-logic">183. Propositional Logic</a></li>
  <li><a href="#predicate-logic-and-quantifiers" id="toc-predicate-logic-and-quantifiers" class="nav-link" data-scroll-target="#predicate-logic-and-quantifiers">184. Predicate Logic and Quantifiers</a></li>
  <li><a href="#logical-inference-and-deduction" id="toc-logical-inference-and-deduction" class="nav-link" data-scroll-target="#logical-inference-and-deduction">185. Logical Inference and Deduction</a></li>
  <li><a href="#proof-techniques-direct-contradiction-induction" id="toc-proof-techniques-direct-contradiction-induction" class="nav-link" data-scroll-target="#proof-techniques-direct-contradiction-induction">186. Proof Techniques: Direct, Contradiction, Induction</a></li>
  <li><a href="#mathematical-induction-in-depth" id="toc-mathematical-induction-in-depth" class="nav-link" data-scroll-target="#mathematical-induction-in-depth">187. Mathematical Induction in Depth</a></li>
  <li><a href="#recursion-and-well-foundedness" id="toc-recursion-and-well-foundedness" class="nav-link" data-scroll-target="#recursion-and-well-foundedness">188. Recursion and Well-Foundedness</a></li>
  <li><a href="#formal-systems-and-completeness" id="toc-formal-systems-and-completeness" class="nav-link" data-scroll-target="#formal-systems-and-completeness">189. Formal Systems and Completeness</a></li>
  <li><a href="#logic-in-ai-reasoning-systems" id="toc-logic-in-ai-reasoning-systems" class="nav-link" data-scroll-target="#logic-in-ai-reasoning-systems">190. Logic in AI Reasoning Systems</a></li>
  </ul></li>
  <li><a href="#chapter-20.-stochastic-process-and-markov-chains" id="toc-chapter-20.-stochastic-process-and-markov-chains" class="nav-link" data-scroll-target="#chapter-20.-stochastic-process-and-markov-chains">Chapter 20. Stochastic Process and Markov chains</a>
  <ul class="collapse">
  <li><a href="#random-processes-and-sequences" id="toc-random-processes-and-sequences" class="nav-link" data-scroll-target="#random-processes-and-sequences">191. Random Processes and Sequences</a></li>
  <li><a href="#stationarity-and-ergodicity" id="toc-stationarity-and-ergodicity" class="nav-link" data-scroll-target="#stationarity-and-ergodicity">192. Stationarity and Ergodicity</a></li>
  <li><a href="#discrete-time-markov-chains" id="toc-discrete-time-markov-chains" class="nav-link" data-scroll-target="#discrete-time-markov-chains">193. Discrete-Time Markov Chains</a></li>
  <li><a href="#continuous-time-markov-processes" id="toc-continuous-time-markov-processes" class="nav-link" data-scroll-target="#continuous-time-markov-processes">194. Continuous-Time Markov Processes</a></li>
  <li><a href="#transition-matrices-and-probabilities" id="toc-transition-matrices-and-probabilities" class="nav-link" data-scroll-target="#transition-matrices-and-probabilities">195. Transition Matrices and Probabilities</a></li>
  <li><a href="#markov-property-and-memorylessness" id="toc-markov-property-and-memorylessness" class="nav-link" data-scroll-target="#markov-property-and-memorylessness">196. Markov Property and Memorylessness</a></li>
  <li><a href="#martingales-and-applications" id="toc-martingales-and-applications" class="nav-link" data-scroll-target="#martingales-and-applications">197. Martingales and Applications</a></li>
  <li><a href="#hidden-markov-models" id="toc-hidden-markov-models" class="nav-link" data-scroll-target="#hidden-markov-models">198. Hidden Markov Models</a></li>
  <li><a href="#stochastic-differential-equations" id="toc-stochastic-differential-equations" class="nav-link" data-scroll-target="#stochastic-differential-equations">199. Stochastic Differential Equations</a></li>
  <li><a href="#monte-carlo-methods-1" id="toc-monte-carlo-methods-1" class="nav-link" data-scroll-target="#monte-carlo-methods-1">200. Monte Carlo Methods</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Volume 2. Mathematicial Foundations</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="chapter-11.-linear-algebra-for-representations" class="level2">
<h2 class="anchored" data-anchor-id="chapter-11.-linear-algebra-for-representations">Chapter 11. Linear Algebra for Representations</h2>
<section id="scalars-vectors-and-matrices" class="level3">
<h3 class="anchored" data-anchor-id="scalars-vectors-and-matrices">101. Scalars, Vectors, and Matrices</h3>
<p>At the foundation of AI mathematics are three objects: scalars, vectors, and matrices. A scalar is a single number. A vector is an ordered list of numbers, representing direction and magnitude in space. A matrix is a rectangular grid of numbers, capable of transforming vectors and encoding relationships. These are the raw building blocks for almost every algorithm in AI, from linear regression to deep neural networks.</p>
<section id="picture-in-your-head" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head">Picture in Your Head</h4>
<p>Imagine scalars as simple dots on a number line. A vector is like an arrow pointing from the origin in a plane or space, with both length and direction. A matrix is a whole system of arrows: a transformation machine that can rotate, stretch, or compress the space around it. In AI, data points are vectors, and learning often comes down to finding the right matrices to transform them.</p>
</section>
<section id="deep-dive" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive">Deep Dive</h4>
<p>Scalars are elements of the real (ℝ) or complex (ℂ) number systems. They describe quantities such as weights, probabilities, or losses. Vectors extend this by grouping scalars into n-dimensional objects. A vector x ∈ ℝⁿ can encode features of a data sample (age, height, income). Operations like dot products measure similarity, and norms measure magnitude. Matrices generalize further: an m×n matrix holds m rows and n columns. Multiplying a vector by a matrix performs a linear transformation. In AI, these transformations express learned parameters—weights in neural networks, transition probabilities in Markov models, or coefficients in regression.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 14%">
<col style="width: 65%">
</colgroup>
<thead>
<tr class="header">
<th>Object</th>
<th>Symbol</th>
<th>Dimension</th>
<th>Example in AI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Scalar</td>
<td><em>a</em></td>
<td>1×1</td>
<td>Learning rate, single probability</td>
</tr>
<tr class="even">
<td>Vector</td>
<td>x</td>
<td>n×1</td>
<td>Feature vector (e.g., pixel intensities)</td>
</tr>
<tr class="odd">
<td>Matrix</td>
<td>W</td>
<td>m×n</td>
<td>Neural network weights, adjacency matrix</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Scalar</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> <span class="fl">3.14</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Vector</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrix</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">2</span>, <span class="dv">3</span>,  <span class="dv">4</span>]])</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Operations</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>dot_product <span class="op">=</span> np.dot(x, x)         <span class="co"># 1*1 + 2*2 + 3*3 = 14</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>transformed <span class="op">=</span> np.dot(W, x)         <span class="co"># matrix-vector multiplication</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>norm <span class="op">=</span> np.linalg.norm(x)           <span class="co"># vector magnitude</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Scalar:"</span>, a)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Vector:"</span>, x)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Matrix:</span><span class="ch">\n</span><span class="st">"</span>, W)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Dot product:"</span>, dot_product)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Transformed:"</span>, transformed)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Norm:"</span>, norm)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="try-it-yourself" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself">Try It Yourself</h4>
<ol type="1">
<li><p>Take the vector x = [4, 3]. What is its norm? (Hint: √(4²+3²))</p></li>
<li><p>Multiply the matrix</p>
<p><span class="math display">\[
A = \begin{bmatrix}2 &amp; 0 \\ 0 &amp; 2\end{bmatrix}
\]</span></p>
<p>by x = [1, 1]. What does the result look like?</p></li>
</ol>
</section>
</section>
<section id="vector-operations-and-norms" class="level3">
<h3 class="anchored" data-anchor-id="vector-operations-and-norms">102. Vector Operations and Norms</h3>
<p>Vectors are not just lists of numbers; they are objects on which we define operations. Adding and scaling vectors lets us move and stretch directions in space. Dot products measure similarity, while norms measure size. These operations form the foundation of geometry and distance in machine learning.</p>
<section id="picture-in-your-head-1" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-1">Picture in Your Head</h4>
<p>Picture two arrows drawn from the origin. Adding them means placing one arrow’s tail at the other’s head, forming a diagonal. Scaling a vector stretches or shrinks its arrow. The dot product measures how aligned two arrows are: large if they point in the same direction, zero if they’re perpendicular, negative if they point opposite. A norm is simply the length of the arrow.</p>
</section>
<section id="deep-dive-1" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-1">Deep Dive</h4>
<p>Vector addition: x + y = [x₁ + y₁, …, xₙ + yₙ]. Scalar multiplication: a·x = [a·x₁, …, a·xₙ]. Dot product: x·y = Σ xᵢyᵢ, capturing both length and alignment. Norms:</p>
<ul>
<li>L2 norm: ‖x‖₂ = √(Σ xᵢ²), the Euclidean length.</li>
<li>L1 norm: ‖x‖₁ = Σ |xᵢ|, often used for sparsity.</li>
<li>L∞ norm: max |xᵢ|, measuring the largest component.</li>
</ul>
<p>In AI, norms define distances for clustering, regularization penalties, and robustness to perturbations.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 12%">
<col style="width: 33%">
<col style="width: 0%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Operation</th>
<th>Formula</th>
<th>Interpretation in AI</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Addition</td>
<td>x + y</td>
<td>Combining features</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Scalar multiplication</td>
<td>a·x</td>
<td>Scaling magnitude</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Dot product</td>
<td>x·y = ‖x‖‖y‖cosθ</td>
<td>Similarity / projection</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>L2 norm</td>
<td>√(Σ xᵢ²)</td>
<td>Standard distance, used in Euclidean space</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>L1 norm</td>
<td>Σ</td>
<td>xᵢ</td>
<td></td>
<td>Promotes sparsity, robust to outliers</td>
</tr>
<tr class="even">
<td>L∞ norm</td>
<td>max</td>
<td>xᵢ</td>
<td></td>
<td>Worst-case deviation, adversarial robustness</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-1" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-1">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="dv">3</span>, <span class="dv">4</span>])</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Vector addition and scaling</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>sum_xy <span class="op">=</span> x <span class="op">+</span> y</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>scaled_x <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> x</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Dot product and norms</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>dot <span class="op">=</span> np.dot(x, y)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>l2 <span class="op">=</span> np.linalg.norm(x, <span class="dv">2</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>l1 <span class="op">=</span> np.linalg.norm(x, <span class="dv">1</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>linf <span class="op">=</span> np.linalg.norm(x, np.inf)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"x + y:"</span>, sum_xy)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"2 * x:"</span>, scaled_x)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Dot product:"</span>, dot)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"L2 norm:"</span>, l2)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"L1 norm:"</span>, l1)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"L∞ norm:"</span>, linf)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="try-it-yourself-1" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-1">Try It Yourself</h4>
<ol type="1">
<li>Compute the dot product of x = [1, 0] and y = [0, 1]. What does the result tell you?</li>
<li>Find the L2 norm of x = [5, 12].</li>
<li>Compare the L1 and L2 norms for x = [1, -1, 1, -1]. Which is larger, and why?</li>
</ol>
</section>
</section>
<section id="matrix-multiplication-and-properties" class="level3">
<h3 class="anchored" data-anchor-id="matrix-multiplication-and-properties">103. Matrix Multiplication and Properties</h3>
<p>Matrix multiplication is the central operation that ties linear algebra to AI. Multiplying a matrix by a vector applies a linear transformation: rotation, scaling, or projection. Multiplying two matrices composes transformations. Understanding how this works and what properties it preserves is essential for reasoning about model weights, layers, and data transformations.</p>
<section id="picture-in-your-head-2" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-2">Picture in Your Head</h4>
<p>Think of a matrix as a machine that takes an input arrow (vector) and outputs a new arrow. Applying one machine after another corresponds to multiplying matrices. If you rotate by 90° and then scale by 2, the combined effect is another matrix. The rows of the matrix act like filters, each producing a weighted combination of the input vector’s components.</p>
</section>
<section id="deep-dive-2" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-2">Deep Dive</h4>
<p>Given an m×n matrix A and an n×p matrix B, the product C = AB is an m×p matrix. Each entry is</p>
<p><span class="math display">\[
c_{ij} = \sum_{k=1}^{n} a_{ik} b_{kj}.
\]</span></p>
<p>Key properties:</p>
<ul>
<li>Associativity: (AB)C = A(BC)</li>
<li>Distributivity: A(B + C) = AB + AC</li>
<li>Non-commutativity: AB ≠ BA in general</li>
<li>Identity: AI = IA = A</li>
<li>Transpose rules: (AB)ᵀ = BᵀAᵀ</li>
</ul>
<p>In AI, matrix multiplication encodes layer operations: inputs × weights = activations. Batch processing is also matrix multiplication, where many vectors are transformed at once.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 22%">
<col style="width: 56%">
</colgroup>
<thead>
<tr class="header">
<th>Property</th>
<th>Formula</th>
<th>Meaning in AI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Associativity</td>
<td>(AB)C = A(BC)</td>
<td>Order of chaining layers doesn’t matter</td>
</tr>
<tr class="even">
<td>Distributivity</td>
<td>A(B+C) = AB + AC</td>
<td>Parallel transformations combine linearly</td>
</tr>
<tr class="odd">
<td>Non-commutative</td>
<td>AB ≠ BA</td>
<td>Order of layers matters</td>
</tr>
<tr class="even">
<td>Identity</td>
<td>AI = IA = A</td>
<td>No transformation applied</td>
</tr>
<tr class="odd">
<td>Transpose rule</td>
<td>(AB)ᵀ = BᵀAᵀ</td>
<td>Useful for gradients/backprop</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-2" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-2">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define matrices</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">2</span>],</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">3</span>, <span class="dv">4</span>]])</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">0</span>]])</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrix-vector multiplication</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>Ax <span class="op">=</span> np.dot(A, x)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrix-matrix multiplication</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>AB <span class="op">=</span> np.dot(A, B)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Properties</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>assoc <span class="op">=</span> np.allclose(np.dot(np.dot(A, B), A), np.dot(A, np.dot(B, A)))</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"A @ x ="</span>, Ax)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"A @ B =</span><span class="ch">\n</span><span class="st">"</span>, AB)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Associativity holds?"</span>, assoc)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters">Why It Matters</h4>
<p>Matrix multiplication is the language of neural networks. Each layer’s parameters form a matrix that transforms input vectors into hidden representations. The non-commutativity explains why order of layers changes outcomes. Properties like associativity enable efficient computation, and transpose rules are the backbone of backpropagation. Without mastering matrix multiplication, it is impossible to understand how AI models propagate signals and gradients.</p>
</section>
<section id="try-it-yourself-2" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-2">Try It Yourself</h4>
<ol type="1">
<li>Multiply A = [[2, 0], [0, 2]] by x = [3, 4]. What happens to the vector?</li>
<li>Show that AB ≠ BA using A = [[1, 2], [0, 1]], B = [[0, 1], [1, 0]].</li>
<li>Verify that (AB)ᵀ = BᵀAᵀ with small 2×2 matrices.</li>
</ol>
</section>
</section>
<section id="linear-independence-and-span" class="level3">
<h3 class="anchored" data-anchor-id="linear-independence-and-span">104. Linear Independence and Span</h3>
<p>Linear independence is about whether vectors bring new information. If one vector can be written as a combination of others, it adds nothing new. The span of a set of vectors is all possible linear combinations of them—essentially the space they generate. Together, independence and span tell us how many unique directions we have and how big a space they cover.</p>
<section id="picture-in-your-head-3" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-3">Picture in Your Head</h4>
<p>Imagine two arrows in the plane. If both point in different directions, they can combine to reach any point in 2D space—the whole plane. If they both lie on the same line, one is redundant, and you can’t reach the full plane. In higher dimensions, independence tells you whether your set of vectors truly spans the whole space or just a smaller subspace.</p>
</section>
<section id="deep-dive-3" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-3">Deep Dive</h4>
<ul>
<li>Linear Combination: a₁v₁ + a₂v₂ + … + aₖvₖ.</li>
<li>Span: The set of all linear combinations of {v₁, …, vₖ}.</li>
<li>Linear Dependence: If there exist coefficients, not all zero, such that a₁v₁ + … + aₖvₖ = 0, then the vectors are dependent.</li>
<li>Linear Independence: No such nontrivial combination exists.</li>
</ul>
<p>Dimension of a span = number of independent vectors. In AI, feature spaces often have redundant dimensions; PCA and other dimensionality reduction methods identify smaller independent sets.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 46%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Formal Definition</th>
<th>Example in AI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Span</td>
<td>All linear combinations of given vectors</td>
<td>Feature space coverage</td>
</tr>
<tr class="even">
<td>Linear dependence</td>
<td>Some vector is a combination of others</td>
<td>Redundant features</td>
</tr>
<tr class="odd">
<td>Linear independence</td>
<td>No redundancy; minimal unique directions</td>
<td>Basis vectors in embeddings</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-3" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-3">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define vectors</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>v1 <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>v2 <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>v3 <span class="op">=</span> np.array([<span class="dv">2</span>, <span class="dv">0</span>])  <span class="co"># dependent on v1</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Stack into matrix</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>M <span class="op">=</span> np.column_stack([v1, v2, v3])</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Rank gives dimension of span</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>rank <span class="op">=</span> np.linalg.matrix_rank(M)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Matrix:</span><span class="ch">\n</span><span class="st">"</span>, M)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Rank (dimension of span):"</span>, rank)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-1" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-1">Why It Matters</h4>
<p>Redundant features inflate dimensionality without adding new information. Independent features, by contrast, capture the true structure of data. Recognizing independence helps in feature selection, dimensionality reduction, and efficient representation learning. In neural networks, basis-like transformations underpin embeddings and compressed representations.</p>
</section>
<section id="try-it-yourself-3" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-3">Try It Yourself</h4>
<ol type="1">
<li>Are v₁ = [1, 2], v₂ = [2, 4] independent or dependent?</li>
<li>What is the span of v₁ = [1, 0], v₂ = [0, 1] in 2D space?</li>
<li>For vectors v₁ = [1, 0, 0], v₂ = [0, 1, 0], v₃ = [1, 1, 0], what is the dimension of their span?</li>
</ol>
</section>
</section>
<section id="rank-null-space-and-solutions-of-ax-b" class="level3">
<h3 class="anchored" data-anchor-id="rank-null-space-and-solutions-of-ax-b">105. Rank, Null Space, and Solutions of Ax = b</h3>
<p>The rank of a matrix measures how much independent information it contains. The null space consists of all vectors that the matrix sends to zero. Together, rank and null space determine whether a system of linear equations Ax = b has solutions, and if so, whether they are unique or infinite.</p>
<section id="picture-in-your-head-4" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-4">Picture in Your Head</h4>
<p>Think of a matrix as a machine that transforms space. If its rank is full, the machine covers the entire output space—every target vector b is reachable. If its rank is deficient, the machine squashes some dimensions, leaving gaps. The null space represents the hidden tunnel: vectors that go in but vanish to zero at the output.</p>
</section>
<section id="deep-dive-4" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-4">Deep Dive</h4>
<ul>
<li><p>Rank(A): number of independent rows/columns of A.</p></li>
<li><p>Null Space: {x ∈ ℝⁿ | Ax = 0}.</p></li>
<li><p>Rank-Nullity Theorem: For A (m×n), rank(A) + nullity(A) = n.</p></li>
<li><p>Solutions to Ax = b:</p>
<ul>
<li>If rank(A) = rank([A|b]) = n → unique solution.</li>
<li>If rank(A) = rank([A|b]) &lt; n → infinite solutions.</li>
<li>If rank(A) &lt; rank([A|b]) → no solution.</li>
</ul></li>
</ul>
<p>In AI, rank relates to model capacity: a low-rank weight matrix cannot represent all possible mappings, while null space directions correspond to variations in input that a model ignores.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 39%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Meaning</th>
<th>AI Connection</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Rank</td>
<td>Independent directions preserved</td>
<td>Expressive power of layers</td>
</tr>
<tr class="even">
<td>Null space</td>
<td>Inputs mapped to zero</td>
<td>Features discarded by model</td>
</tr>
<tr class="odd">
<td>Rank-nullity</td>
<td>Rank + nullity = number of variables</td>
<td>Trade-off between information and redundancy</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-4" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-4">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>],</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>],</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.array([<span class="dv">6</span>, <span class="dv">12</span>, <span class="dv">4</span>])</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Rank of A</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>rank_A <span class="op">=</span> np.linalg.matrix_rank(A)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Augmented matrix [A|b]</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>Ab <span class="op">=</span> np.column_stack([A, b])</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>rank_Ab <span class="op">=</span> np.linalg.matrix_rank(Ab)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Solve if consistent</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>solution <span class="op">=</span> <span class="va">None</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> rank_A <span class="op">==</span> rank_Ab:</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    solution <span class="op">=</span> np.linalg.lstsq(A, b, rcond<span class="op">=</span><span class="va">None</span>)[<span class="dv">0</span>]</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Rank(A):"</span>, rank_A)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Rank([A|b]):"</span>, rank_Ab)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Solution:"</span>, solution)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-2" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-2">Why It Matters</h4>
<p>In machine learning, rank restrictions show up in low-rank approximations for compression, in covariance matrices that reveal correlations, and in singular value decomposition used for embeddings. Null spaces matter because they identify directions in the data that models cannot see—critical for robustness and feature engineering.</p>
</section>
<section id="try-it-yourself-4" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-4">Try It Yourself</h4>
<ol type="1">
<li>For A = [[1, 0], [0, 1]], what is rank(A) and null space?</li>
<li>Solve Ax = b for A = [[1, 2], [2, 4]], b = [3, 6]. How many solutions exist?</li>
<li>Consider A = [[1, 1], [1, 1]], b = [1, 0]. Does a solution exist? Why or why not?</li>
</ol>
</section>
</section>
<section id="orthogonality-and-projections" class="level3">
<h3 class="anchored" data-anchor-id="orthogonality-and-projections">106. Orthogonality and Projections</h3>
<p>Orthogonality describes vectors that are perpendicular—sharing no overlap in direction. Projection is the operation of expressing one vector in terms of another, by dropping a shadow onto it. Orthogonality and projections are the basis of decomposing data into independent components, simplifying geometry, and designing efficient algorithms.</p>
<section id="picture-in-your-head-5" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-5">Picture in Your Head</h4>
<p>Imagine standing in the sun: your shadow on the ground is the projection of you onto the plane. If the ground is at a right angle to your height, the shadow contains only the part of you aligned with that surface. Two orthogonal arrows, like the x- and y-axis, stand perfectly independent; projecting onto one ignores the other completely.</p>
</section>
<section id="deep-dive-5" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-5">Deep Dive</h4>
<ul>
<li>Orthogonality: Vectors x and y are orthogonal if x·y = 0.</li>
<li>Projection of y onto x:</li>
</ul>
<p><span class="math display">\[
\text{proj}_x(y) = \frac{x \cdot y}{x \cdot x} x
\]</span></p>
<ul>
<li>Orthogonal Basis: A set of mutually perpendicular vectors; simplifies calculations because coordinates don’t interfere.</li>
<li>Orthogonal Matrices: Matrices whose columns form an orthonormal set; preserve lengths and angles.</li>
</ul>
<p>Applications:</p>
<ul>
<li>PCA: data projected onto principal components.</li>
<li>Least squares: projecting data onto subspaces spanned by features.</li>
<li>Orthogonal transforms (e.g., Fourier, wavelets) simplify computation.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 33%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Formula / Rule</th>
<th>AI Application</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Orthogonality</td>
<td>x·y = 0</td>
<td>Independence of features or embeddings</td>
</tr>
<tr class="even">
<td>Projection</td>
<td>projₓ(y) = (x·y / x·x) x</td>
<td>Dimensionality reduction, regression</td>
</tr>
<tr class="odd">
<td>Orthogonal basis</td>
<td>Set of perpendicular vectors</td>
<td>PCA, spectral decomposition</td>
</tr>
<tr class="even">
<td>Orthogonal matrix</td>
<td>QᵀQ = I</td>
<td>Stable rotations in optimization</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-5" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-5">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">3</span>, <span class="dv">4</span>])</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Check orthogonality</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>dot <span class="op">=</span> np.dot(x, y)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Projection of y onto x</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>proj <span class="op">=</span> (np.dot(x, y) <span class="op">/</span> np.dot(x, x)) <span class="op">*</span> x</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Dot product (x·y):"</span>, dot)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Projection of y onto x:"</span>, proj)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-3" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-3">Why It Matters</h4>
<p>Orthogonality underlies the idea of uncorrelated features: one doesn’t explain the other. Projections explain regression, dimensionality reduction, and embedding models. When models work with orthogonal directions, learning is efficient and stable. When features are not orthogonal, redundancy and collinearity can cause instability in optimization.</p>
</section>
<section id="try-it-yourself-5" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-5">Try It Yourself</h4>
<ol type="1">
<li>Compute the projection of y = [2, 3] onto x = [1, 1].</li>
<li>Are [1, 2] and [2, -1] orthogonal? Check using the dot product.</li>
<li>Show that multiplying a vector by an orthogonal matrix preserves its length.</li>
</ol>
</section>
</section>
<section id="eigenvalues-and-eigenvectors" class="level3">
<h3 class="anchored" data-anchor-id="eigenvalues-and-eigenvectors">107. Eigenvalues and Eigenvectors</h3>
<p>Eigenvalues and eigenvectors reveal the “natural modes” of a transformation. An eigenvector is a special direction that does not change orientation when a matrix acts on it, only its length is scaled. The scaling factor is the eigenvalue. They expose the geometry hidden inside matrices and are key to understanding stability, dimensionality reduction, and spectral methods.</p>
<section id="picture-in-your-head-6" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-6">Picture in Your Head</h4>
<p>Imagine stretching a rubber sheet with arrows drawn on it. Most arrows bend and twist, but some special arrows only get longer or shorter, never changing their direction. These are eigenvectors, and the stretch factor is the eigenvalue. They describe the fundamental axes along which transformations act most cleanly.</p>
</section>
<section id="deep-dive-6" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-6">Deep Dive</h4>
<ul>
<li><p>Definition: For matrix A, if</p>
<p><span class="math display">\[
A v = \lambda v
\]</span></p>
<p>then v is an eigenvector and λ is the corresponding eigenvalue.</p></li>
<li><p>Not all matrices have real eigenvalues, but symmetric matrices always do, with orthogonal eigenvectors.</p></li>
<li><p>Diagonalization: A = PDP⁻¹, where D is diagonal with eigenvalues, P contains eigenvectors.</p></li>
<li><p>Spectral theorem: Symmetric A = QΛQᵀ.</p></li>
<li><p>Applications:</p>
<ul>
<li>PCA: eigenvectors of covariance matrix = principal components.</li>
<li>PageRank: dominant eigenvector of web graph transition matrix.</li>
<li>Stability: eigenvalues of Jacobians predict system behavior.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 30%">
<col style="width: 49%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Formula</th>
<th>AI Application</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Eigenvector</td>
<td>Av = λv</td>
<td>Principal components, stable directions</td>
</tr>
<tr class="even">
<td>Eigenvalue</td>
<td>λ = scaling factor</td>
<td>Strength of component or mode</td>
</tr>
<tr class="odd">
<td>Diagonalization</td>
<td>A = PDP⁻¹</td>
<td>Simplifies powers of matrices, dynamics</td>
</tr>
<tr class="even">
<td>Spectral theorem</td>
<td>A = QΛQᵀ for symmetric A</td>
<td>PCA, graph Laplacians</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-6" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-6">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">1</span>],</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">2</span>]])</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute eigenvalues and eigenvectors</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>vals, vecs <span class="op">=</span> np.linalg.eig(A)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Eigenvalues:"</span>, vals)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Eigenvectors:</span><span class="ch">\n</span><span class="st">"</span>, vecs)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-4" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-4">Why It Matters</h4>
<p>Eigenvalues and eigenvectors uncover hidden structure. In AI, they identify dominant directions in data (PCA), measure graph connectivity (spectral clustering), and evaluate stability of optimization. Neural networks exploit low-rank and spectral properties to compress weights and speed up learning.</p>
</section>
<section id="try-it-yourself-6" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-6">Try It Yourself</h4>
<ol type="1">
<li>Find eigenvalues and eigenvectors of A = [[1, 0], [0, 2]]. What do they represent?</li>
<li>For covariance matrix of data points [[1, 0], [0, 1]], what are the eigenvectors?</li>
<li>Compute eigenvalues of [[0, 1], [1, 0]]. How do they relate to flipping coordinates?</li>
</ol>
</section>
</section>
<section id="singular-value-decomposition-svd" class="level3">
<h3 class="anchored" data-anchor-id="singular-value-decomposition-svd">108. Singular Value Decomposition (SVD)</h3>
<p>Singular Value Decomposition is a powerful factorization that expresses any matrix as a combination of rotations (or reflections) and scalings. Unlike eigen decomposition, SVD applies to all rectangular matrices, not just square ones. It breaks a matrix into orthogonal directions of input and output, linked by singular values that measure the strength of each direction.</p>
<section id="picture-in-your-head-7" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-7">Picture in Your Head</h4>
<p>Think of a block of clay being pressed through a mold. The mold rotates and aligns the clay, stretches it differently along key directions, and then rotates it again. Those directions are the singular vectors, and the stretching factors are the singular values. SVD reveals the essential axes of action of any transformation.</p>
</section>
<section id="deep-dive-7" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-7">Deep Dive</h4>
<p>For a matrix A (m×n),</p>
<p><span class="math display">\[
A = U \Sigma V^T
\]</span></p>
<ul>
<li>U (m×m): orthogonal, columns = left singular vectors.</li>
<li>Σ (m×n): diagonal with singular values (σ₁ ≥ σ₂ ≥ … ≥ 0).</li>
<li>V (n×n): orthogonal, columns = right singular vectors.</li>
</ul>
<p>Properties:</p>
<ul>
<li>Rank(A) = number of nonzero singular values.</li>
<li>Condition number = σ₁ / σ_min, measures numerical stability.</li>
<li>Low-rank approximation: keep top k singular values to compress A.</li>
</ul>
<p>Applications:</p>
<ul>
<li>PCA: covariance matrix factorized via SVD.</li>
<li>Recommender systems: latent factors via matrix factorization.</li>
<li>Noise reduction and compression: discard small singular values.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 36%">
<col style="width: 57%">
</colgroup>
<thead>
<tr class="header">
<th>Part</th>
<th>Role</th>
<th>AI Application</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>U</td>
<td>Orthogonal basis for outputs</td>
<td>Principal directions in data space</td>
</tr>
<tr class="even">
<td>Σ</td>
<td>Strength of each component</td>
<td>Variance captured by each latent factor</td>
</tr>
<tr class="odd">
<td>V</td>
<td>Orthogonal basis for inputs</td>
<td>Feature embeddings or latent representations</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-7" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-7">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>              [<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">1</span>]])</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute SVD</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>U, S, Vt <span class="op">=</span> np.linalg.svd(A)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"U:</span><span class="ch">\n</span><span class="st">"</span>, U)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Singular values:"</span>, S)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"V^T:</span><span class="ch">\n</span><span class="st">"</span>, Vt)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Low-rank approximation (rank-1)</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>rank1 <span class="op">=</span> np.outer(U[:,<span class="dv">0</span>], Vt[<span class="dv">0</span>,:]) <span class="op">*</span> S[<span class="dv">0</span>]</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Rank-1 approximation:</span><span class="ch">\n</span><span class="st">"</span>, rank1)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-5" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-5">Why It Matters</h4>
<p>SVD underpins dimensionality reduction, matrix completion, and compression. It helps uncover latent structures in data (topics, embeddings), makes computations stable, and explains why certain transformations amplify or suppress information. In deep learning, truncated SVD approximates large weight matrices to reduce memory and computation.</p>
</section>
<section id="try-it-yourself-7" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-7">Try It Yourself</h4>
<ol type="1">
<li>Compute the SVD of A = [[1, 0], [0, 1]]. What are the singular values?</li>
<li>Take matrix [[2, 0], [0, 1]] and reconstruct it from UΣVᵀ. Which direction is stretched more?</li>
<li>Apply rank-1 approximation to a 3×3 random matrix. How close is it to the original?</li>
</ol>
</section>
</section>
<section id="tensors-and-higher-order-structures" class="level3">
<h3 class="anchored" data-anchor-id="tensors-and-higher-order-structures">109. Tensors and Higher-Order Structures</h3>
<p>Tensors generalize scalars, vectors, and matrices to higher dimensions. A scalar is a 0th-order tensor, a vector is a 1st-order tensor, and a matrix is a 2nd-order tensor. Higher-order tensors (3rd-order and beyond) represent multi-dimensional data arrays. They are essential in AI for modeling structured data such as images, sequences, and multimodal information.</p>
<section id="picture-in-your-head-8" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-8">Picture in Your Head</h4>
<p>Picture a line of numbers: that’s a vector. Arrange numbers into a grid: that’s a matrix. Stack matrices like pages in a book: that’s a 3D tensor. Add more axes, and you get higher-order tensors. In AI, these extra dimensions represent channels, time steps, or feature groups—all in one object.</p>
</section>
<section id="deep-dive-8" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-8">Deep Dive</h4>
<ul>
<li><p>Order: number of indices needed to address an element.</p>
<ul>
<li>Scalar: 0th order (a).</li>
<li>Vector: 1st order (aᵢ).</li>
<li>Matrix: 2nd order (aᵢⱼ).</li>
<li>Tensor: 3rd+ order (aᵢⱼₖ…).</li>
</ul></li>
<li><p>Shape: tuple of dimensions, e.g., (batch, height, width, channels).</p></li>
<li><p>Operations:</p>
<ul>
<li>Element-wise addition and multiplication.</li>
<li>Contractions (generalized dot products).</li>
<li>Tensor decompositions (e.g., CP, Tucker).</li>
</ul></li>
<li><p>Applications in AI:</p>
<ul>
<li>Images: 3rd-order tensors (height × width × channels).</li>
<li>Videos: 4th-order tensors (frames × height × width × channels).</li>
<li>Transformers: attention weights stored as 4D tensors.</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Order</th>
<th>Example Object</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>Scalar</td>
<td>Loss value, learning rate</td>
</tr>
<tr class="even">
<td>1</td>
<td>Vector</td>
<td>Word embedding</td>
</tr>
<tr class="odd">
<td>2</td>
<td>Matrix</td>
<td>Weight matrix</td>
</tr>
<tr class="even">
<td>3</td>
<td>Tensor (3D)</td>
<td>RGB image (H×W×3)</td>
</tr>
<tr class="odd">
<td>4+</td>
<td>Higher-order</td>
<td>Batch of videos, attention scores</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-8" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-8">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Scalars, vectors, matrices, tensors</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>scalar <span class="op">=</span> np.array(<span class="dv">5</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>vector <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>matrix <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]])</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>tensor3 <span class="op">=</span> np.random.rand(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>)   <span class="co"># 3rd-order tensor</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>tensor4 <span class="op">=</span> np.random.rand(<span class="dv">10</span>, <span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">3</span>)  <span class="co"># batch of 10 RGB images</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Scalar:"</span>, scalar)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Vector:"</span>, vector)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Matrix:</span><span class="ch">\n</span><span class="st">"</span>, matrix)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"3D Tensor shape:"</span>, tensor3.shape)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"4D Tensor shape:"</span>, tensor4.shape)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-6" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-6">Why It Matters</h4>
<p>Tensors are the core data structure in modern AI frameworks like TensorFlow and PyTorch. Every dataset and model parameter is expressed as tensors, enabling efficient GPU computation. Mastering tensors means understanding how data flows through deep learning systems, from raw input to final prediction.</p>
</section>
<section id="try-it-yourself-8" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-8">Try It Yourself</h4>
<ol type="1">
<li>Represent a grayscale image of size 28×28 as a tensor. What is its order and shape?</li>
<li>Extend it to a batch of 100 RGB images. What is the new tensor shape?</li>
<li>Compute the contraction (generalized dot product) between two 3D tensors of compatible shapes. What does the result represent?</li>
</ol>
</section>
</section>
<section id="applications-in-ai-representations" class="level3">
<h3 class="anchored" data-anchor-id="applications-in-ai-representations">110. Applications in AI Representations</h3>
<p>Linear algebra objects—scalars, vectors, matrices, and tensors—are not abstract math curiosities. They directly represent data, parameters, and operations in AI systems. Vectors hold features, matrices encode transformations, and tensors capture complex structured inputs. Understanding these correspondences turns math into an intuitive language for modeling intelligence.</p>
<section id="picture-in-your-head-9" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-9">Picture in Your Head</h4>
<p>Imagine an AI model as a factory. Scalars are like single control knobs (learning rate, bias terms). Vectors are conveyor belts carrying rows of features. Matrices are the machinery applying transformations—rotating, stretching, mixing inputs. Tensors are entire stacks of conveyor belts handling images, sequences, or multimodal signals at once.</p>
</section>
<section id="deep-dive-9" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-9">Deep Dive</h4>
<ul>
<li><p>Scalars in AI:</p>
<ul>
<li>Learning rates control optimization steps.</li>
<li>Loss values quantify performance.</li>
</ul></li>
<li><p>Vectors in AI:</p>
<ul>
<li>Embeddings for words, users, or items.</li>
<li>Feature vectors for tabular data or single images.</li>
</ul></li>
<li><p>Matrices in AI:</p>
<ul>
<li>Weight matrices of fully connected layers.</li>
<li>Transition matrices in Markov models.</li>
</ul></li>
<li><p>Tensors in AI:</p>
<ul>
<li>Image batches (N×H×W×C).</li>
<li>Attention maps (Batch×Heads×Seq×Seq).</li>
<li>Multimodal data (e.g., video with audio channels).</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Object</th>
<th>AI Role Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Scalar</td>
<td>Learning rate = 0.001, single prediction value</td>
</tr>
<tr class="even">
<td>Vector</td>
<td>Word embedding = [0.2, -0.1, 0.5, …]</td>
</tr>
<tr class="odd">
<td>Matrix</td>
<td>Neural layer weights, 512×1024</td>
</tr>
<tr class="even">
<td>Tensor</td>
<td>Batch of 64 images, 64×224×224×3</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-9" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-9">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Scalar: loss</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> <span class="fl">0.23</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Vector: embedding for a word</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> np.random.rand(<span class="dv">128</span>)  <span class="co"># 128-dim word embedding</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrix: weights in a dense layer</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> np.random.rand(<span class="dv">128</span>, <span class="dv">64</span>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Tensor: batch of 32 RGB images, 64x64 pixels</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>images <span class="op">=</span> np.random.rand(<span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">64</span>, <span class="dv">3</span>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loss (scalar):"</span>, loss)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Embedding (vector) shape:"</span>, embedding.shape)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Weights (matrix) shape:"</span>, weights.shape)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Images (tensor) shape:"</span>, images.shape)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-7" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-7">Why It Matters</h4>
<p>Every modern AI framework is built on top of tensor operations. Training a model means applying matrix multiplications, summing losses, and updating weights. Recognizing the role of scalars, vectors, matrices, and tensors in representations lets you map theory directly to practice, and reason about computation, memory, and scalability.</p>
</section>
<section id="try-it-yourself-9" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-9">Try It Yourself</h4>
<ol type="1">
<li>Represent a mini-batch of 16 grayscale MNIST digits (28×28 each). What tensor shape do you get?</li>
<li>If a dense layer has 300 input features and 100 outputs, what is the shape of its weight matrix?</li>
<li>Construct a tensor representing a 10-second audio clip sampled at 16 kHz, split into 1-second frames with 13 MFCC coefficients each. What would its order and shape be?</li>
</ol>
</section>
</section>
</section>
<section id="chapter-12.-differential-and-integral-calculus" class="level2">
<h2 class="anchored" data-anchor-id="chapter-12.-differential-and-integral-calculus">Chapter 12. Differential and Integral Calculus</h2>
<section id="functions-limits-and-continuity" class="level3">
<h3 class="anchored" data-anchor-id="functions-limits-and-continuity">111. Functions, Limits, and Continuity</h3>
<p>Calculus begins with functions: rules that assign inputs to outputs. Limits describe how functions behave near a point, even if the function is undefined there. Continuity ensures no sudden jumps—the function flows smoothly without gaps. These concepts form the groundwork for derivatives, gradients, and optimization in AI.</p>
<section id="picture-in-your-head-10" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-10">Picture in Your Head</h4>
<p>Think of walking along a curve drawn on paper. A continuous function means you can trace the entire curve without lifting your pencil. A limit is like approaching a tunnel: even if the tunnel entrance is blocked at the exact spot, you can still describe where the path was heading.</p>
</section>
<section id="deep-dive-10" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-10">Deep Dive</h4>
<ul>
<li><p>Function: f: ℝ → ℝ, mapping x ↦ f(x).</p></li>
<li><p>Limit:</p>
<p><span class="math display">\[
\lim_{x \to a} f(x) = L
\]</span></p>
<p>if values of f(x) approach L as x approaches a.</p></li>
<li><p>Continuity: f is continuous at x=a if</p>
<p><span class="math display">\[
\lim_{x \to a} f(x) = f(a).
\]</span></p></li>
<li><p>Discontinuities: removable (hole), jump, or infinite.</p></li>
<li><p>In AI: limits ensure stability in gradient descent, continuity ensures smooth loss surfaces.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 35%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th>Idea</th>
<th>Formal Definition</th>
<th>AI Role</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Function</td>
<td>f(x) assigns outputs to inputs</td>
<td>Loss, activation functions</td>
</tr>
<tr class="even">
<td>Limit</td>
<td>Values approach L as x → a</td>
<td>Gradient approximations, convergence</td>
</tr>
<tr class="odd">
<td>Continuity</td>
<td>Limit at a = f(a)</td>
<td>Smooth learning curves, differentiability</td>
</tr>
<tr class="even">
<td>Discontinuity</td>
<td>Jumps, holes, asymptotes</td>
<td>Non-smooth activations (ReLU kinks, etc.)</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-10" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-10">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function with a removable discontinuity at x=0</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x):</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (np.sin(x)) <span class="op">/</span> x <span class="cf">if</span> x <span class="op">!=</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">1</span>  <span class="co"># define f(0)=1</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Approximate limit near 0</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> [<span class="fl">0.1</span>, <span class="fl">0.01</span>, <span class="fl">0.001</span>, <span class="op">-</span><span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.01</span>]</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>limits <span class="op">=</span> [f(val) <span class="cf">for</span> val <span class="kw">in</span> xs]</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Values near 0:"</span>, limits)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"f(0):"</span>, f(<span class="dv">0</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-8" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-8">Why It Matters</h4>
<p>Optimization in AI depends on smooth, continuous loss functions. Gradient-based algorithms need limits and continuity to define derivatives. Activation functions like sigmoid and tanh are continuous, while piecewise ones like ReLU are continuous but not smooth at zero—still useful because continuity is preserved.</p>
</section>
<section id="try-it-yourself-10" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-10">Try It Yourself</h4>
<ol type="1">
<li>Evaluate the left and right limits of f(x) = 1/x as x → 0. Why do they differ?</li>
<li>Is ReLU(x) = max(0, x) continuous everywhere? Where is it not differentiable?</li>
<li>Construct a function with a jump discontinuity and explain why gradient descent would fail on it.</li>
</ol>
</section>
</section>
<section id="derivatives-and-gradients" class="level3">
<h3 class="anchored" data-anchor-id="derivatives-and-gradients">112. Derivatives and Gradients</h3>
<p>The derivative measures how a function changes as its input changes. It captures slope—the rate of change at a point. In multiple dimensions, this generalizes to gradients: vectors of partial derivatives that describe the steepest direction of change. Derivatives and gradients are the engines of optimization in AI.</p>
<section id="picture-in-your-head-11" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-11">Picture in Your Head</h4>
<p>Imagine a curve on a hill. At each point, the slope of the tangent line tells you whether you’re climbing up or sliding down. In higher dimensions, picture standing on a mountain surface: the gradient points in the direction of steepest ascent, while its negative points toward steepest descent—the path optimization algorithms follow.</p>
</section>
<section id="deep-dive-11" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-11">Deep Dive</h4>
<ul>
<li><p>Derivative (1D):</p>
<p><span class="math display">\[
f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
\]</span></p></li>
<li><p>Partial derivative: rate of change with respect to one variable while holding others constant.</p></li>
<li><p>Gradient:</p>
<p><span class="math display">\[
\nabla f(x) = \left(\frac{\partial f}{\partial x_1}, \dots, \frac{\partial f}{\partial x_n}\right)
\]</span></p></li>
<li><p>Geometric meaning: gradient is perpendicular to level sets of f.</p></li>
<li><p>In AI: gradients guide backpropagation, parameter updates, and loss minimization.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 35%">
<col style="width: 51%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Formula / Definition</th>
<th>AI Application</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Derivative</td>
<td>f′(x) = lim (f(x+h) - f(x))/h</td>
<td>Slope of loss curve in 1D optimization</td>
</tr>
<tr class="even">
<td>Partial</td>
<td>∂f/∂xᵢ</td>
<td>Effect of one feature/parameter</td>
</tr>
<tr class="odd">
<td>Gradient</td>
<td>(∂f/∂x₁, …, ∂f/∂xₙ)</td>
<td>Direction of steepest change in parameters</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-11" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-11">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function f(x, y) = x^2 + y^2</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x, y):</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x2 <span class="op">+</span> y2</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Numerical gradient at (1,2)</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> <span class="fl">1e-5</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>df_dx <span class="op">=</span> (f(<span class="dv">1</span><span class="op">+</span>h, <span class="dv">2</span>) <span class="op">-</span> f(<span class="dv">1</span><span class="op">-</span>h, <span class="dv">2</span>)) <span class="op">/</span> (<span class="dv">2</span><span class="op">*</span>h)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>df_dy <span class="op">=</span> (f(<span class="dv">1</span>, <span class="dv">2</span><span class="op">+</span>h) <span class="op">-</span> f(<span class="dv">1</span>, <span class="dv">2</span><span class="op">-</span>h)) <span class="op">/</span> (<span class="dv">2</span><span class="op">*</span>h)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>gradient <span class="op">=</span> np.array([df_dx, df_dy])</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Gradient at (1,2):"</span>, gradient)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-9" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-9">Why It Matters</h4>
<p>Every AI model learns by following gradients. Training is essentially moving through a high-dimensional landscape of parameters, guided by derivatives of the loss. Understanding derivatives explains why optimization converges—or gets stuck—and why techniques like momentum or adaptive learning rates are necessary.</p>
</section>
<section id="try-it-yourself-11" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-11">Try It Yourself</h4>
<ol type="1">
<li>Compute the derivative of f(x) = x² at x=3.</li>
<li>For f(x,y) = 3x + 4y, what is the gradient? What direction does it point?</li>
<li>Explain why the gradient of f(x,y) = x² + y² at (0,0) is the zero vector.</li>
</ol>
</section>
</section>
<section id="partial-derivatives-and-multivariable-calculus" class="level3">
<h3 class="anchored" data-anchor-id="partial-derivatives-and-multivariable-calculus">113. Partial Derivatives and Multivariable Calculus</h3>
<p>When functions depend on several variables, we study how the output changes with respect to each input separately. Partial derivatives measure change along one axis at a time, while holding others fixed. Together they form the foundation of multivariable calculus, which models curved surfaces and multidimensional landscapes.</p>
<section id="picture-in-your-head-12" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-12">Picture in Your Head</h4>
<p>Imagine a mountain surface described by height f(x,y). Walking east measures ∂f/∂x, walking north measures ∂f/∂y. Each partial derivative is like slicing the mountain in one direction and asking how steep the slope is in that slice. By combining all directions, we can describe the terrain fully.</p>
</section>
<section id="deep-dive-12" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-12">Deep Dive</h4>
<ul>
<li><p>Partial derivative:</p>
<p><span class="math display">\[
\frac{\partial f}{\partial x_i}(x_1,\dots,x_n) = \lim_{h \to 0}\frac{f(\dots,x_i+h,\dots) - f(\dots,x_i,\dots)}{h}
\]</span></p></li>
<li><p>Gradient vector: collects all partial derivatives.</p></li>
<li><p>Mixed partials: ∂²f/∂x∂y = ∂²f/∂y∂x (under smoothness assumptions, Clairaut’s theorem).</p></li>
<li><p>Level sets: curves/surfaces where f(x) = constant; gradient is perpendicular to these.</p></li>
<li><p>In AI: loss functions often depend on thousands or millions of parameters; partial derivatives tell how sensitive the loss is to each parameter individually.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 36%">
<col style="width: 41%">
</colgroup>
<thead>
<tr class="header">
<th>Idea</th>
<th>Formula/Rule</th>
<th>AI Role</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Partial derivative</td>
<td>∂f/∂xᵢ</td>
<td>Effect of one parameter or feature</td>
</tr>
<tr class="even">
<td>Gradient</td>
<td>(∂f/∂x₁, …, ∂f/∂xₙ)</td>
<td>Used in backpropagation</td>
</tr>
<tr class="odd">
<td>Mixed partials</td>
<td>∂²f/∂x∂y = ∂²f/∂y∂x (if smooth)</td>
<td>Second-order methods, curvature</td>
</tr>
<tr class="even">
<td>Level sets</td>
<td>f(x)=c, gradient ⟂ level set</td>
<td>Visualizing optimization landscapes</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-12" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-12">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define variables</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> sp.symbols(<span class="st">'x y'</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> x2 <span class="op">*</span> y <span class="op">+</span> sp.sin(y)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Partial derivatives</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>df_dx <span class="op">=</span> sp.diff(f, x)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>df_dy <span class="op">=</span> sp.diff(f, y)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"∂f/∂x ="</span>, df_dx)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"∂f/∂y ="</span>, df_dy)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-10" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-10">Why It Matters</h4>
<p>Partial derivatives explain how each weight in a neural network influences the loss. Backpropagation computes them efficiently layer by layer. Without partial derivatives, training deep models would be impossible: they are the numerical levers that let optimization adjust millions of parameters simultaneously.</p>
</section>
<section id="try-it-yourself-12" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-12">Try It Yourself</h4>
<ol type="1">
<li>Compute ∂/∂x of f(x,y) = x²y at (2,1).</li>
<li>For f(x,y) = sin(xy), find ∂f/∂y.</li>
<li>Check whether mixed partial derivatives commute for f(x,y) = x²y³.</li>
</ol>
</section>
</section>
<section id="gradient-vectors-and-directional-derivatives" class="level3">
<h3 class="anchored" data-anchor-id="gradient-vectors-and-directional-derivatives">114. Gradient Vectors and Directional Derivatives</h3>
<p>The gradient vector extends derivatives to multiple dimensions. It points in the direction of steepest increase of a function. Directional derivatives generalize further, asking: how does the function change if we move in <em>any</em> chosen direction? Together, they provide the compass for navigating multidimensional landscapes.</p>
<section id="picture-in-your-head-13" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-13">Picture in Your Head</h4>
<p>Imagine standing on a hill. The gradient is the arrow on the ground pointing directly uphill. If you decide to walk northeast, the directional derivative tells you how steep the slope is in that chosen direction. It’s the projection of the gradient onto your direction of travel.</p>
</section>
<section id="deep-dive-13" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-13">Deep Dive</h4>
<ul>
<li><p>Gradient:</p>
<p><span class="math display">\[
\nabla f(x) = \left( \frac{\partial f}{\partial x_1}, \dots, \frac{\partial f}{\partial x_n} \right)
\]</span></p></li>
<li><p>Directional derivative in direction u:</p>
<p><span class="math display">\[
D_u f(x) = \nabla f(x) \cdot u
\]</span></p>
<p>where u is a unit vector.</p></li>
<li><p>Gradient points to steepest ascent; -∇f points to steepest descent.</p></li>
<li><p>Level sets (contours of constant f): gradient is perpendicular to them.</p></li>
<li><p>In AI: gradient descent updates parameters in direction of -∇f; directional derivatives explain sensitivity along specific parameter combinations.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 29%">
<col style="width: 25%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Formula</th>
<th>AI Application</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Gradient</td>
<td>(∂f/∂x₁, …, ∂f/∂xₙ)</td>
<td>Backpropagation, training updates</td>
</tr>
<tr class="even">
<td>Directional derivative</td>
<td>Dᵤf(x) = ∇f(x)·u</td>
<td>Sensitivity along chosen direction</td>
</tr>
<tr class="odd">
<td>Steepest ascent</td>
<td>Direction of ∇f</td>
<td>Climbing optimization landscapes</td>
</tr>
<tr class="even">
<td>Steepest descent</td>
<td>Direction of -∇f</td>
<td>Gradient descent learning</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-13" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-13">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define f(x,y) = x^2 + y^2</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x, y):</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x2 <span class="op">+</span> y2</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient at (1,2)</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>grad <span class="op">=</span> np.array([<span class="dv">2</span><span class="op">*</span><span class="dv">1</span>, <span class="dv">2</span><span class="op">*</span><span class="dv">2</span>])</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Direction u (normalized)</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>u <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">1</span>]) <span class="op">/</span> np.sqrt(<span class="dv">2</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Directional derivative</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>Du <span class="op">=</span> np.dot(grad, u)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Gradient at (1,2):"</span>, grad)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Directional derivative in direction (1,1):"</span>, Du)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-11" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-11">Why It Matters</h4>
<p>Gradients drive every learning algorithm: they show how to change parameters to reduce error fastest. Directional derivatives give insight into how models respond to combined changes, such as adjusting multiple weights together. This underpins second-order methods, sensitivity analysis, and robustness checks.</p>
</section>
<section id="try-it-yourself-13" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-13">Try It Yourself</h4>
<ol type="1">
<li>For f(x,y) = x² + y², compute the gradient at (3,4). What direction does it point?</li>
<li>Using u = (0,1), compute the directional derivative at (1,2). How does it compare to ∂f/∂y?</li>
<li>Explain why gradient descent always chooses -∇f rather than another direction.</li>
</ol>
</section>
</section>
<section id="jacobians-and-hessians" class="level3">
<h3 class="anchored" data-anchor-id="jacobians-and-hessians">115. Jacobians and Hessians</h3>
<p>The Jacobian and Hessian extend derivatives into structured, matrix forms. The Jacobian collects all first-order partial derivatives of a multivariable function, while the Hessian gathers all second-order partial derivatives. Together, they describe both the slope and curvature of high-dimensional functions.</p>
<section id="picture-in-your-head-14" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-14">Picture in Your Head</h4>
<p>Think of the Jacobian as a map of slopes pointing in every direction, like a compass at each point of a surface. The Hessian adds a second layer: it tells you whether the surface is bowl-shaped (convex), saddle-shaped, or inverted bowl (concave). The Jacobian points you downhill, the Hessian tells you how the ground curves beneath your feet.</p>
</section>
<section id="deep-dive-14" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-14">Deep Dive</h4>
<ul>
<li><p>Jacobian: For f: ℝⁿ → ℝᵐ,</p>
<p><span class="math display">\[
J_{ij} = \frac{\partial f_i}{\partial x_j}
\]</span></p>
<p>It’s an m×n matrix capturing how each output changes with each input.</p></li>
<li><p>Hessian: For scalar f: ℝⁿ → ℝ,</p>
<p><span class="math display">\[
H_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}
\]</span></p>
<p>It’s an n×n symmetric matrix (if f is smooth).</p></li>
<li><p>Properties:</p>
<ul>
<li>Jacobian linearizes functions locally.</li>
<li>Hessian encodes curvature, used in Newton’s method.</li>
</ul></li>
<li><p>In AI:</p>
<ul>
<li>Jacobians: used in backpropagation through vector-valued layers.</li>
<li>Hessians: characterize loss landscapes, stability, and convergence.</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Concept</th>
<th>Shape</th>
<th>AI Role</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Jacobian</td>
<td>m×n</td>
<td>Sensitivity of outputs to inputs</td>
</tr>
<tr class="even">
<td>Hessian</td>
<td>n×n</td>
<td>Curvature of loss function</td>
</tr>
<tr class="odd">
<td>Gradient</td>
<td>1×n</td>
<td>Special case of Jacobian (m=1)</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-14" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-14">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define variables</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> sp.symbols(<span class="st">'x y'</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>f1 <span class="op">=</span> x2 <span class="op">+</span> y</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>f2 <span class="op">=</span> sp.sin(x) <span class="op">*</span> y</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>F <span class="op">=</span> sp.Matrix([f1, f2])</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Jacobian of F wrt (x,y)</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>J <span class="op">=</span> F.jacobian([x, y])</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Hessian of scalar f1</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>H <span class="op">=</span> sp.hessian(f1, (x, y))</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Jacobian:</span><span class="ch">\n</span><span class="st">"</span>, J)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Hessian of f1:</span><span class="ch">\n</span><span class="st">"</span>, H)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-12" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-12">Why It Matters</h4>
<p>The Jacobian underlies backpropagation: it’s how gradients flow through each layer of a neural network. The Hessian reveals whether minima are sharp or flat, explaining generalization and optimization difficulty. Many advanced algorithms—Newton’s method, natural gradients, curvature-aware optimizers—rely on these structures.</p>
</section>
<section id="try-it-yourself-14" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-14">Try It Yourself</h4>
<ol type="1">
<li>Compute the Jacobian of F(x,y) = (x², y²) at (1,2).</li>
<li>For f(x,y) = x² + y², write down the Hessian. What does it say about curvature?</li>
<li>Explain how the Hessian helps distinguish between a minimum, maximum, and saddle point.</li>
</ol>
</section>
</section>
<section id="optimization-and-critical-points" class="level3">
<h3 class="anchored" data-anchor-id="optimization-and-critical-points">116. Optimization and Critical Points</h3>
<p>Optimization is about finding inputs that minimize or maximize a function. Critical points are where the gradient vanishes (∇f = 0). These points can be minima, maxima, or saddle points. Understanding them is central to training AI models, since learning is optimization over a loss surface.</p>
<section id="picture-in-your-head-15" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-15">Picture in Your Head</h4>
<p>Imagine a landscape of hills and valleys. Critical points are the flat spots where the slope disappears: the bottom of a valley, the top of a hill, or the center of a saddle. Optimization is like dropping a ball into this landscape and watching where it rolls. The type of critical point determines whether the ball comes to rest in a stable valley or balances precariously on a ridge.</p>
</section>
<section id="deep-dive-15" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-15">Deep Dive</h4>
<ul>
<li><p>Critical point: x* where ∇f(x*) = 0.</p></li>
<li><p>Classification via Hessian:</p>
<ul>
<li>Positive definite → local minimum.</li>
<li>Negative definite → local maximum.</li>
<li>Indefinite → saddle point.</li>
</ul></li>
<li><p>Global vs local: Local minima are valleys nearby; global minimum is the deepest valley.</p></li>
<li><p>Convex functions: any local minimum is also global.</p></li>
<li><p>In AI: neural networks often converge to local minima or saddle points; optimization aims for low-loss basins that generalize well.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 33%">
<col style="width: 49%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Test (using Hessian)</th>
<th>Meaning in AI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Local minimum</td>
<td>H positive definite</td>
<td>Stable learned model, low loss</td>
</tr>
<tr class="even">
<td>Local maximum</td>
<td>H negative definite</td>
<td>Rare in training; undesired peak</td>
</tr>
<tr class="odd">
<td>Saddle point</td>
<td>H indefinite</td>
<td>Common in high dimensions, slows training</td>
</tr>
<tr class="even">
<td>Global minimum</td>
<td>Lowest value over all inputs</td>
<td>Best achievable performance</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-15" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-15">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> sp.symbols(<span class="st">'x y'</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> x2 <span class="op">+</span> y2 <span class="op">-</span> x<span class="op">*</span>y</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient and Hessian</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>grad <span class="op">=</span> [sp.diff(f, var) <span class="cf">for</span> var <span class="kw">in</span> (x, y)]</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>H <span class="op">=</span> sp.hessian(f, (x, y))</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Solve for critical points</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>critical_points <span class="op">=</span> sp.solve(grad, (x, y))</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Critical points:"</span>, critical_points)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Hessian:</span><span class="ch">\n</span><span class="st">"</span>, H)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-13" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-13">Why It Matters</h4>
<p>Training neural networks is about navigating a massive landscape of parameters. Knowing how to identify minima, maxima, and saddles explains why optimization sometimes gets stuck or converges slowly. Techniques like momentum and adaptive learning rates help escape saddles and find flatter minima, which often generalize better.</p>
</section>
<section id="try-it-yourself-15" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-15">Try It Yourself</h4>
<ol type="1">
<li>Find critical points of f(x) = x². What type are they?</li>
<li>For f(x,y) = x² − y², compute the gradient and Hessian at (0,0). What type of point is this?</li>
<li>Explain why convex loss functions are easier to optimize than non-convex ones.</li>
</ol>
</section>
</section>
<section id="integrals-and-areas-under-curves" class="level3">
<h3 class="anchored" data-anchor-id="integrals-and-areas-under-curves">117. Integrals and Areas under Curves</h3>
<p>Integration is the process of accumulating quantities, often visualized as the area under a curve. While derivatives measure instantaneous change, integrals measure total accumulation. In AI, integrals appear in probability (areas under density functions), expected values, and continuous approximations of sums.</p>
<section id="picture-in-your-head-16" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-16">Picture in Your Head</h4>
<p>Imagine pouring water under a curve until it touches the graph: the filled region is the integral. If the curve goes above and below the axis, areas above count positive and areas below count negative, balancing out like gains and losses over time.</p>
</section>
<section id="deep-dive-16" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-16">Deep Dive</h4>
<ul>
<li><p>Definite integral:</p>
<p><span class="math display">\[
\int_a^b f(x)\,dx
\]</span></p>
<p>is the net area under f(x) between a and b.</p></li>
<li><p>Indefinite integral:</p>
<p><span class="math display">\[
\int f(x)\,dx = F(x) + C
\]</span></p>
<p>where F′(x) = f(x).</p></li>
<li><p>Fundamental Theorem of Calculus: connects integrals and derivatives:</p>
<p><span class="math display">\[
\frac{d}{dx}\int_a^x f(t)\,dt = f(x).
\]</span></p></li>
<li><p>In AI:</p>
<ul>
<li>Probability densities integrate to 1.</li>
<li>Expectations are integrals over random variables.</li>
<li>Continuous-time models (differential equations, neural ODEs) rely on integration.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 25%">
<col style="width: 51%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Formula</th>
<th>AI Role</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Definite integral</td>
<td>∫ₐᵇ f(x) dx</td>
<td>Probability mass, expected outcomes</td>
</tr>
<tr class="even">
<td>Indefinite integral</td>
<td>∫ f(x) dx = F(x) + C</td>
<td>Antiderivative, symbolic computation</td>
</tr>
<tr class="odd">
<td>Fundamental theorem</td>
<td>d/dx ∫ f(t) dt = f(x)</td>
<td>Links change (derivatives) and accumulation</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-16" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-16">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> sp.symbols(<span class="st">'x'</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> sp.sin(x)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Indefinite integral</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>F <span class="op">=</span> sp.integrate(f, x)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Definite integral from 0 to pi</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>area <span class="op">=</span> sp.integrate(f, (x, <span class="dv">0</span>, sp.pi))</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Indefinite integral of sin(x):"</span>, F)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Definite integral from 0 to pi:"</span>, area)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-14" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-14">Why It Matters</h4>
<p>Integrals explain how continuous distributions accumulate probability, why loss functions like cross-entropy involve expectations, and how continuous dynamics are modeled in AI. Without integrals, probability theory and continuous optimization would collapse, leaving only crude approximations.</p>
</section>
<section id="try-it-yourself-16" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-16">Try It Yourself</h4>
<ol type="1">
<li>Compute ∫₀¹ x² dx.</li>
<li>For probability density f(x) = 2x on [0,1], check that ∫₀¹ f(x) dx = 1.</li>
<li>Find ∫ cos(x) dx and verify by differentiation.</li>
</ol>
</section>
</section>
<section id="multiple-integrals-and-volumes" class="level3">
<h3 class="anchored" data-anchor-id="multiple-integrals-and-volumes">118. Multiple Integrals and Volumes</h3>
<p>Multiple integrals extend the idea of integration to higher dimensions. Instead of the area under a curve, we compute volumes under surfaces or hyper-volumes in higher-dimensional spaces. They let us measure total mass, probability, or accumulation over multidimensional regions.</p>
<section id="picture-in-your-head-17" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-17">Picture in Your Head</h4>
<p>Imagine a bumpy sheet stretched over the xy-plane. The double integral sums the “pillars” of volume beneath the surface, filling the region like pouring sand until the surface is reached. Triple integrals push this further, measuring the volume inside 3D solids. Higher-order integrals generalize the same idea into abstract feature spaces.</p>
</section>
<section id="deep-dive-17" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-17">Deep Dive</h4>
<ul>
<li><p>Double integral:</p>
<p><span class="math display">\[
\iint_R f(x,y)\,dx\,dy
\]</span></p>
<p>sums over a region R in 2D.</p></li>
<li><p>Triple integral:</p>
<p><span class="math display">\[
\iiint_V f(x,y,z)\,dx\,dy\,dz
\]</span></p>
<p>over volume V.</p></li>
<li><p>Fubini’s theorem: allows evaluating multiple integrals as iterated single integrals, e.g.</p>
<p><span class="math display">\[
\iint_R f(x,y)\,dx\,dy = \int_a^b \int_c^d f(x,y)\,dx\,dy.
\]</span></p></li>
<li><p>Applications in AI:</p>
<ul>
<li>Probability distributions in multiple variables (joint densities).</li>
<li>Normalization constants in Bayesian inference.</li>
<li>Expectation over multivariate spaces.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 29%">
<col style="width: 54%">
</colgroup>
<thead>
<tr class="header">
<th>Integral Type</th>
<th>Formula Example</th>
<th>AI Application</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Double</td>
<td>∬ f(x,y) dx dy</td>
<td>Joint probability of two features</td>
</tr>
<tr class="even">
<td>Triple</td>
<td>∭ f(x,y,z) dx dy dz</td>
<td>Volumes, multivariate Gaussian normalization</td>
</tr>
<tr class="odd">
<td>Higher-order</td>
<td>∫ … ∫ f(x₁,…,xₙ) dx₁…dxₙ</td>
<td>Expectation in high-dimensional models</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-17" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-17">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> sp.symbols(<span class="st">'x y'</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> x <span class="op">+</span> y</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Double integral over square [0,1]x[0,1]</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>area <span class="op">=</span> sp.integrate(sp.integrate(f, (x, <span class="dv">0</span>, <span class="dv">1</span>)), (y, <span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Double integral over [0,1]x[0,1]:"</span>, area)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-15" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-15">Why It Matters</h4>
<p>Many AI models operate on high-dimensional data, where probabilities are defined via integrals across feature spaces. Normalizing Gaussian densities, computing evidence in Bayesian models, or estimating expectations all require multiple integrals. They connect geometry with probability in the spaces AI systems navigate.</p>
</section>
<section id="try-it-yourself-17" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-17">Try It Yourself</h4>
<ol type="1">
<li>Evaluate ∬ (x² + y²) dx dy over [0,1]×[0,1].</li>
<li>Compute ∭ 1 dx dy dz over the cube [0,1]³. What does it represent?</li>
<li>For joint density f(x,y) = 6xy on [0,1]×[0,1], check that its double integral equals 1.</li>
</ol>
</section>
</section>
<section id="differential-equations-basics" class="level3">
<h3 class="anchored" data-anchor-id="differential-equations-basics">119. Differential Equations Basics</h3>
<p>Differential equations describe how quantities change with respect to one another. Instead of just functions, they define relationships between a function and its derivatives. Solutions to differential equations capture dynamic processes evolving over time or space.</p>
<section id="picture-in-your-head-18" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-18">Picture in Your Head</h4>
<p>Think of a swinging pendulum. Its position changes, but its rate of change depends on velocity, and velocity depends on forces. A differential equation encodes this chain of dependencies, like a rulebook that governs motion rather than a single trajectory.</p>
</section>
<section id="deep-dive-18" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-18">Deep Dive</h4>
<ul>
<li><p>Ordinary Differential Equation (ODE): involves derivatives with respect to one variable (usually time). Example:</p>
<p><span class="math display">\[
\frac{dy}{dt} = ky
\]</span></p>
<p>has solution y(t) = Ce^{kt}.</p></li>
<li><p>Partial Differential Equation (PDE): involves derivatives with respect to multiple variables. Example: heat equation:</p>
<p><span class="math display">\[
\frac{\partial u}{\partial t} = \alpha \nabla^2 u.
\]</span></p></li>
<li><p>Initial value problem (IVP): specify conditions at a starting point to determine a unique solution.</p></li>
<li><p>Linear vs nonlinear: linear equations superpose solutions; nonlinear ones often create complex behaviors.</p></li>
<li><p>In AI: neural ODEs, diffusion models, and continuous-time dynamics all rest on differential equations.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 6%">
<col style="width: 26%">
<col style="width: 66%">
</colgroup>
<thead>
<tr class="header">
<th>Type</th>
<th>General Form</th>
<th>Example Use in AI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ODE</td>
<td>dy/dt = f(y,t)</td>
<td>Neural ODEs for continuous-depth models</td>
</tr>
<tr class="even">
<td>PDE</td>
<td>∂u/∂t = f(u,∇u,…)</td>
<td>Diffusion models for generative AI</td>
</tr>
<tr class="odd">
<td>IVP</td>
<td>y(t₀)=y₀</td>
<td>Simulating trajectories from initial state</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-18" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-18">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.integrate <span class="im">import</span> solve_ivp</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co"># ODE: dy/dt = -y</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(t, y):</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>y</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>sol <span class="op">=</span> solve_ivp(f, (<span class="dv">0</span>, <span class="dv">5</span>), [<span class="fl">1.0</span>], t_eval<span class="op">=</span>np.linspace(<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">6</span>))</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"t:"</span>, sol.t)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"y:"</span>, sol.y[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-16" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-16">Why It Matters</h4>
<p>Differential equations connect AI to physics and natural processes. They explain how continuous-time systems evolve and allow models like diffusion probabilistic models or neural ODEs to simulate dynamics. Mastery of differential equations equips AI practitioners to model beyond static data, into evolving systems.</p>
</section>
<section id="try-it-yourself-18" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-18">Try It Yourself</h4>
<ol type="1">
<li>Solve dy/dt = 2y with y(0)=1.</li>
<li>Write down the PDE governing heat diffusion in 1D.</li>
<li>Explain how an ODE solver could be used inside a neural network layer.</li>
</ol>
</section>
</section>
<section id="calculus-in-machine-learning-applications" class="level3">
<h3 class="anchored" data-anchor-id="calculus-in-machine-learning-applications">120. Calculus in Machine Learning Applications</h3>
<p>Calculus is not just abstract math—it powers nearly every algorithm in machine learning. Derivatives guide optimization, integrals handle probabilities, and multivariable calculus shapes how we train and regularize models. Understanding these connections makes the mathematical backbone of AI visible.</p>
<section id="picture-in-your-head-19" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-19">Picture in Your Head</h4>
<p>Imagine training a neural network as hiking down a mountain blindfolded. Derivatives tell you which way is downhill (gradient descent). Integrals measure the area you’ve already crossed (expectation over data). Together, they form the invisible GPS guiding your steps toward a valley of lower loss.</p>
</section>
<section id="deep-dive-19" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-19">Deep Dive</h4>
<ul>
<li><p>Derivatives in ML:</p>
<ul>
<li>Gradients of loss functions guide parameter updates.</li>
<li>Backpropagation applies the chain rule across layers.</li>
</ul></li>
<li><p>Integrals in ML:</p>
<ul>
<li><p>Probabilities as areas under density functions.</p></li>
<li><p>Expectations:</p>
<p><span class="math display">\[
\mathbb{E}[f(x)] = \int f(x) p(x) dx.
\]</span></p></li>
<li><p>Partition functions in probabilistic models.</p></li>
</ul></li>
<li><p>Optimization: finding minima of loss surfaces through derivatives.</p></li>
<li><p>Regularization: penalty terms often involve norms, tied to integrals of squared functions.</p></li>
<li><p>Continuous-time models: neural ODEs and diffusion models integrate dynamics.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 45%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Calculus Tool</th>
<th>Role in ML</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Derivative</td>
<td>Guides optimization</td>
<td>Gradient descent in neural networks</td>
</tr>
<tr class="even">
<td>Chain rule</td>
<td>Efficient backpropagation</td>
<td>Training deep nets</td>
</tr>
<tr class="odd">
<td>Integral</td>
<td>Probability and expectation</td>
<td>Likelihood, Bayesian inference</td>
</tr>
<tr class="even">
<td>Multivariable</td>
<td>Handles high-dimensional parameter spaces</td>
<td>Vectorized gradients in large models</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-19" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-19">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Loss function: mean squared error</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loss(w, x, y):</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> w <span class="op">*</span> x</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean((y <span class="op">-</span> y_pred)<span class="dv">2</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient of loss wrt w</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grad(w, x, y):</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span><span class="dv">2</span> <span class="op">*</span> np.mean(x <span class="op">*</span> (y <span class="op">-</span> w <span class="op">*</span> x))</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>])</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">6</span>,<span class="dv">8</span>])</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>    w <span class="op">-=</span> lr <span class="op">*</span> grad(w, x, y)</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, w=</span><span class="sc">{</span>w<span class="sc">:.4f}</span><span class="ss">, loss=</span><span class="sc">{</span>loss(w,x,y)<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-17" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-17">Why It Matters</h4>
<p>Calculus is the language of change, and machine learning is about changing parameters to fit data. Derivatives let us learn efficiently in high dimensions. Integrals make probability models consistent. Without calculus, optimization, probabilistic inference, and even basic learning algorithms would be impossible.</p>
</section>
<section id="try-it-yourself-19" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-19">Try It Yourself</h4>
<ol type="1">
<li>Show how the chain rule applies to f(x) = (3x+1)².</li>
<li>Express the expectation of f(x) = x under uniform distribution on [0,1] as an integral.</li>
<li>Compute the derivative of cross-entropy loss with respect to predicted probability p.</li>
</ol>
</section>
</section>
</section>
<section id="chapter-13.-probability-theory-fundamentals" class="level2">
<h2 class="anchored" data-anchor-id="chapter-13.-probability-theory-fundamentals">Chapter 13. Probability Theory Fundamentals</h2>
<section id="probability-axioms-and-sample-spaces" class="level3">
<h3 class="anchored" data-anchor-id="probability-axioms-and-sample-spaces">121. Probability Axioms and Sample Spaces</h3>
<p>Probability provides a formal framework for reasoning about uncertainty. At its core are three axioms that define how probabilities behave, and a sample space that captures all possible outcomes. Together, they turn randomness into a rigorous system we can compute with.</p>
<section id="picture-in-your-head-20" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-20">Picture in Your Head</h4>
<p>Imagine rolling a die. The sample space is the set of all possible faces {1,2,3,4,5,6}. Assigning probabilities is like pouring paint onto these outcomes so that the total paint equals 1. The axioms ensure the paint spreads consistently: nonnegative, complete, and additive.</p>
</section>
<section id="deep-dive-20" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-20">Deep Dive</h4>
<ul>
<li><p>Sample space (Ω): set of all possible outcomes.</p></li>
<li><p>Event: subset of Ω. Example: rolling an even number = {2,4,6}.</p></li>
<li><p>Axioms of probability (Kolmogorov):</p>
<ol type="1">
<li><p>Non-negativity: P(A) ≥ 0 for all events A.</p></li>
<li><p>Normalization: P(Ω) = 1.</p></li>
<li><p>Additivity: For disjoint events A, B:</p>
<p><span class="math display">\[
P(A \cup B) = P(A) + P(B).
\]</span></p></li>
</ol></li>
</ul>
<p>From these axioms, all other probability rules follow, such as complement, conditional probability, and independence.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 35%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Definition / Rule</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sample space Ω</td>
<td>All possible outcomes</td>
<td>Coin toss: {H, T}</td>
</tr>
<tr class="even">
<td>Event</td>
<td>Subset of Ω</td>
<td>Even number on die: {2,4,6}</td>
</tr>
<tr class="odd">
<td>Non-negativity</td>
<td>P(A) ≥ 0</td>
<td>Probability can’t be negative</td>
</tr>
<tr class="even">
<td>Normalization</td>
<td>P(Ω) = 1</td>
<td>Total probability of all die faces = 1</td>
</tr>
<tr class="odd">
<td>Additivity</td>
<td>P(A∪B) = P(A)+P(B), if A∩B=∅</td>
<td>P(odd ∪ even) = 1</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-20" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-20">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample space: fair six-sided die</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>sample_space <span class="op">=</span> {<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>}</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Uniform probability distribution</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>prob <span class="op">=</span> {outcome: <span class="dv">1</span><span class="op">/</span><span class="dv">6</span> <span class="cf">for</span> outcome <span class="kw">in</span> sample_space}</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability of event A = {2,4,6}</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> {<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>}</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>P_A <span class="op">=</span> <span class="bu">sum</span>(prob[x] <span class="cf">for</span> x <span class="kw">in</span> A)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"P(A):"</span>, P_A)   <span class="co"># 0.5</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Normalization check:"</span>, <span class="bu">sum</span>(prob.values()))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-18" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-18">Why It Matters</h4>
<p>AI systems constantly reason under uncertainty: predicting outcomes, estimating likelihoods, or sampling from models. The axioms guarantee consistency in these calculations. Without them, probability would collapse into contradictions, and machine learning models built on probabilistic foundations would be meaningless.</p>
</section>
<section id="try-it-yourself-20" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-20">Try It Yourself</h4>
<ol type="1">
<li>Define the sample space for flipping two coins. List all possible events.</li>
<li>If a biased coin has P(H) = 0.7 and P(T) = 0.3, check normalization.</li>
<li>Roll a die. What is the probability of getting a number divisible by 3?</li>
</ol>
</section>
</section>
<section id="random-variables-and-distributions" class="level3">
<h3 class="anchored" data-anchor-id="random-variables-and-distributions">122. Random Variables and Distributions</h3>
<p>Random variables assign numerical values to outcomes of a random experiment. They let us translate abstract events into numbers we can calculate with. The distribution of a random variable tells us how likely each value is, shaping the behavior of probabilistic models.</p>
<section id="picture-in-your-head-21" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-21">Picture in Your Head</h4>
<p>Think of rolling a die. The outcome is a symbol like “3,” but the random variable X maps this to the number 3. Now imagine throwing darts at a dartboard: the random variable could be the distance from the center. Distributions describe whether outcomes are spread evenly, clustered, or skewed.</p>
</section>
<section id="deep-dive-21" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-21">Deep Dive</h4>
<ul>
<li><p>Random variable (RV): A function X: Ω → ℝ.</p></li>
<li><p>Discrete RV: takes countable values (coin toss, die roll).</p></li>
<li><p>Continuous RV: takes values in intervals of ℝ (height, time).</p></li>
<li><p>Probability Mass Function (PMF):</p>
<p><span class="math display">\[
P(X = x) = p(x), \quad \sum_x p(x) = 1.
\]</span></p></li>
<li><p>Probability Density Function (PDF):</p>
<p><span class="math display">\[
P(a \leq X \leq b) = \int_a^b f(x)\,dx, \quad \int_{-\infty}^\infty f(x)\,dx = 1.
\]</span></p></li>
<li><p>Cumulative Distribution Function (CDF):</p>
<p><span class="math display">\[
F(x) = P(X \leq x).
\]</span></p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 21%">
<col style="width: 63%">
</colgroup>
<thead>
<tr class="header">
<th>Type</th>
<th>Representation</th>
<th>Example in AI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Discrete</td>
<td>PMF p(x)</td>
<td>Word counts, categorical labels</td>
</tr>
<tr class="even">
<td>Continuous</td>
<td>PDF f(x)</td>
<td>Feature distributions (height, signal value)</td>
</tr>
<tr class="odd">
<td>CDF</td>
<td>F(x) = P(X ≤ x)</td>
<td>Threshold probabilities, quantiles</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-21" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-21">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Discrete: fair die</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>die_outcomes <span class="op">=</span> [<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>]</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>pmf <span class="op">=</span> {x: <span class="dv">1</span><span class="op">/</span><span class="dv">6</span> <span class="cf">for</span> x <span class="kw">in</span> die_outcomes}</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Continuous: Normal distribution</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>mu, sigma <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">5</span>)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>pdf_values <span class="op">=</span> norm.pdf(x, mu, sigma)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>cdf_values <span class="op">=</span> norm.cdf(x, mu, sigma)</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Die PMF:"</span>, pmf)</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Normal PDF:"</span>, pdf_values)</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Normal CDF:"</span>, cdf_values)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-19" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-19">Why It Matters</h4>
<p>Machine learning depends on modeling data distributions. Random variables turn uncertainty into analyzable numbers, while distributions tell us how data is spread. Class probabilities in classifiers, Gaussian assumptions in regression, and sampling in generative models all rely on these ideas.</p>
</section>
<section id="try-it-yourself-21" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-21">Try It Yourself</h4>
<ol type="1">
<li>Define a random variable for tossing a coin twice. What values can it take?</li>
<li>For a fair die, what is the PMF of X = “die roll”?</li>
<li>For a continuous variable X ∼ Uniform(0,1), compute P(0.2 ≤ X ≤ 0.5).</li>
</ol>
</section>
</section>
<section id="expectation-variance-and-moments" class="level3">
<h3 class="anchored" data-anchor-id="expectation-variance-and-moments">123. Expectation, Variance, and Moments</h3>
<p>Expectation measures the average value of a random variable in the long run. Variance quantifies how spread out the values are around that average. Higher moments (like skewness and kurtosis) describe asymmetry and tail heaviness. These statistics summarize distributions into interpretable quantities.</p>
<section id="picture-in-your-head-22" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-22">Picture in Your Head</h4>
<p>Imagine tossing a coin thousands of times and recording 1 for heads, 0 for tails. The expectation is the long-run fraction of heads, the variance tells how often results deviate from that average, and higher moments reveal whether the distribution is balanced or skewed. It’s like reducing a noisy dataset to a handful of meaningful descriptors.</p>
</section>
<section id="deep-dive-22" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-22">Deep Dive</h4>
<ul>
<li><p>Expectation (mean):</p>
<ul>
<li><p>Discrete:</p>
<p><span class="math display">\[
\mathbb{E}[X] = \sum_x x \, p(x).
\]</span></p></li>
<li><p>Continuous:</p>
<p><span class="math display">\[
\mathbb{E}[X] = \int_{-\infty}^\infty x \, f(x) \, dx.
\]</span></p></li>
</ul></li>
<li><p>Variance:</p>
<p><span class="math display">\[
\text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2.
\]</span></p></li>
<li><p>Standard deviation: square root of variance.</p></li>
<li><p>Higher moments:</p>
<ul>
<li>Skewness: asymmetry.</li>
<li>Kurtosis: heaviness of tails.</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Statistic</th>
<th>Formula</th>
<th>Interpretation in AI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Expectation</td>
<td>E[X]</td>
<td>Predicted output, mean loss</td>
</tr>
<tr class="even">
<td>Variance</td>
<td>E[(X−μ)²]</td>
<td>Uncertainty in predictions</td>
</tr>
<tr class="odd">
<td>Skewness</td>
<td>E[((X−μ)/σ)³]</td>
<td>Bias toward one side</td>
</tr>
<tr class="even">
<td>Kurtosis</td>
<td>E[((X−μ)/σ)⁴]</td>
<td>Outlier sensitivity</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-22" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-22">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample data: simulated predictions</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array([<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">9</span>])</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Expectation</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> np.mean(data)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Variance and standard deviation</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>var <span class="op">=</span> np.var(data)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>std <span class="op">=</span> np.std(data)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Higher moments</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>skew <span class="op">=</span> ((data <span class="op">-</span> mean)<span class="dv">3</span>).mean() <span class="op">/</span> (std3)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>kurt <span class="op">=</span> ((data <span class="op">-</span> mean)<span class="dv">4</span>).mean() <span class="op">/</span> (std4)</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Mean:"</span>, mean)</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Variance:"</span>, var)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Skewness:"</span>, skew)</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Kurtosis:"</span>, kurt)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-20" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-20">Why It Matters</h4>
<p>Expectations are used in defining loss functions, variances quantify uncertainty in probabilistic models, and higher moments detect distributional shifts. For example, expected risk underlies learning theory, variance is minimized in ensemble methods, and kurtosis signals heavy-tailed data often found in real-world datasets.</p>
</section>
<section id="try-it-yourself-22" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-22">Try It Yourself</h4>
<ol type="1">
<li>Compute the expectation of rolling a fair die.</li>
<li>What is the variance of a Bernoulli random variable with p=0.3?</li>
<li>Explain why minimizing expected loss (not variance) is the goal in training, but variance still matters for model stability.</li>
</ol>
</section>
</section>
<section id="common-distributions-bernoulli-binomial-gaussian" class="level3">
<h3 class="anchored" data-anchor-id="common-distributions-bernoulli-binomial-gaussian">124. Common Distributions (Bernoulli, Binomial, Gaussian)</h3>
<p>Certain probability distributions occur so often in real-world problems that they are considered “canonical.” The Bernoulli models a single yes/no event, the Binomial models repeated independent trials, and the Gaussian (Normal) models continuous data clustered around a mean. Mastering these is essential for building and interpreting AI models.</p>
<section id="picture-in-your-head-23" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-23">Picture in Your Head</h4>
<p>Imagine flipping a single coin: that’s Bernoulli. Flip the coin ten times and count heads: that’s Binomial. Measure people’s heights: most cluster near average with some shorter and taller outliers—that’s Gaussian. These three form the basic vocabulary of probability.</p>
</section>
<section id="deep-dive-23" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-23">Deep Dive</h4>
<ul>
<li><p>Bernoulli(p):</p>
<ul>
<li>Values: {0,1}, success probability p.</li>
<li>PMF: P(X=1)=p, P(X=0)=1−p.</li>
<li>Mean: p, Variance: p(1−p).</li>
</ul></li>
<li><p>Binomial(n,p):</p>
<ul>
<li><p>Number of successes in n independent Bernoulli trials.</p></li>
<li><p>PMF:</p>
<p><span class="math display">\[
P(X=k) = \binom{n}{k} p^k (1-p)^{n-k}.
\]</span></p></li>
<li><p>Mean: np, Variance: np(1−p).</p></li>
</ul></li>
<li><p>Gaussian(μ,σ²):</p>
<ul>
<li><p>Continuous distribution with PDF:</p>
<p><span class="math display">\[
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right).
\]</span></p></li>
<li><p>Mean: μ, Variance: σ².</p></li>
<li><p>Appears by Central Limit Theorem.</p></li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 33%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th>Distribution</th>
<th>Formula</th>
<th>Example in AI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bernoulli</td>
<td>P(X=1)=p, P(X=0)=1−p</td>
<td>Binary labels, dropout masks</td>
</tr>
<tr class="even">
<td>Binomial</td>
<td>P(X=k)=C(n,k)pᵏ(1−p)ⁿ⁻ᵏ</td>
<td>Number of successes in trials</td>
</tr>
<tr class="odd">
<td>Gaussian</td>
<td>f(x) ∝ exp(−(x−μ)²/2σ²)</td>
<td>Noise models, continuous features</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-23" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-23">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> bernoulli, binom, norm</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Bernoulli trial</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="fl">0.7</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>sample <span class="op">=</span> bernoulli.rvs(p, size<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Binomial: 10 trials, p=0.5</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>binom_samples <span class="op">=</span> binom.rvs(<span class="dv">10</span>, <span class="fl">0.5</span>, size<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Gaussian: mu=0, sigma=1</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>gauss_samples <span class="op">=</span> norm.rvs(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">1</span>, size<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Bernoulli samples:"</span>, sample)</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Binomial samples:"</span>, binom_samples)</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Gaussian samples:"</span>, gauss_samples)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-21" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-21">Why It Matters</h4>
<p>Many machine learning algorithms assume specific distributions: logistic regression assumes Bernoulli outputs, Naive Bayes uses Binomial/Multinomial, and Gaussian assumptions appear in linear regression, PCA, and generative models. Recognizing these distributions connects statistical modeling to practical AI.</p>
</section>
<section id="try-it-yourself-23" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-23">Try It Yourself</h4>
<ol type="1">
<li>What are the mean and variance of a Binomial(20, 0.4) distribution?</li>
<li>Simulate 1000 Gaussian samples with μ=5, σ=2 and compute their sample mean. How close is it to the true mean?</li>
<li>Explain why the Gaussian is often used to model noise in data.</li>
</ol>
</section>
</section>
<section id="joint-marginal-and-conditional-probability" class="level3">
<h3 class="anchored" data-anchor-id="joint-marginal-and-conditional-probability">125. Joint, Marginal, and Conditional Probability</h3>
<p>When dealing with multiple random variables, probabilities can be combined (joint), reduced (marginal), or conditioned (conditional). These operations form the grammar of probabilistic reasoning, allowing us to express how variables interact and how knowledge of one affects belief about another.</p>
<section id="picture-in-your-head-24" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-24">Picture in Your Head</h4>
<p>Think of two dice rolled together. The joint probability is the full grid of all 36 outcomes. Marginal probability is like looking only at one die’s values, ignoring the other. Conditional probability is asking: if the first die shows a 6, what is the probability that the sum is greater than 10?</p>
</section>
<section id="deep-dive-24" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-24">Deep Dive</h4>
<ul>
<li><p>Joint probability: probability of events happening together.</p>
<ul>
<li>Discrete: P(X=x, Y=y).</li>
<li>Continuous: joint density f(x,y).</li>
</ul></li>
<li><p>Marginal probability: probability of a subset of variables, obtained by summing/integrating over others.</p>
<ul>
<li>Discrete: P(X=x) = Σ_y P(X=x, Y=y).</li>
<li>Continuous: f_X(x) = ∫ f(x,y) dy.</li>
</ul></li>
<li><p>Conditional probability:</p>
<p><span class="math display">\[
P(X|Y) = \frac{P(X,Y)}{P(Y)}, \quad P(Y)&gt;0.
\]</span></p></li>
<li><p>Chain rule of probability:</p>
<p><span class="math display">\[
P(X_1, …, X_n) = \prod_{i=1}^n P(X_i | X_1, …, X_{i-1}).
\]</span></p></li>
<li><p>In AI: joint models define distributions over data, marginals appear in feature distributions, and conditionals are central to Bayesian inference.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 19%">
<col style="width: 34%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Formula</th>
<th>Example in AI</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Joint</td>
<td>P(X,Y)</td>
<td>Image pixel + label distribution</td>
<td></td>
</tr>
<tr class="even">
<td>Marginal</td>
<td>P(X) = Σ_y P(X,Y)</td>
<td>Distribution of one feature alone</td>
<td></td>
</tr>
<tr class="odd">
<td>Conditional</td>
<td>P(X</td>
<td>Y) = P(X,Y)/P(Y)</td>
<td>Class probabilities given features</td>
</tr>
<tr class="even">
<td>Chain rule</td>
<td>P(X₁,…,Xₙ) = Π P(Xᵢ</td>
<td>X₁…Xᵢ₋₁)</td>
<td>Generative sequence models</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-24" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-24">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Joint distribution for two binary variables X,Y</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>joint <span class="op">=</span> np.array([[<span class="fl">0.1</span>, <span class="fl">0.2</span>],</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>                  [<span class="fl">0.3</span>, <span class="fl">0.4</span>]])  <span class="co"># rows=X, cols=Y</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Marginals</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>P_X <span class="op">=</span> joint.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>P_Y <span class="op">=</span> joint.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Conditional P(X|Y=1)</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>P_X_given_Y1 <span class="op">=</span> joint[:,<span class="dv">1</span>] <span class="op">/</span> P_Y[<span class="dv">1</span>]</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Joint:</span><span class="ch">\n</span><span class="st">"</span>, joint)</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Marginal P(X):"</span>, P_X)</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Marginal P(Y):"</span>, P_Y)</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Conditional P(X|Y=1):"</span>, P_X_given_Y1)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-22" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-22">Why It Matters</h4>
<p>Probabilistic models in AI—from Bayesian networks to hidden Markov models—are built from joint, marginal, and conditional probabilities. Classification is essentially conditional probability estimation (P(label | features)). Generative models learn joint distributions, while inference often involves computing marginals.</p>
</section>
<section id="try-it-yourself-24" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-24">Try It Yourself</h4>
<ol type="1">
<li>For a fair die and coin, what is the joint probability of rolling a 3 and flipping heads?</li>
<li>From joint distribution P(X,Y), derive P(X) by marginalization.</li>
<li>Explain why P(A|B) ≠ P(B|A), with an example from medical diagnosis.</li>
</ol>
</section>
</section>
<section id="independence-and-correlation" class="level3">
<h3 class="anchored" data-anchor-id="independence-and-correlation">126. Independence and Correlation</h3>
<p>Independence means two random variables do not influence each other: knowing one tells you nothing about the other. Correlation measures the strength and direction of linear dependence. Together, they help us characterize whether features or events are related, redundant, or informative.</p>
<section id="picture-in-your-head-25" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-25">Picture in Your Head</h4>
<p>Imagine rolling two dice. The result of one die does not affect the other—this is independence. Now imagine height and weight: they are not independent, because taller people tend to weigh more. The correlation quantifies this relationship on a scale from −1 (perfect negative) to +1 (perfect positive).</p>
</section>
<section id="deep-dive-25" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-25">Deep Dive</h4>
<ul>
<li><p>Independence:</p>
<p><span class="math display">\[
P(X,Y) = P(X)P(Y), \quad \text{or equivalently } P(X|Y)=P(X).
\]</span></p></li>
<li><p>Correlation coefficient (Pearson’s ρ):</p>
<p><span class="math display">\[
\rho(X,Y) = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}.
\]</span></p></li>
<li><p>Covariance:</p>
<p><span class="math display">\[
\text{Cov}(X,Y) = \mathbb{E}[(X-\mu_X)(Y-\mu_Y)].
\]</span></p></li>
<li><p>Independence ⇒ zero correlation (for uncorrelated distributions), but zero correlation does not imply independence in general.</p></li>
<li><p>In AI: independence assumptions simplify models (Naive Bayes). Correlation analysis detects redundant features and spurious relationships.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 19%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Formula</th>
<th>AI Role</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Independence</td>
<td>P(X,Y)=P(X)P(Y)</td>
<td>Feature independence in Naive Bayes</td>
</tr>
<tr class="even">
<td>Covariance</td>
<td>E[(X−μX)(Y−μY)]</td>
<td>Relationship strength</td>
</tr>
<tr class="odd">
<td>Correlation ρ</td>
<td>Cov(X,Y)/(σXσY)</td>
<td>Normalized measure (−1 to 1)</td>
</tr>
<tr class="even">
<td>Zero correlation</td>
<td>ρ=0</td>
<td>No linear relation, but not necessarily independent</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-25" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-25">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Example data</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>])</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.array([<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">8</span>, <span class="dv">10</span>])  <span class="co"># perfectly correlated</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Covariance</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>cov <span class="op">=</span> np.cov(X, Y, bias<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>,<span class="dv">1</span>]</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Correlation</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>corr <span class="op">=</span> np.corrcoef(X, Y)[<span class="dv">0</span>,<span class="dv">1</span>]</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Covariance:"</span>, cov)</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Correlation:"</span>, corr)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-23" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-23">Why It Matters</h4>
<p>Understanding independence allows us to simplify joint distributions and design tractable probabilistic models. Correlation helps in feature engineering—removing redundant features or identifying signals. Misinterpreting correlation as causation can lead to faulty AI conclusions, so distinguishing the two is critical.</p>
</section>
<section id="try-it-yourself-25" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-25">Try It Yourself</h4>
<ol type="1">
<li>If X = coin toss, Y = die roll, are X and Y independent? Why?</li>
<li>Compute the correlation between X = [1,2,3] and Y = [3,2,1]. What does the sign indicate?</li>
<li>Give an example where two variables have zero correlation but are not independent.</li>
</ol>
</section>
</section>
<section id="law-of-large-numbers" class="level3">
<h3 class="anchored" data-anchor-id="law-of-large-numbers">127. Law of Large Numbers</h3>
<p>The Law of Large Numbers (LLN) states that as the number of trials grows, the average of observed outcomes converges to the expected value. Randomness dominates in the short run, but averages stabilize in the long run. This principle explains why empirical data approximates true probabilities.</p>
<section id="picture-in-your-head-26" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-26">Picture in Your Head</h4>
<p>Imagine flipping a fair coin. In 10 flips, you might get 7 heads. In 1000 flips, you’ll be close to 500 heads. The noise of chance evens out, and the proportion of heads converges to 0.5. It’s like blurry vision becoming clearer as more data accumulates.</p>
</section>
<section id="deep-dive-26" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-26">Deep Dive</h4>
<ul>
<li><p>Weak Law of Large Numbers (WLLN): For i.i.d. random variables X₁,…,Xₙ with mean μ,</p>
<p><span class="math display">\[
\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i \to μ \quad \text{in probability as } n→∞.
\]</span></p></li>
<li><p>Strong Law of Large Numbers (SLLN):</p>
<p><span class="math display">\[
\bar{X}_n \to μ \quad \text{almost surely as } n→∞.
\]</span></p></li>
<li><p>Conditions: finite expectation μ.</p></li>
<li><p>In AI: LLN underlies empirical risk minimization—training loss approximates expected loss as dataset size grows.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 21%">
<col style="width: 64%">
</colgroup>
<thead>
<tr class="header">
<th>Form</th>
<th>Convergence Type</th>
<th>Meaning in AI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Weak LLN</td>
<td>In probability</td>
<td>Training error ≈ expected error with enough data</td>
</tr>
<tr class="even">
<td>Strong LLN</td>
<td>Almost surely</td>
<td>Guarantees convergence on almost every sequence</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-26" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-26">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate coin flips (Bernoulli trials)</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>n_trials <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>coin_flips <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, <span class="fl">0.5</span>, n_trials)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Running averages</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>running_avg <span class="op">=</span> np.cumsum(coin_flips) <span class="op">/</span> np.arange(<span class="dv">1</span>, n_trials<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Final running average:"</span>, running_avg[<span class="op">-</span><span class="dv">1</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-24" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-24">Why It Matters</h4>
<p>LLN explains why training on larger datasets improves reliability. It guarantees that averages of noisy observations approximate true expectations, making probability-based models feasible. Without LLN, empirical statistics like mean accuracy or loss would never stabilize.</p>
</section>
<section id="try-it-yourself-26" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-26">Try It Yourself</h4>
<ol type="1">
<li>Simulate 100 rolls of a fair die and compute the running average. Does it approach 3.5?</li>
<li>Explain how LLN justifies using validation accuracy to estimate generalization.</li>
<li>If a random variable has infinite variance, does the LLN still hold?</li>
</ol>
</section>
</section>
<section id="central-limit-theorem" class="level3">
<h3 class="anchored" data-anchor-id="central-limit-theorem">128. Central Limit Theorem</h3>
<p>The Central Limit Theorem (CLT) states that the distribution of the sum (or average) of many independent, identically distributed random variables tends toward a normal distribution, regardless of the original distribution. This explains why the Gaussian distribution appears so frequently in statistics and AI.</p>
<section id="picture-in-your-head-27" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-27">Picture in Your Head</h4>
<p>Imagine sampling numbers from any strange distribution—uniform, skewed, even discrete. If you average enough samples, the histogram of those averages begins to form the familiar bell curve. It’s as if nature smooths out irregularities when many random effects combine.</p>
</section>
<section id="deep-dive-27" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-27">Deep Dive</h4>
<ul>
<li><p>Statement (simplified): Let X₁,…,Xₙ be i.i.d. with mean μ and variance σ². Then</p>
<p><span class="math display">\[
\frac{\bar{X}_n - μ}{σ/\sqrt{n}} \to \mathcal{N}(0,1) \quad \text{as } n \to ∞.
\]</span></p></li>
<li><p>Requirements: finite mean and variance.</p></li>
<li><p>Generalizations exist for weaker assumptions.</p></li>
<li><p>In AI: CLT justifies approximating distributions with Gaussians, motivates confidence intervals, and explains why stochastic gradients behave as noisy normal variables.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 42%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Formula</th>
<th>AI Application</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sample mean distribution</td>
<td>(X̄ − μ)/(σ/√n) → N(0,1)</td>
<td>Confidence bounds on model accuracy</td>
</tr>
<tr class="even">
<td>Gaussian emergence</td>
<td>Sums/averages of random variables look normal</td>
<td>Approximation in inference &amp; learning</td>
</tr>
<tr class="odd">
<td>Variance scaling</td>
<td>Std. error = σ/√n</td>
<td>More data = less uncertainty</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-27" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-27">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw from uniform distribution</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">1</span>, (<span class="dv">10000</span>, <span class="dv">50</span>))  <span class="co"># 50 samples each</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>averages <span class="op">=</span> samples.mean(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Check mean and std</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Sample mean:"</span>, np.mean(averages))</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Sample std:"</span>, np.std(averages))</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot histogram</span></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>plt.hist(averages, bins<span class="op">=</span><span class="dv">30</span>, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"CLT: Distribution of Averages (Uniform → Gaussian)"</span>)</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-25" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-25">Why It Matters</h4>
<p>The CLT explains why Gaussian assumptions are safe in many models, even if underlying data is not Gaussian. It powers statistical testing, confidence intervals, and uncertainty estimation. In machine learning, it justifies treating stochastic gradient noise as Gaussian and simplifies analysis of large models.</p>
</section>
<section id="try-it-yourself-27" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-27">Try It Yourself</h4>
<ol type="1">
<li>Simulate 1000 averages of 10 coin tosses (Bernoulli p=0.5). What does the histogram look like?</li>
<li>Explain why the CLT makes the Gaussian central to Bayesian inference.</li>
<li>How does increasing n (sample size) change the standard error of the sample mean?</li>
</ol>
</section>
</section>
<section id="bayes-theorem-and-conditional-inference" class="level3">
<h3 class="anchored" data-anchor-id="bayes-theorem-and-conditional-inference">129. Bayes’ Theorem and Conditional Inference</h3>
<p>Bayes’ Theorem provides a way to update beliefs when new evidence arrives. It relates prior knowledge, likelihood of data, and posterior beliefs. This simple formula underpins probabilistic reasoning, classification, and modern Bayesian machine learning.</p>
<section id="picture-in-your-head-28" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-28">Picture in Your Head</h4>
<p>Imagine a medical test for a rare disease. Before testing, you know the disease is rare (prior). If the test comes back positive (evidence), Bayes’ Theorem updates your belief about whether the person is actually sick (posterior). It’s like recalculating odds every time you learn something new.</p>
</section>
<section id="deep-dive-28" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-28">Deep Dive</h4>
<ul>
<li><p>Bayes’ Theorem:</p>
<p><span class="math display">\[
P(A|B) = \frac{P(B|A)P(A)}{P(B)}.
\]</span></p>
<ul>
<li>P(A): prior probability of event A.</li>
<li>P(B|A): likelihood of evidence given A.</li>
<li>P(B): normalizing constant = Σ P(B|Ai)P(Ai).</li>
<li>P(A|B): posterior probability after seeing B.</li>
</ul></li>
<li><p>Odds form:</p>
<p><span class="math display">\[
\text{Posterior odds} = \text{Prior odds} \times \text{Likelihood ratio}.
\]</span></p></li>
<li><p>In AI:</p>
<ul>
<li>Naive Bayes classifiers use conditional independence to simplify P(X|Y).</li>
<li>Bayesian inference updates model parameters.</li>
<li>Probabilistic reasoning systems (e.g., spam filtering, diagnostics).</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 7%">
<col style="width: 27%">
<col style="width: 30%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th>Term</th>
<th>Meaning</th>
<th>AI Example</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Prior P(A)</td>
<td>Belief before seeing evidence</td>
<td>Spam rate before checking email</td>
<td></td>
</tr>
<tr class="even">
<td>Likelihood</td>
<td>P(B</td>
<td>A): evidence given hypothesis</td>
<td>Probability email contains “free” if spam</td>
</tr>
<tr class="odd">
<td>Posterior</td>
<td>P(A</td>
<td>B): updated belief after evidence</td>
<td>Probability email is spam given “free” word</td>
</tr>
<tr class="even">
<td>Normalizer</td>
<td>P(B) ensures probabilities sum to 1</td>
<td>Adjust for total frequency of evidence</td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-28" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-28">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Disease testing</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>P_disease <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>P_pos_given_disease <span class="op">=</span> <span class="fl">0.95</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>P_pos_given_no <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Total probability of positive test</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>P_pos <span class="op">=</span> P_pos_given_disease<span class="op">*</span>P_disease <span class="op">+</span> P_pos_given_no<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>P_disease)</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>P_disease_given_pos <span class="op">=</span> (P_pos_given_disease<span class="op">*</span>P_disease) <span class="op">/</span> P_pos</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"P(disease | positive test):"</span>, P_disease_given_pos)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-26" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-26">Why It Matters</h4>
<p>Bayes’ Theorem is the foundation of probabilistic AI. It explains how classifiers infer labels from features, how models incorporate uncertainty, and how predictions adjust with new evidence. Without Bayes, probabilistic reasoning in AI would be fragmented and incoherent.</p>
</section>
<section id="try-it-yourself-28" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-28">Try It Yourself</h4>
<ol type="1">
<li>A spam filter assigns prior P(spam)=0.2. If P(“win”|spam)=0.6 and P(“win”|not spam)=0.05, compute P(spam|“win”).</li>
<li>Why is P(A|B) ≠ P(B|A)? Give an everyday example.</li>
<li>Explain how Naive Bayes simplifies computing P(X|Y) in high dimensions.</li>
</ol>
</section>
</section>
<section id="probabilistic-models-in-ai" class="level3">
<h3 class="anchored" data-anchor-id="probabilistic-models-in-ai">130. Probabilistic Models in AI</h3>
<p>Probabilistic models describe data and uncertainty using distributions. They provide structured ways to capture randomness, model dependencies, and make predictions with confidence levels. These models are central to AI, where uncertainty is the norm rather than the exception.</p>
<section id="picture-in-your-head-29" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-29">Picture in Your Head</h4>
<p>Think of predicting tomorrow’s weather. Instead of saying “It will rain,” a probabilistic model says, “There’s a 70% chance of rain.” This uncertainty-aware prediction is more realistic. Probabilistic models act like maps with probabilities attached to each possible future.</p>
</section>
<section id="deep-dive-29" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-29">Deep Dive</h4>
<ul>
<li><p>Generative models: learn joint distributions P(X,Y). Example: Naive Bayes, Hidden Markov Models, Variational Autoencoders.</p></li>
<li><p>Discriminative models: focus on conditional probability P(Y|X). Example: Logistic Regression, Conditional Random Fields.</p></li>
<li><p>Graphical models: represent dependencies with graphs. Example: Bayesian Networks, Markov Random Fields.</p></li>
<li><p>Probabilistic inference: computing marginals, posteriors, or MAP estimates.</p></li>
<li><p>In AI pipelines:</p>
<ul>
<li>Uncertainty estimation in predictions.</li>
<li>Decision-making under uncertainty.</li>
<li>Data generation and simulation.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 27%">
<col style="width: 26%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th>Model Type</th>
<th>Focus</th>
<th>Example in AI</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Generative</td>
<td>Joint P(X,Y)</td>
<td>Naive Bayes, VAEs</td>
<td></td>
</tr>
<tr class="even">
<td>Discriminative</td>
<td>Conditional P(Y</td>
<td>X)</td>
<td>Logistic regression, CRFs</td>
</tr>
<tr class="odd">
<td>Graphical</td>
<td>Structure + dependencies</td>
<td>HMMs, Bayesian networks</td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-29" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-29">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> GaussianNB</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: simple Naive Bayes classifier</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="fl">1.8</span>, <span class="dv">80</span>], [<span class="fl">1.6</span>, <span class="dv">60</span>], [<span class="fl">1.7</span>, <span class="dv">65</span>], [<span class="fl">1.5</span>, <span class="dv">50</span>]])  <span class="co"># features: height, weight</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>])  <span class="co"># labels: 1=male, 0=female</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> GaussianNB()</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict probabilities</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> model.predict_proba([[<span class="fl">1.7</span>, <span class="dv">70</span>]])</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predicted probabilities:"</span>, probs)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-27" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-27">Why It Matters</h4>
<p>Probabilistic models let AI systems express confidence, combine prior knowledge with new evidence, and reason about incomplete information. From spam filters to speech recognition and modern generative AI, probability provides the mathematical backbone for making reliable predictions.</p>
</section>
<section id="try-it-yourself-29" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-29">Try It Yourself</h4>
<ol type="1">
<li>Explain how Naive Bayes assumes independence among features.</li>
<li>What is the difference between modeling P(X,Y) vs P(Y|X)?</li>
<li>Describe how a probabilistic model could handle missing data.</li>
</ol>
</section>
</section>
</section>
<section id="chapter-14.-statistics-and-estimation" class="level2">
<h2 class="anchored" data-anchor-id="chapter-14.-statistics-and-estimation">Chapter 14. Statistics and Estimation</h2>
<section id="descriptive-statistics-and-summaries" class="level3">
<h3 class="anchored" data-anchor-id="descriptive-statistics-and-summaries">131. Descriptive Statistics and Summaries</h3>
<p>Descriptive statistics condense raw data into interpretable summaries. Instead of staring at thousands of numbers, we reduce them to measures like mean, median, variance, and quantiles. These summaries highlight central tendencies, variability, and patterns, making datasets comprehensible.</p>
<section id="picture-in-your-head-30" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-30">Picture in Your Head</h4>
<p>Think of a classroom’s exam scores. Instead of listing every score, you might say, “The average was 75, most students scored between 70 and 80, and the highest was 95.” These summaries give a clear picture without overwhelming detail.</p>
</section>
<section id="deep-dive-30" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-30">Deep Dive</h4>
<ul>
<li>Measures of central tendency: mean (average), median (middle), mode (most frequent).</li>
<li>Measures of dispersion: range, variance, standard deviation, interquartile range.</li>
<li>Shape descriptors: skewness (asymmetry), kurtosis (tail heaviness).</li>
<li>Visualization aids: histograms, box plots, summary tables.</li>
<li>In AI: descriptive stats guide feature engineering, outlier detection, and data preprocessing.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 28%">
<col style="width: 55%">
</colgroup>
<thead>
<tr class="header">
<th>Statistic</th>
<th>Formula / Definition</th>
<th>AI Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Mean (μ)</td>
<td>(1/n) Σ xi</td>
<td>Baseline average performance</td>
</tr>
<tr class="even">
<td>Median</td>
<td>Middle value when sorted</td>
<td>Robust measure against outliers</td>
</tr>
<tr class="odd">
<td>Variance (σ²)</td>
<td>(1/n) Σ (xi−μ)²</td>
<td>Spread of feature distributions</td>
</tr>
<tr class="even">
<td>IQR</td>
<td>Q3 − Q1</td>
<td>Detecting outliers</td>
</tr>
<tr class="odd">
<td>Skewness</td>
<td>E[((X−μ)/σ)³]</td>
<td>Identifying asymmetry in feature distributions</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-30" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-30">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> skew, kurtosis</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array([<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">10</span>])</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> np.mean(data)</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>median <span class="op">=</span> np.median(data)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>var <span class="op">=</span> np.var(data)</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>sk <span class="op">=</span> skew(data)</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>kt <span class="op">=</span> kurtosis(data)</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Mean:"</span>, mean)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Median:"</span>, median)</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Variance:"</span>, var)</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Skewness:"</span>, sk)</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Kurtosis:"</span>, kt)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-28" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-28">Why It Matters</h4>
<p>Before training a model, understanding your dataset is crucial. Descriptive statistics reveal biases, anomalies, and trends. They are the first checkpoint in exploratory data analysis (EDA), helping practitioners avoid errors caused by misunderstood or skewed data.</p>
</section>
<section id="try-it-yourself-30" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-30">Try It Yourself</h4>
<ol type="1">
<li>Compute the mean, median, and variance of exam scores: [60, 65, 70, 80, 85, 90, 100].</li>
<li>Which is more robust to outliers: mean or median? Why?</li>
<li>Plot a histogram of 1000 random Gaussian samples and describe its shape.</li>
</ol>
</section>
</section>
<section id="sampling-distributions" class="level3">
<h3 class="anchored" data-anchor-id="sampling-distributions">132. Sampling Distributions</h3>
<p>A sampling distribution is the probability distribution of a statistic (like the mean or variance) computed from repeated random samples of the same population. It explains how statistics vary from sample to sample and provides the foundation for statistical inference.</p>
<section id="picture-in-your-head-31" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-31">Picture in Your Head</h4>
<p>Imagine repeatedly drawing small groups of students from a university and calculating their average height. Each group will have a slightly different average. If you plot all these averages, you’ll see a new distribution—the sampling distribution of the mean.</p>
</section>
<section id="deep-dive-31" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-31">Deep Dive</h4>
<ul>
<li><p>Statistic vs parameter: parameter = fixed property of population, statistic = estimate from sample.</p></li>
<li><p>Sampling distribution: distribution of a statistic across repeated samples.</p></li>
<li><p>Key result: the sampling distribution of the sample mean has mean μ and variance σ²/n.</p></li>
<li><p>Central Limit Theorem: ensures the sampling distribution of the mean approaches normality for large n.</p></li>
<li><p>Standard error (SE): standard deviation of the sampling distribution:</p>
<p><span class="math display">\[
SE = \frac{\sigma}{\sqrt{n}}.
\]</span></p></li>
<li><p>In AI: sampling distributions explain variability in validation accuracy, generalization gaps, and performance metrics.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 34%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Formula / Rule</th>
<th>AI Connection</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sampling distribution</td>
<td>Distribution of statistics</td>
<td>Variability of model metrics</td>
</tr>
<tr class="even">
<td>Standard error (SE)</td>
<td>σ/√n</td>
<td>Confidence in accuracy estimates</td>
</tr>
<tr class="odd">
<td>CLT link</td>
<td>Mean sampling distribution ≈ normal</td>
<td>Justifies Gaussian assumptions in experiments</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-31" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-31">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Population: pretend test scores</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>population <span class="op">=</span> np.random.normal(<span class="dv">70</span>, <span class="dv">10</span>, <span class="dv">10000</span>)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw repeated samples and compute means</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>sample_means <span class="op">=</span> [np.mean(np.random.choice(population, <span class="dv">50</span>)) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>)]</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Mean of sample means:"</span>, np.mean(sample_means))</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Std of sample means (SE):"</span>, np.std(sample_means))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-29" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-29">Why It Matters</h4>
<p>Model evaluation relies on samples of data, not entire populations. Sampling distributions quantify how much reported metrics (accuracy, loss) can fluctuate by chance, guiding confidence intervals and hypothesis tests. They help distinguish true improvements from random variation.</p>
</section>
<section id="try-it-yourself-31" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-31">Try It Yourself</h4>
<ol type="1">
<li>Simulate rolling a die 30 times, compute the sample mean, and repeat 500 times. Plot the distribution of means.</li>
<li>Explain why the standard error decreases as sample size increases.</li>
<li>How does the CLT connect sampling distributions to the normal distribution?</li>
</ol>
</section>
</section>
<section id="point-estimation-and-properties" class="level3">
<h3 class="anchored" data-anchor-id="point-estimation-and-properties">133. Point Estimation and Properties</h3>
<p>Point estimation provides single-value guesses of population parameters (like mean or variance) from data. Good estimators should be accurate, stable, and efficient. Properties such as unbiasedness, consistency, and efficiency define their quality.</p>
<section id="picture-in-your-head-32" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-32">Picture in Your Head</h4>
<p>Imagine trying to guess the average height of all students in a school. You take a sample and compute the sample mean—it’s your “best guess.” Sometimes it’s too high, sometimes too low, but with enough data, it hovers around the true average.</p>
</section>
<section id="deep-dive-32" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-32">Deep Dive</h4>
<ul>
<li><p>Estimator: a rule (function of data) to estimate a parameter θ.</p></li>
<li><p>Point estimate: realized value of the estimator.</p></li>
<li><p>Desirable properties:</p>
<ul>
<li>Unbiasedness: E[θ̂] = θ.</li>
<li>Consistency: θ̂ → θ as n→∞.</li>
<li>Efficiency: estimator has the smallest variance among unbiased estimators.</li>
<li>Sufficiency: θ̂ captures all information about θ in the data.</li>
</ul></li>
<li><p>Examples:</p>
<ul>
<li>Sample mean for μ is unbiased and consistent.</li>
<li>Sample variance (with denominator n−1) is unbiased for σ².</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 42%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>Property</th>
<th>Definition</th>
<th>Example in AI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Unbiasedness</td>
<td>E[θ̂] = θ</td>
<td>Sample mean as unbiased estimator of true μ</td>
</tr>
<tr class="even">
<td>Consistency</td>
<td>θ̂ → θ as n→∞</td>
<td>Validation accuracy converging with data size</td>
</tr>
<tr class="odd">
<td>Efficiency</td>
<td>Minimum variance among unbiased estimators</td>
<td>MLE often efficient in large samples</td>
</tr>
<tr class="even">
<td>Sufficiency</td>
<td>Captures all information about θ</td>
<td>Sufficient statistics in probabilistic models</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-32" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-32">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="co"># True population</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>population <span class="op">=</span> np.random.normal(<span class="dv">100</span>, <span class="dv">15</span>, <span class="dv">100000</span>)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw sample</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>sample <span class="op">=</span> np.random.choice(population, <span class="dv">50</span>)</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Point estimators</span></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>mean_est <span class="op">=</span> np.mean(sample)</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>var_est <span class="op">=</span> np.var(sample, ddof<span class="op">=</span><span class="dv">1</span>)  <span class="co"># unbiased variance</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Sample mean (estimator of μ):"</span>, mean_est)</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Sample variance (estimator of σ²):"</span>, var_est)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-30" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-30">Why It Matters</h4>
<p>Point estimation underlies nearly all machine learning parameter fitting. From estimating regression weights to learning probabilities in Naive Bayes, we rely on estimators. Knowing their properties ensures our models don’t just fit data but provide reliable generalizations.</p>
</section>
<section id="try-it-yourself-32" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-32">Try It Yourself</h4>
<ol type="1">
<li>Show that the sample mean is an unbiased estimator of the population mean.</li>
<li>Why do we divide by (n−1) instead of n when computing sample variance?</li>
<li>Explain how maximum likelihood estimation is a general framework for point estimation.</li>
</ol>
</section>
</section>
<section id="maximum-likelihood-estimation-mle" class="level3">
<h3 class="anchored" data-anchor-id="maximum-likelihood-estimation-mle">134. Maximum Likelihood Estimation (MLE)</h3>
<p>Maximum Likelihood Estimation is a method for finding parameter values that make the observed data most probable. It transforms learning into an optimization problem: choose parameters θ that maximize the likelihood of data under a model.</p>
<section id="picture-in-your-head-33" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-33">Picture in Your Head</h4>
<p>Imagine tuning the parameters of a Gaussian curve to fit a histogram of data. If the curve is too wide or shifted, the probability of observing the actual data is low. Adjusting until the curve “hugs” the data maximizes the likelihood—it’s like aligning a mold to fit scattered points.</p>
</section>
<section id="deep-dive-33" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-33">Deep Dive</h4>
<ul>
<li><p>Likelihood function: For data x₁,…,xₙ from distribution P(x|θ):</p>
<p><span class="math display">\[
L(θ) = \prod_{i=1}^n P(x_i | θ).
\]</span></p></li>
<li><p>Log-likelihood (easier to optimize):</p>
<p><span class="math display">\[
\ell(θ) = \sum_{i=1}^n \log P(x_i | θ).
\]</span></p></li>
<li><p>MLE estimator:</p>
<p><span class="math display">\[
\hat{θ}_{MLE} = \arg\max_θ \ell(θ).
\]</span></p></li>
<li><p>Properties:</p>
<ul>
<li>Consistent: converges to true θ as n→∞.</li>
<li>Asymptotically efficient: achieves minimum variance.</li>
<li>Invariant: if θ̂ is MLE of θ, then g(θ̂) is MLE of g(θ).</li>
</ul></li>
<li><p>Example: For Gaussian(μ,σ²), MLE of μ is sample mean, and of σ² is (1/n) Σ(xᵢ−μ)².</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 15%">
<col style="width: 36%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Formula</th>
<th>AI Connection</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Likelihood</td>
<td>L(θ)=Π P(xᵢ</td>
<td>θ)</td>
<td>Fit parameters to maximize data fit</td>
</tr>
<tr class="even">
<td>Log-likelihood</td>
<td>ℓ(θ)=Σ log P(xᵢ</td>
<td>θ)</td>
<td>Used in optimization algorithms</td>
</tr>
<tr class="odd">
<td>Estimator</td>
<td>θ̂=argmax ℓ(θ)</td>
<td>Logistic regression, HMMs, deep nets</td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-33" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-33">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample data</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array([<span class="fl">2.3</span>, <span class="fl">2.5</span>, <span class="fl">2.8</span>, <span class="fl">3.0</span>, <span class="fl">3.1</span>])</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Negative log-likelihood for Gaussian(μ,σ)</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> nll(params):</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>    mu, sigma <span class="op">=</span> params</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.<span class="bu">sum</span>(norm.logpdf(data, mu, sigma))</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimize</span></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> minimize(nll, x0<span class="op">=</span>[<span class="dv">0</span>,<span class="dv">1</span>], bounds<span class="op">=</span>[(<span class="va">None</span>,<span class="va">None</span>),(<span class="fl">1e-6</span>,<span class="va">None</span>)])</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>mu_mle, sigma_mle <span class="op">=</span> result.x</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"MLE μ:"</span>, mu_mle)</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"MLE σ:"</span>, sigma_mle)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-31" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-31">Why It Matters</h4>
<p>MLE is the foundation of statistical learning. Logistic regression, Gaussian Mixture Models, and Hidden Markov Models all rely on MLE. Even deep learning loss functions (like cross-entropy) can be derived from MLE principles, framing training as maximizing likelihood of observed labels.</p>
</section>
<section id="try-it-yourself-33" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-33">Try It Yourself</h4>
<ol type="1">
<li>Derive the MLE for the Bernoulli parameter p from n coin flips.</li>
<li>Show that the MLE for μ in a Gaussian is the sample mean.</li>
<li>Explain why taking the log of the likelihood simplifies optimization.</li>
</ol>
</section>
</section>
<section id="confidence-intervals" class="level3">
<h3 class="anchored" data-anchor-id="confidence-intervals">135. Confidence Intervals</h3>
<p>A confidence interval (CI) gives a range of plausible values for a population parameter, based on sample data. Instead of a single point estimate, it quantifies uncertainty, reflecting how sample variability affects inference.</p>
<section id="picture-in-your-head-34" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-34">Picture in Your Head</h4>
<p>Imagine shooting arrows at a target. A point estimate is one arrow at the bullseye. A confidence interval is a band around the bullseye, acknowledging that you might miss a little, but you’re likely to land within the band most of the time.</p>
</section>
<section id="deep-dive-34" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-34">Deep Dive</h4>
<ul>
<li><p>Definition: A 95% confidence interval for θ means that if we repeated the sampling process many times, about 95% of such intervals would contain the true θ.</p></li>
<li><p>General form:</p>
<p><span class="math display">\[
\hat{θ} \pm z_{\alpha/2} \cdot SE(\hat{θ}),
\]</span></p>
<p>where SE = standard error, and z depends on confidence level.</p></li>
<li><p>For mean with known σ:</p>
<p><span class="math display">\[
CI = \bar{x} \pm z_{\alpha/2} \frac{σ}{\sqrt{n}}.
\]</span></p></li>
<li><p>For mean with unknown σ: use t-distribution.</p></li>
<li><p>In AI: confidence intervals quantify reliability of reported metrics like accuracy, precision, or AUC.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 24%">
<col style="width: 51%">
</colgroup>
<thead>
<tr class="header">
<th>Confidence Level</th>
<th>z-score (approx)</th>
<th>Meaning in AI results</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>90%</td>
<td>1.64</td>
<td>Narrower interval, less certain</td>
</tr>
<tr class="even">
<td>95%</td>
<td>1.96</td>
<td>Standard reporting level</td>
</tr>
<tr class="odd">
<td>99%</td>
<td>2.58</td>
<td>Wider interval, stronger certainty</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-34" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-34">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats <span class="im">as</span> st</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample data</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array([<span class="fl">2.3</span>, <span class="fl">2.5</span>, <span class="fl">2.8</span>, <span class="fl">3.0</span>, <span class="fl">3.1</span>])</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> np.mean(data)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>sem <span class="op">=</span> st.sem(data)  <span class="co"># standard error</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 95% CI using t-distribution</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>ci <span class="op">=</span> st.t.interval(<span class="fl">0.95</span>, <span class="bu">len</span>(data)<span class="op">-</span><span class="dv">1</span>, loc<span class="op">=</span>mean, scale<span class="op">=</span>sem)</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Sample mean:"</span>, mean)</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"95</span><span class="sc">% c</span><span class="st">onfidence interval:"</span>, ci)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-32" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-32">Why It Matters</h4>
<p>Point estimates can be misleading if not accompanied by uncertainty. Confidence intervals prevent overconfidence, enabling better decisions in model evaluation and comparison. They ensure we know not just what our estimate is, but how trustworthy it is.</p>
</section>
<section id="try-it-yourself-34" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-34">Try It Yourself</h4>
<ol type="1">
<li>Compute a 95% confidence interval for the mean of 100 coin tosses (with p=0.5).</li>
<li>Compare intervals at 90% and 99% confidence. Which is wider? Why?</li>
<li>Explain how confidence intervals help interpret differences between two classifiers’ accuracies.</li>
</ol>
</section>
</section>
<section id="hypothesis-testing" class="level3">
<h3 class="anchored" data-anchor-id="hypothesis-testing">136. Hypothesis Testing</h3>
<p>Hypothesis testing is a formal procedure for deciding whether data supports a claim about a population. It pits two competing statements against each other: the null hypothesis (status quo) and the alternative hypothesis (the effect or difference we are testing for). Statistical evidence then determines whether to reject the null.</p>
<section id="picture-in-your-head-35" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-35">Picture in Your Head</h4>
<p>Imagine a courtroom. The null hypothesis is the presumption of innocence. The alternative is the claim of guilt. The jury (our data) doesn’t have to prove guilt with certainty, only beyond a reasonable doubt (statistical significance). Rejecting the null is like delivering a guilty verdict.</p>
</section>
<section id="deep-dive-35" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-35">Deep Dive</h4>
<ul>
<li><p>Null hypothesis (H₀): baseline claim, e.g., μ = μ₀.</p></li>
<li><p>Alternative hypothesis (H₁): competing claim, e.g., μ ≠ μ₀.</p></li>
<li><p>Test statistic: summarizes evidence from sample.</p></li>
<li><p>p-value: probability of seeing data as extreme as observed, if H₀ is true.</p></li>
<li><p>Decision rule: reject H₀ if p-value &lt; α (significance level, often 0.05).</p></li>
<li><p>Errors:</p>
<ul>
<li>Type I error: rejecting H₀ when true (false positive).</li>
<li>Type II error: failing to reject H₀ when false (false negative).</li>
</ul></li>
<li><p>In AI: hypothesis tests validate model improvements, check feature effects, and compare algorithms.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 37%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Definition</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Null (H₀)</td>
<td>Baseline assumption</td>
<td>“Model A = Model B in accuracy”</td>
</tr>
<tr class="even">
<td>Alternative (H₁)</td>
<td>Competing claim</td>
<td>“Model A &gt; Model B”</td>
</tr>
<tr class="odd">
<td>Test statistic</td>
<td>Derived measure (t, z, χ²)</td>
<td>Difference in means between models</td>
</tr>
<tr class="even">
<td>p-value</td>
<td>Evidence strength</td>
<td>Probability improvement is due to chance</td>
</tr>
<tr class="odd">
<td>Type I error</td>
<td>False positive (reject true H₀)</td>
<td>Claiming feature matters when it doesn’t</td>
</tr>
<tr class="even">
<td>Type II error</td>
<td>False negative (miss true effect)</td>
<td>Overlooking a real model improvement</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-35" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-35">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Accuracy of two models on 10 runs</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>model_a <span class="op">=</span> np.array([<span class="fl">0.82</span>, <span class="fl">0.81</span>, <span class="fl">0.80</span>, <span class="fl">0.83</span>, <span class="fl">0.82</span>, <span class="fl">0.81</span>, <span class="fl">0.84</span>, <span class="fl">0.83</span>, <span class="fl">0.82</span>, <span class="fl">0.81</span>])</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>model_b <span class="op">=</span> np.array([<span class="fl">0.79</span>, <span class="fl">0.78</span>, <span class="fl">0.80</span>, <span class="fl">0.77</span>, <span class="fl">0.79</span>, <span class="fl">0.80</span>, <span class="fl">0.78</span>, <span class="fl">0.79</span>, <span class="fl">0.77</span>, <span class="fl">0.78</span>])</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Two-sample t-test</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>t_stat, p_val <span class="op">=</span> stats.ttest_ind(model_a, model_b)</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"t-statistic:"</span>, t_stat, <span class="st">"p-value:"</span>, p_val)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-33" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-33">Why It Matters</h4>
<p>Hypothesis testing prevents AI practitioners from overclaiming results. Improvements in accuracy may be due to randomness unless confirmed statistically. Tests provide a disciplined framework for distinguishing true effects from noise, ensuring reliable scientific progress.</p>
</section>
<section id="try-it-yourself-35" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-35">Try It Yourself</h4>
<ol type="1">
<li>Toss a coin 100 times and test if it’s fair (p=0.5).</li>
<li>Compare two classifiers with accuracies of 0.85 and 0.87 over 20 runs. Is the difference significant?</li>
<li>Explain the difference between Type I and Type II errors in model evaluation.</li>
</ol>
</section>
</section>
<section id="bayesian-estimation" class="level3">
<h3 class="anchored" data-anchor-id="bayesian-estimation">137. Bayesian Estimation</h3>
<p>Bayesian estimation updates beliefs about parameters by combining prior knowledge with observed data. Instead of producing just a single point estimate, it gives a full posterior distribution, reflecting both what we assumed before and what the data tells us.</p>
<section id="picture-in-your-head-36" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-36">Picture in Your Head</h4>
<p>Imagine guessing the weight of an object. Before weighing, you already have a prior belief (it’s probably around 1 kg). After measuring, you update that belief to account for the evidence. The result isn’t one number but a refined probability curve centered closer to the truth.</p>
</section>
<section id="deep-dive-36" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-36">Deep Dive</h4>
<ul>
<li><p>Bayes’ theorem for parameters θ:</p>
<p><span class="math display">\[
P(θ|D) = \frac{P(D|θ)P(θ)}{P(D)}.
\]</span></p>
<ul>
<li>Prior P(θ): belief before data.</li>
<li>Likelihood P(D|θ): probability of data given θ.</li>
<li>Posterior P(θ|D): updated belief after seeing data.</li>
</ul></li>
<li><p>Point estimates from posterior:</p>
<ul>
<li>MAP (Maximum A Posteriori): θ̂ = argmax P(θ|D).</li>
<li>Posterior mean: E[θ|D].</li>
</ul></li>
<li><p>Conjugate priors: priors chosen to make posterior distribution same family as prior (e.g., Beta prior with Binomial likelihood).</p></li>
<li><p>In AI: Bayesian estimation appears in Naive Bayes, Bayesian neural networks, and hierarchical models.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 46%">
<col style="width: 38%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Role</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Prior</td>
<td>Assumptions before data</td>
<td>Belief in feature importance</td>
</tr>
<tr class="even">
<td>Likelihood</td>
<td>Data fit</td>
<td>Logistic regression likelihood</td>
</tr>
<tr class="odd">
<td>Posterior</td>
<td>Updated distribution</td>
<td>Updated model weights</td>
</tr>
<tr class="even">
<td>MAP estimate</td>
<td>Most probable parameter after evidence</td>
<td>Regularized parameter estimates</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-36" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-36">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> beta</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: coin flips</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior: Beta(2,2) ~ uniformish belief</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>prior_a, prior_b <span class="op">=</span> <span class="dv">2</span>, <span class="dv">2</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Data: 7 heads, 3 tails</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>heads, tails <span class="op">=</span> <span class="dv">7</span>, <span class="dv">3</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior parameters</span></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>post_a <span class="op">=</span> prior_a <span class="op">+</span> heads</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>post_b <span class="op">=</span> prior_b <span class="op">+</span> tails</span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior distribution</span></span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a>posterior <span class="op">=</span> beta(post_a, post_b)</span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Posterior mean:"</span>, posterior.mean())</span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"MAP estimate:"</span>, (post_a <span class="op">-</span> <span class="dv">1</span>) <span class="op">/</span> (post_a <span class="op">+</span> post_b <span class="op">-</span> <span class="dv">2</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-34" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-34">Why It Matters</h4>
<p>Bayesian estimation provides a principled way to incorporate prior knowledge, quantify uncertainty, and avoid overfitting. In machine learning, it enables robust predictions even with small datasets, while posterior distributions guide decisions under uncertainty.</p>
</section>
<section id="try-it-yourself-36" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-36">Try It Yourself</h4>
<ol type="1">
<li>For 5 coin flips with 4 heads, use a Beta(1,1) prior to compute the posterior.</li>
<li>Compare MAP vs posterior mean estimates—when do they differ?</li>
<li>Explain how Bayesian estimation could help when training data is scarce.</li>
</ol>
</section>
</section>
<section id="resampling-methods-bootstrap-jackknife" class="level3">
<h3 class="anchored" data-anchor-id="resampling-methods-bootstrap-jackknife">138. Resampling Methods (Bootstrap, Jackknife)</h3>
<p>Resampling methods estimate the variability of a statistic by repeatedly drawing new samples from the observed data. Instead of relying on strict formulas, they use computation to approximate confidence intervals, standard errors, and bias.</p>
<section id="picture-in-your-head-37" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-37">Picture in Your Head</h4>
<p>Imagine you only have one class of 30 students and their exam scores. To estimate the variability of the average score, you can “resample” from those 30 scores with replacement many times, creating many pseudo-classes. The spread of these averages shows how uncertain your estimate is.</p>
</section>
<section id="deep-dive-37" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-37">Deep Dive</h4>
<ul>
<li><p>Bootstrap:</p>
<ul>
<li>Resample with replacement from the dataset.</li>
<li>Compute statistic for each resample.</li>
<li>Approximate distribution of statistic across resamples.</li>
</ul></li>
<li><p>Jackknife:</p>
<ul>
<li>Systematically leave one observation out at a time.</li>
<li>Compute statistic for each reduced dataset.</li>
<li>Useful for bias and variance estimation.</li>
</ul></li>
<li><p>Advantages: fewer assumptions, works with complex estimators.</p></li>
<li><p>Limitations: computationally expensive, less effective with very small datasets.</p></li>
<li><p>In AI: used for model evaluation, confidence intervals of performance metrics, and ensemble methods like bagging.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 41%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>How It Works</th>
<th>AI Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bootstrap</td>
<td>Sample with replacement, many times</td>
<td>Confidence intervals for accuracy or AUC</td>
</tr>
<tr class="even">
<td>Jackknife</td>
<td>Leave-one-out resampling</td>
<td>Variance estimation for small datasets</td>
</tr>
<tr class="odd">
<td>Bagging</td>
<td>Bootstrap applied to ML models</td>
<td>Random forests, ensemble learning</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-37" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-37">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array([<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">9</span>])</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Bootstrap mean estimates</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>bootstrap_means <span class="op">=</span> [np.mean(np.random.choice(data, size<span class="op">=</span><span class="bu">len</span>(data), replace<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>                   <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>)]</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Jackknife mean estimates</span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>jackknife_means <span class="op">=</span> [(np.mean(np.delete(data, i))) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(data))]</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Bootstrap mean (approx):"</span>, np.mean(bootstrap_means))</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Jackknife mean (approx):"</span>, np.mean(jackknife_means))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-35" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-35">Why It Matters</h4>
<p>Resampling frees us from restrictive assumptions about distributions. In AI, where data may not follow textbook distributions, resampling methods provide reliable uncertainty estimates. Bootstrap underlies ensemble learning, while jackknife gives insights into bias and stability of estimators.</p>
</section>
<section id="try-it-yourself-37" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-37">Try It Yourself</h4>
<ol type="1">
<li>Compute bootstrap confidence intervals for the median of a dataset.</li>
<li>Apply the jackknife to estimate the variance of the sample mean for a dataset of 20 numbers.</li>
<li>Explain how bagging in random forests is essentially bootstrap applied to decision trees.</li>
</ol>
</section>
</section>
<section id="statistical-significance-and-p-values" class="level3">
<h3 class="anchored" data-anchor-id="statistical-significance-and-p-values">139. Statistical Significance and p-Values</h3>
<p>Statistical significance is a way to decide whether an observed effect is likely real or just due to random chance. The p-value measures how extreme the data is under the null hypothesis. A small p-value suggests the null is unlikely, providing evidence for the alternative.</p>
<section id="picture-in-your-head-38" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-38">Picture in Your Head</h4>
<p>Imagine tossing a fair coin. If it lands heads 9 out of 10 times, you’d be suspicious. The p-value answers: “If the coin were truly fair, how likely is it to see a result at least this extreme?” A very small probability means the fairness assumption (null) may not hold.</p>
</section>
<section id="deep-dive-38" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-38">Deep Dive</h4>
<ul>
<li><p>p-value:</p>
<p><span class="math display">\[
p = P(\text{data or more extreme} | H_0).
\]</span></p></li>
<li><p>Decision rule: Reject H₀ if p &lt; α (commonly α=0.05).</p></li>
<li><p>Significance level (α): threshold chosen before the test.</p></li>
<li><p>Misinterpretations:</p>
<ul>
<li>p ≠ probability that H₀ is true.</li>
<li>p ≠ strength of effect size.</li>
</ul></li>
<li><p>In AI: used in A/B testing, comparing algorithms, and evaluating new features.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 34%">
<col style="width: 41%">
</colgroup>
<thead>
<tr class="header">
<th>Term</th>
<th>Meaning</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Null hypothesis</td>
<td>No effect or difference</td>
<td>“Model A = Model B in accuracy”</td>
</tr>
<tr class="even">
<td>p-value</td>
<td>Likelihood of observed data under H₀</td>
<td>Probability new feature effect is by chance</td>
</tr>
<tr class="odd">
<td>α = 0.05</td>
<td>5% tolerance for false positives</td>
<td>Standard cutoff in ML experiments</td>
</tr>
<tr class="even">
<td>Statistical significance</td>
<td>Evidence strong enough to reject H₀</td>
<td>Model improvement deemed meaningful</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-38" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-38">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Two models' accuracies across 8 runs</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>model_a <span class="op">=</span> np.array([<span class="fl">0.82</span>, <span class="fl">0.81</span>, <span class="fl">0.83</span>, <span class="fl">0.84</span>, <span class="fl">0.82</span>, <span class="fl">0.81</span>, <span class="fl">0.83</span>, <span class="fl">0.82</span>])</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>model_b <span class="op">=</span> np.array([<span class="fl">0.79</span>, <span class="fl">0.78</span>, <span class="fl">0.80</span>, <span class="fl">0.79</span>, <span class="fl">0.78</span>, <span class="fl">0.80</span>, <span class="fl">0.79</span>, <span class="fl">0.78</span>])</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Independent t-test</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>t_stat, p_val <span class="op">=</span> stats.ttest_ind(model_a, model_b)</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"t-statistic:"</span>, t_stat)</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"p-value:"</span>, p_val)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-36" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-36">Why It Matters</h4>
<p>p-values and significance levels prevent us from overclaiming improvements. In AI research and production, results must be statistically significant before rollout. They provide a disciplined way to guard against randomness being mistaken for progress.</p>
</section>
<section id="try-it-yourself-38" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-38">Try It Yourself</h4>
<ol type="1">
<li>Flip a coin 20 times, observe 16 heads. Compute the p-value under H₀: fair coin.</li>
<li>Compare two classifiers with 0.80 vs 0.82 accuracy on 100 samples each. Is the difference significant?</li>
<li>Explain why a very small p-value does not always mean a large or important effect.</li>
</ol>
</section>
</section>
<section id="applications-in-data-driven-ai" class="level3">
<h3 class="anchored" data-anchor-id="applications-in-data-driven-ai">140. Applications in Data-Driven AI</h3>
<p>Statistical methods turn raw data into actionable insights in AI. From estimating parameters to testing hypotheses, they provide the tools for making decisions under uncertainty. Statistics ensures that models are not only trained but also validated, interpreted, and trusted.</p>
<section id="picture-in-your-head-39" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-39">Picture in Your Head</h4>
<p>Think of building a recommendation system. Descriptive stats summarize user behavior, sampling distributions explain uncertainty, confidence intervals quantify reliability, and hypothesis testing checks if a new algorithm truly improves engagement. Each statistical tool plays a part in the lifecycle.</p>
</section>
<section id="deep-dive-39" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-39">Deep Dive</h4>
<ul>
<li>Exploratory Data Analysis (EDA): descriptive statistics and visualization to understand data.</li>
<li>Parameter Estimation: point and Bayesian estimators for model parameters.</li>
<li>Uncertainty Quantification: confidence intervals and Bayesian posteriors.</li>
<li>Model Evaluation: hypothesis testing and p-values to compare models.</li>
<li>Resampling: bootstrap methods to assess variability and support ensemble methods.</li>
<li>Decision-Making: statistical significance guides deployment choices.</li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Statistical Tool</th>
<th>AI Application</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Descriptive stats</td>
<td>Detecting skew, anomalies, data preprocessing</td>
</tr>
<tr class="even">
<td>Estimation</td>
<td>Parameter fitting in regression, Naive Bayes</td>
</tr>
<tr class="odd">
<td>Confidence intervals</td>
<td>Reliable accuracy reports</td>
</tr>
<tr class="even">
<td>Hypothesis testing</td>
<td>Validating improvements in A/B testing</td>
</tr>
<tr class="odd">
<td>Resampling</td>
<td>Random forests, bagging, model robustness</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-39" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-39">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.utils <span class="im">import</span> resample</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: bootstrap confidence interval for accuracy</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>accuracies <span class="op">=</span> np.array([<span class="fl">0.81</span>, <span class="fl">0.82</span>, <span class="fl">0.80</span>, <span class="fl">0.83</span>, <span class="fl">0.81</span>, <span class="fl">0.82</span>])</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>boot_means <span class="op">=</span> [np.mean(resample(accuracies)) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>)]</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>ci_low, ci_high <span class="op">=</span> np.percentile(boot_means, [<span class="fl">2.5</span>, <span class="fl">97.5</span>])</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Mean accuracy:"</span>, np.mean(accuracies))</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"95% CI:"</span>, (ci_low, ci_high))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-37" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-37">Why It Matters</h4>
<p>Without statistics, AI risks overfitting, overclaiming, or misinterpreting results. Statistical thinking ensures that conclusions drawn from data are robust, reproducible, and reliable. It turns machine learning from heuristic curve-fitting into a scientific discipline.</p>
</section>
<section id="try-it-yourself-39" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-39">Try It Yourself</h4>
<ol type="1">
<li>Use bootstrap to estimate a 95% confidence interval for model precision.</li>
<li>Explain how hypothesis testing prevents deploying a worse-performing model in A/B testing.</li>
<li>Give an example where descriptive statistics alone could mislead AI evaluation without deeper inference.</li>
</ol>
</section>
</section>
</section>
<section id="chapter-15.-optimization-and-convex-analysis" class="level2">
<h2 class="anchored" data-anchor-id="chapter-15.-optimization-and-convex-analysis">Chapter 15. Optimization and convex analysis</h2>
<section id="optimization-problem-formulation" class="level3">
<h3 class="anchored" data-anchor-id="optimization-problem-formulation">141. Optimization Problem Formulation</h3>
<p>Optimization is the process of finding the best solution among many possibilities, guided by an objective function. Formulating a problem in optimization terms means defining variables to adjust, constraints to respect, and an objective to minimize or maximize.</p>
<section id="picture-in-your-head-40" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-40">Picture in Your Head</h4>
<p>Imagine packing items into a suitcase. The goal is to maximize how much value you carry while keeping within the weight limit. The items are variables, the weight restriction is a constraint, and the total value is the objective. Optimization frames this decision-making precisely.</p>
</section>
<section id="deep-dive-40" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-40">Deep Dive</h4>
<ul>
<li><p>General form of optimization problem:</p>
<p><span class="math display">\[
\min_{x \in \mathbb{R}^n} f(x) \quad \text{subject to } g_i(x) \leq 0, \; h_j(x)=0.
\]</span></p>
<ul>
<li><p>Objective function f(x): quantity to minimize or maximize.</p></li>
<li><p>Decision variables x: parameters to choose.</p></li>
<li><p>Constraints:</p>
<ul>
<li>Inequalities gᵢ(x) ≤ 0.</li>
<li>Equalities hⱼ(x) = 0.</li>
</ul></li>
</ul></li>
<li><p>Types of optimization problems:</p>
<ul>
<li>Unconstrained: no restrictions, e.g.&nbsp;minimizing f(x)=‖Ax−b‖².</li>
<li>Constrained: restrictions present, e.g.&nbsp;resource allocation.</li>
<li>Convex vs non-convex: convex problems are easier, global solutions guaranteed.</li>
</ul></li>
<li><p>In AI: optimization underlies training (loss minimization), hyperparameter tuning, and resource scheduling.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 30%">
<col style="width: 51%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Role</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Objective function</td>
<td>Defines what is being optimized</td>
<td>Loss function in neural network training</td>
</tr>
<tr class="even">
<td>Variables</td>
<td>Parameters to adjust</td>
<td>Model weights, feature weights</td>
</tr>
<tr class="odd">
<td>Constraints</td>
<td>Rules to satisfy</td>
<td>Fairness, resource limits</td>
</tr>
<tr class="even">
<td>Convexity</td>
<td>Guarantees easier optimization</td>
<td>Logistic regression (convex), deep nets (non-convex)</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-40" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-40">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: unconstrained optimization</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> x: (x[<span class="dv">0</span>]<span class="op">-</span><span class="dv">2</span>)<span class="dv">2</span> <span class="op">+</span> (x[<span class="dv">1</span>]<span class="op">+</span><span class="dv">3</span>)<span class="dv">2</span>  <span class="co"># objective function</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> minimize(f, x0<span class="op">=</span>[<span class="dv">0</span>,<span class="dv">0</span>])  <span class="co"># initial guess</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Optimal solution:"</span>, result.x)</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Minimum value:"</span>, result.fun)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-38" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-38">Why It Matters</h4>
<p>Every AI model is trained by solving an optimization problem: parameters are tuned to minimize loss. Understanding how to frame objectives and constraints transforms vague goals (“make accurate predictions”) into solvable problems. Without proper formulation, optimization may fail or produce meaningless results.</p>
</section>
<section id="try-it-yourself-40" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-40">Try It Yourself</h4>
<ol type="1">
<li>Write the optimization problem for training linear regression with squared error loss.</li>
<li>Formulate logistic regression as a constrained optimization problem.</li>
<li>Explain why convex optimization problems are more desirable than non-convex ones in AI.</li>
</ol>
</section>
</section>
<section id="convex-sets-and-convex-functions" class="level3">
<h3 class="anchored" data-anchor-id="convex-sets-and-convex-functions">142. Convex Sets and Convex Functions</h3>
<p>Convexity is the cornerstone of modern optimization. A set is convex if any line segment between two points in it stays entirely inside. A function is convex if its epigraph (region above its graph) is convex. Convex problems are attractive because every local minimum is also a global minimum.</p>
<section id="picture-in-your-head-41" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-41">Picture in Your Head</h4>
<p>Imagine a smooth bowl-shaped surface. Drop a marble anywhere, and it will roll down to the bottom—the unique global minimum. Contrast this with a rugged mountain range (non-convex), where marbles can get stuck in local dips.</p>
</section>
<section id="deep-dive-41" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-41">Deep Dive</h4>
<ul>
<li><p>Convex set: A set C ⊆ ℝⁿ is convex if ∀ x,y ∈ C and ∀ λ ∈ [0,1]:</p>
<p><span class="math display">\[
λx + (1−λ)y ∈ C.
\]</span></p></li>
<li><p>Convex function: f is convex if its domain is convex and ∀ x,y and λ ∈ [0,1]:</p>
<p><span class="math display">\[
f(λx + (1−λ)y) ≤ λf(x) + (1−λ)f(y).
\]</span></p></li>
<li><p>Strict convexity: inequality is strict for x ≠ y.</p></li>
<li><p>Properties:</p>
<ul>
<li>Sublevel sets of convex functions are convex.</li>
<li>Convex functions have no “false valleys.”</li>
</ul></li>
<li><p>In AI: many loss functions (squared error, logistic loss) are convex; guarantees on convergence exist for convex optimization.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 40%">
<col style="width: 41%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Definition</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Convex set</td>
<td>Line segment stays inside</td>
<td>Feasible region in linear programming</td>
</tr>
<tr class="even">
<td>Convex function</td>
<td>Weighted average lies above graph</td>
<td>Mean squared error loss</td>
</tr>
<tr class="odd">
<td>Strict convexity</td>
<td>Unique minimum</td>
<td>Ridge regression objective</td>
</tr>
<tr class="even">
<td>Non-convex</td>
<td>Many local minima, hard optimization</td>
<td>Deep neural networks</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-41" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-41">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>f_convex <span class="op">=</span> x2        <span class="co"># convex (bowl)</span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>f_nonconvex <span class="op">=</span> np.sin(x)<span class="co"># non-convex (wiggly)</span></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>plt.plot(x, f_convex, label<span class="op">=</span><span class="st">"Convex: x^2"</span>)</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>plt.plot(x, f_nonconvex, label<span class="op">=</span><span class="st">"Non-convex: sin(x)"</span>)</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-39" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-39">Why It Matters</h4>
<p>Convexity is what makes optimization reliable and efficient. Algorithms like gradient descent and interior-point methods come with guarantees for convex problems. Even though deep learning is non-convex, convex analysis still provides intuition and local approximations that guide practice.</p>
</section>
<section id="try-it-yourself-41" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-41">Try It Yourself</h4>
<ol type="1">
<li>Prove that the set of solutions to Ax ≤ b is convex.</li>
<li>Show that f(x)=‖x‖² is convex using the definition.</li>
<li>Give an example of a convex loss function and explain why convexity helps optimization.</li>
</ol>
</section>
</section>
<section id="gradient-descent-and-variants" class="level3">
<h3 class="anchored" data-anchor-id="gradient-descent-and-variants">143. Gradient Descent and Variants</h3>
<p>Gradient descent is an iterative method for minimizing functions. By following the negative gradient—the direction of steepest descent—we approach a local (and sometimes global) minimum. Variants improve speed, stability, and scalability in large-scale machine learning.</p>
<section id="picture-in-your-head-42" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-42">Picture in Your Head</h4>
<p>Imagine hiking down a foggy mountain with only a slope detector in your hand. At each step, you move in the direction that goes downhill the fastest. If your steps are too small, progress is slow; too big, and you overshoot the valley. Variants of gradient descent adjust how you step.</p>
</section>
<section id="deep-dive-42" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-42">Deep Dive</h4>
<ul>
<li><p>Basic gradient descent:</p>
<p><span class="math display">\[
x_{k+1} = x_k - η \nabla f(x_k),
\]</span></p>
<p>where η is the learning rate.</p></li>
<li><p>Variants:</p>
<ul>
<li>Stochastic Gradient Descent (SGD): uses one sample at a time.</li>
<li>Mini-batch GD: compromise between batch and SGD.</li>
<li>Momentum: accelerates by remembering past gradients.</li>
<li>Adaptive methods (AdaGrad, RMSProp, Adam): scale learning rate per parameter.</li>
</ul></li>
<li><p>Convergence: guaranteed for convex, smooth functions with proper η; trickier for non-convex.</p></li>
<li><p>In AI: the default optimizer for training neural networks and many statistical models.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 36%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Update Rule</th>
<th>AI Application</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Batch GD</td>
<td>Uses full dataset per step</td>
<td>Small datasets, convex optimization</td>
</tr>
<tr class="even">
<td>SGD</td>
<td>One sample per step</td>
<td>Online learning, large-scale ML</td>
</tr>
<tr class="odd">
<td>Mini-batch</td>
<td>Subset of data per step</td>
<td>Neural network training</td>
</tr>
<tr class="even">
<td>Momentum</td>
<td>Adds velocity term</td>
<td>Faster convergence, less oscillation</td>
</tr>
<tr class="odd">
<td>Adam</td>
<td>Adaptive learning rates</td>
<td>Standard in deep learning</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-42" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-42">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Function f(x) = (x-3)^2</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> x: (x<span class="op">-</span><span class="dv">3</span>)<span class="dv">2</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>grad <span class="op">=</span> <span class="kw">lambda</span> x: <span class="dv">2</span><span class="op">*</span>(x<span class="op">-</span><span class="dv">3</span>)</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="fl">0.0</span>  <span class="co"># start point</span></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>    x <span class="op">-=</span> eta <span class="op">*</span> grad(x)</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"x=</span><span class="sc">{</span>x<span class="sc">:.4f}</span><span class="ss">, f(x)=</span><span class="sc">{</span>f(x)<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-40" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-40">Why It Matters</h4>
<p>Gradient descent is the workhorse of machine learning. Without it, training models with millions of parameters would be impossible. Variants like Adam make optimization robust to noisy gradients and poor scaling, critical in deep learning.</p>
</section>
<section id="try-it-yourself-42" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-42">Try It Yourself</h4>
<ol type="1">
<li>Run gradient descent on f(x)=x² starting from x=10 with η=0.1. Does it converge to 0?</li>
<li>Compare SGD and batch GD for logistic regression. What are the trade-offs?</li>
<li>Explain why Adam is often chosen as the default optimizer in deep learning.</li>
</ol>
</section>
</section>
<section id="constrained-optimization-and-lagrange-multipliers" class="level3">
<h3 class="anchored" data-anchor-id="constrained-optimization-and-lagrange-multipliers">144. Constrained Optimization and Lagrange Multipliers</h3>
<p>Constrained optimization extends standard optimization by adding conditions that the solution must satisfy. Lagrange multipliers transform constrained problems into unconstrained ones by incorporating the constraints into the objective, enabling powerful analytical and computational methods.</p>
<section id="picture-in-your-head-43" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-43">Picture in Your Head</h4>
<p>Imagine trying to find the lowest point in a valley, but you’re restricted to walking along a fence. You can’t just follow the valley downward—you must stay on the fence. Lagrange multipliers act like weights on the constraints, balancing the pull of the objective and the restrictions.</p>
</section>
<section id="deep-dive-43" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-43">Deep Dive</h4>
<ul>
<li><p>Problem form:</p>
<p><span class="math display">\[
\min f(x) \quad \text{s.t. } g_i(x)=0, \; h_j(x) \leq 0.
\]</span></p></li>
<li><p>Lagrangian function:</p>
<p><span class="math display">\[
\mathcal{L}(x,λ,μ) = f(x) + \sum_i λ_i g_i(x) + \sum_j μ_j h_j(x),
\]</span></p>
<p>where λ, μ ≥ 0 are multipliers.</p></li>
<li><p>Karush-Kuhn-Tucker (KKT) conditions: generalization of first-order conditions for constrained problems.</p>
<ul>
<li>Stationarity: ∇f(x*) + Σ λᵢ∇gᵢ(x*) + Σ μⱼ∇hⱼ(x*) = 0.</li>
<li>Primal feasibility: constraints satisfied.</li>
<li>Dual feasibility: μ ≥ 0.</li>
<li>Complementary slackness: μⱼhⱼ(x*) = 0.</li>
</ul></li>
<li><p>In AI: constraints enforce fairness, resource limits, or structured predictions.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 42%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Element</th>
<th>Meaning</th>
<th>AI Application</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Lagrangian</td>
<td>Combines objective + constraints</td>
<td>Training with fairness constraints</td>
</tr>
<tr class="even">
<td>Multipliers (λ, μ)</td>
<td>Shadow prices: trade-off between goals</td>
<td>Resource allocation in ML systems</td>
</tr>
<tr class="odd">
<td>KKT conditions</td>
<td>Optimality conditions under constraints</td>
<td>Support Vector Machines (SVMs)</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-43" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-43">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>x, y, λ <span class="op">=</span> sp.symbols(<span class="st">'x y λ'</span>)</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> x2 <span class="op">+</span> y2  <span class="co"># objective</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> x <span class="op">+</span> y <span class="op">-</span> <span class="dv">1</span>    <span class="co"># constraint</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Lagrangian</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> f <span class="op">+</span> λ<span class="op">*</span>g</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Solve system: ∂L/∂x = 0, ∂L/∂y = 0, g=0</span></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>solutions <span class="op">=</span> sp.solve([sp.diff(L, x), sp.diff(L, y), g], [x, y, λ])</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Optimal solution:"</span>, solutions)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-41" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-41">Why It Matters</h4>
<p>Most real-world AI problems have constraints: fairness in predictions, limited memory in deployment, or interpretability requirements. Lagrange multipliers and KKT conditions give a systematic way to handle such problems without brute force.</p>
</section>
<section id="try-it-yourself-43" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-43">Try It Yourself</h4>
<ol type="1">
<li>Minimize f(x,y) = x² + y² subject to x+y=1. Solve using Lagrange multipliers.</li>
<li>Explain how SVMs use constrained optimization to separate data with a margin.</li>
<li>Give an AI example where inequality constraints are essential.</li>
</ol>
</section>
</section>
<section id="duality-in-optimization" class="level3">
<h3 class="anchored" data-anchor-id="duality-in-optimization">145. Duality in Optimization</h3>
<p>Duality provides an alternative perspective on optimization problems by transforming them into related “dual” problems. The dual often offers deeper insight, easier computation, or guarantees about the original (primal) problem. In many cases, solving the dual is equivalent to solving the primal.</p>
<section id="picture-in-your-head-44" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-44">Picture in Your Head</h4>
<p>Think of haggling in a marketplace. The seller wants to maximize profit (primal problem), while the buyer wants to minimize cost (dual problem). Their negotiations converge to a price where both objectives meet—illustrating primal-dual optimality.</p>
</section>
<section id="deep-dive-44" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-44">Deep Dive</h4>
<ul>
<li><p>Primal problem (general form):</p>
<p><span class="math display">\[
\min_x f(x) \quad \text{s.t. } g_i(x) \leq 0, \; h_j(x)=0.
\]</span></p></li>
<li><p>Lagrangian:</p>
<p><span class="math display">\[
\mathcal{L}(x,λ,μ) = f(x) + \sum_i λ_i g_i(x) + \sum_j μ_j h_j(x).
\]</span></p></li>
<li><p>Dual function:</p>
<p><span class="math display">\[
q(λ,μ) = \inf_x \mathcal{L}(x,λ,μ).
\]</span></p></li>
<li><p>Dual problem:</p>
<p><span class="math display">\[
\max_{λ \geq 0, μ} q(λ,μ).
\]</span></p></li>
<li><p>Weak duality: dual optimum ≤ primal optimum.</p></li>
<li><p>Strong duality: equality holds under convexity + regularity (Slater’s condition).</p></li>
<li><p>In AI: duality is central to SVMs, resource allocation, and distributed optimization.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 38%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Role</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Primal problem</td>
<td>Original optimization goal</td>
<td>Training SVM in feature space</td>
</tr>
<tr class="even">
<td>Dual problem</td>
<td>Alternative view with multipliers</td>
<td>Kernel trick applied in SVM dual form</td>
</tr>
<tr class="odd">
<td>Weak duality</td>
<td>Dual ≤ primal</td>
<td>Bound on objective value</td>
</tr>
<tr class="even">
<td>Strong duality</td>
<td>Dual = primal (convex problems)</td>
<td>Guarantees optimal solution equivalence</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-44" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-44">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cvxpy <span class="im">as</span> cp</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Primal: minimize x^2 subject to x &gt;= 1</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> cp.Variable()</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>objective <span class="op">=</span> cp.Minimize(x2)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>constraints <span class="op">=</span> [x <span class="op">&gt;=</span> <span class="dv">1</span>]</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>prob <span class="op">=</span> cp.Problem(objective, constraints)</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>primal_val <span class="op">=</span> prob.solve()</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Dual variables</span></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>dual_val <span class="op">=</span> constraints[<span class="dv">0</span>].dual_value</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Primal optimum:"</span>, primal_val)</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Dual variable (λ):"</span>, dual_val)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-42" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-42">Why It Matters</h4>
<p>Duality gives bounds, simplifies complex problems, and enables distributed computation. For example, SVM training is usually solved in the dual because kernels appear naturally there. In large-scale AI, dual formulations often reduce computational burden.</p>
</section>
<section id="try-it-yourself-44" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-44">Try It Yourself</h4>
<ol type="1">
<li>Write the dual of the problem: minimize x² subject to x ≥ 1.</li>
<li>Explain why the kernel trick works naturally in the SVM dual formulation.</li>
<li>Give an example where weak duality holds but strong duality fails.</li>
</ol>
</section>
</section>
<section id="convex-optimization-algorithms-interior-point-etc." class="level3">
<h3 class="anchored" data-anchor-id="convex-optimization-algorithms-interior-point-etc.">146. Convex Optimization Algorithms (Interior Point, etc.)</h3>
<p>Convex optimization problems can be solved efficiently with specialized algorithms that exploit convexity. Unlike generic search, these methods guarantee convergence to the global optimum. Interior point methods, gradient-based algorithms, and barrier functions are among the most powerful tools.</p>
<section id="picture-in-your-head-45" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-45">Picture in Your Head</h4>
<p>Imagine navigating a smooth valley bounded by steep cliffs. Instead of walking along the edge (constraints), interior point methods guide you smoothly through the interior, avoiding walls but still respecting the boundaries. Each step moves closer to the lowest point without hitting constraints head-on.</p>
</section>
<section id="deep-dive-45" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-45">Deep Dive</h4>
<ul>
<li><p>First-order methods:</p>
<ul>
<li>Gradient descent, projected gradient descent.</li>
<li>Scalable but may converge slowly.</li>
</ul></li>
<li><p>Second-order methods:</p>
<ul>
<li><p>Newton’s method: uses curvature (Hessian).</p></li>
<li><p>Interior point methods: transform constraints into smooth barrier terms.</p>
<p><span class="math display">\[
\min f(x) - μ \sum \log(-g_i(x))
\]</span></p>
<p>with μ shrinking → enforces feasibility.</p></li>
</ul></li>
<li><p>Complexity: convex optimization can be solved in polynomial time; interior point methods are efficient for medium-scale problems.</p></li>
<li><p>Modern solvers: CVX, Gurobi, OSQP.</p></li>
<li><p>In AI: used in SVM training, logistic regression, optimal transport, and constrained learning.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 37%">
<col style="width: 43%">
</colgroup>
<thead>
<tr class="header">
<th>Algorithm</th>
<th>Idea</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Gradient methods</td>
<td>Follow slopes</td>
<td>Large-scale convex problems</td>
</tr>
<tr class="even">
<td>Newton’s method</td>
<td>Use curvature for fast convergence</td>
<td>Logistic regression</td>
</tr>
<tr class="odd">
<td>Interior point</td>
<td>Barrier functions enforce constraints</td>
<td>Support Vector Machines, linear programming</td>
</tr>
<tr class="even">
<td>Projected gradient</td>
<td>Project steps back into feasible set</td>
<td>Constrained parameter tuning</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-45" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-45">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cvxpy <span class="im">as</span> cp</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: minimize x^2 + y^2 subject to x+y &gt;= 1</span></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> cp.Variable(), cp.Variable()</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>objective <span class="op">=</span> cp.Minimize(x2 <span class="op">+</span> y2)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>constraints <span class="op">=</span> [x <span class="op">+</span> y <span class="op">&gt;=</span> <span class="dv">1</span>]</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>prob <span class="op">=</span> cp.Problem(objective, constraints)</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> prob.solve()</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Optimal x, y:"</span>, x.value, y.value)</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Optimal value:"</span>, result)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-43" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-43">Why It Matters</h4>
<p>Convex optimization algorithms provide the mathematical backbone of many classical ML models. They make training provably efficient and reliable—qualities often lost in non-convex deep learning. Even there, convex methods appear in components like convex relaxations and regularized losses.</p>
</section>
<section id="try-it-yourself-45" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-45">Try It Yourself</h4>
<ol type="1">
<li>Solve min (x−2)²+(y−1)² subject to x+y=2 using CVX or by hand.</li>
<li>Explain how barrier functions prevent violating inequality constraints.</li>
<li>Compare gradient descent and interior point methods in terms of scalability and accuracy.</li>
</ol>
</section>
</section>
<section id="non-convex-optimization-challenges" class="level3">
<h3 class="anchored" data-anchor-id="non-convex-optimization-challenges">147. Non-Convex Optimization Challenges</h3>
<p>Unlike convex problems, non-convex optimization involves rugged landscapes with many local minima, saddle points, and flat regions. Finding the global optimum is often intractable, but practical methods aim for “good enough” solutions that generalize well.</p>
<section id="picture-in-your-head-46" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-46">Picture in Your Head</h4>
<p>Think of a hiker navigating a mountain range filled with peaks, valleys, and plateaus. Unlike a simple bowl-shaped valley (convex), here the hiker might get trapped in a small dip (local minimum) or wander aimlessly on a flat ridge (saddle point).</p>
</section>
<section id="deep-dive-46" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-46">Deep Dive</h4>
<ul>
<li><p>Local minima vs global minimum: Non-convex functions may have many local minima; algorithms risk getting stuck.</p></li>
<li><p>Saddle points: places where gradient = 0 but not optimal; common in high dimensions.</p></li>
<li><p>Plateaus and flat regions: slow convergence due to vanishing gradients.</p></li>
<li><p>No guarantees: non-convex optimization is generally NP-hard.</p></li>
<li><p>Heuristics &amp; strategies:</p>
<ul>
<li>Random restarts, stochasticity (SGD helps escape saddles).</li>
<li>Momentum-based methods.</li>
<li>Regularization and good initialization.</li>
<li>Relaxations to convex problems.</li>
</ul></li>
<li><p>In AI: deep learning is fundamentally non-convex, yet SGD finds solutions that generalize.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 43%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Challenge</th>
<th>Explanation</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Local minima</td>
<td>Algorithm stuck in suboptimal valley</td>
<td>Training small neural networks</td>
</tr>
<tr class="even">
<td>Saddle points</td>
<td>Flat ridges, slow escape</td>
<td>High-dimensional deep nets</td>
</tr>
<tr class="odd">
<td>Flat plateaus</td>
<td>Gradients vanish, slow convergence</td>
<td>Vanishing gradient problem in RNNs</td>
</tr>
<tr class="even">
<td>Non-convexity</td>
<td>NP-hard in general</td>
<td>Training deep generative models</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-46" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-46">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">400</span>)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">400</span>)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> np.meshgrid(x, y)</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> np.sin(X) <span class="op">*</span> np.cos(Y)  <span class="co"># non-convex surface</span></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>plt.contourf(X, Y, Z, levels<span class="op">=</span><span class="dv">20</span>, cmap<span class="op">=</span><span class="st">"RdBu"</span>)</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Non-Convex Optimization Landscape"</span>)</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-44" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-44">Why It Matters</h4>
<p>Most modern AI models—from deep nets to reinforcement learning—are trained by solving non-convex problems. Understanding the challenges helps explain why training may be unstable, why initialization matters, and why methods like SGD succeed despite theoretical hardness.</p>
</section>
<section id="try-it-yourself-46" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-46">Try It Yourself</h4>
<ol type="1">
<li>Plot f(x)=sin(x) for x∈[−10,10]. Identify local minima and the global minimum.</li>
<li>Explain why SGD can escape saddle points more easily than batch gradient descent.</li>
<li>Give an example of a convex relaxation used to approximate a non-convex problem.</li>
</ol>
</section>
</section>
<section id="stochastic-optimization" class="level3">
<h3 class="anchored" data-anchor-id="stochastic-optimization">148. Stochastic Optimization</h3>
<p>Stochastic optimization uses randomness to handle large or uncertain problems where exact computation is impractical. Instead of evaluating the full objective, it samples parts of the data or uses noisy approximations, making it scalable for modern machine learning.</p>
<section id="picture-in-your-head-47" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-47">Picture in Your Head</h4>
<p>Imagine trying to find the lowest point in a vast landscape. Checking every inch is impossible. Instead, you take random walks, each giving a rough sense of direction. With enough steps, the randomness averages out, guiding you downhill efficiently.</p>
</section>
<section id="deep-dive-47" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-47">Deep Dive</h4>
<ul>
<li><p>Stochastic Gradient Descent (SGD):</p>
<p><span class="math display">\[
x_{k+1} = x_k - η \nabla f_i(x_k),
\]</span></p>
<p>where gradient is estimated from a random sample i.</p></li>
<li><p>Mini-batch SGD: balances variance reduction and efficiency.</p></li>
<li><p>Variance reduction methods: SVRG, SAG, Adam adapt stochastic updates.</p></li>
<li><p>Monte Carlo optimization: approximates expectations with random samples.</p></li>
<li><p>Reinforcement learning: stochastic optimization used in policy gradient methods.</p></li>
<li><p>Advantages: scalable, handles noisy data.</p></li>
<li><p>Disadvantages: randomness may slow convergence, requires tuning.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 26%">
<col style="width: 38%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Key Idea</th>
<th>AI Application</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>SGD</td>
<td>Update using random sample</td>
<td>Neural network training</td>
</tr>
<tr class="even">
<td>Mini-batch SGD</td>
<td>Small batch gradient estimate</td>
<td>Standard deep learning practice</td>
</tr>
<tr class="odd">
<td>Variance reduction (SVRG)</td>
<td>Reduce noise in stochastic gradients</td>
<td>Faster convergence in ML training</td>
</tr>
<tr class="even">
<td>Monte Carlo optimization</td>
<td>Approximate expectation via sampling</td>
<td>RL, generative models</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-47" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-47">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Function f(x) = (x-3)^2</span></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>grad <span class="op">=</span> <span class="kw">lambda</span> x, i: <span class="dv">2</span><span class="op">*</span>(x<span class="op">-</span><span class="dv">3</span>) <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>)  <span class="co"># noisy gradient</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>    x <span class="op">-=</span> eta <span class="op">*</span> grad(x, _)</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"x=</span><span class="sc">{</span>x<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-45" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-45">Why It Matters</h4>
<p>AI models are trained on massive datasets where exact optimization is infeasible. Stochastic optimization makes learning tractable by trading exactness for scalability. It powers deep learning, reinforcement learning, and online algorithms.</p>
</section>
<section id="try-it-yourself-47" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-47">Try It Yourself</h4>
<ol type="1">
<li>Compare convergence of batch gradient descent and SGD on a quadratic function.</li>
<li>Explain why adding noise in optimization can help escape local minima.</li>
<li>Implement mini-batch SGD for logistic regression on a toy dataset.</li>
</ol>
</section>
</section>
<section id="optimization-in-high-dimensions" class="level3">
<h3 class="anchored" data-anchor-id="optimization-in-high-dimensions">149. Optimization in High Dimensions</h3>
<p>High-dimensional optimization is challenging because the geometry of space changes as dimensions grow. Distances concentrate, gradients may vanish, and searching the landscape becomes exponentially harder. Yet, most modern AI models, especially deep neural networks, live in very high-dimensional spaces.</p>
<section id="picture-in-your-head-48" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-48">Picture in Your Head</h4>
<p>Imagine trying to search for a marble in a huge warehouse. In two dimensions, you can scan rows and columns quickly. In a thousand dimensions, nearly all points look equally far apart, and the marble hides in an enormous volume that’s impossible to search exhaustively.</p>
</section>
<section id="deep-dive-48" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-48">Deep Dive</h4>
<ul>
<li><p>Curse of dimensionality: computational cost and data requirements grow exponentially with dimension.</p></li>
<li><p>Distance concentration: in high dimensions, distances between points become nearly identical, complicating nearest-neighbor methods.</p></li>
<li><p>Gradient issues: gradients can vanish or explode in deep networks.</p></li>
<li><p>Optimization challenges:</p>
<ul>
<li>Saddle points become more common than local minima.</li>
<li>Flat regions slow convergence.</li>
<li>Regularization needed to control overfitting.</li>
</ul></li>
<li><p>Techniques:</p>
<ul>
<li>Dimensionality reduction (PCA, autoencoders).</li>
<li>Adaptive learning rates (Adam, RMSProp).</li>
<li>Normalization layers (BatchNorm, LayerNorm).</li>
<li>Random projections and low-rank approximations.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 30%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>Challenge</th>
<th>Effect in High Dimensions</th>
<th>AI Connection</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Curse of dimensionality</td>
<td>Requires exponential data</td>
<td>Feature engineering, embeddings</td>
</tr>
<tr class="even">
<td>Distance concentration</td>
<td>Points look equally far</td>
<td>Vector similarity search, nearest neighbors</td>
</tr>
<tr class="odd">
<td>Saddle points dominance</td>
<td>Slows optimization</td>
<td>Deep network training</td>
</tr>
<tr class="even">
<td>Gradient issues</td>
<td>Vanishing/exploding gradients</td>
<td>RNN training, weight initialization</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-48" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-48">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Distance concentration demo</span></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> <span class="dv">1000</span>  <span class="co"># dimension</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>points <span class="op">=</span> np.random.randn(<span class="dv">1000</span>, d)</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Pairwise distances</span></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial.distance <span class="im">import</span> pdist</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>distances <span class="op">=</span> pdist(points, <span class="st">'euclidean'</span>)</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Mean distance:"</span>, np.mean(distances))</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Std of distances:"</span>, np.std(distances))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-46" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-46">Why It Matters</h4>
<p>Most AI problems—from embeddings to deep nets—are inherently high-dimensional. Understanding how optimization behaves in these spaces explains why naive algorithms fail, why regularization is essential, and why specialized techniques like normalization and adaptive methods succeed.</p>
</section>
<section id="try-it-yourself-48" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-48">Try It Yourself</h4>
<ol type="1">
<li>Simulate distances in 10, 100, and 1000 dimensions. How does the variance change?</li>
<li>Explain why PCA can help optimization in high-dimensional feature spaces.</li>
<li>Give an example where high-dimensional embeddings improve AI performance despite optimization challenges.</li>
</ol>
</section>
</section>
<section id="applications-in-ml-training" class="level3">
<h3 class="anchored" data-anchor-id="applications-in-ml-training">150. Applications in ML Training</h3>
<p>Optimization is the engine behind machine learning. Training a model means defining a loss function and using optimization algorithms to minimize it with respect to the model’s parameters. From linear regression to deep neural networks, optimization turns data into predictive power.</p>
<section id="picture-in-your-head-49" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-49">Picture in Your Head</h4>
<p>Think of sculpting a statue from a block of marble. The raw block is the initial model with random parameters. Each optimization step chisels away error, gradually shaping the model to fit the data.</p>
</section>
<section id="deep-dive-49" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-49">Deep Dive</h4>
<ul>
<li>Linear models: closed-form solutions exist (e.g., least squares), but gradient descent is often used for scalability.</li>
<li>Logistic regression: convex optimization with log-loss.</li>
<li>Support Vector Machines: quadratic programming solved via dual optimization.</li>
<li>Neural networks: non-convex optimization with SGD and adaptive methods.</li>
<li>Regularization: adds penalties (L1, L2) to the objective, improving generalization.</li>
<li>Hyperparameter optimization: grid search, random search, Bayesian optimization.</li>
<li>Distributed optimization: data-parallel SGD, asynchronous updates for large-scale training.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 41%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Model/Task</th>
<th>Optimization Formulation</th>
<th>Example Algorithm</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear regression</td>
<td>Minimize squared error</td>
<td>Gradient descent, closed form</td>
</tr>
<tr class="even">
<td>Logistic regression</td>
<td>Minimize log-loss</td>
<td>Newton’s method, gradient descent</td>
</tr>
<tr class="odd">
<td>SVM</td>
<td>Maximize margin, quadratic constraints</td>
<td>Interior point, dual optimization</td>
</tr>
<tr class="even">
<td>Neural networks</td>
<td>Minimize cross-entropy or MSE</td>
<td>SGD, Adam, RMSProp</td>
</tr>
<tr class="odd">
<td>Hyperparameter tuning</td>
<td>Black-box optimization</td>
<td>Bayesian optimization</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-49" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-49">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple classification with logistic regression</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">1</span>,<span class="dv">2</span>],[<span class="dv">2</span>,<span class="dv">1</span>],[<span class="dv">2</span>,<span class="dv">3</span>],[<span class="dv">3</span>,<span class="dv">5</span>],[<span class="dv">5</span>,<span class="dv">4</span>],[<span class="dv">6</span>,<span class="dv">5</span>]])</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>])</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Optimized coefficients:"</span>, model.coef_)</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Intercept:"</span>, model.intercept_)</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Accuracy:"</span>, model.score(X, y))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-47" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-47">Why It Matters</h4>
<p>Optimization is what makes learning feasible. Without it, models would remain abstract definitions with no way to adjust parameters from data. Every breakthrough in AI—from logistic regression to transformers—relies on advances in optimization techniques.</p>
</section>
<section id="try-it-yourself-49" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-49">Try It Yourself</h4>
<ol type="1">
<li>Write the optimization objective for linear regression and solve for the closed-form solution.</li>
<li>Explain why SVM training is solved using a dual formulation.</li>
<li>Compare training with SGD vs Adam on a small neural network—what differences do you observe?</li>
</ol>
</section>
</section>
</section>
<section id="chapter-16.-numerical-methods-and-stability" class="level2">
<h2 class="anchored" data-anchor-id="chapter-16.-numerical-methods-and-stability">Chapter 16. Numerical methods and stability</h2>
<section id="numerical-representation-and-rounding-errors" class="level3">
<h3 class="anchored" data-anchor-id="numerical-representation-and-rounding-errors">151. Numerical Representation and Rounding Errors</h3>
<p>Computers represent numbers with finite precision, which introduces rounding errors. While small individually, these errors accumulate in iterative algorithms, sometimes destabilizing optimization or inference. Numerical analysis studies how to represent and control such errors.</p>
<section id="picture-in-your-head-50" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-50">Picture in Your Head</h4>
<p>Imagine pouring water into a cup but spilling a drop each time. One spill seems negligible, but after thousands of pours, the missing water adds up. Similarly, tiny rounding errors in floating-point arithmetic can snowball into significant inaccuracies.</p>
</section>
<section id="deep-dive-50" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-50">Deep Dive</h4>
<ul>
<li><p>Floating-point representation (IEEE 754): numbers stored with finite bits for sign, exponent, and mantissa.</p></li>
<li><p>Machine epsilon (ε): smallest number such that 1+ε &gt; 1 in machine precision.</p></li>
<li><p>Types of errors:</p>
<ul>
<li>Rounding error: due to truncation of digits.</li>
<li>Cancellation: subtracting nearly equal numbers magnifies error.</li>
<li>Overflow/underflow: exceeding representable range.</li>
</ul></li>
<li><p>Stability concerns: iterative methods (like gradient descent) can accumulate error.</p></li>
<li><p>Mitigations: scaling, normalization, higher precision, numerically stable algorithms.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 41%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Issue</th>
<th>Description</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Rounding error</td>
<td>Truncation of decimals</td>
<td>Summing large feature vectors</td>
</tr>
<tr class="even">
<td>Cancellation</td>
<td>Loss of significance in subtraction</td>
<td>Variance computation with large numbers</td>
</tr>
<tr class="odd">
<td>Overflow/underflow</td>
<td>Exceeding float limits</td>
<td>Softmax with very large/small logits</td>
</tr>
<tr class="even">
<td>Machine epsilon</td>
<td>Limit of precision (~1e-16 for float64)</td>
<td>Convergence thresholds in optimization</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-50" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-50">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Machine epsilon</span></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>eps <span class="op">=</span> np.finfo(<span class="bu">float</span>).eps</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Machine epsilon:"</span>, eps)</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Cancellation example</span></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>a, b <span class="op">=</span> <span class="fl">1e16</span>, <span class="fl">1e16</span> <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>diff1 <span class="op">=</span> b <span class="op">-</span> a         <span class="co"># exact difference should be 1</span></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>diff2 <span class="op">=</span> (b <span class="op">-</span> a) <span class="op">+</span> <span class="dv">1</span>   <span class="co"># accumulation with error</span></span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Cancellation error example:"</span>, diff1, diff2)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-48" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-48">Why It Matters</h4>
<p>AI systems rely on numerical computation at scale. Floating-point limitations explain instabilities in training (exploding/vanishing gradients) and motivate techniques like log-sum-exp for stable probability calculations. Awareness of rounding errors prevents subtle but serious bugs.</p>
</section>
<section id="try-it-yourself-50" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-50">Try It Yourself</h4>
<ol type="1">
<li>Compute softmax(1000, 1001) directly and with log-sum-exp. Compare results.</li>
<li>Find machine epsilon for float32 and float64 in Python.</li>
<li>Explain why subtracting nearly equal probabilities can lead to unstable results.</li>
</ol>
</section>
</section>
<section id="root-finding-methods-newton-raphson-bisection" class="level3">
<h3 class="anchored" data-anchor-id="root-finding-methods-newton-raphson-bisection">152. Root-Finding Methods (Newton-Raphson, Bisection)</h3>
<p>Root-finding algorithms locate solutions to equations of the form f(x)=0. These methods are essential for optimization, solving nonlinear equations, and iterative methods in AI. Different algorithms trade speed, stability, and reliance on derivatives.</p>
<section id="picture-in-your-head-51" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-51">Picture in Your Head</h4>
<p>Imagine standing at a river, looking for the shallowest crossing. You test different spots: if the water is too deep, move closer to the bank; if it’s shallow, you’re near the crossing. Root-finding works the same way—adjust guesses until the function value crosses zero.</p>
</section>
<section id="deep-dive-51" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-51">Deep Dive</h4>
<ul>
<li><p>Bisection method:</p>
<ul>
<li>Interval-based, guaranteed convergence if f is continuous and sign changes on [a,b].</li>
<li>Update: repeatedly halve the interval.</li>
<li>Converges slowly (linear rate).</li>
</ul></li>
<li><p>Newton-Raphson method:</p>
<ul>
<li><p>Iterative update:</p>
<p><span class="math display">\[
x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}.
\]</span></p></li>
<li><p>Quadratic convergence if derivative is available and initial guess is good.</p></li>
<li><p>Can diverge if poorly initialized.</p></li>
</ul></li>
<li><p>Secant method:</p>
<ul>
<li>Approximates derivative numerically.</li>
</ul></li>
<li><p>In AI: solving logistic regression likelihood equations, computing eigenvalues, backpropagation steps.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 12%">
<col style="width: 20%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Convergence</th>
<th>Needs derivative?</th>
<th>AI Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bisection</td>
<td>Linear</td>
<td>No</td>
<td>Robust threshold finding</td>
</tr>
<tr class="even">
<td>Newton-Raphson</td>
<td>Quadratic</td>
<td>Yes</td>
<td>Logistic regression optimization</td>
</tr>
<tr class="odd">
<td>Secant</td>
<td>Superlinear</td>
<td>Approximate</td>
<td>Parameter estimation when derivative costly</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-51" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-51">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Newton-Raphson for sqrt(2)</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> x: x2 <span class="op">-</span> <span class="dv">2</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>f_prime <span class="op">=</span> <span class="kw">lambda</span> x: <span class="dv">2</span><span class="op">*</span>x</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x <span class="op">-</span> f(x)<span class="op">/</span>f_prime(x)</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Approximation:"</span>, x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-49" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-49">Why It Matters</h4>
<p>Root-finding is a building block for optimization and inference. Newton’s method accelerates convergence in training convex models, while bisection provides safety when robustness is more important than speed.</p>
</section>
<section id="try-it-yourself-51" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-51">Try It Yourself</h4>
<ol type="1">
<li>Use bisection to find the root of f(x)=cos(x)−x.</li>
<li>Derive Newton’s method for solving log-likelihood equations in logistic regression.</li>
<li>Compare convergence speed of bisection vs Newton on f(x)=x²−2.</li>
</ol>
</section>
</section>
<section id="numerical-linear-algebra-lu-qr-decomposition" class="level3">
<h3 class="anchored" data-anchor-id="numerical-linear-algebra-lu-qr-decomposition">153. Numerical Linear Algebra (LU, QR Decomposition)</h3>
<p>Numerical linear algebra develops stable and efficient ways to solve systems of linear equations, factorize matrices, and compute decompositions. These methods form the computational backbone of optimization, statistics, and machine learning.</p>
<section id="picture-in-your-head-52" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-52">Picture in Your Head</h4>
<p>Imagine trying to solve a puzzle by breaking it into smaller, easier sub-puzzles. Instead of directly inverting a giant matrix, decompositions split it into triangular or orthogonal pieces that are simpler to work with.</p>
</section>
<section id="deep-dive-52" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-52">Deep Dive</h4>
<ul>
<li><p>LU decomposition:</p>
<ul>
<li>Factorizes A into L (lower triangular) and U (upper triangular).</li>
<li>Solves Ax=b efficiently by forward + backward substitution.</li>
</ul></li>
<li><p>QR decomposition:</p>
<ul>
<li>Factorizes A into Q (orthogonal) and R (upper triangular).</li>
<li>Useful for least-squares problems.</li>
</ul></li>
<li><p>Cholesky decomposition:</p>
<ul>
<li>Special case for symmetric positive definite matrices: A=LLᵀ.</li>
</ul></li>
<li><p>SVD (Singular Value Decomposition): more general, stable but expensive.</p></li>
<li><p>Numerical concerns:</p>
<ul>
<li>Pivoting improves stability.</li>
<li>Condition number indicates sensitivity to perturbations.</li>
</ul></li>
<li><p>In AI: used in PCA, linear regression, matrix factorization, spectral methods.</p></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Decomposition</th>
<th>Form</th>
<th>Use Case in AI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>LU</td>
<td>A = LU</td>
<td>Solving linear systems</td>
</tr>
<tr class="even">
<td>QR</td>
<td>A = QR</td>
<td>Least squares, orthogonalization</td>
</tr>
<tr class="odd">
<td>Cholesky</td>
<td>A = LLᵀ</td>
<td>Gaussian processes, covariance matrices</td>
</tr>
<tr class="even">
<td>SVD</td>
<td>A = UΣVᵀ</td>
<td>Dimensionality reduction, embeddings</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-52" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-52">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.linalg <span class="im">import</span> lu, qr</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">3</span>]])</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a><span class="co"># LU decomposition</span></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>P, L, U <span class="op">=</span> lu(A)</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"L:</span><span class="ch">\n</span><span class="st">"</span>, L)</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"U:</span><span class="ch">\n</span><span class="st">"</span>, U)</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a><span class="co"># QR decomposition</span></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>Q, R <span class="op">=</span> qr(A)</span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Q:</span><span class="ch">\n</span><span class="st">"</span>, Q)</span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"R:</span><span class="ch">\n</span><span class="st">"</span>, R)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-50" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-50">Why It Matters</h4>
<p>Machine learning workflows rely on efficient linear algebra. From solving regression equations to training large models, numerical decompositions provide scalable, stable methods where naive matrix inversion would fail.</p>
</section>
<section id="try-it-yourself-52" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-52">Try It Yourself</h4>
<ol type="1">
<li>Solve Ax=b using LU decomposition for A=[[4,2],[3,1]], b=[1,2].</li>
<li>Explain why QR decomposition is more stable than solving normal equations directly in least squares.</li>
<li>Compute the Cholesky decomposition of a covariance matrix and explain its role in Gaussian sampling.</li>
</ol>
</section>
</section>
<section id="iterative-methods-for-linear-systems" class="level3">
<h3 class="anchored" data-anchor-id="iterative-methods-for-linear-systems">154. Iterative Methods for Linear Systems</h3>
<p>Iterative methods solve large systems of linear equations without directly factorizing the matrix. Instead, they refine an approximate solution step by step. These methods are essential when matrices are too large or sparse for direct approaches like LU or QR.</p>
<section id="picture-in-your-head-53" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-53">Picture in Your Head</h4>
<p>Imagine adjusting the volume knob on a radio: you start with a guess, then keep tuning slightly up or down until the signal comes in clearly. Iterative solvers do the same—gradually refining estimates until the solution is “clear enough.”</p>
</section>
<section id="deep-dive-53" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-53">Deep Dive</h4>
<ul>
<li><p>Problem: Solve Ax = b, where A is large and sparse.</p></li>
<li><p>Basic iterative methods:</p>
<ul>
<li>Jacobi method: update each variable using the previous iteration.</li>
<li>Gauss-Seidel method: uses latest updated values for faster convergence.</li>
<li>Successive Over-Relaxation (SOR): accelerates Gauss-Seidel with relaxation factor.</li>
</ul></li>
<li><p>Krylov subspace methods:</p>
<ul>
<li>Conjugate Gradient (CG): efficient for symmetric positive definite matrices.</li>
<li>GMRES (Generalized Minimal Residual): for general nonsymmetric matrices.</li>
</ul></li>
<li><p>Convergence: depends on matrix properties (diagonal dominance, conditioning).</p></li>
<li><p>In AI: used in large-scale optimization, graph algorithms, Gaussian processes, and PDE-based models.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 35%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Requirement</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Jacobi</td>
<td>Diagonal dominance</td>
<td>Approximate inference in graphical models</td>
</tr>
<tr class="even">
<td>Gauss-Seidel</td>
<td>Stronger convergence than Jacobi</td>
<td>Sparse system solvers in ML pipelines</td>
</tr>
<tr class="odd">
<td>Conjugate Gradient</td>
<td>Symmetric positive definite</td>
<td>Kernel methods, Gaussian processes</td>
</tr>
<tr class="even">
<td>GMRES</td>
<td>General sparse systems</td>
<td>Large-scale graph embeddings</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-53" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-53">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.sparse.linalg <span class="im">import</span> cg</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Example system Ax = b</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">4</span>,<span class="dv">1</span>],[<span class="dv">1</span>,<span class="dv">3</span>]])</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.array([<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Conjugate Gradient</span></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>x, info <span class="op">=</span> cg(A, b)</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Solution:"</span>, x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-51" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-51">Why It Matters</h4>
<p>Iterative solvers scale where direct methods fail. In AI, datasets can involve millions of variables and sparse matrices. Efficient iterative algorithms enable training kernel machines, performing inference in probabilistic models, and solving high-dimensional optimization problems.</p>
</section>
<section id="try-it-yourself-53" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-53">Try It Yourself</h4>
<ol type="1">
<li>Implement the Jacobi method for a 3×3 diagonally dominant system.</li>
<li>Compare convergence of Jacobi vs Gauss-Seidel on the same system.</li>
<li>Explain why Conjugate Gradient is preferred for symmetric positive definite matrices.</li>
</ol>
</section>
</section>
<section id="numerical-differentiation-and-integration" class="level3">
<h3 class="anchored" data-anchor-id="numerical-differentiation-and-integration">155. Numerical Differentiation and Integration</h3>
<p>When analytical solutions are unavailable, numerical methods approximate derivatives and integrals. Differentiation estimates slopes using nearby points, while integration approximates areas under curves. These methods are essential for simulation, optimization, and probabilistic inference.</p>
<section id="picture-in-your-head-54" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-54">Picture in Your Head</h4>
<p>Think of measuring the slope of a hill without a formula. You check two nearby altitudes and estimate the incline. Or, to measure land area, you cut it into small strips and sum them up. Numerical differentiation and integration work in the same way.</p>
</section>
<section id="deep-dive-54" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-54">Deep Dive</h4>
<ul>
<li><p>Numerical differentiation:</p>
<ul>
<li><p>Forward difference:</p>
<p><span class="math display">\[
f'(x) \approx \frac{f(x+h)-f(x)}{h}.
\]</span></p></li>
<li><p>Central difference (more accurate):</p>
<p><span class="math display">\[
f'(x) \approx \frac{f(x+h)-f(x-h)}{2h}.
\]</span></p></li>
<li><p>Trade-off: small h reduces truncation error but increases round-off error.</p></li>
</ul></li>
<li><p>Numerical integration:</p>
<ul>
<li>Rectangle/Trapezoidal rule: approximate area under curve.</li>
<li>Simpson’s rule: quadratic approximation, higher accuracy.</li>
<li>Monte Carlo integration: estimate integral by random sampling, useful in high dimensions.</li>
</ul></li>
<li><p>In AI: used in gradient estimation, reinforcement learning (policy gradients), Bayesian inference, and sampling methods.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 31%">
<col style="width: 43%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Formula / Idea</th>
<th>AI Application</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Central difference</td>
<td>(f(x+h)-f(x-h))/(2h)</td>
<td>Gradient-free optimization</td>
</tr>
<tr class="even">
<td>Trapezoidal rule</td>
<td>Avg height × width</td>
<td>Numerical expectation in small problems</td>
</tr>
<tr class="odd">
<td>Simpson’s rule</td>
<td>Quadratic fit over intervals</td>
<td>Smooth density integration</td>
</tr>
<tr class="even">
<td>Monte Carlo integration</td>
<td>Random sampling approximation</td>
<td>Probabilistic models, Bayesian inference</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-54" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-54">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Function</span></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> x: np.sin(x)</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Numerical derivative at x=1</span></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> <span class="fl">1e-5</span></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>derivative <span class="op">=</span> (f(<span class="dv">1</span><span class="op">+</span>h) <span class="op">-</span> f(<span class="dv">1</span><span class="op">-</span>h)) <span class="op">/</span> (<span class="dv">2</span><span class="op">*</span>h)</span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Numerical integration of sin(x) from 0 to pi</span></span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> np.linspace(<span class="dv">0</span>, np.pi, <span class="dv">1000</span>)</span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a>trapezoid <span class="op">=</span> np.trapz(np.sin(xs), xs)</span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Derivative of sin at x=1 ≈"</span>, derivative)</span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Integral of sin from 0 to pi ≈"</span>, trapezoid)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-52" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-52">Why It Matters</h4>
<p>Many AI models rely on gradients and expectations where closed forms don’t exist. Numerical differentiation provides approximate gradients, while Monte Carlo integration handles high-dimensional expectations central to probabilistic inference and generative modeling.</p>
</section>
<section id="try-it-yourself-54" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-54">Try It Yourself</h4>
<ol type="1">
<li>Estimate derivative of f(x)=exp(x) at x=0 using central difference.</li>
<li>Compute ∫₀¹ x² dx numerically with trapezoidal and Simpson’s rule—compare accuracy.</li>
<li>Use Monte Carlo to approximate π by integrating the unit circle area.</li>
</ol>
</section>
</section>
<section id="stability-and-conditioning-of-problems" class="level3">
<h3 class="anchored" data-anchor-id="stability-and-conditioning-of-problems">156. Stability and Conditioning of Problems</h3>
<p>Stability and conditioning describe how sensitive a numerical problem is to small changes. Conditioning is a property of the problem itself, while stability concerns the algorithm used to solve it. Together, they determine whether numerical answers can be trusted.</p>
<section id="picture-in-your-head-55" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-55">Picture in Your Head</h4>
<p>Imagine balancing a pencil on its tip. The system (problem) is ill-conditioned—tiny nudges cause big changes. Now imagine the floor is also shaky (algorithm instability). Even with a well-posed problem, an unstable method could still topple your pencil.</p>
</section>
<section id="deep-dive-55" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-55">Deep Dive</h4>
<ul>
<li><p>Conditioning:</p>
<ul>
<li><p>A problem is well-conditioned if small input changes cause small output changes.</p></li>
<li><p>Ill-conditioned if small errors in input cause large deviations in output.</p></li>
<li><p>Condition number (κ):</p>
<p><span class="math display">\[
κ(A) = \|A\|\|A^{-1}\|.
\]</span></p>
<p>Large κ ⇒ ill-conditioned.</p></li>
</ul></li>
<li><p>Stability:</p>
<ul>
<li>An algorithm is stable if it produces nearly correct results for nearly correct data.</li>
<li>Example: Gaussian elimination with partial pivoting is more stable than without pivoting.</li>
</ul></li>
<li><p>Well-posedness (Hadamard): a problem must have existence, uniqueness, and continuous dependence on data.</p></li>
<li><p>In AI: conditioning affects gradient-based training, covariance estimation, and inversion of kernel matrices.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 40%">
<col style="width: 42%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Definition</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Well-conditioned</td>
<td>Small errors → small output change</td>
<td>PCA on normalized data</td>
</tr>
<tr class="even">
<td>Ill-conditioned</td>
<td>Small errors → large output change</td>
<td>Inverting covariance in Gaussian processes</td>
</tr>
<tr class="odd">
<td>Stable algorithm</td>
<td>Doesn’t magnify rounding errors</td>
<td>Pivoted LU for regression problems</td>
</tr>
<tr class="even">
<td>Unstable algo</td>
<td>Propagates or amplifies numerical errors</td>
<td>Naive Gaussian elimination</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-55" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-55">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Ill-conditioned matrix</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="fl">1.001</span>], [<span class="fl">1.001</span>, <span class="fl">1.002</span>]])</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>cond <span class="op">=</span> np.linalg.cond(A)</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.array([<span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linalg.solve(A, b)</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Condition number:"</span>, cond)</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Solution:"</span>, x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-53" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-53">Why It Matters</h4>
<p>AI systems often rely on solving large linear systems or optimizing high-dimensional objectives. Poor conditioning leads to unstable training (exploding/vanishing gradients). Stable algorithms and preconditioning improve reliability.</p>
</section>
<section id="try-it-yourself-55" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-55">Try It Yourself</h4>
<ol type="1">
<li>Compute condition numbers of random matrices of size 5×5. Which are ill-conditioned?</li>
<li>Explain why normalization improves conditioning in linear regression.</li>
<li>Give an AI example where unstable algorithms could cause misleading results.</li>
</ol>
</section>
</section>
<section id="floating-point-arithmetic-and-precision" class="level3">
<h3 class="anchored" data-anchor-id="floating-point-arithmetic-and-precision">157. Floating-Point Arithmetic and Precision</h3>
<p>Floating-point arithmetic allows computers to represent real numbers approximately using a finite number of bits. While flexible, it introduces rounding and precision issues that can accumulate, affecting the reliability of numerical algorithms.</p>
<section id="picture-in-your-head-56" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-56">Picture in Your Head</h4>
<p>Think of measuring with a ruler that only has centimeter markings. If you measure something 10 times and add the results, each small rounding error adds up. Floating-point numbers work similarly—precise enough for most tasks, but never exact.</p>
</section>
<section id="deep-dive-56" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-56">Deep Dive</h4>
<ul>
<li><p>IEEE 754 format:</p>
<ul>
<li>Single precision (float32): 1 sign bit, 8 exponent bits, 23 fraction bits (~7 decimal digits).</li>
<li>Double precision (float64): 1 sign bit, 11 exponent bits, 52 fraction bits (~16 decimal digits).</li>
</ul></li>
<li><p>Precision limits: machine epsilon ε ≈ 1.19×10⁻⁷ (float32), ≈ 2.22×10⁻¹⁶ (float64).</p></li>
<li><p>Common pitfalls:</p>
<ul>
<li>Rounding error in sums/products.</li>
<li>Cancellation when subtracting close numbers.</li>
<li>Overflow/underflow for very large/small numbers.</li>
</ul></li>
<li><p>Workarounds:</p>
<ul>
<li>Use higher precision if needed.</li>
<li>Reorder operations for numerical stability.</li>
<li>Apply log transformations for probabilities (log-sum-exp trick).</li>
</ul></li>
<li><p>In AI: float32 dominates training neural networks; float16 and bfloat16 reduce memory and speed up training with some precision trade-offs.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 8%">
<col style="width: 21%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Precision Type</th>
<th>Digits</th>
<th>Range Approx.</th>
<th>AI Usage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>float16</td>
<td>~3-4</td>
<td>10⁻⁵ to 10⁵</td>
<td>Mixed precision deep learning</td>
</tr>
<tr class="even">
<td>float32</td>
<td>~7</td>
<td>10⁻³⁸ to 10³⁸</td>
<td>Standard for training</td>
</tr>
<tr class="odd">
<td>float64</td>
<td>~16</td>
<td>10⁻³⁰⁸ to 10³⁰⁸</td>
<td>Scientific computing, kernel methods</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-56" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-56">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Precision comparison</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>x32 <span class="op">=</span> np.float32(<span class="fl">1.0</span>) <span class="op">+</span> np.float32(<span class="fl">1e-8</span>)</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>x64 <span class="op">=</span> np.float64(<span class="fl">1.0</span>) <span class="op">+</span> np.float64(<span class="fl">1e-8</span>)</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Float32 result:"</span>, x32)  <span class="co"># rounds away</span></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Float64 result:"</span>, x64)  <span class="co"># keeps precision</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-54" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-54">Why It Matters</h4>
<p>Precision trade-offs influence speed, memory, and stability. Deep learning thrives on float32/float16 for efficiency, but numerical algorithms (like kernel methods or Gaussian processes) often require float64 to avoid instability.</p>
</section>
<section id="try-it-yourself-56" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-56">Try It Yourself</h4>
<ol type="1">
<li>Add 1e-8 to 1.0 using float32 and float64. What happens?</li>
<li>Compute softmax([1000,1001]) with and without log-sum-exp. Compare results.</li>
<li>Explain why mixed precision training works despite reduced numerical accuracy.</li>
</ol>
</section>
</section>
<section id="monte-carlo-methods" class="level3">
<h3 class="anchored" data-anchor-id="monte-carlo-methods">158. Monte Carlo Methods</h3>
<p>Monte Carlo methods use random sampling to approximate quantities that are hard to compute exactly. By averaging many random trials, they estimate integrals, expectations, or probabilities, making them invaluable in high-dimensional and complex AI problems.</p>
<section id="picture-in-your-head-57" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-57">Picture in Your Head</h4>
<p>Imagine trying to measure the area of an irregular pond. Instead of using formulas, you throw pebbles randomly in a bounding box. The proportion that lands in the pond estimates its area. Monte Carlo methods do the same with randomness and computation.</p>
</section>
<section id="deep-dive-57" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-57">Deep Dive</h4>
<ul>
<li><p>Monte Carlo integration:</p>
<p><span class="math display">\[
\int f(x) dx \approx \frac{1}{N}\sum_{i=1}^N f(x_i), \quad x_i \sim p(x).
\]</span></p></li>
<li><p>Law of Large Numbers: guarantees convergence as N→∞.</p></li>
<li><p>Variance reduction techniques: importance sampling, stratified sampling, control variates.</p></li>
<li><p>Markov Chain Monte Carlo (MCMC): generates samples from complex distributions (e.g., Metropolis-Hastings, Gibbs sampling).</p></li>
<li><p>Applications in AI:</p>
<ul>
<li>Bayesian inference.</li>
<li>Policy evaluation in reinforcement learning.</li>
<li>Probabilistic graphical models.</li>
<li>Simulation for uncertainty quantification.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 41%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Idea</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Plain Monte Carlo</td>
<td>Random uniform sampling</td>
<td>Estimating π or integrals</td>
</tr>
<tr class="even">
<td>Importance sampling</td>
<td>Bias sampling toward important regions</td>
<td>Rare event probability in risk models</td>
</tr>
<tr class="odd">
<td>Stratified sampling</td>
<td>Divide space into strata for efficiency</td>
<td>Variance reduction in simulation</td>
</tr>
<tr class="even">
<td>MCMC</td>
<td>Construct Markov chain with target dist.</td>
<td>Bayesian neural networks, topic models</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-57" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-57">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Monte Carlo estimate of pi</span></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">100000</span></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>points <span class="op">=</span> np.random.rand(N, <span class="dv">2</span>)</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>inside <span class="op">=</span> np.<span class="bu">sum</span>(points[:,<span class="dv">0</span>]<span class="dv">2</span> <span class="op">+</span> points[:,<span class="dv">1</span>]<span class="dv">2</span> <span class="op">&lt;=</span> <span class="dv">1</span>)</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>pi_est <span class="op">=</span> <span class="dv">4</span> <span class="op">*</span> inside <span class="op">/</span> N</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Monte Carlo estimate of pi:"</span>, pi_est)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-55" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-55">Why It Matters</h4>
<p>Monte Carlo makes the intractable tractable. High-dimensional integrals appear in Bayesian models, reinforcement learning, and generative AI; Monte Carlo is often the only feasible tool. It trades exactness for scalability, a cornerstone of modern probabilistic AI.</p>
</section>
<section id="try-it-yourself-57" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-57">Try It Yourself</h4>
<ol type="1">
<li>Use Monte Carlo to estimate the integral of f(x)=exp(−x²) from −2 to 2.</li>
<li>Implement importance sampling for rare-event probability estimation.</li>
<li>Run Gibbs sampling for a simple two-variable Gaussian distribution.</li>
</ol>
</section>
</section>
<section id="error-propagation-and-analysis" class="level3">
<h3 class="anchored" data-anchor-id="error-propagation-and-analysis">159. Error Propagation and Analysis</h3>
<p>Error propagation studies how small inaccuracies in inputs—whether from measurement, rounding, or approximation—affect outputs of computations. In numerical methods, understanding how errors accumulate is essential for ensuring trustworthy results.</p>
<section id="picture-in-your-head-58" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-58">Picture in Your Head</h4>
<p>Imagine passing a message along a chain of people. Each person whispers it slightly differently. By the time it reaches the end, the message may have drifted far from the original. Computational pipelines behave the same way—small errors compound through successive operations.</p>
</section>
<section id="deep-dive-58" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-58">Deep Dive</h4>
<ul>
<li><p>Sources of error:</p>
<ul>
<li>Input error: noisy data or imprecise measurements.</li>
<li>Truncation error: approximating infinite processes (e.g., Taylor series).</li>
<li>Rounding error: finite precision arithmetic.</li>
</ul></li>
<li><p>Error propagation formula (first-order): For y = f(x₁,…,xₙ):</p>
<p><span class="math display">\[
\Delta y \approx \sum_{i=1}^n \frac{\partial f}{\partial x_i} \Delta x_i.
\]</span></p></li>
<li><p>Condition number link: higher sensitivity ⇒ greater error amplification.</p></li>
<li><p>Monte Carlo error analysis: simulate error distributions via sampling.</p></li>
<li><p>In AI: affects stability of optimization, uncertainty in predictions, and reliability of simulations.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 36%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>Error Type</th>
<th>Description</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Input error</td>
<td>Noisy or approximate measurements</td>
<td>Sensor data for robotics</td>
</tr>
<tr class="even">
<td>Truncation error</td>
<td>Approximation cutoff</td>
<td>Numerical gradient estimation</td>
</tr>
<tr class="odd">
<td>Rounding error</td>
<td>Finite precision representation</td>
<td>Softmax probabilities in deep learning</td>
</tr>
<tr class="even">
<td>Propagation</td>
<td>Errors amplify through computation</td>
<td>Long training pipelines, iterative solvers</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-58" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-58">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Function sensitive to input errors</span></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> x: np.exp(x) <span class="op">-</span> np.exp(x<span class="op">-</span><span class="fl">0.00001</span>)</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>x_true <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>perturbations <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">1e-5</span>, <span class="fl">1e-5</span>, <span class="dv">5</span>)</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> dx <span class="kw">in</span> perturbations:</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> f(x_true <span class="op">+</span> dx)</span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"x=</span><span class="sc">{</span>x_true<span class="op">+</span>dx<span class="sc">:.8f}</span><span class="ss">, f(x)=</span><span class="sc">{</span>y<span class="sc">:.8e}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-56" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-56">Why It Matters</h4>
<p>Error propagation explains why some algorithms are stable while others collapse under noise. In AI, where models rely on massive computations, unchecked error growth can lead to unreliable predictions, exploding gradients, or divergence in training.</p>
</section>
<section id="try-it-yourself-58" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-58">Try It Yourself</h4>
<ol type="1">
<li>Use the propagation formula to estimate error in y = x² when x=1000 with Δx=0.01.</li>
<li>Compare numerical and symbolic differentiation for small step sizes—observe truncation error.</li>
<li>Simulate how float32 rounding affects the cumulative sum of 1 million random numbers.</li>
</ol>
</section>
</section>
<section id="numerical-methods-in-ai-systems" class="level3">
<h3 class="anchored" data-anchor-id="numerical-methods-in-ai-systems">160. Numerical Methods in AI Systems</h3>
<p>Numerical methods are the hidden engines inside AI systems, enabling efficient optimization, stable learning, and scalable inference. From solving linear systems to approximating integrals, they bridge the gap between mathematical models and practical computation.</p>
<section id="picture-in-your-head-59" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-59">Picture in Your Head</h4>
<p>Think of AI as a skyscraper. The visible structure is the model—neural networks, decision trees, probabilistic graphs. But the unseen foundation is numerical methods: without solid algorithms for computation, the skyscraper would collapse.</p>
</section>
<section id="deep-dive-59" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-59">Deep Dive</h4>
<ul>
<li>Linear algebra methods: matrix factorizations (LU, QR, SVD) for regression, PCA, embeddings.</li>
<li>Optimization algorithms: gradient descent, interior point, stochastic optimization for model training.</li>
<li>Probability and statistics tools: Monte Carlo integration, resampling, numerical differentiation for uncertainty estimation.</li>
<li>Stability and conditioning: ensuring models remain reliable when data or computations are noisy.</li>
<li>Precision management: choosing float16, float32, or float64 depending on trade-offs between efficiency and accuracy.</li>
<li>Scalability: iterative solvers and distributed numerical methods allow AI to handle massive datasets.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 35%">
<col style="width: 64%">
</colgroup>
<thead>
<tr class="header">
<th>Numerical Method</th>
<th>Role in AI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear solvers</td>
<td>Regression, covariance estimation</td>
</tr>
<tr class="even">
<td>Optimization routines</td>
<td>Training neural networks, tuning hyperparams</td>
</tr>
<tr class="odd">
<td>Monte Carlo methods</td>
<td>Bayesian inference, RL simulations</td>
</tr>
<tr class="even">
<td>Error/stability analysis</td>
<td>Reliable model evaluation</td>
</tr>
<tr class="odd">
<td>Mixed precision</td>
<td>Faster deep learning training</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-59" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-59">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="co"># PCA using SVD under the hood (numerical linear algebra)</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.randn(<span class="dv">100</span>, <span class="dv">10</span>)</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>X_reduced <span class="op">=</span> pca.fit_transform(X)</span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Original shape:"</span>, X.shape)</span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Reduced shape:"</span>, X_reduced.shape)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-57" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-57">Why It Matters</h4>
<p>Without robust numerical methods, AI would be brittle, slow, and unreliable. Training transformers, running reinforcement learning simulations, or doing large-scale probabilistic inference all depend on efficient numerical algorithms that tame complexity.</p>
</section>
<section id="try-it-yourself-59" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-59">Try It Yourself</h4>
<ol type="1">
<li>Implement PCA manually using SVD and compare with sklearn’s PCA.</li>
<li>Train a small neural network using float16 and float32—compare speed and stability.</li>
<li>Explain how Monte Carlo integration enables probabilistic inference in Bayesian models.</li>
</ol>
</section>
</section>
</section>
<section id="chapter-17.-information-theory" class="level2">
<h2 class="anchored" data-anchor-id="chapter-17.-information-theory">Chapter 17. Information Theory</h2>
<section id="entropy-and-information-content" class="level3">
<h3 class="anchored" data-anchor-id="entropy-and-information-content">161. Entropy and Information Content</h3>
<p>Entropy measures the average uncertainty or surprise in a random variable. Information content quantifies how much “news” an event provides: rare events carry more information than common ones. Together, they form the foundation of information theory.</p>
<section id="picture-in-your-head-60" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-60">Picture in Your Head</h4>
<p>Imagine guessing a number someone is thinking of. If they choose uniformly between 1 and 1000, each answer feels surprising and informative. If they always pick 7, there’s no surprise—and no information gained.</p>
</section>
<section id="deep-dive-60" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-60">Deep Dive</h4>
<ul>
<li><p>Information content (self-information): For event <span class="math inline">\(x\)</span> with probability <span class="math inline">\(p(x)\)</span>,</p>
<p><span class="math display">\[
I(x) = -\log p(x)
\]</span></p>
<p>Rare events (low <span class="math inline">\(p(x)\)</span>) yield higher <span class="math inline">\(I(x)\)</span>.</p></li>
<li><p>Entropy (Shannon entropy): Average information of random variable <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[
H(X) = -\sum_x p(x)\log p(x)
\]</span></p>
<ul>
<li>Maximum when all outcomes are equally likely.</li>
<li>Minimum (0) when outcome is certain.</li>
</ul></li>
<li><p>Interpretations:</p>
<ul>
<li>Average uncertainty.</li>
<li>Expected code length in optimal compression.</li>
<li>Measure of unpredictability in systems.</li>
</ul></li>
<li><p>Properties:</p>
<ul>
<li><span class="math inline">\(H(X) \geq 0\)</span>.</li>
<li><span class="math inline">\(H(X)\)</span> is maximized for uniform distribution.</li>
<li>Units: bits (log base 2), nats (log base <span class="math inline">\(e\)</span>).</li>
</ul></li>
<li><p>In AI: used in decision trees (information gain), language modeling, reinforcement learning, and uncertainty quantification.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 38%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th>Distribution</th>
<th>Entropy Value</th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Certain outcome</td>
<td><span class="math inline">\(H=0\)</span></td>
<td>No uncertainty</td>
</tr>
<tr class="even">
<td>Fair coin toss</td>
<td><span class="math inline">\(H=1\)</span> bit</td>
<td>One bit needed per toss</td>
</tr>
<tr class="odd">
<td>Fair 6-sided die</td>
<td><span class="math inline">\(H=\log_2 6 \approx 2.58\)</span> bits</td>
<td>Average surprise per roll</td>
</tr>
<tr class="even">
<td>Biased coin (p=0.9)</td>
<td><span class="math inline">\(H \approx 0.47\)</span> bits</td>
<td>Less surprise than fair coin</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-60" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-60">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> entropy(probs):</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.<span class="bu">sum</span>([p<span class="op">*</span>np.log2(p) <span class="cf">for</span> p <span class="kw">in</span> probs <span class="cf">if</span> p <span class="op">&gt;</span> <span class="dv">0</span>])</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Entropy fair coin:"</span>, entropy([<span class="fl">0.5</span>, <span class="fl">0.5</span>]))</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Entropy biased coin:"</span>, entropy([<span class="fl">0.9</span>, <span class="fl">0.1</span>]))</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Entropy fair die:"</span>, entropy([<span class="dv">1</span><span class="op">/</span><span class="dv">6</span>]<span class="op">*</span><span class="dv">6</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-58" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-58">Why It Matters</h4>
<p>Entropy provides a universal measure of uncertainty and compressibility. In AI, it quantifies uncertainty in predictions, guides model training, and connects probability with coding and decision-making. Without entropy, concepts like information gain, cross-entropy loss, and probabilistic learning would lack foundation.</p>
</section>
<section id="try-it-yourself-60" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-60">Try It Yourself</h4>
<ol type="1">
<li>Compute entropy for a dataset where 80% of labels are “A” and 20% are “B.”</li>
<li>Compare entropy of a uniform distribution vs a highly skewed one.</li>
<li>Explain why entropy measures the lower bound of lossless data compression.</li>
</ol>
</section>
</section>
<section id="joint-and-conditional-entropy" class="level3">
<h3 class="anchored" data-anchor-id="joint-and-conditional-entropy">162. Joint and Conditional Entropy</h3>
<p>Joint entropy measures the uncertainty of two random variables considered together. Conditional entropy refines this by asking: given knowledge of one variable, how much uncertainty remains about the other? These concepts extend entropy to relationships between variables.</p>
<section id="picture-in-your-head-61" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-61">Picture in Your Head</h4>
<p>Imagine rolling two dice. The joint entropy reflects the total unpredictability of the pair. Now, suppose you already know the result of the first die—how uncertain are you about the second? That remaining uncertainty is the conditional entropy.</p>
</section>
<section id="deep-dive-61" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-61">Deep Dive</h4>
<ul>
<li><p>Joint entropy: For random variables <span class="math inline">\(X, Y\)</span>:</p>
<p><span class="math display">\[
H(X, Y) = -\sum_{x,y} p(x,y) \log p(x,y)
\]</span></p>
<ul>
<li>Captures combined uncertainty of both variables.</li>
</ul></li>
<li><p>Conditional entropy: Uncertainty in <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[
H(Y \mid X) = -\sum_{x,y} p(x,y) \log p(y \mid x)
\]</span></p>
<ul>
<li>Measures average uncertainty left in <span class="math inline">\(Y\)</span> once <span class="math inline">\(X\)</span> is known.</li>
</ul></li>
<li><p>Relationships:</p>
<ul>
<li>Chain rule: <span class="math inline">\(H(X, Y) = H(X) + H(Y \mid X)\)</span>.</li>
<li>Symmetry: <span class="math inline">\(H(X, Y) = H(Y, X)\)</span>.</li>
</ul></li>
<li><p>Properties:</p>
<ul>
<li><span class="math inline">\(H(Y \mid X) \leq H(Y)\)</span>.</li>
<li>Equality if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.</li>
</ul></li>
<li><p>In AI:</p>
<ul>
<li>Joint entropy: modeling uncertainty across features.</li>
<li>Conditional entropy: decision trees (information gain), communication efficiency, Bayesian networks.</li>
</ul></li>
</ul>
<!--
| Concept             | Formula                            | Interpretation                |                                 |                                            |
| ------------------- | ---------------------------------- | ----------------------------- | ------------------------------- | ------------------------------------------ |
| Joint entropy       | $H(X,Y) = -\sum p(x,y)\log p(x,y)$ | Uncertainty of the pair (X,Y) |                                 |                                            |
| Conditional entropy | (H(Y                               | X) = -\sum p(x,y)\log p(y     | x))                             | Remaining uncertainty in Y after knowing X |
| Chain rule          | (H(X,Y) = H(X) + H(Y               | X))                           | Breaks joint entropy into parts |                                            |
-->
</section>
<section id="tiny-code-61" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-61">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Example joint distribution for X,Y (binary variables)</span></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> np.array([[<span class="fl">0.25</span>, <span class="fl">0.25</span>],</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>              [<span class="fl">0.25</span>, <span class="fl">0.25</span>]])  <span class="co"># independent uniform</span></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> entropy(probs):</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.<span class="bu">sum</span>([p<span class="op">*</span>np.log2(p) <span class="cf">for</span> p <span class="kw">in</span> probs.flatten() <span class="cf">if</span> p <span class="op">&gt;</span> <span class="dv">0</span>])</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> joint_entropy(p):</span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> entropy(p)</span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conditional_entropy(p):</span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a>    H <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a>    row_sums <span class="op">=</span> p.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(row_sums)):</span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> row_sums[i] <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb62-18"><a href="#cb62-18" aria-hidden="true" tabindex="-1"></a>            cond_probs <span class="op">=</span> p[i]<span class="op">/</span>row_sums[i]</span>
<span id="cb62-19"><a href="#cb62-19" aria-hidden="true" tabindex="-1"></a>            H <span class="op">+=</span> row_sums[i] <span class="op">*</span> entropy(cond_probs)</span>
<span id="cb62-20"><a href="#cb62-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> H</span>
<span id="cb62-21"><a href="#cb62-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-22"><a href="#cb62-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Joint entropy:"</span>, joint_entropy(p))</span>
<span id="cb62-23"><a href="#cb62-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Conditional entropy H(Y|X):"</span>, conditional_entropy(p))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-59" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-59">Why It Matters</h4>
<p>Joint and conditional entropy extend uncertainty beyond single variables, capturing relationships and dependencies. They underpin information gain in machine learning, compression schemes, and probabilistic reasoning frameworks like Bayesian networks.</p>
</section>
<section id="try-it-yourself-61" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-61">Try It Yourself</h4>
<ol type="1">
<li>Calculate joint entropy for two independent coin tosses.</li>
<li>Compute conditional entropy for a biased coin where you’re told whether the outcome is heads.</li>
<li>Explain why <span class="math inline">\(H(Y|X)=0\)</span> when <span class="math inline">\(Y\)</span> is a deterministic function of <span class="math inline">\(X\)</span>.</li>
</ol>
</section>
</section>
<section id="mutual-information" class="level3">
<h3 class="anchored" data-anchor-id="mutual-information">163. Mutual Information</h3>
<p>Mutual information (MI) quantifies how much knowing one random variable reduces uncertainty about another. It measures dependence: if two variables are independent, their mutual information is zero; if perfectly correlated, MI is maximized.</p>
<section id="picture-in-your-head-62" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-62">Picture in Your Head</h4>
<p>Think of two overlapping circles representing uncertainty about variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The overlap region is the mutual information—it’s the shared knowledge between the two.</p>
</section>
<section id="deep-dive-62" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-62">Deep Dive</h4>
<ul>
<li><p>Definition:</p>
<p><span class="math display">\[
I(X;Y) = \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}
\]</span></p></li>
<li><p>Equivalent forms:</p>
<p><span class="math display">\[
I(X;Y) = H(X) + H(Y) - H(X,Y)
\]</span></p>
<p><span class="math display">\[
I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
\]</span></p></li>
<li><p>Properties:</p>
<ul>
<li>Always nonnegative.</li>
<li>Symmetric: <span class="math inline">\(I(X;Y) = I(Y;X)\)</span>.</li>
<li>Zero iff <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.</li>
</ul></li>
<li><p>Interpretation:</p>
<ul>
<li>Reduction in uncertainty about one variable given the other.</li>
<li>Shared information content.</li>
</ul></li>
<li><p>In AI:</p>
<ul>
<li>Feature selection: pick features with high MI with labels.</li>
<li>Clustering: measure similarity between variables.</li>
<li>Representation learning: InfoNCE loss, variational bounds on MI.</li>
<li>Communication: efficiency of transmitting signals.</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Expression</th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(I(X;Y)=0\)</span></td>
<td>X and Y are independent</td>
</tr>
<tr class="even">
<td>Large <span class="math inline">\(I(X;Y)\)</span></td>
<td>Strong dependence between X and Y</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(I(X;Y)=H(X)\)</span></td>
<td>X completely determined by Y</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-62" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-62">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mutual_info_score</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Example joint distribution: correlated binary variables</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, <span class="fl">0.7</span>, size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> X <span class="op">^</span> np.random.binomial(<span class="dv">1</span>, <span class="fl">0.1</span>, size<span class="op">=</span><span class="dv">1000</span>)  <span class="co"># noisy copy of X</span></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>mi <span class="op">=</span> mutual_info_score(X, Y)</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Mutual Information:"</span>, mi)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-60" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-60">Why It Matters</h4>
<p>Mutual information generalizes correlation to capture both linear and nonlinear dependencies. In AI, it guides feature selection, helps design efficient encodings, and powers modern unsupervised and self-supervised learning methods.</p>
</section>
<section id="try-it-yourself-62" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-62">Try It Yourself</h4>
<ol type="1">
<li>Compute MI between two independent coin tosses—why is it zero?</li>
<li>Compute MI between a variable and its noisy copy—how does noise affect the value?</li>
<li>Explain how maximizing mutual information can improve learned representations.</li>
</ol>
</section>
</section>
<section id="kullbackleibler-divergence" class="level3">
<h3 class="anchored" data-anchor-id="kullbackleibler-divergence">164. Kullback–Leibler Divergence</h3>
<p>Kullback–Leibler (KL) divergence measures how one probability distribution diverges from another. It quantifies the inefficiency of assuming distribution <span class="math inline">\(Q\)</span> when the true distribution is <span class="math inline">\(P\)</span>.</p>
<section id="picture-in-your-head-63" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-63">Picture in Your Head</h4>
<p>Imagine packing luggage with the wrong-sized suitcases. If you assume people pack small items (distribution <span class="math inline">\(Q\)</span>), but in reality, they bring bulky clothes (distribution <span class="math inline">\(P\)</span>), you’ll waste space or run out of room. KL divergence measures that mismatch.</p>
</section>
<section id="deep-dive-63" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-63">Deep Dive</h4>
<ul>
<li><p>Definition: For discrete distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>:</p>
<p><span class="math display">\[
D_{KL}(P \parallel Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
\]</span></p>
<p>For continuous:</p>
<p><span class="math display">\[
D_{KL}(P \parallel Q) = \int p(x) \log \frac{p(x)}{q(x)} dx
\]</span></p></li>
<li><p>Properties:</p>
<ul>
<li><span class="math inline">\(D_{KL}(P \parallel Q) \geq 0\)</span> (Gibbs inequality).</li>
<li>Asymmetric: <span class="math inline">\(D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P)\)</span>.</li>
<li>Zero iff <span class="math inline">\(P=Q\)</span> almost everywhere.</li>
</ul></li>
<li><p>Interpretations:</p>
<ul>
<li>Extra bits required when coding samples from <span class="math inline">\(P\)</span> using code optimized for <span class="math inline">\(Q\)</span>.</li>
<li>Measure of distance (though not a true metric).</li>
</ul></li>
<li><p>In AI:</p>
<ul>
<li>Variational inference (ELBO minimization).</li>
<li>Regularizer in VAEs (match approximate posterior to prior).</li>
<li>Policy optimization in RL (trust region methods).</li>
<li>Comparing probability models.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 37%">
<col style="width: 62%">
</colgroup>
<thead>
<tr class="header">
<th>Expression</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(D_{KL}(P \parallel Q)=0\)</span></td>
<td>Perfect match between P and Q</td>
</tr>
<tr class="even">
<td>Large <span class="math inline">\(D_{KL}(P \parallel Q)\)</span></td>
<td>Q is a poor approximation of P</td>
</tr>
<tr class="odd">
<td>Asymmetry</td>
<td>Forward vs reverse KL lead to different behaviors</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-63" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-63">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> entropy</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> np.array([<span class="fl">0.5</span>, <span class="fl">0.5</span>])       <span class="co"># True distribution</span></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> np.array([<span class="fl">0.9</span>, <span class="fl">0.1</span>])       <span class="co"># Approximate distribution</span></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>kl <span class="op">=</span> entropy(P, Q)  <span class="co"># KL(P||Q)</span></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"KL Divergence:"</span>, kl)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-61" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-61">Why It Matters</h4>
<p>KL divergence underpins much of probabilistic AI, from Bayesian inference to deep generative models. It provides a bridge between probability theory, coding theory, and optimization. Understanding it is key to modern machine learning.</p>
</section>
<section id="try-it-yourself-63" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-63">Try It Yourself</h4>
<ol type="1">
<li>Compute KL divergence between two biased coins (e.g., P=[0.6,0.4], Q=[0.5,0.5]).</li>
<li>Compare forward KL (P||Q) and reverse KL (Q||P). Which penalizes mode-covering vs mode-seeking?</li>
<li>Explain how KL divergence is used in training variational autoencoders.</li>
</ol>
</section>
</section>
<section id="cross-entropy-and-likelihood" class="level3">
<h3 class="anchored" data-anchor-id="cross-entropy-and-likelihood">165. Cross-Entropy and Likelihood</h3>
<p>Cross-entropy measures the average number of bits needed to encode events from a true distribution <span class="math inline">\(P\)</span> using a model distribution <span class="math inline">\(Q\)</span>. It is directly related to likelihood: minimizing cross-entropy is equivalent to maximizing the likelihood of the model given the data.</p>
<section id="picture-in-your-head-64" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-64">Picture in Your Head</h4>
<p>Imagine trying to compress text with a code designed for English, but your text is actually in French. The mismatch wastes space. Cross-entropy quantifies that inefficiency, and likelihood measures how well your model explains the observed text.</p>
</section>
<section id="deep-dive-64" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-64">Deep Dive</h4>
<ul>
<li><p>Cross-entropy definition:</p>
<p><span class="math display">\[
H(P, Q) = - \sum_x P(x) \log Q(x)
\]</span></p>
<ul>
<li><p>Equals entropy <span class="math inline">\(H(P)\)</span> plus KL divergence:</p>
<p><span class="math display">\[
H(P, Q) = H(P) + D_{KL}(P \parallel Q)
\]</span></p></li>
</ul></li>
<li><p>Maximum likelihood connection:</p>
<ul>
<li><p>Given samples <span class="math inline">\(\{x_i\}\)</span>, maximizing likelihood</p>
<p><span class="math display">\[
\hat{\theta} = \arg\max_\theta \prod_i Q(x_i;\theta)
\]</span></p>
<p>is equivalent to minimizing cross-entropy between empirical distribution and model.</p></li>
</ul></li>
<li><p>Loss functions in AI:</p>
<ul>
<li><p>Binary cross-entropy:</p>
<p><span class="math display">\[
L = -[y \log \hat{y} + (1-y)\log(1-\hat{y})]
\]</span></p></li>
<li><p>Categorical cross-entropy:</p>
<p><span class="math display">\[
L = -\sum_{k} y_k \log \hat{y}_k
\]</span></p></li>
</ul></li>
<li><p>Applications:</p>
<ul>
<li>Classification tasks (logistic regression, neural networks).</li>
<li>Language modeling (predicting next token).</li>
<li>Probabilistic forecasting.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 37%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Formula</th>
<th>AI Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Cross-entropy <span class="math inline">\(H(P,Q)\)</span></td>
<td><span class="math inline">\(-\sum P(x)\log Q(x)\)</span></td>
<td>Model evaluation and training</td>
</tr>
<tr class="even">
<td>Relation to KL</td>
<td><span class="math inline">\(H(P,Q) = H(P) + D_{KL}(P\parallel Q)\)</span></td>
<td>Shows inefficiency when using wrong model</td>
</tr>
<tr class="odd">
<td>Likelihood</td>
<td>Product of probabilities under model</td>
<td>Basis of parameter estimation</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-64" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-64">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> log_loss</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a><span class="co"># True labels and predicted probabilities</span></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>]</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> [<span class="fl">0.1</span>, <span class="fl">0.9</span>, <span class="fl">0.8</span>, <span class="fl">0.2</span>]</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Binary cross-entropy</span></span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> log_loss(y_true, y_pred)</span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Cross-Entropy Loss:"</span>, loss)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-62" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-62">Why It Matters</h4>
<p>Cross-entropy ties together coding theory and statistical learning. It is the standard loss function for classification because minimizing it maximizes likelihood, ensuring the model aligns as closely as possible with the true data distribution.</p>
</section>
<section id="try-it-yourself-64" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-64">Try It Yourself</h4>
<ol type="1">
<li>Compute cross-entropy for a biased coin with true p=0.7 but model q=0.5.</li>
<li>Show how minimizing cross-entropy improves a classifier’s predictions.</li>
<li>Explain why cross-entropy is preferred over mean squared error for probability outputs.</li>
</ol>
</section>
</section>
<section id="channel-capacity-and-coding-theorems" class="level3">
<h3 class="anchored" data-anchor-id="channel-capacity-and-coding-theorems">166. Channel Capacity and Coding Theorems</h3>
<p>Channel capacity is the maximum rate at which information can be reliably transmitted over a noisy communication channel. Coding theorems guarantee that, with clever encoding, we can approach this limit while keeping the error probability arbitrarily small.</p>
<section id="picture-in-your-head-65" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-65">Picture in Your Head</h4>
<p>Imagine trying to talk to a friend across a noisy café. If you speak too fast, they’ll miss words. But if you speak at or below a certain pace—the channel capacity—they’ll catch everything with the right decoding strategy.</p>
</section>
<section id="deep-dive-65" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-65">Deep Dive</h4>
<ul>
<li><p>Channel capacity:</p>
<ul>
<li><p>Defined as the maximum mutual information between input <span class="math inline">\(X\)</span> and output <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[
C = \max_{p(x)} I(X;Y)
\]</span></p></li>
<li><p>Represents highest achievable communication rate (bits per channel use).</p></li>
</ul></li>
<li><p>Shannon’s Channel Coding Theorem:</p>
<ul>
<li>If rate <span class="math inline">\(R &lt; C\)</span>, there exist coding schemes with error probability → 0 as block length grows.</li>
<li>If <span class="math inline">\(R &gt; C\)</span>, reliable communication is impossible.</li>
</ul></li>
<li><p>Types of channels:</p>
<ul>
<li>Binary symmetric channel (BSC): flips bits with probability <span class="math inline">\(p\)</span>.</li>
<li>Binary erasure channel (BEC): deletes bits with probability <span class="math inline">\(p\)</span>.</li>
<li>Gaussian channel: continuous noise added to signal.</li>
</ul></li>
<li><p>Coding schemes:</p>
<ul>
<li>Error-correcting codes: Hamming codes, Reed–Solomon, LDPC, Turbo, Polar codes.</li>
<li>Trade-off between redundancy, efficiency, and error correction.</li>
</ul></li>
<li><p>In AI:</p>
<ul>
<li>Inspiration for regularization (information bottleneck).</li>
<li>Understanding data transmission in distributed learning.</li>
<li>Analogies for generalization and noise robustness.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 40%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th>Channel Type</th>
<th>Capacity Formula</th>
<th>Example Use</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Binary Symmetric (BSC)</td>
<td><span class="math inline">\(C = 1 - H(p)\)</span></td>
<td>Noisy bit transmission</td>
</tr>
<tr class="even">
<td>Binary Erasure (BEC)</td>
<td><span class="math inline">\(C = 1 - p\)</span></td>
<td>Packet loss in networks</td>
</tr>
<tr class="odd">
<td>Gaussian</td>
<td><span class="math inline">\(C = \tfrac{1}{2}\log_2(1+SNR)\)</span></td>
<td>Wireless communications</td>
</tr>
</tbody>
</table>
<p>Tiny Code Sample (Python, simulate BSC capacity)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> math <span class="im">import</span> log2</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> binary_entropy(p):</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> p <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> p <span class="op">==</span> <span class="dv">1</span>: <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>p<span class="op">*</span>log2(p) <span class="op">-</span> (<span class="dv">1</span><span class="op">-</span>p)<span class="op">*</span>log2(<span class="dv">1</span><span class="op">-</span>p)</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Capacity of Binary Symmetric Channel</span></span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># bit flip probability</span></span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> binary_entropy(p)</span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"BSC Capacity:"</span>, C, <span class="st">"bits per channel use"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-63" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-63">Why It Matters</h4>
<p>Channel capacity sets a fundamental limit: no algorithm can surpass it. The coding theorems show how close we can get, forming the backbone of digital communication. In AI, these ideas echo in information bottlenecks, compression, and error-tolerant learning systems.</p>
</section>
<section id="try-it-yourself-65" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-65">Try It Yourself</h4>
<ol type="1">
<li>Compute capacity of a BSC with error probability <span class="math inline">\(p=0.2\)</span>.</li>
<li>Compare capacity of a Gaussian channel with SNR = 10 dB and 20 dB.</li>
<li>Explain how redundancy in coding relates to regularization in machine learning.</li>
</ol>
</section>
</section>
<section id="ratedistortion-theory" class="level3">
<h3 class="anchored" data-anchor-id="ratedistortion-theory">167. Rate–Distortion Theory</h3>
<p>Rate–distortion theory studies the trade-off between compression rate (how many bits you use) and distortion (how much information is lost). It answers: what is the minimum number of bits per symbol required to represent data within a given tolerance of error?</p>
<section id="picture-in-your-head-66" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-66">Picture in Your Head</h4>
<p>Imagine saving a photo. If you compress it heavily, the file is small but blurry. If you save it losslessly, the file is large but perfect. Rate–distortion theory formalizes this compromise between size and quality.</p>
</section>
<section id="deep-dive-66" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-66">Deep Dive</h4>
<ul>
<li><p>Distortion measure: Quantifies error between original <span class="math inline">\(x\)</span> and reconstruction <span class="math inline">\(\hat{x}\)</span>. Example: mean squared error (MSE), Hamming distance.</p></li>
<li><p>Rate–distortion function: Minimum rate needed for distortion <span class="math inline">\(D\)</span>:</p>
<p><span class="math display">\[
R(D) = \min_{p(\hat{x}|x): E[d(x,\hat{x})] \leq D} I(X;\hat{X})
\]</span></p></li>
<li><p>Interpretations:</p>
<ul>
<li>At <span class="math inline">\(D=0\)</span>: <span class="math inline">\(R(D)=H(X)\)</span> (lossless compression).</li>
<li>As <span class="math inline">\(D\)</span> increases, fewer bits are needed.</li>
</ul></li>
<li><p>Shannon’s Rate–Distortion Theorem:</p>
<ul>
<li>Provides theoretical lower bound on compression efficiency.</li>
</ul></li>
<li><p>Applications in AI:</p>
<ul>
<li>Image/audio compression (JPEG, MP3).</li>
<li>Variational autoencoders (ELBO resembles rate–distortion trade-off).</li>
<li>Information bottleneck method (trade-off between relevance and compression).</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 31%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>Distortion Level</th>
<th>Bits per Symbol (Rate)</th>
<th>Example in Practice</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0 (perfect)</td>
<td><span class="math inline">\(H(X)\)</span></td>
<td>Lossless compression (PNG, FLAC)</td>
</tr>
<tr class="even">
<td>Low</td>
<td>Slightly &lt; <span class="math inline">\(H(X)\)</span></td>
<td>High-quality JPEG</td>
</tr>
<tr class="odd">
<td>High</td>
<td>Much smaller</td>
<td>Aggressive lossy compression</td>
</tr>
</tbody>
</table>
<p>Tiny Code Sample (Python, toy rate–distortion curve)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">50</span>)  <span class="co"># distortion</span></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>R <span class="op">=</span> np.maximum(<span class="dv">0</span>, <span class="dv">1</span> <span class="op">-</span> D)   <span class="co"># toy linear approx for illustration</span></span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>plt.plot(D, R)</span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Distortion"</span>)</span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Rate (bits/symbol)"</span>)</span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Toy Rate–Distortion Trade-off"</span>)</span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-64" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-64">Why It Matters</h4>
<p>Rate–distortion theory reveals the limits of lossy compression: how much data can be removed without exceeding a distortion threshold. In AI, it inspires representation learning methods that balance expressiveness with efficiency.</p>
</section>
<section id="try-it-yourself-66" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-66">Try It Yourself</h4>
<ol type="1">
<li>Compute the rate–distortion function for a binary source with Hamming distortion.</li>
<li>Compare distortion tolerance in JPEG vs PNG for the same image.</li>
<li>Explain how rate–distortion ideas appear in the variational autoencoder objective.</li>
</ol>
</section>
</section>
<section id="information-bottleneck-principle" class="level3">
<h3 class="anchored" data-anchor-id="information-bottleneck-principle">168. Information Bottleneck Principle</h3>
<p>The Information Bottleneck (IB) principle describes how to extract the most relevant information from an input while compressing away irrelevant details. It formalizes learning as balancing two goals: retain information about the target variable while discarding noise.</p>
<section id="picture-in-your-head-67" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-67">Picture in Your Head</h4>
<p>Imagine squeezing water through a filter. The wide stream of input data passes through a narrow bottleneck that only lets essential drops through—enough to reconstruct what matters, but not every detail.</p>
</section>
<section id="deep-dive-67" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-67">Deep Dive</h4>
<ul>
<li><p>Formal objective: Given input <span class="math inline">\(X\)</span> and target <span class="math inline">\(Y\)</span>, find compressed representation <span class="math inline">\(T\)</span>:</p>
<p><span class="math display">\[
\min I(X;T) - \beta I(T;Y)
\]</span></p>
<ul>
<li><span class="math inline">\(I(X;T)\)</span>: how much input information is kept.</li>
<li><span class="math inline">\(I(T;Y)\)</span>: how useful the representation is for predicting <span class="math inline">\(Y\)</span>.</li>
<li><span class="math inline">\(\beta\)</span>: trade-off parameter between compression and relevance.</li>
</ul></li>
<li><p>Connections:</p>
<ul>
<li>At <span class="math inline">\(\beta=0\)</span>: keep all information (<span class="math inline">\(T=X\)</span>).</li>
<li>Large <span class="math inline">\(\beta\)</span>: compress aggressively, retain only predictive parts.</li>
<li>Related to rate–distortion theory with “distortion” defined by prediction error.</li>
</ul></li>
<li><p>In AI:</p>
<ul>
<li>Neural networks: hidden layers act as information bottlenecks.</li>
<li>Variational Information Bottleneck (VIB): practical approximation for deep learning.</li>
<li>Regularization: prevents overfitting by discarding irrelevant detail.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 34%">
<col style="width: 46%">
</colgroup>
<thead>
<tr class="header">
<th>Term</th>
<th>Meaning</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(I(X;T)\)</span></td>
<td>Info retained from input</td>
<td>Latent representation complexity</td>
</tr>
<tr class="even">
<td><span class="math inline">\(I(T;Y)\)</span></td>
<td>Info relevant for prediction</td>
<td>Accuracy of classifier</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\beta\)</span> trade-off</td>
<td>Compression vs predictive power</td>
<td>Tuning representation learning objectives</td>
</tr>
</tbody>
</table>
<p>Tiny Code Sample (Python, sketch of VIB loss)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> vib_loss(p_y_given_t, q_t_given_x, p_t, y, beta<span class="op">=</span><span class="fl">1e-3</span>):</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Prediction loss (cross-entropy)</span></span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>    pred_loss <span class="op">=</span> F.nll_loss(p_y_given_t, y)</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># KL divergence term for compression</span></span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>    kl <span class="op">=</span> torch.distributions.kl.kl_divergence(q_t_given_x, p_t).mean()</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pred_loss <span class="op">+</span> beta <span class="op">*</span> kl</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-65" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-65">Why It Matters</h4>
<p>The IB principle provides a unifying view of representation learning: good models should compress inputs while preserving what matters for outputs. It bridges coding theory, statistics, and deep learning, and explains why deep networks generalize well despite huge capacity.</p>
</section>
<section id="try-it-yourself-67" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-67">Try It Yourself</h4>
<ol type="1">
<li>Explain why the hidden representation of a neural net can be seen as a bottleneck.</li>
<li>Modify <span class="math inline">\(\beta\)</span> in the VIB objective—what happens to compression vs accuracy?</li>
<li>Compare IB to rate–distortion theory: how do they differ in purpose?</li>
</ol>
</section>
</section>
<section id="minimum-description-length-mdl" class="level3">
<h3 class="anchored" data-anchor-id="minimum-description-length-mdl">169. Minimum Description Length (MDL)</h3>
<p>The Minimum Description Length principle views learning as compression: the best model is the one that provides the shortest description of the data plus the model itself. MDL formalizes Occam’s razor—prefer simpler models unless complexity is justified by better fit.</p>
<section id="picture-in-your-head-68" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-68">Picture in Your Head</h4>
<p>Imagine trying to explain a dataset to a friend. If you just read out all the numbers, that’s long. If you fit a simple pattern (“all numbers are even up to 100”), your explanation is shorter. MDL says the best explanation is the one that minimizes total description length.</p>
</section>
<section id="deep-dive-68" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-68">Deep Dive</h4>
<ul>
<li><p>Formal principle: Total description length = model complexity + data encoding under model.</p>
<p><span class="math display">\[
L(M, D) = L(M) + L(D \mid M)
\]</span></p>
<ul>
<li><span class="math inline">\(L(M)\)</span>: bits to describe the model.</li>
<li><span class="math inline">\(L(D|M)\)</span>: bits to encode the data given the model.</li>
</ul></li>
<li><p>Connections:</p>
<ul>
<li>Equivalent to maximizing posterior probability in Bayesian inference.</li>
<li>Related to Kolmogorov complexity (shortest program producing the data).</li>
<li>Generalizes to stochastic models: choose the one with minimal codelength.</li>
</ul></li>
<li><p>Applications in AI:</p>
<ul>
<li>Model selection (balancing bias–variance).</li>
<li>Avoiding overfitting in machine learning.</li>
<li>Feature selection via compressibility.</li>
<li>Information-theoretic foundations of regularization.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 30%">
<col style="width: 33%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th>Term</th>
<th>Meaning</th>
<th>AI Example</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(L(M)\)</span></td>
<td>Complexity cost of the model</td>
<td>Number of parameters in neural net</td>
<td></td>
</tr>
<tr class="even">
<td>(L(D</td>
<td>M))</td>
<td>Encoding cost of data given model</td>
<td>Log-likelihood under model</td>
</tr>
<tr class="odd">
<td>MDL principle</td>
<td>Minimize total description length</td>
<td>Trade-off between fit and simplicity</td>
<td></td>
</tr>
</tbody>
</table>
<p>Tiny Code Sample (Python, toy MDL for polynomial fit)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate noisy quadratic data</span></span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">20</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>X[:,<span class="dv">0</span>]<span class="dv">2</span> <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>np.random.randn(<span class="dv">20</span>)</span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mdl_cost(degree):</span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a>    poly <span class="op">=</span> PolynomialFeatures(degree)</span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a>    X_poly <span class="op">=</span> poly.fit_transform(X)</span>
<span id="cb69-15"><a href="#cb69-15" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> LinearRegression().fit(X_poly, y)</span>
<span id="cb69-16"><a href="#cb69-16" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(X_poly)</span>
<span id="cb69-17"><a href="#cb69-17" aria-hidden="true" tabindex="-1"></a>    mse <span class="op">=</span> mean_squared_error(y, y_pred)</span>
<span id="cb69-18"><a href="#cb69-18" aria-hidden="true" tabindex="-1"></a>    L_D_given_M <span class="op">=</span> <span class="bu">len</span>(y)<span class="op">*</span>math.log(mse<span class="op">+</span><span class="fl">1e-6</span>)   <span class="co"># data fit cost</span></span>
<span id="cb69-19"><a href="#cb69-19" aria-hidden="true" tabindex="-1"></a>    L_M <span class="op">=</span> degree                              <span class="co"># model complexity proxy</span></span>
<span id="cb69-20"><a href="#cb69-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> L_M <span class="op">+</span> L_D_given_M</span>
<span id="cb69-21"><a href="#cb69-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-22"><a href="#cb69-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">6</span>):</span>
<span id="cb69-23"><a href="#cb69-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Degree </span><span class="sc">{</span>d<span class="sc">}</span><span class="ss">, MDL cost: </span><span class="sc">{</span>mdl_cost(d)<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-66" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-66">Why It Matters</h4>
<p>MDL offers a principled, universal way to balance model complexity with data fit. It justifies why simpler models generalize better, and underlies practical methods like AIC, BIC, and regularization penalties in modern machine learning.</p>
</section>
<section id="try-it-yourself-68" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-68">Try It Yourself</h4>
<ol type="1">
<li>Compare MDL costs for fitting linear vs quadratic models to data.</li>
<li>Explain how MDL prevents overfitting in decision trees.</li>
<li>Relate MDL to deep learning regularization: how do weight penalties mimic description length?</li>
</ol>
</section>
</section>
<section id="applications-in-machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="applications-in-machine-learning">170. Applications in Machine Learning</h3>
<p>Information theory provides the language and tools to quantify uncertainty, dependence, and efficiency. In machine learning, these concepts directly translate into loss functions, regularization, and representation learning.</p>
<section id="picture-in-your-head-69" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-69">Picture in Your Head</h4>
<p>Imagine teaching a child new words. You want to give them enough examples to reduce uncertainty (entropy), focus on the most relevant clues (mutual information), and avoid wasting effort on noise. Machine learning systems operate under the same principles.</p>
</section>
<section id="deep-dive-69" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-69">Deep Dive</h4>
<ul>
<li><p>Entropy &amp; Cross-Entropy:</p>
<ul>
<li>Classification uses cross-entropy loss to align predicted and true distributions.</li>
<li>Entropy measures model uncertainty, guiding exploration in reinforcement learning.</li>
</ul></li>
<li><p>Mutual Information:</p>
<ul>
<li>Feature selection: choose variables with high MI with labels.</li>
<li>Representation learning: InfoNCE and contrastive learning maximize MI between views.</li>
</ul></li>
<li><p>KL Divergence:</p>
<ul>
<li>Core of variational inference and VAEs.</li>
<li>Regularizes approximate posteriors toward priors.</li>
</ul></li>
<li><p>Channel Capacity:</p>
<ul>
<li>Analogy for limits of model generalization.</li>
<li>Bottleneck layers in deep nets function like constrained channels.</li>
</ul></li>
<li><p>Rate–Distortion &amp; Bottleneck:</p>
<ul>
<li>Variational Information Bottleneck (VIB) balances compression and relevance.</li>
<li>Applied in disentangled representation learning.</li>
</ul></li>
<li><p>MDL Principle:</p>
<ul>
<li>Guides model selection by trading complexity for fit.</li>
<li>Explains regularization penalties (L1, L2) as description length constraints.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 36%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th>Information Concept</th>
<th>Machine Learning Role</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Entropy</td>
<td>Quantify uncertainty</td>
<td>Exploration in RL</td>
</tr>
<tr class="even">
<td>Cross-Entropy</td>
<td>Training objective</td>
<td>Classification, language modeling</td>
</tr>
<tr class="odd">
<td>Mutual Information</td>
<td>Feature/repr. relevance</td>
<td>Contrastive learning, clustering</td>
</tr>
<tr class="even">
<td>KL Divergence</td>
<td>Approximate inference</td>
<td>VAEs, Bayesian deep learning</td>
</tr>
<tr class="odd">
<td>Channel Capacity</td>
<td>Limit of reliable info transfer</td>
<td>Neural bottlenecks, compression</td>
</tr>
<tr class="even">
<td>Rate–Distortion / IB</td>
<td>Compress yet preserve relevance</td>
<td>Representation learning, VAEs</td>
</tr>
<tr class="odd">
<td>MDL</td>
<td>Model selection, generalization</td>
<td>Regularization, pruning</td>
</tr>
</tbody>
</table>
<p>Tiny Code Sample (Python, InfoNCE Loss)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> info_nce_loss(z_i, z_j, temperature<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># z_i, z_j are embeddings from two augmented views</span></span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> z_i.shape[<span class="dv">0</span>]</span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> torch.cat([z_i, z_j], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>    sim <span class="op">=</span> F.cosine_similarity(z.unsqueeze(<span class="dv">1</span>), z.unsqueeze(<span class="dv">0</span>), dim<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a>    sim <span class="op">/=</span> temperature</span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> torch.arange(batch_size, device<span class="op">=</span>z.device)</span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> torch.cat([labels, labels], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> F.cross_entropy(sim, labels)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-67" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-67">Why It Matters</h4>
<p>Information theory explains <em>why</em> machine learning works. It unifies compression, prediction, and generalization, showing that learning is fundamentally about extracting, transmitting, and representing information efficiently.</p>
</section>
<section id="try-it-yourself-69" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-69">Try It Yourself</h4>
<ol type="1">
<li>Train a classifier with cross-entropy loss and measure entropy of predictions on uncertain data.</li>
<li>Use mutual information to rank features in a dataset.</li>
<li>Relate the concept of channel capacity to overfitting in deep networks.</li>
</ol>
</section>
</section>
</section>
<section id="chapter-18.-graphs-matrices-and-special-methods" class="level2">
<h2 class="anchored" data-anchor-id="chapter-18.-graphs-matrices-and-special-methods">Chapter 18. Graphs, Matrices and Special Methods</h2>
<section id="graphs-nodes-edges-and-paths" class="level3">
<h3 class="anchored" data-anchor-id="graphs-nodes-edges-and-paths">171. Graphs: Nodes, Edges, and Paths</h3>
<p>Graphs are mathematical structures that capture relationships between entities. A graph consists of nodes (vertices) and edges (links). They can be directed or undirected, weighted or unweighted, and form the foundation for reasoning about connectivity, flow, and structure.</p>
<section id="picture-in-your-head-70" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-70">Picture in Your Head</h4>
<p>Imagine a social network. Each person is a node, and each friendship is an edge connecting two people. A path is just a chain of friendships—how you get from one person to another through mutual friends.</p>
</section>
<section id="deep-dive-70" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-70">Deep Dive</h4>
<ul>
<li><p>Graph definition: <span class="math inline">\(G = (V, E)\)</span> with vertex set <span class="math inline">\(V\)</span> and edge set <span class="math inline">\(E\)</span>.</p></li>
<li><p>Nodes (vertices): fundamental units (people, cities, states).</p></li>
<li><p>Edges (links): represent relationships, can be:</p>
<ul>
<li>Directed: (u,v) ≠ (v,u) → Twitter follow.</li>
<li>Undirected: (u,v) = (v,u) → Facebook friendship.</li>
</ul></li>
<li><p>Weighted graphs: edges have values (distance, cost, similarity).</p></li>
<li><p>Paths and connectivity:</p>
<ul>
<li>Path = sequence of edges between nodes.</li>
<li>Cycle = path that starts and ends at same node.</li>
<li>Connected graph = path exists between any two nodes.</li>
</ul></li>
<li><p>Special graphs: trees, bipartite graphs, complete graphs.</p></li>
<li><p>In AI: graphs model knowledge bases, molecules, neural nets, logistics, and interactions in multi-agent systems.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 34%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Element</th>
<th>Meaning</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Node (vertex)</td>
<td>Entity</td>
<td>User in social network, word in NLP</td>
</tr>
<tr class="even">
<td>Edge (link)</td>
<td>Relationship between entities</td>
<td>Friendship, co-occurrence, road connection</td>
</tr>
<tr class="odd">
<td>Weighted edge</td>
<td>Strength or cost of relation</td>
<td>Distance between cities, attention score</td>
</tr>
<tr class="even">
<td>Path</td>
<td>Sequence of nodes/edges</td>
<td>Inference chain in knowledge graph</td>
</tr>
<tr class="odd">
<td>Cycle</td>
<td>Path that returns to start</td>
<td>Feedback loop in causal models</td>
</tr>
</tbody>
</table>
<p>Tiny Code Sample (Python, using NetworkX)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create graph</span></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> nx.Graph()</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>G.add_edges_from([(<span class="st">"Alice"</span>,<span class="st">"Bob"</span>), (<span class="st">"Bob"</span>,<span class="st">"Carol"</span>), (<span class="st">"Alice"</span>,<span class="st">"Dan"</span>)])</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Nodes:"</span>, G.nodes())</span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Edges:"</span>, G.edges())</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Check paths</span></span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Path Alice -&gt; Carol:"</span>, nx.shortest_path(G, <span class="st">"Alice"</span>, <span class="st">"Carol"</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-68" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-68">Why It Matters</h4>
<p>Graphs are the universal language of structure and relationships. In AI, they support reasoning (knowledge graphs), learning (graph neural networks), and optimization (routing, scheduling). Without graphs, many AI systems would lack the ability to represent and reason about complex connections.</p>
</section>
<section id="try-it-yourself-70" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-70">Try It Yourself</h4>
<ol type="1">
<li>Construct a graph of five cities and connect them with distances as edge weights. Find the shortest path between two cities.</li>
<li>Build a bipartite graph of users and movies. What does a path from user A to user B mean?</li>
<li>Give an example where cycles in a graph model feedback in a real system (e.g., economy, ecology).</li>
</ol>
</section>
</section>
<section id="adjacency-and-incidence-matrices" class="level3">
<h3 class="anchored" data-anchor-id="adjacency-and-incidence-matrices">172. Adjacency and Incidence Matrices</h3>
<p>Graphs can be represented algebraically using matrices. The adjacency matrix encodes which nodes are connected, while the incidence matrix captures relationships between nodes and edges. These matrix forms enable powerful linear algebra techniques for analyzing graphs.</p>
<section id="picture-in-your-head-71" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-71">Picture in Your Head</h4>
<p>Think of a city map. You could describe it with a list of roads (edges) connecting intersections (nodes), or you could build a big table. Each row and column of the table represents intersections, and you mark a “1” whenever a road connects two intersections. That table is the adjacency matrix.</p>
</section>
<section id="deep-dive-71" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-71">Deep Dive</h4>
<ul>
<li><p>Adjacency matrix (A):</p>
<ul>
<li><p>For graph <span class="math inline">\(G=(V,E)\)</span> with <span class="math inline">\(|V|=n\)</span>:</p>
<p><span class="math display">\[
A_{ij} = \begin{cases}
  1 &amp; \text{if edge } (i,j) \in E, \\
  0 &amp; \text{otherwise.}
\end{cases}
\]</span></p></li>
<li><p>For weighted graphs, entries contain weights instead of 1s.</p></li>
<li><p>Properties: symmetric for undirected graphs; row sums give node degrees.</p></li>
</ul></li>
<li><p>Incidence matrix (B):</p>
<ul>
<li><p>Rows = nodes, columns = edges.</p></li>
<li><p>For edge <span class="math inline">\(e=(i,j)\)</span>:</p>
<ul>
<li><span class="math inline">\(B_{i,e} = +1\)</span>, <span class="math inline">\(B_{j,e} = -1\)</span>, all others 0 (for directed graphs).</li>
</ul></li>
<li><p>Captures how edges connect vertices.</p></li>
</ul></li>
<li><p>Linear algebra links:</p>
<ul>
<li>Degree matrix: <span class="math inline">\(D_{ii} = \sum_j A_{ij}\)</span>.</li>
<li>Graph Laplacian: <span class="math inline">\(L = D - A\)</span>.</li>
</ul></li>
<li><p>In AI: used in spectral clustering, graph convolutional networks, knowledge graph embeddings.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 35%">
<col style="width: 43%">
</colgroup>
<thead>
<tr class="header">
<th>Matrix</th>
<th>Definition</th>
<th>Use Case in AI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Adjacency (A)</td>
<td>Node-to-node connectivity</td>
<td>Graph neural networks, node embeddings</td>
</tr>
<tr class="even">
<td>Weighted adjacency</td>
<td>Edge weights as entries</td>
<td>Shortest paths, recommender systems</td>
</tr>
<tr class="odd">
<td>Incidence (B)</td>
<td>Node-to-edge mapping</td>
<td>Flow problems, electrical circuits</td>
</tr>
<tr class="even">
<td>Laplacian (L=D−A)</td>
<td>Derived from adjacency + degree</td>
<td>Spectral methods, clustering, GNNs</td>
</tr>
</tbody>
</table>
<p>Tiny Code Sample (Python, using NetworkX &amp; NumPy)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Build graph</span></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> nx.Graph()</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>G.add_edges_from([(<span class="dv">0</span>,<span class="dv">1</span>),(<span class="dv">1</span>,<span class="dv">2</span>),(<span class="dv">2</span>,<span class="dv">0</span>),(<span class="dv">2</span>,<span class="dv">3</span>)])</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjacency matrix</span></span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> nx.to_numpy_array(G)</span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Adjacency matrix:</span><span class="ch">\n</span><span class="st">"</span>, A)</span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Incidence matrix</span></span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> nx.incidence_matrix(G, oriented<span class="op">=</span><span class="va">True</span>).toarray()</span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Incidence matrix:</span><span class="ch">\n</span><span class="st">"</span>, B)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-69" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-69">Why It Matters</h4>
<p>Matrix representations let us apply linear algebra to graphs, unlocking tools for clustering, spectral analysis, and graph neural networks. This algebraic viewpoint turns structural problems into numerical ones, making them solvable with efficient algorithms.</p>
</section>
<section id="try-it-yourself-71" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-71">Try It Yourself</h4>
<ol type="1">
<li>Construct the adjacency matrix for a triangle graph (3 nodes, fully connected). What are its eigenvalues?</li>
<li>Build the incidence matrix for a 4-node chain graph. How do its columns reflect edge connections?</li>
<li>Use the Laplacian <span class="math inline">\(L=D-A\)</span> of a small graph to compute its connected components.</li>
</ol>
</section>
</section>
<section id="graph-traversals-dfs-bfs" class="level3">
<h3 class="anchored" data-anchor-id="graph-traversals-dfs-bfs">173. Graph Traversals (DFS, BFS)</h3>
<p>Graph traversal algorithms systematically explore nodes and edges. Depth-First Search (DFS) goes as far as possible along one path before backtracking, while Breadth-First Search (BFS) explores neighbors layer by layer. These two strategies underpin many higher-level graph algorithms.</p>
<section id="picture-in-your-head-72" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-72">Picture in Your Head</h4>
<p>Imagine searching a maze. DFS is like always taking the next hallway until you hit a dead end, then backtracking. BFS is like exploring all hallways one step at a time, ensuring you find the shortest way out.</p>
</section>
<section id="deep-dive-72" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-72">Deep Dive</h4>
<ul>
<li><p>DFS (Depth-First Search):</p>
<ul>
<li>Explores deep into a branch before backtracking.</li>
<li>Implemented recursively or with a stack.</li>
<li>Useful for detecting cycles, topological sorting, connected components.</li>
</ul></li>
<li><p>BFS (Breadth-First Search):</p>
<ul>
<li>Explores all neighbors of current node before moving deeper.</li>
<li>Uses a queue.</li>
<li>Finds shortest paths in unweighted graphs.</li>
</ul></li>
<li><p>Complexity: <span class="math inline">\(O(|V| + |E|)\)</span> for both.</p></li>
<li><p>In AI: used in search (state spaces, planning), social network analysis, knowledge graph queries.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 8%">
<col style="width: 16%">
<col style="width: 37%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Traversal</th>
<th>Mechanism</th>
<th>Strengths</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>DFS</td>
<td>Stack/recursion</td>
<td>Memory-efficient, explores deeply</td>
<td>Topological sort, constraint satisfaction</td>
</tr>
<tr class="even">
<td>BFS</td>
<td>Queue, level-order</td>
<td>Finds shortest path in unweighted graphs</td>
<td>Shortest queries in knowledge graphs</td>
</tr>
</tbody>
</table>
<p>Tiny Code Sample (Python, DFS &amp; BFS with NetworkX)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> deque</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> nx.Graph()</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>G.add_edges_from([(<span class="dv">0</span>,<span class="dv">1</span>),(<span class="dv">0</span>,<span class="dv">2</span>),(<span class="dv">1</span>,<span class="dv">3</span>),(<span class="dv">2</span>,<span class="dv">3</span>),(<span class="dv">3</span>,<span class="dv">4</span>)])</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a><span class="co"># DFS</span></span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dfs(graph, start, visited<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> visited <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>        visited <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>    visited.add(start)</span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> neighbor <span class="kw">in</span> graph.neighbors(start):</span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> neighbor <span class="kw">not</span> <span class="kw">in</span> visited:</span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a>            dfs(graph, neighbor, visited)</span>
<span id="cb73-15"><a href="#cb73-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> visited</span>
<span id="cb73-16"><a href="#cb73-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-17"><a href="#cb73-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"DFS from 0:"</span>, dfs(G, <span class="dv">0</span>))</span>
<span id="cb73-18"><a href="#cb73-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-19"><a href="#cb73-19" aria-hidden="true" tabindex="-1"></a><span class="co"># BFS</span></span>
<span id="cb73-20"><a href="#cb73-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bfs(graph, start):</span>
<span id="cb73-21"><a href="#cb73-21" aria-hidden="true" tabindex="-1"></a>    visited, queue <span class="op">=</span> <span class="bu">set</span>([start]), deque([start])</span>
<span id="cb73-22"><a href="#cb73-22" aria-hidden="true" tabindex="-1"></a>    order <span class="op">=</span> []</span>
<span id="cb73-23"><a href="#cb73-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> queue:</span>
<span id="cb73-24"><a href="#cb73-24" aria-hidden="true" tabindex="-1"></a>        node <span class="op">=</span> queue.popleft()</span>
<span id="cb73-25"><a href="#cb73-25" aria-hidden="true" tabindex="-1"></a>        order.append(node)</span>
<span id="cb73-26"><a href="#cb73-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> neighbor <span class="kw">in</span> graph.neighbors(node):</span>
<span id="cb73-27"><a href="#cb73-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> neighbor <span class="kw">not</span> <span class="kw">in</span> visited:</span>
<span id="cb73-28"><a href="#cb73-28" aria-hidden="true" tabindex="-1"></a>                visited.add(neighbor)</span>
<span id="cb73-29"><a href="#cb73-29" aria-hidden="true" tabindex="-1"></a>                queue.append(neighbor)</span>
<span id="cb73-30"><a href="#cb73-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> order</span>
<span id="cb73-31"><a href="#cb73-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-32"><a href="#cb73-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"BFS from 0:"</span>, bfs(G, <span class="dv">0</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-70" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-70">Why It Matters</h4>
<p>Traversal is the backbone of graph algorithms. Whether navigating a state space in AI search, analyzing social networks, or querying knowledge graphs, DFS and BFS provide the exploration strategies on which more complex reasoning is built.</p>
</section>
<section id="try-it-yourself-72" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-72">Try It Yourself</h4>
<ol type="1">
<li>Use BFS to find the shortest path between two nodes in an unweighted graph.</li>
<li>Modify DFS to detect cycles in a directed graph.</li>
<li>Compare the traversal order of BFS vs DFS on a binary tree—what insights do you gain?</li>
</ol>
</section>
</section>
<section id="connectivity-and-components" class="level3">
<h3 class="anchored" data-anchor-id="connectivity-and-components">174. Connectivity and Components</h3>
<p>Connectivity describes whether nodes in a graph are reachable from one another. A connected component is a maximal set of nodes where each pair has a path between them. In directed graphs, we distinguish between strongly and weakly connected components.</p>
<section id="picture-in-your-head-73" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-73">Picture in Your Head</h4>
<p>Think of islands connected by bridges. Each island cluster where you can walk from any town to any other without leaving the cluster is a connected component. If some islands are cut off, they form separate components.</p>
</section>
<section id="deep-dive-73" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-73">Deep Dive</h4>
<ul>
<li><p>Undirected graphs:</p>
<ul>
<li>A graph is connected if every pair of nodes has a path.</li>
<li>Otherwise, it splits into multiple connected components.</li>
</ul></li>
<li><p>Directed graphs:</p>
<ul>
<li>Strongly connected component (SCC): every node reachable from every other node.</li>
<li>Weakly connected component: connectivity holds if edge directions are ignored.</li>
</ul></li>
<li><p>Algorithms:</p>
<ul>
<li>BFS/DFS to find connected components in undirected graphs.</li>
<li>Kosaraju’s, Tarjan’s, or Gabow’s algorithm for SCCs in directed graphs.</li>
</ul></li>
<li><p>Applications in AI:</p>
<ul>
<li>Social network analysis (friendship clusters).</li>
<li>Knowledge graphs (isolated subgraphs).</li>
<li>Computer vision (connected pixel regions).</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 41%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Type</th>
<th>Definition</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Connected graph</td>
<td>All nodes reachable</td>
<td>Communication networks</td>
</tr>
<tr class="even">
<td>Connected component</td>
<td>Maximal subset of mutually reachable nodes</td>
<td>Community detection in social graphs</td>
</tr>
<tr class="odd">
<td>Strongly connected comp.</td>
<td>Directed paths in both directions exist</td>
<td>Web graph link cycles</td>
</tr>
<tr class="even">
<td>Weakly connected comp.</td>
<td>Paths exist if direction is ignored</td>
<td>Isolated knowledge graph partitions</td>
</tr>
</tbody>
</table>
<p>Tiny Code Sample (Python, NetworkX)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Undirected graph with two components</span></span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> nx.Graph()</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>G.add_edges_from([(<span class="dv">0</span>,<span class="dv">1</span>),(<span class="dv">1</span>,<span class="dv">2</span>),(<span class="dv">3</span>,<span class="dv">4</span>)])</span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>components <span class="op">=</span> <span class="bu">list</span>(nx.connected_components(G))</span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Connected components:"</span>, components)</span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Directed graph SCCs</span></span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a>DG <span class="op">=</span> nx.DiGraph()</span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a>DG.add_edges_from([(<span class="dv">0</span>,<span class="dv">1</span>),(<span class="dv">1</span>,<span class="dv">2</span>),(<span class="dv">2</span>,<span class="dv">0</span>),(<span class="dv">3</span>,<span class="dv">4</span>)])</span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a>sccs <span class="op">=</span> <span class="bu">list</span>(nx.strongly_connected_components(DG))</span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Strongly connected components:"</span>, sccs)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-71" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-71">Why It Matters</h4>
<p>Understanding connectivity helps identify whether a system is unified or fragmented. In AI, it reveals isolated data clusters, ensures graph search completeness, and supports robustness analysis in networks and multi-agent systems.</p>
</section>
<section id="try-it-yourself-73" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-73">Try It Yourself</h4>
<ol type="1">
<li>Build a graph with three disconnected subgraphs and identify its connected components.</li>
<li>Create a directed cycle (A→B→C→A). Is it strongly connected? Weakly connected?</li>
<li>Explain how identifying SCCs might help in optimizing web crawlers or knowledge graph queries.</li>
</ol>
</section>
</section>
<section id="graph-laplacians" class="level3">
<h3 class="anchored" data-anchor-id="graph-laplacians">175. Graph Laplacians</h3>
<p>The graph Laplacian is a matrix that encodes both connectivity and structure of a graph. It is central to spectral graph theory, linking graph properties with eigenvalues and eigenvectors. Laplacians underpin clustering, graph embeddings, and diffusion processes in AI.</p>
<section id="picture-in-your-head-74" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-74">Picture in Your Head</h4>
<p>Imagine pouring dye on one node of a network of pipes. The way the dye diffuses over time depends on how the pipes connect. The Laplacian matrix mathematically describes that diffusion across the graph.</p>
</section>
<section id="deep-dive-74" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-74">Deep Dive</h4>
<ul>
<li><p>Definition: For graph <span class="math inline">\(G=(V,E)\)</span> with adjacency matrix <span class="math inline">\(A\)</span> and degree matrix <span class="math inline">\(D\)</span>:</p>
<p><span class="math display">\[
L = D - A
\]</span></p></li>
<li><p>Normalized forms:</p>
<ul>
<li>Symmetric: <span class="math inline">\(L_{sym} = D^{-1/2} L D^{-1/2}\)</span>.</li>
<li>Random-walk: <span class="math inline">\(L_{rw} = D^{-1} L\)</span>.</li>
</ul></li>
<li><p>Key properties:</p>
<ul>
<li><span class="math inline">\(L\)</span> is symmetric and positive semi-definite.</li>
<li>The smallest eigenvalue is always 0, with multiplicity equal to the number of connected components.</li>
</ul></li>
<li><p>Applications:</p>
<ul>
<li>Spectral clustering: uses eigenvectors of Laplacian to partition graphs.</li>
<li>Graph embeddings: Laplacian Eigenmaps for dimensionality reduction.</li>
<li>Physics: models heat diffusion and random walks.</li>
</ul></li>
<li><p>In AI: community detection, semi-supervised learning, manifold learning, graph neural networks.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 26%">
<col style="width: 46%">
</colgroup>
<thead>
<tr class="header">
<th>Variant</th>
<th>Formula</th>
<th>Application in AI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Unnormalized L</td>
<td><span class="math inline">\(D - A\)</span></td>
<td>General graph analysis</td>
</tr>
<tr class="even">
<td>Normalized <span class="math inline">\(L_{sym}\)</span></td>
<td><span class="math inline">\(D^{-1/2}LD^{-1/2}\)</span></td>
<td>Spectral clustering</td>
</tr>
<tr class="odd">
<td>Random-walk <span class="math inline">\(L_{rw}\)</span></td>
<td><span class="math inline">\(D^{-1}L\)</span></td>
<td>Markov processes, diffusion models</td>
</tr>
</tbody>
</table>
<p>Tiny Code Sample (Python, NumPy + NetworkX)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Build simple graph</span></span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> nx.Graph()</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>G.add_edges_from([(<span class="dv">0</span>,<span class="dv">1</span>),(<span class="dv">1</span>,<span class="dv">2</span>),(<span class="dv">2</span>,<span class="dv">0</span>),(<span class="dv">2</span>,<span class="dv">3</span>)])</span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Degree and adjacency matrices</span></span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> nx.to_numpy_array(G)</span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> np.diag(A.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Laplacian</span></span>
<span id="cb75-13"><a href="#cb75-13" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> D <span class="op">-</span> A</span>
<span id="cb75-14"><a href="#cb75-14" aria-hidden="true" tabindex="-1"></a>eigs, vecs <span class="op">=</span> np.linalg.eigh(L)</span>
<span id="cb75-15"><a href="#cb75-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-16"><a href="#cb75-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Laplacian:</span><span class="ch">\n</span><span class="st">"</span>, L)</span>
<span id="cb75-17"><a href="#cb75-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Eigenvalues:"</span>, eigs)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-72" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-72">Why It Matters</h4>
<p>The Laplacian turns graph problems into linear algebra problems. Its spectral properties reveal clusters, connectivity, and diffusion dynamics. This makes it indispensable in AI methods that rely on graph structure, from GNNs to semi-supervised learning.</p>
</section>
<section id="try-it-yourself-74" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-74">Try It Yourself</h4>
<ol type="1">
<li>Construct the Laplacian of a chain of 4 nodes and compute its eigenvalues.</li>
<li>Use the Fiedler vector (second-smallest eigenvector) to partition a graph into two clusters.</li>
<li>Explain how the Laplacian relates to random walks and Markov chains.</li>
</ol>
</section>
</section>
<section id="spectral-decomposition-of-graphs" class="level3">
<h3 class="anchored" data-anchor-id="spectral-decomposition-of-graphs">176. Spectral Decomposition of Graphs</h3>
<p>Spectral graph theory studies the eigenvalues and eigenvectors of matrices associated with graphs, especially the Laplacian and adjacency matrices. These spectral properties reveal structure, connectivity, and clustering in graphs.</p>
<section id="picture-in-your-head-75" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-75">Picture in Your Head</h4>
<p>Imagine plucking a guitar string. The vibration frequencies are determined by the string’s structure. Similarly, the “frequencies” (eigenvalues) of a graph come from its Laplacian, and the “modes” (eigenvectors) reveal how the graph naturally partitions.</p>
</section>
<section id="deep-dive-75" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-75">Deep Dive</h4>
<ul>
<li><p>Adjacency spectrum: eigenvalues of adjacency matrix <span class="math inline">\(A\)</span>.</p>
<ul>
<li>Capture connectivity patterns.</li>
</ul></li>
<li><p>Laplacian spectrum: eigenvalues of <span class="math inline">\(L=D-A\)</span>.</p>
<ul>
<li>Smallest eigenvalue is always 0.</li>
<li>Multiplicity of 0 equals number of connected components.</li>
<li>Second-smallest eigenvalue (Fiedler value) measures graph connectivity.</li>
</ul></li>
<li><p>Eigenvectors:</p>
<ul>
<li>Fiedler vector used to partition graphs (spectral clustering).</li>
<li>Eigenvectors represent smooth variations across nodes.</li>
</ul></li>
<li><p>Applications:</p>
<ul>
<li>Graph partitioning, community detection.</li>
<li>Embeddings (Laplacian eigenmaps).</li>
<li>Analyzing diffusion and random walks.</li>
<li>Designing Graph Neural Networks with spectral filters.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 37%">
<col style="width: 38%">
</colgroup>
<thead>
<tr class="header">
<th>Spectrum Type</th>
<th>Information Provided</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Adjacency eigenvalues</td>
<td>Density, degree distribution</td>
<td>Social network analysis</td>
</tr>
<tr class="even">
<td>Laplacian eigenvalues</td>
<td>Connectivity, clustering structure</td>
<td>Spectral clustering in ML</td>
</tr>
<tr class="odd">
<td>Eigenvectors</td>
<td>Node embeddings, smooth functions</td>
<td>Semi-supervised node classification</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-65" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-65">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Build simple graph</span></span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> nx.path_graph(<span class="dv">5</span>)  <span class="co"># 5 nodes in a chain</span></span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Laplacian</span></span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> nx.laplacian_matrix(G).toarray()</span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Eigen-decomposition</span></span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>eigs, vecs <span class="op">=</span> np.linalg.eigh(L)</span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Eigenvalues:"</span>, eigs)</span>
<span id="cb76-14"><a href="#cb76-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Fiedler vector (2nd eigenvector):"</span>, vecs[:,<span class="dv">1</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-73" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-73">Why It Matters</h4>
<p>Spectral methods provide a bridge between graph theory and linear algebra. In AI, they enable powerful techniques for clustering, embeddings, and GNN architectures. Understanding the spectral view of graphs is key to analyzing structure beyond simple connectivity.</p>
</section>
<section id="try-it-yourself-75" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-75">Try It Yourself</h4>
<ol type="1">
<li>Compute Laplacian eigenvalues of a complete graph with 4 nodes. How many zeros appear?</li>
<li>Use the Fiedler vector to split a graph into two communities.</li>
<li>Explain how eigenvalues can indicate robustness of networks to node/edge removal.</li>
</ol>
</section>
</section>
<section id="eigenvalues-and-graph-partitioning" class="level3">
<h3 class="anchored" data-anchor-id="eigenvalues-and-graph-partitioning">177. Eigenvalues and Graph Partitioning</h3>
<p>Graph partitioning divides a graph into groups of nodes while minimizing connections between groups. Eigenvalues and eigenvectors of the Laplacian provide a principled way to achieve this, forming the basis of spectral clustering.</p>
<section id="picture-in-your-head-76" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-76">Picture in Your Head</h4>
<p>Imagine a city split by a river. People within each side interact more with each other than across the river. The graph Laplacian’s eigenvalues reveal this “natural cut,” and the corresponding eigenvector helps assign nodes to their side.</p>
</section>
<section id="deep-dive-76" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-76">Deep Dive</h4>
<ul>
<li><p>Fiedler value (λ₂):</p>
<ul>
<li>Second-smallest eigenvalue of Laplacian.</li>
<li>Measures algebraic connectivity: small λ₂ means graph is loosely connected.</li>
</ul></li>
<li><p>Fiedler vector:</p>
<ul>
<li>Corresponding eigenvector partitions nodes into two sets based on sign (or value threshold).</li>
<li>Defines a “spectral cut” of the graph.</li>
</ul></li>
<li><p>Graph partitioning problem:</p>
<ul>
<li>Minimize edge cuts between partitions while balancing group sizes.</li>
<li>NP-hard in general, but spectral relaxation makes it tractable.</li>
</ul></li>
<li><p>Spectral clustering:</p>
<ul>
<li>Use top k eigenvectors of normalized Laplacian as features.</li>
<li>Apply k-means to cluster nodes.</li>
</ul></li>
<li><p>Applications in AI:</p>
<ul>
<li>Community detection in social networks.</li>
<li>Document clustering in NLP.</li>
<li>Image segmentation (pixels as graph nodes).</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 44%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Role in Partitioning</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Fiedler value λ₂</td>
<td>Strength of connectivity</td>
<td>Detecting weakly linked communities</td>
</tr>
<tr class="even">
<td>Fiedler vector</td>
<td>Partition nodes into two sets</td>
<td>Splitting social networks into groups</td>
</tr>
<tr class="odd">
<td>Spectral clustering</td>
<td>Uses eigenvectors of Laplacian for clustering</td>
<td>Image segmentation, topic modeling</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-66" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-66">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Build graph</span></span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> nx.karate_club_graph()</span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> nx.normalized_laplacian_matrix(G).toarray()</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Eigen-decomposition</span></span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a>eigs, vecs <span class="op">=</span> np.linalg.eigh(L)</span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Use second eigenvector for 2-way partition</span></span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a>fiedler_vector <span class="op">=</span> vecs[:,<span class="dv">1</span>]</span>
<span id="cb77-14"><a href="#cb77-14" aria-hidden="true" tabindex="-1"></a>partition <span class="op">=</span> fiedler_vector <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb77-15"><a href="#cb77-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-16"><a href="#cb77-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Partition groups:"</span>, partition.astype(<span class="bu">int</span>))</span>
<span id="cb77-17"><a href="#cb77-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-18"><a href="#cb77-18" aria-hidden="true" tabindex="-1"></a><span class="co"># k-means spectral clustering (k=2)</span></span>
<span id="cb77-19"><a href="#cb77-19" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> vecs[:,<span class="dv">1</span>:<span class="dv">3</span>]</span>
<span id="cb77-20"><a href="#cb77-20" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">2</span>, n_init<span class="op">=</span><span class="dv">10</span>).fit_predict(features)</span>
<span id="cb77-21"><a href="#cb77-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Spectral clustering labels:"</span>, labels)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-74" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-74">Why It Matters</h4>
<p>Graph partitioning via eigenvalues is more robust than naive heuristics. It reveals hidden communities and patterns, enabling AI systems to learn structure in complex data. Without spectral methods, clustering high-dimensional relational data would often be intractable.</p>
</section>
<section id="try-it-yourself-76" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-76">Try It Yourself</h4>
<ol type="1">
<li>Compute λ₂ for a chain of 5 nodes and explain its meaning.</li>
<li>Use the Fiedler vector to partition a graph with two weakly connected clusters.</li>
<li>Apply spectral clustering to a pixel graph of an image—what structures emerge?</li>
</ol>
</section>
</section>
<section id="random-walks-and-markov-chains-on-graphs" class="level3">
<h3 class="anchored" data-anchor-id="random-walks-and-markov-chains-on-graphs">178. Random Walks and Markov Chains on Graphs</h3>
<p>A random walk is a process of moving through a graph by randomly choosing edges. When repeated indefinitely, it forms a Markov chain—a stochastic process where the next state depends only on the current one. Random walks connect graph structure with probability, enabling ranking, clustering, and learning.</p>
<section id="picture-in-your-head-77" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-77">Picture in Your Head</h4>
<p>Imagine a tourist wandering a city. At every intersection (node), they pick a random road (edge) to walk down. Over time, the frequency with which they visit each place reflects the structure of the city.</p>
</section>
<section id="deep-dive-77" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-77">Deep Dive</h4>
<ul>
<li><p>Random walk definition:</p>
<ul>
<li>From node <span class="math inline">\(i\)</span>, move to neighbor <span class="math inline">\(j\)</span> with probability <span class="math inline">\(1/\deg(i)\)</span> (uniform case).</li>
<li>Transition matrix: <span class="math inline">\(P = D^{-1}A\)</span>.</li>
</ul></li>
<li><p>Stationary distribution:</p>
<ul>
<li>Probability distribution <span class="math inline">\(\pi\)</span> where <span class="math inline">\(\pi = \pi P\)</span>.</li>
<li>In undirected graphs, <span class="math inline">\(\pi_i \propto \deg(i)\)</span>.</li>
</ul></li>
<li><p>Markov chains:</p>
<ul>
<li>Irreducible: all nodes reachable.</li>
<li>Aperiodic: no fixed cycle.</li>
<li>Converges to stationary distribution under these conditions.</li>
</ul></li>
<li><p>Applications in AI:</p>
<ul>
<li>PageRank (random surfer model).</li>
<li>Semi-supervised learning on graphs.</li>
<li>Node embeddings (DeepWalk, node2vec).</li>
<li>Sampling for large-scale graph analysis.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 38%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Definition/Formula</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Transition matrix (P)</td>
<td><span class="math inline">\(P=D^{-1}A\)</span></td>
<td>Defines step probabilities</td>
</tr>
<tr class="even">
<td>Stationary distribution</td>
<td><span class="math inline">\(\pi = \pi P\)</span></td>
<td>Long-run importance of nodes (PageRank)</td>
</tr>
<tr class="odd">
<td>Mixing time</td>
<td>Steps to reach near-stationarity</td>
<td>Efficiency of random-walk sampling</td>
</tr>
<tr class="even">
<td>Biased random walk</td>
<td>Probabilities adjusted by weights/bias</td>
<td>node2vec embeddings</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-67" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-67">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple graph</span></span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> nx.path_graph(<span class="dv">4</span>)</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> nx.to_numpy_array(G)</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> np.diag(A.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> np.linalg.inv(D) <span class="op">@</span> A</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Random walk simulation</span></span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>n_steps <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>state <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>trajectory <span class="op">=</span> [state]</span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_steps):</span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a>    state <span class="op">=</span> np.random.choice(<span class="bu">range</span>(<span class="bu">len</span>(G)), p<span class="op">=</span>P[state])</span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a>    trajectory.append(state)</span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-18"><a href="#cb78-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Transition matrix:</span><span class="ch">\n</span><span class="st">"</span>, P)</span>
<span id="cb78-19"><a href="#cb78-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Random walk trajectory:"</span>, trajectory)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-75" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-75">Why It Matters</h4>
<p>Random walks connect probabilistic reasoning with graph structure. They enable scalable algorithms for ranking, clustering, and representation learning, powering search engines, recommendation systems, and graph-based AI.</p>
</section>
<section id="try-it-yourself-77" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-77">Try It Yourself</h4>
<ol type="1">
<li>Simulate a random walk on a triangle graph. Does the stationary distribution match degree proportions?</li>
<li>Compute PageRank scores on a small directed graph using the random walk model.</li>
<li>Explain how biased random walks in node2vec capture both local and global graph structure.</li>
</ol>
</section>
</section>
<section id="spectral-clustering" class="level3">
<h3 class="anchored" data-anchor-id="spectral-clustering">179. Spectral Clustering</h3>
<p>Spectral clustering partitions a graph using the eigenvalues and eigenvectors of its Laplacian. Instead of clustering directly in the raw feature space, it embeds nodes into a low-dimensional spectral space where structure is easier to separate.</p>
<section id="picture-in-your-head-78" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-78">Picture in Your Head</h4>
<p>Think of shining light through a prism. The light splits into clear, separated colors. Similarly, spectral clustering transforms graph data into a space where groups become naturally separable.</p>
</section>
<section id="deep-dive-78" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-78">Deep Dive</h4>
<ul>
<li><p>Steps of spectral clustering:</p>
<ol type="1">
<li>Construct similarity graph and adjacency matrix <span class="math inline">\(A\)</span>.</li>
<li>Compute Laplacian <span class="math inline">\(L = D - A\)</span> (or normalized versions).</li>
<li>Find eigenvectors corresponding to the smallest nonzero eigenvalues.</li>
<li>Use these eigenvectors as features in k-means clustering.</li>
</ol></li>
<li><p>Why it works:</p>
<ul>
<li>Eigenvectors encode smooth variations across the graph.</li>
<li>Fiedler vector separates weakly connected groups.</li>
</ul></li>
<li><p>Normalized variants:</p>
<ul>
<li>Shi–Malik (normalized cut): uses random-walk Laplacian.</li>
<li>Ng–Jordan–Weiss: uses symmetric Laplacian.</li>
</ul></li>
<li><p>Applications in AI:</p>
<ul>
<li>Image segmentation (pixels as graph nodes).</li>
<li>Social/community detection.</li>
<li>Document clustering.</li>
<li>Semi-supervised learning.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 33%">
<col style="width: 42%">
</colgroup>
<thead>
<tr class="header">
<th>Variant</th>
<th>Laplacian Used</th>
<th>Typical Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Unnormalized spectral</td>
<td><span class="math inline">\(L = D - A\)</span></td>
<td>Small, balanced graphs</td>
</tr>
<tr class="even">
<td>Shi–Malik (Ncut)</td>
<td><span class="math inline">\(L_{rw} = D^{-1}L\)</span></td>
<td>Image segmentation, partitioning</td>
</tr>
<tr class="odd">
<td>Ng–Jordan–Weiss</td>
<td><span class="math inline">\(L_{sym} = D^{-1/2}LD^{-1/2}\)</span></td>
<td>General clustering with normalization</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-68" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-68">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Build simple graph</span></span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> nx.karate_club_graph()</span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> nx.normalized_laplacian_matrix(G).toarray()</span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Eigen-decomposition</span></span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a>eigs, vecs <span class="op">=</span> np.linalg.eigh(L)</span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Use k=2 smallest nonzero eigenvectors</span></span>
<span id="cb79-13"><a href="#cb79-13" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> vecs[:,<span class="dv">1</span>:<span class="dv">3</span>]</span>
<span id="cb79-14"><a href="#cb79-14" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">2</span>, n_init<span class="op">=</span><span class="dv">10</span>).fit_predict(X)</span>
<span id="cb79-15"><a href="#cb79-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-16"><a href="#cb79-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Spectral clustering labels:"</span>, labels[:<span class="dv">10</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-76" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-76">Why It Matters</h4>
<p>Spectral clustering harnesses graph structure hidden in data, outperforming traditional clustering in non-Euclidean or highly structured datasets. It is a cornerstone method linking graph theory with machine learning.</p>
</section>
<section id="try-it-yourself-78" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-78">Try It Yourself</h4>
<ol type="1">
<li>Perform spectral clustering on a graph with two loosely connected clusters. Does the Fiedler vector split them?</li>
<li>Compare spectral clustering with k-means directly on raw coordinates—what differences emerge?</li>
<li>Apply spectral clustering to an image (treating pixels as nodes). How do the clusters map to regions?</li>
</ol>
</section>
</section>
<section id="graph-based-ai-applications" class="level3">
<h3 class="anchored" data-anchor-id="graph-based-ai-applications">180. Graph-Based AI Applications</h3>
<p>Graphs naturally capture relationships, making them a central structure for AI. From social networks to molecules, many domains are best modeled as nodes and edges. Graph-based AI leverages algorithms and neural architectures to reason, predict, and learn from such structured data.</p>
<section id="picture-in-your-head-79" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-79">Picture in Your Head</h4>
<p>Imagine a detective’s board with people, places, and events connected by strings. Graph-based AI is like training an assistant who not only remembers all the connections but can also infer missing links and predict what might happen next.</p>
</section>
<section id="deep-dive-79" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-79">Deep Dive</h4>
<ul>
<li><p>Knowledge graphs: structured representations of entities and relations.</p>
<ul>
<li>Used in search engines, question answering, and recommender systems.</li>
</ul></li>
<li><p>Graph Neural Networks (GNNs): extend deep learning to graphs.</p>
<ul>
<li>Message-passing framework: nodes update embeddings based on neighbors.</li>
<li>Variants: GCN, GAT, GraphSAGE.</li>
</ul></li>
<li><p>Graph embeddings: map nodes/edges/subgraphs into continuous space.</p>
<ul>
<li>Enable link prediction, clustering, classification.</li>
</ul></li>
<li><p>Graph-based algorithms:</p>
<ul>
<li>PageRank: ranking nodes by importance.</li>
<li>Community detection: finding clusters of related nodes.</li>
<li>Random walks: for node embeddings and sampling.</li>
</ul></li>
<li><p>Applications across AI:</p>
<ul>
<li>NLP: semantic parsing, knowledge graphs.</li>
<li>Vision: scene graphs, object relationships.</li>
<li>Science: molecular property prediction, drug discovery.</li>
<li>Robotics: planning with state-space graphs.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 38%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th>Domain</th>
<th>Graph Representation</th>
<th>AI Application</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Social networks</td>
<td>Users as nodes, friendships as edges</td>
<td>Influence prediction, community detection</td>
</tr>
<tr class="even">
<td>Knowledge graphs</td>
<td>Entities + relations</td>
<td>Question answering, semantic search</td>
</tr>
<tr class="odd">
<td>Molecules</td>
<td>Atoms as nodes, bonds as edges</td>
<td>Drug discovery, materials science</td>
</tr>
<tr class="even">
<td>Scenes</td>
<td>Objects and their relationships</td>
<td>Visual question answering, scene reasoning</td>
</tr>
<tr class="odd">
<td>Planning</td>
<td>States as nodes, actions as edges</td>
<td>Robotics, reinforcement learning</td>
</tr>
</tbody>
</table>
<p>Tiny Code Sample (Python, Graph Neural Network with PyTorch Geometric)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch_geometric.data <span class="im">import</span> Data</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch_geometric.nn <span class="im">import</span> GCNConv</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple graph with 3 nodes and 2 edges</span></span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>edge_index <span class="op">=</span> torch.tensor([[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>],</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>                           [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>]], dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([[<span class="dv">1</span>], [<span class="dv">2</span>], [<span class="dv">3</span>]], dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> Data(x<span class="op">=</span>x, edge_index<span class="op">=</span>edge_index)</span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GCN(torch.nn.Module):</span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb80-15"><a href="#cb80-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> GCNConv(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb80-16"><a href="#cb80-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, data):</span>
<span id="cb80-17"><a href="#cb80-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.conv1(data.x, data.edge_index)</span>
<span id="cb80-18"><a href="#cb80-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-19"><a href="#cb80-19" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> GCN()</span>
<span id="cb80-20"><a href="#cb80-20" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> model(data)</span>
<span id="cb80-21"><a href="#cb80-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Node embeddings:</span><span class="ch">\n</span><span class="st">"</span>, out)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-77" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-77">Why It Matters</h4>
<p>Graphs bridge symbolic reasoning and statistical learning, making them a powerful tool for AI. They enable AI systems to capture structure, context, and relationships—crucial for understanding language, vision, and complex real-world systems.</p>
</section>
<section id="try-it-yourself-79" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-79">Try It Yourself</h4>
<ol type="1">
<li>Build a small knowledge graph of three entities and use it to answer simple queries.</li>
<li>Train a GNN on a citation graph dataset and compare with logistic regression on node features.</li>
<li>Explain why graphs are a more natural representation than tables for molecules or social networks.</li>
</ol>
</section>
</section>
</section>
<section id="chapter-19.-logic-sets-and-proof-techniques" class="level2">
<h2 class="anchored" data-anchor-id="chapter-19.-logic-sets-and-proof-techniques">Chapter 19. Logic, Sets and Proof Techniques</h2>
<section id="set-theory-fundamentals" class="level3">
<h3 class="anchored" data-anchor-id="set-theory-fundamentals">181. Set Theory Fundamentals</h3>
<p>Set theory provides the foundation for modern mathematics, describing collections of objects and the rules for manipulating them. In AI, sets underlie probability, logic, databases, and knowledge representation.</p>
<section id="picture-in-your-head-80" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-80">Picture in Your Head</h4>
<p>Think of a basket of fruit. The basket is the set, and the fruits are its elements. You can combine baskets (union), find fruits in both baskets (intersection), or look at fruits missing from one basket (difference).</p>
</section>
<section id="deep-dive-80" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-80">Deep Dive</h4>
<ul>
<li><p>Basic definitions:</p>
<ul>
<li>Set = collection of distinct elements.</li>
<li>Notation: <span class="math inline">\(A = \{a, b, c\}\)</span>.</li>
<li>Empty set: <span class="math inline">\(\varnothing\)</span>.</li>
</ul></li>
<li><p>Operations:</p>
<ul>
<li>Union: <span class="math inline">\(A \cup B\)</span>.</li>
<li>Intersection: <span class="math inline">\(A \cap B\)</span>.</li>
<li>Difference: <span class="math inline">\(A \setminus B\)</span>.</li>
<li>Complement: <span class="math inline">\(\overline{A}\)</span>.</li>
</ul></li>
<li><p>Special sets:</p>
<ul>
<li>Universal set <span class="math inline">\(U\)</span>.</li>
<li>Subsets: <span class="math inline">\(A \subseteq B\)</span>.</li>
<li>Power set: set of all subsets of <span class="math inline">\(A\)</span>.</li>
</ul></li>
<li><p>Properties:</p>
<ul>
<li>Commutativity, associativity, distributivity.</li>
<li>De Morgan’s laws: <span class="math inline">\(\overline{A \cup B} = \overline{A} \cap \overline{B}\)</span>.</li>
</ul></li>
<li><p>In AI: forming knowledge bases, defining probability events, representing state spaces.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 21%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th>Operation</th>
<th>Formula</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Union</td>
<td><span class="math inline">\(A \cup B\)</span></td>
<td>Merging candidate features from two sources</td>
</tr>
<tr class="even">
<td>Intersection</td>
<td><span class="math inline">\(A \cap B\)</span></td>
<td>Common tokens in NLP vocabulary</td>
</tr>
<tr class="odd">
<td>Difference</td>
<td><span class="math inline">\(A \setminus B\)</span></td>
<td>Features unique to one dataset</td>
</tr>
<tr class="even">
<td>Power set</td>
<td><span class="math inline">\(2^A\)</span></td>
<td>All possible feature subsets</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-69" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-69">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> {<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>}</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> {<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>}</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Union:"</span>, A <span class="op">|</span> B)</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Intersection:"</span>, A <span class="op">&amp;</span> B)</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Difference:"</span>, A <span class="op">-</span> B)</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Power set:"</span>, [{x <span class="cf">for</span> i,x <span class="kw">in</span> <span class="bu">enumerate</span>(A) <span class="cf">if</span> (mask<span class="op">&gt;&gt;</span>i)<span class="op">&amp;</span><span class="dv">1</span>} </span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a>                     <span class="cf">for</span> mask <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span><span class="op">&lt;&lt;</span><span class="bu">len</span>(A))])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-78" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-78">Why It Matters</h4>
<p>Set theory provides the language for probability, logic, and data representation in AI. From defining event spaces in machine learning to structuring knowledge graphs, sets offer a precise way to reason about collections.</p>
</section>
<section id="try-it-yourself-80" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-80">Try It Yourself</h4>
<ol type="1">
<li>Write down two sets of words (e.g., {cat, dog, fish}, {dog, bird}). Compute their union and intersection.</li>
<li>List the power set of {a, b}.</li>
<li>Use De Morgan’s law to simplify <span class="math inline">\(\overline{(A \cup B)}\)</span> when <span class="math inline">\(A={1,2}\)</span>, <span class="math inline">\(B={2,3}\)</span>, <span class="math inline">\(U={1,2,3,4}\)</span>.</li>
</ol>
</section>
</section>
<section id="relations-and-functions" class="level3">
<h3 class="anchored" data-anchor-id="relations-and-functions">182. Relations and Functions</h3>
<p>Relations describe connections between elements of sets, while functions are special relations that assign exactly one output to each input. These ideas underpin mappings, transformations, and dependencies across mathematics and AI.</p>
<section id="picture-in-your-head-81" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-81">Picture in Your Head</h4>
<p>Imagine a school roster. A relation could pair each student with every course they take. A function is stricter: each student gets exactly one unique ID number.</p>
</section>
<section id="deep-dive-81" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-81">Deep Dive</h4>
<ul>
<li><p>Relations:</p>
<ul>
<li>A relation <span class="math inline">\(R\)</span> between sets <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is a subset of <span class="math inline">\(A \times B\)</span>.</li>
<li>Examples: “is a friend of,” “is greater than.”</li>
<li>Properties: reflexive, symmetric, transitive, antisymmetric.</li>
</ul></li>
<li><p>Equivalence relations: reflexive, symmetric, transitive → partition set into equivalence classes.</p></li>
<li><p>Partial orders: reflexive, antisymmetric, transitive → define hierarchies.</p></li>
<li><p>Functions:</p>
<ul>
<li>Special relation: <span class="math inline">\(f: A \to B\)</span>.</li>
<li>Each <span class="math inline">\(a \in A\)</span> has exactly one <span class="math inline">\(b \in B\)</span>.</li>
<li>Surjective (onto), injective (one-to-one), bijective (both).</li>
</ul></li>
<li><p>In AI:</p>
<ul>
<li>Relations: knowledge graphs (entities + relations).</li>
<li>Functions: mappings from input features to predictions.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 35%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Definition</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Relation</td>
<td>Subset of <span class="math inline">\(A \times B\)</span></td>
<td>User–item rating pairs in recommender systems</td>
</tr>
<tr class="even">
<td>Equivalence relation</td>
<td>Reflexive, symmetric, transitive</td>
<td>Grouping synonyms in NLP</td>
</tr>
<tr class="odd">
<td>Partial order</td>
<td>Reflexive, antisymmetric, transitive</td>
<td>Task dependency graph in scheduling</td>
</tr>
<tr class="even">
<td>Function</td>
<td>Maps input to single output</td>
<td>Neural network mapping x → y</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-70" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-70">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Relation: list of pairs</span></span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>students <span class="op">=</span> {<span class="st">"Alice"</span>, <span class="st">"Bob"</span>}</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>courses <span class="op">=</span> {<span class="st">"Math"</span>, <span class="st">"CS"</span>}</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a>relation <span class="op">=</span> {(<span class="st">"Alice"</span>, <span class="st">"Math"</span>), (<span class="st">"Bob"</span>, <span class="st">"CS"</span>), (<span class="st">"Alice"</span>, <span class="st">"CS"</span>)}</span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Function: mapping</span></span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> {<span class="st">"Alice"</span>: <span class="st">"ID001"</span>, <span class="st">"Bob"</span>: <span class="st">"ID002"</span>}</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Relation:"</span>, relation)</span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Function mapping:"</span>, f)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-79" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-79">Why It Matters</h4>
<p>Relations give AI systems the ability to represent structured connections like “works at” or “is similar to.” Functions guarantee consistent mappings, essential in deterministic prediction tasks. This distinction underlies both symbolic and statistical approaches to AI.</p>
</section>
<section id="try-it-yourself-81" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-81">Try It Yourself</h4>
<ol type="1">
<li>Give an example of a relation that is symmetric but not transitive.</li>
<li>Define a function <span class="math inline">\(f: \{1,2,3\} \to \{a,b\}\)</span>. Is it surjective? Injective?</li>
<li>Explain why equivalence relations are useful for clustering in AI.</li>
</ol>
</section>
</section>
<section id="propositional-logic" class="level3">
<h3 class="anchored" data-anchor-id="propositional-logic">183. Propositional Logic</h3>
<p>Propositional logic formalizes reasoning with statements that can be true or false. It uses logical operators to build complex expressions and determine truth systematically.</p>
<section id="picture-in-your-head-82" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-82">Picture in Your Head</h4>
<p>Imagine a set of switches that can be either ON (true) or OFF (false). Combining them with rules like “AND,” “OR,” and “NOT” lets you create more complex circuits. Propositional logic works like that: simple truths combine into structured reasoning.</p>
</section>
<section id="deep-dive-82" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-82">Deep Dive</h4>
<ul>
<li><p>Propositions: declarative statements with truth values (e.g., “It is raining”).</p></li>
<li><p>Logical connectives:</p>
<ul>
<li>NOT (¬p): true if p is false.</li>
<li>AND (p ∧ q): true if both are true.</li>
<li>OR (p ∨ q): true if at least one is true.</li>
<li>IMPLIES (p → q): false only if p is true and q is false.</li>
<li>IFF (p ↔︎ q): true if p and q have same truth value.</li>
</ul></li>
<li><p>Truth tables: define behavior of operators.</p></li>
<li><p>Normal forms:</p>
<ul>
<li>CNF (conjunctive normal form): AND of ORs.</li>
<li>DNF (disjunctive normal form): OR of ANDs.</li>
</ul></li>
<li><p>Inference: rules like modus ponens (p → q, p ⇒ q).</p></li>
<li><p>In AI: SAT solvers, planning, rule-based expert systems.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 9%">
<col style="width: 27%">
<col style="width: 41%">
</colgroup>
<thead>
<tr class="header">
<th>Operator</th>
<th>Symbol</th>
<th>Meaning</th>
<th>Example (p=Rain, q=Cloudy)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Negation</td>
<td>¬p</td>
<td>Opposite truth</td>
<td>¬p = “Not raining”</td>
</tr>
<tr class="even">
<td>Conjunction</td>
<td>p ∧ q</td>
<td>Both true</td>
<td>“Raining AND Cloudy”</td>
</tr>
<tr class="odd">
<td>Disjunction</td>
<td>p ∨ q</td>
<td>At least one true</td>
<td>“Raining OR Cloudy”</td>
</tr>
<tr class="even">
<td>Implication</td>
<td>p → q</td>
<td>If p then q</td>
<td>“If raining then cloudy”</td>
</tr>
<tr class="odd">
<td>Biconditional</td>
<td>p ↔︎ q</td>
<td>Both same truth</td>
<td>“Raining iff cloudy”</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-71" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-71">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Truth table for implication</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> itertools</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> implies(p, q):</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (<span class="kw">not</span> p) <span class="kw">or</span> q</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"p q | p→q"</span>)</span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p, q <span class="kw">in</span> itertools.product([<span class="va">False</span>, <span class="va">True</span>], repeat<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(p, q, <span class="st">"|"</span>, implies(p,q))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-80" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-80">Why It Matters</h4>
<p>Propositional logic is the simplest formal system of reasoning and the foundation for more expressive logics. In AI, it powers SAT solvers, which in turn drive verification, planning, and optimization engines at scale.</p>
</section>
<section id="try-it-yourself-82" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-82">Try It Yourself</h4>
<ol type="1">
<li>Build a truth table for (p ∧ q) → r.</li>
<li>Convert (¬p ∨ q) into CNF and DNF.</li>
<li>Explain how propositional logic could represent constraints in a scheduling problem.</li>
</ol>
</section>
</section>
<section id="predicate-logic-and-quantifiers" class="level3">
<h3 class="anchored" data-anchor-id="predicate-logic-and-quantifiers">184. Predicate Logic and Quantifiers</h3>
<p>Predicate logic (first-order logic) extends propositional logic by allowing statements about objects and their properties, using quantifiers to express generality. It can capture more complex relationships and forms the backbone of formal reasoning in AI.</p>
<section id="picture-in-your-head-83" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-83">Picture in Your Head</h4>
<p>Think of propositional logic as reasoning with whole sentences: “It is raining.” Predicate logic opens them up: “For every city, if it is cloudy, then it rains.” Quantifiers let us say “for all” or “there exists,” making reasoning far richer.</p>
</section>
<section id="deep-dive-83" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-83">Deep Dive</h4>
<ul>
<li><p>Predicates: functions that return true/false depending on input.</p>
<ul>
<li>Example: Likes(Alice, IceCream).</li>
</ul></li>
<li><p>Quantifiers:</p>
<ul>
<li>Universal (∀x P(x)): P(x) holds for all x.</li>
<li>Existential (∃x P(x)): P(x) holds for at least one x.</li>
</ul></li>
<li><p>Syntax examples:</p>
<ul>
<li>∀x (Human(x) → Mortal(x))</li>
<li>∃y (Student(y) ∧ Studies(y, AI))</li>
</ul></li>
<li><p>Semantics: defined over domains of discourse.</p></li>
<li><p>Inference rules:</p>
<ul>
<li>Universal instantiation: from ∀x P(x), infer P(a).</li>
<li>Existential generalization: from P(a), infer ∃x P(x).</li>
</ul></li>
<li><p>In AI: knowledge representation, natural language understanding, automated reasoning.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 7%">
<col style="width: 40%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th>Element</th>
<th>Symbol</th>
<th>Meaning</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Predicate</td>
<td>P(x)</td>
<td>Property or relation of object x</td>
<td>Human(Socrates)</td>
</tr>
<tr class="even">
<td>Universal quant.</td>
<td>∀x</td>
<td>For all x</td>
<td>∀x Human(x) → Mortal(x)</td>
</tr>
<tr class="odd">
<td>Existential quant.</td>
<td>∃x</td>
<td>There exists x</td>
<td>∃x Loves(x, IceCream)</td>
</tr>
<tr class="even">
<td>Nested quantifiers</td>
<td>∀x∃y</td>
<td>For each x, there is a y</td>
<td>∀x ∃y Parent(y,x)</td>
</tr>
</tbody>
</table>
<p>Tiny Code Sample (Python, simple predicate logic)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Domain of people and properties</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>people <span class="op">=</span> [<span class="st">"Alice"</span>, <span class="st">"Bob"</span>, <span class="st">"Charlie"</span>]</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>likes_icecream <span class="op">=</span> {<span class="st">"Alice"</span>, <span class="st">"Charlie"</span>}</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicate</span></span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> LikesIcecream(x):</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x <span class="kw">in</span> likes_icecream</span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Universal quantifier</span></span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a>all_like <span class="op">=</span> <span class="bu">all</span>(LikesIcecream(p) <span class="cf">for</span> p <span class="kw">in</span> people)</span>
<span id="cb84-11"><a href="#cb84-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-12"><a href="#cb84-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Existential quantifier</span></span>
<span id="cb84-13"><a href="#cb84-13" aria-hidden="true" tabindex="-1"></a>exists_like <span class="op">=</span> <span class="bu">any</span>(LikesIcecream(p) <span class="cf">for</span> p <span class="kw">in</span> people)</span>
<span id="cb84-14"><a href="#cb84-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-15"><a href="#cb84-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"∀x LikesIcecream(x):"</span>, all_like)</span>
<span id="cb84-16"><a href="#cb84-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"∃x LikesIcecream(x):"</span>, exists_like)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-81" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-81">Why It Matters</h4>
<p>Predicate logic allows AI systems to represent structured knowledge and reason with it. Unlike propositional logic, it scales to domains with many objects and relationships, making it essential for semantic parsing, theorem proving, and symbolic AI.</p>
</section>
<section id="try-it-yourself-83" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-83">Try It Yourself</h4>
<ol type="1">
<li>Express “All cats are mammals, some mammals are pets” in predicate logic.</li>
<li>Translate “Every student studies some course” into formal notation.</li>
<li>Explain why predicate logic is more powerful than propositional logic for knowledge graphs.</li>
</ol>
</section>
</section>
<section id="logical-inference-and-deduction" class="level3">
<h3 class="anchored" data-anchor-id="logical-inference-and-deduction">185. Logical Inference and Deduction</h3>
<p>Logical inference is the process of deriving new truths from known ones using formal rules of deduction. Deduction ensures that if the premises are true, the conclusion must also be true, providing a foundation for automated reasoning in AI.</p>
<section id="picture-in-your-head-84" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-84">Picture in Your Head</h4>
<p>Think of a chain of dominoes. Each piece represents a logical statement. If the first falls (premise is true), the rules ensure that the next falls, and eventually the conclusion is reached without contradiction.</p>
</section>
<section id="deep-dive-84" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-84">Deep Dive</h4>
<ul>
<li><p>Inference rules:</p>
<ul>
<li>Modus Ponens: from <span class="math inline">\(p → q\)</span> and <span class="math inline">\(p\)</span>, infer <span class="math inline">\(q\)</span>.</li>
<li>Modus Tollens: from <span class="math inline">\(p → q\)</span> and ¬q, infer ¬p.</li>
<li>Hypothetical Syllogism: from <span class="math inline">\(p → q\)</span>, <span class="math inline">\(q → r\)</span>, infer <span class="math inline">\(p → r\)</span>.</li>
<li>Universal Instantiation: from ∀x P(x), infer P(a).</li>
</ul></li>
<li><p>Deduction systems:</p>
<ul>
<li>Natural deduction (step-by-step reasoning).</li>
<li>Resolution (refutation-based).</li>
<li>Sequent calculus.</li>
</ul></li>
<li><p>Soundness: if a conclusion can be derived, it must be true in all models.</p></li>
<li><p>Completeness: all truths in the system can, in principle, be derived.</p></li>
<li><p>In AI: SAT solvers, expert systems, theorem proving, program verification.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 25%">
<col style="width: 51%">
</colgroup>
<thead>
<tr class="header">
<th>Rule</th>
<th>Formulation</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Modus Ponens</td>
<td><span class="math inline">\(p, p → q ⟹ q\)</span></td>
<td>If it rains, the ground gets wet. It rains ⇒ wet</td>
</tr>
<tr class="even">
<td>Modus Tollens</td>
<td><span class="math inline">\(p → q, ¬q ⟹ ¬p\)</span></td>
<td>If rain ⇒ wet. Ground not wet ⇒ no rain</td>
</tr>
<tr class="odd">
<td>Hypothetical Syllogism</td>
<td><span class="math inline">\(p → q, q → r ⟹ p → r\)</span></td>
<td>If A is human ⇒ mortal, mortal ⇒ dies ⇒ A dies</td>
</tr>
<tr class="even">
<td>Resolution</td>
<td>Eliminate contradictions</td>
<td>Used in SAT solving</td>
</tr>
</tbody>
</table>
<p>Tiny Code Sample (Python: Modus Ponens)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> modus_ponens(p, implication):</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># implication in form (p, q)</span></span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>    antecedent, consequent <span class="op">=</span> implication</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> p <span class="op">==</span> antecedent:</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> consequent</span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"From (p → q) and p, infer q:"</span>)</span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(modus_ponens(<span class="st">"It rains"</span>, (<span class="st">"It rains"</span>, <span class="st">"Ground is wet"</span>)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-82" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-82">Why It Matters</h4>
<p>Inference and deduction provide the reasoning backbone for symbolic AI. They allow systems not just to store knowledge but to derive consequences, verify consistency, and explain their reasoning steps—critical for trustworthy AI.</p>
</section>
<section id="try-it-yourself-84" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-84">Try It Yourself</h4>
<ol type="1">
<li>Use Modus Ponens to infer: “If AI learns, it improves. AI learns.”</li>
<li>Show why resolution is powerful for proving contradictions in propositional logic.</li>
<li>Explain how completeness guarantees that no valid inference is left unreachable.</li>
</ol>
</section>
</section>
<section id="proof-techniques-direct-contradiction-induction" class="level3">
<h3 class="anchored" data-anchor-id="proof-techniques-direct-contradiction-induction">186. Proof Techniques: Direct, Contradiction, Induction</h3>
<p>Proof techniques provide structured methods for demonstrating that statements are true. Direct proofs build step-by-step arguments, proof by contradiction shows that denying the claim leads to impossibility, and induction proves statements for all natural numbers by building on simpler cases.</p>
<section id="picture-in-your-head-85" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-85">Picture in Your Head</h4>
<p>Imagine climbing a staircase. Direct proof is like walking up the steps in order. Proof by contradiction is like assuming the staircase ends suddenly and discovering that would make the entire building collapse. Induction is like proving you can step onto the first stair, and if you can move from one stair to the next, you can reach any stair.</p>
</section>
<section id="deep-dive-85" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-85">Deep Dive</h4>
<ul>
<li><p>Direct proof:</p>
<ul>
<li>Assume premises and apply logical rules until the conclusion is reached.</li>
<li>Example: prove that the sum of two even numbers is even.</li>
</ul></li>
<li><p>Proof by contradiction:</p>
<ul>
<li>Assume the negation of the statement.</li>
<li>Show this assumption leads to inconsistency.</li>
<li>Example: proof that √2 is irrational.</li>
</ul></li>
<li><p>Proof by induction:</p>
<ul>
<li>Base case: show statement holds for n=1.</li>
<li>Inductive step: assume it holds for n=k, prove it for n=k+1.</li>
<li>Example: sum of first n integers = n(n+1)/2.</li>
</ul></li>
<li><p>Applications in AI: formal verification of algorithms, correctness proofs, mathematical foundations of learning theory.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 35%">
<col style="width: 51%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Approach</th>
<th>Example in AI/Math</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Direct proof</td>
<td>Build argument step by step</td>
<td>Prove gradient descent converges under assumptions</td>
</tr>
<tr class="even">
<td>Contradiction</td>
<td>Assume false, derive impossibility</td>
<td>Show no smaller counterexample exists</td>
</tr>
<tr class="odd">
<td>Induction</td>
<td>Base case + inductive step</td>
<td>Proof of recursive algorithm correctness</td>
</tr>
</tbody>
</table>
<p>Tiny Code Sample (Python: Induction Idea)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify induction hypothesis for sum of integers</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> formula(n):</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> n<span class="op">*</span>(n<span class="op">+</span><span class="dv">1</span>)<span class="op">//</span><span class="dv">2</span></span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Check base case and a few steps</span></span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">6</span>):</span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"n=</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">, sum=</span><span class="sc">{</span><span class="bu">sum</span>(<span class="bu">range</span>(<span class="dv">1</span>,n<span class="op">+</span><span class="dv">1</span>))<span class="sc">}</span><span class="ss">, formula=</span><span class="sc">{</span>formula(n)<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-83" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-83">Why It Matters</h4>
<p>Proof techniques give rigor to reasoning in AI and computer science. They ensure algorithms behave as expected, prevent hidden contradictions, and provide guarantees—especially important in safety-critical AI systems.</p>
</section>
<section id="try-it-yourself-85" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-85">Try It Yourself</h4>
<ol type="1">
<li>Write a direct proof that the product of two odd numbers is odd.</li>
<li>Use contradiction to prove there is no largest prime number.</li>
<li>Apply induction to show that a binary tree with n nodes has exactly n−1 edges.</li>
</ol>
</section>
</section>
<section id="mathematical-induction-in-depth" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-induction-in-depth">187. Mathematical Induction in Depth</h3>
<p>Mathematical induction is a proof technique tailored to statements about integers or recursively defined structures. It shows that if a property holds for a base case and persists from <span class="math inline">\(n\)</span> to <span class="math inline">\(n+1\)</span>, then it holds universally. Strong induction and structural induction extend the idea further.</p>
<section id="picture-in-your-head-86" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-86">Picture in Your Head</h4>
<p>Think of a row of dominoes. Knocking down the first (base case) and proving each one pushes the next (inductive step) ensures the whole line falls. Induction guarantees the truth of infinitely many cases with just two steps.</p>
</section>
<section id="deep-dive-86" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-86">Deep Dive</h4>
<ul>
<li><p>Ordinary induction:</p>
<ol type="1">
<li>Base case: prove statement for <span class="math inline">\(n=1\)</span>.</li>
<li>Inductive hypothesis: assume statement holds for <span class="math inline">\(n=k\)</span>.</li>
<li>Inductive step: prove statement for <span class="math inline">\(n=k+1\)</span>.</li>
</ol></li>
<li><p>Strong induction:</p>
<ul>
<li>Assume statement holds for all cases up to <span class="math inline">\(k\)</span>, then prove for <span class="math inline">\(k+1\)</span>.</li>
<li>Useful when the <span class="math inline">\(k+1\)</span> case depends on multiple earlier cases.</li>
</ul></li>
<li><p>Structural induction:</p>
<ul>
<li>Extends induction to trees, graphs, or recursively defined data.</li>
<li>Base case: prove for simplest structure.</li>
<li>Inductive step: assume for substructures, prove for larger ones.</li>
</ul></li>
<li><p>Applications in AI:</p>
<ul>
<li>Proving algorithm correctness (e.g., recursive sorting).</li>
<li>Verifying properties of data structures.</li>
<li>Formal reasoning about grammars and logical systems.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 18%">
<col style="width: 21%">
<col style="width: 41%">
</colgroup>
<thead>
<tr class="header">
<th>Type of Induction</th>
<th>Base Case</th>
<th>Inductive Step</th>
<th>Example in AI/CS</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Ordinary induction</td>
<td><span class="math inline">\(n=1\)</span></td>
<td>From <span class="math inline">\(n=k\)</span> ⇒ <span class="math inline">\(n=k+1\)</span></td>
<td>Proof of arithmetic formulas</td>
</tr>
<tr class="even">
<td>Strong induction</td>
<td><span class="math inline">\(n=1\)</span></td>
<td>From all ≤k ⇒ <span class="math inline">\(n=k+1\)</span></td>
<td>Proving correctness of divide-and-conquer</td>
</tr>
<tr class="odd">
<td>Structural induction</td>
<td>Smallest structure</td>
<td>From parts ⇒ whole</td>
<td>Proof of correctness for syntax trees</td>
</tr>
</tbody>
</table>
<p>Tiny Code Sample (Python, checking induction idea)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify sum of first n squares formula by brute force</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sum_squares(n): <span class="cf">return</span> <span class="bu">sum</span>(i<span class="op">*</span>i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,n<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> formula(n): <span class="cf">return</span> n<span class="op">*</span>(n<span class="op">+</span><span class="dv">1</span>)<span class="op">*</span>(<span class="dv">2</span><span class="op">*</span>n<span class="op">+</span><span class="dv">1</span>)<span class="op">//</span><span class="dv">6</span></span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">6</span>):</span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"n=</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">, sum=</span><span class="sc">{</span>sum_squares(n)<span class="sc">}</span><span class="ss">, formula=</span><span class="sc">{</span>formula(n)<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-84" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-84">Why It Matters</h4>
<p>Induction provides a rigorous way to prove correctness of AI algorithms and recursive models. It ensures trust in results across infinite cases, making it essential in theory, programming, and verification.</p>
</section>
<section id="try-it-yourself-86" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-86">Try It Yourself</h4>
<ol type="1">
<li>Prove by induction that <span class="math inline">\(1+2+...+n = n(n+1)/2\)</span>.</li>
<li>Use strong induction to prove that every integer ≥2 is a product of primes.</li>
<li>Apply structural induction to show that a binary tree with n nodes has n−1 edges.</li>
</ol>
</section>
</section>
<section id="recursion-and-well-foundedness" class="level3">
<h3 class="anchored" data-anchor-id="recursion-and-well-foundedness">188. Recursion and Well-Foundedness</h3>
<p>Recursion defines objects or processes in terms of themselves, with a base case anchoring the definition. Well-foundedness ensures recursion doesn’t loop forever: every recursive call must move closer to a base case. Together, they guarantee termination and correctness.</p>
<section id="picture-in-your-head-87" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-87">Picture in Your Head</h4>
<p>Imagine Russian nesting dolls. Each doll contains a smaller one, until you reach the smallest. Recursion works the same way—problems are broken into smaller pieces until the simplest case is reached.</p>
</section>
<section id="deep-dive-87" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-87">Deep Dive</h4>
<ul>
<li><p>Recursive definitions:</p>
<ul>
<li>Factorial: <span class="math inline">\(n! = n \times (n-1)!\)</span>, with <span class="math inline">\(0! = 1\)</span>.</li>
<li>Fibonacci: <span class="math inline">\(F(n) = F(n-1) + F(n-2)\)</span>, with <span class="math inline">\(F(0)=0, F(1)=1\)</span>.</li>
</ul></li>
<li><p>Well-foundedness:</p>
<ul>
<li>Requires a measure (like size of n) that decreases at every step.</li>
<li>Prevents infinite descent.</li>
</ul></li>
<li><p>Structural recursion:</p>
<ul>
<li>Defined on data structures like lists or trees.</li>
<li>Example: sum of list = head + sum(tail).</li>
</ul></li>
<li><p>Applications in AI:</p>
<ul>
<li>Recursive search (DFS, minimax in games).</li>
<li>Recursive neural networks for structured data.</li>
<li>Inductive definitions in knowledge representation.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 38%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Definition</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Base case</td>
<td>Anchor for recursion</td>
<td><span class="math inline">\(F(0)=0\)</span>, <span class="math inline">\(F(1)=1\)</span> in Fibonacci</td>
</tr>
<tr class="even">
<td>Recursive case</td>
<td>Define larger in terms of smaller</td>
<td>DFS visits neighbors recursively</td>
</tr>
<tr class="odd">
<td>Well-foundedness</td>
<td>Guarantees termination</td>
<td>Depth decreases in search</td>
</tr>
<tr class="even">
<td>Structural recursion</td>
<td>Recursion on data structures</td>
<td>Parsing trees in NLP</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-72" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-72">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> factorial(n):</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> n <span class="op">==</span> <span class="dv">0</span>:   <span class="co"># base case</span></span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span></span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> n <span class="op">*</span> factorial(n<span class="op">-</span><span class="dv">1</span>)  <span class="co"># recursive case</span></span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Factorial 5:"</span>, factorial(<span class="dv">5</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-85" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-85">Why It Matters</h4>
<p>Recursion is fundamental to algorithms, data structures, and AI reasoning. Ensuring well-foundedness avoids infinite loops and guarantees correctness—critical for search algorithms, symbolic reasoning, and recursive neural models.</p>
</section>
<section id="try-it-yourself-87" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-87">Try It Yourself</h4>
<ol type="1">
<li>Write a recursive function to compute the nth Fibonacci number. Prove it terminates.</li>
<li>Define a recursive function to count nodes in a binary tree.</li>
<li>Explain how minimax recursion in game AI relies on well-foundedness.</li>
</ol>
</section>
</section>
<section id="formal-systems-and-completeness" class="level3">
<h3 class="anchored" data-anchor-id="formal-systems-and-completeness">189. Formal Systems and Completeness</h3>
<p>A formal system is a framework consisting of symbols, rules for forming expressions, and rules for deriving theorems. Completeness describes whether the system can express and prove all truths within its intended scope. Together, they define the boundaries of formal reasoning in mathematics and AI.</p>
<section id="picture-in-your-head-88" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-88">Picture in Your Head</h4>
<p>Imagine a game with pieces (symbols), rules for valid moves (syntax), and strategies to reach checkmate (proofs). A formal system is like such a game—but instead of chess, it encodes mathematics or logic. Completeness asks: “Can every winning position be reached using the rules?”</p>
</section>
<section id="deep-dive-88" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-88">Deep Dive</h4>
<ul>
<li><p>Components of a formal system:</p>
<ul>
<li>Alphabet: finite set of symbols.</li>
<li>Grammar: rules to build well-formed formulas.</li>
<li>Axioms: starting truths.</li>
<li>Inference rules: how to derive theorems.</li>
</ul></li>
<li><p>Soundness: everything derivable is true.</p></li>
<li><p>Completeness: everything true is derivable.</p></li>
<li><p>Gödel’s completeness theorem (first-order logic): every logically valid formula can be proven.</p></li>
<li><p>Gödel’s incompleteness theorem: in arithmetic, no consistent formal system can be both complete and decidable.</p></li>
<li><p>In AI:</p>
<ul>
<li>Used in theorem provers, logic programming (Prolog).</li>
<li>Defines limits of symbolic reasoning.</li>
<li>Influences design of verification tools and knowledge representation.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 43%">
<col style="width: 41%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Definition</th>
<th>Example in AI/Logic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Formal system</td>
<td>Symbols + rules for expressions + inference</td>
<td>Propositional calculus, first-order logic</td>
</tr>
<tr class="even">
<td>Soundness</td>
<td>Derivations ⊆ truths</td>
<td>No false theorem provable</td>
</tr>
<tr class="odd">
<td>Completeness</td>
<td>Truths ⊆ derivations</td>
<td>All valid statements can be proved</td>
</tr>
<tr class="even">
<td>Incompleteness</td>
<td>Some truths unprovable in system</td>
<td>Gödel’s theorem for arithmetic</td>
</tr>
</tbody>
</table>
<p>Tiny Code Sample (Prolog Example)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb89"><pre class="sourceCode prolog code-with-copy"><code class="sourceCode prolog"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co">% Simple formal system in Prolog</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>parent(alice<span class="kw">,</span> bob)<span class="kw">.</span></span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>parent(bob<span class="kw">,</span> carol)<span class="kw">.</span></span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>ancestor(<span class="dt">X</span><span class="kw">,</span><span class="dt">Y</span>) <span class="kw">:-</span> parent(<span class="dt">X</span><span class="kw">,</span><span class="dt">Y</span>)<span class="kw">.</span></span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>ancestor(<span class="dt">X</span><span class="kw">,</span><span class="dt">Y</span>) <span class="kw">:-</span> parent(<span class="dt">X</span><span class="kw">,</span><span class="dt">Z</span>)<span class="kw">,</span> ancestor(<span class="dt">Z</span><span class="kw">,</span><span class="dt">Y</span>)<span class="kw">.</span></span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a><span class="co">% Query: ?- ancestor(alice, carol).</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-86" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-86">Why It Matters</h4>
<p>Formal systems and completeness define the power and limits of logic-based AI. They ensure reasoning is rigorous but also highlight boundaries—no single system can capture all mathematical truths. This awareness shapes how AI blends symbolic and statistical approaches.</p>
</section>
<section id="try-it-yourself-88" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-88">Try It Yourself</h4>
<ol type="1">
<li>Define axioms and inference rules for propositional logic as a formal system.</li>
<li>Explain the difference between soundness and completeness using an example.</li>
<li>Reflect on why Gödel’s incompleteness is important for AI safety and reasoning.</li>
</ol>
</section>
</section>
<section id="logic-in-ai-reasoning-systems" class="level3">
<h3 class="anchored" data-anchor-id="logic-in-ai-reasoning-systems">190. Logic in AI Reasoning Systems</h3>
<p>Logic provides a structured way for AI systems to represent knowledge and reason with it. From rule-based systems to modern neuro-symbolic AI, logical reasoning enables deduction, consistency checking, and explanation.</p>
<section id="picture-in-your-head-89" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-89">Picture in Your Head</h4>
<p>Think of an AI as a detective. It gathers facts (“Alice is Bob’s parent”), applies rules (“All parents are ancestors”), and deduces new conclusions (“Alice is Carol’s ancestor”). Logic gives the detective both the notebook (representation) and the reasoning rules (inference).</p>
</section>
<section id="deep-dive-89" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-89">Deep Dive</h4>
<ul>
<li><p>Rule-based reasoning:</p>
<ul>
<li>Expert systems represent knowledge as IF–THEN rules.</li>
<li>Inference engines apply forward or backward chaining.</li>
</ul></li>
<li><p>Knowledge representation:</p>
<ul>
<li>Ontologies and semantic networks structure logical relationships.</li>
<li>Description logics form the basis of the Semantic Web.</li>
</ul></li>
<li><p>Uncertainty in logic:</p>
<ul>
<li>Probabilistic logics combine probability with deductive reasoning.</li>
<li>Useful for noisy, real-world AI.</li>
</ul></li>
<li><p>Neuro-symbolic integration:</p>
<ul>
<li>Combines neural networks with logical reasoning.</li>
<li>Example: neural models extract facts, logic enforces consistency.</li>
</ul></li>
<li><p>Applications:</p>
<ul>
<li>Automated planning and scheduling.</li>
<li>Natural language understanding.</li>
<li>Verification of AI models.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 37%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th>Approach</th>
<th>Mechanism</th>
<th>Example in AI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Rule-based expert systems</td>
<td>Forward/backward chaining</td>
<td>Medical diagnosis (MYCIN)</td>
</tr>
<tr class="even">
<td>Description logics</td>
<td>Formal semantics for ontologies</td>
<td>Semantic Web, knowledge graphs</td>
</tr>
<tr class="odd">
<td>Probabilistic logics</td>
<td>Add uncertainty to logical frameworks</td>
<td>AI for robotics in uncertain environments</td>
</tr>
<tr class="even">
<td>Neuro-symbolic AI</td>
<td>Neural + symbolic reasoning integration</td>
<td>Knowledge-grounded NLP</td>
</tr>
</tbody>
</table>
<p>Tiny Code Sample (Prolog)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb90"><pre class="sourceCode prolog code-with-copy"><code class="sourceCode prolog"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="co">% Facts</span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>parent(alice<span class="kw">,</span> bob)<span class="kw">.</span></span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>parent(bob<span class="kw">,</span> carol)<span class="kw">.</span></span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a><span class="co">% Rule</span></span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a>ancestor(<span class="dt">X</span><span class="kw">,</span><span class="dt">Y</span>) <span class="kw">:-</span> parent(<span class="dt">X</span><span class="kw">,</span><span class="dt">Y</span>)<span class="kw">.</span></span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>ancestor(<span class="dt">X</span><span class="kw">,</span><span class="dt">Y</span>) <span class="kw">:-</span> parent(<span class="dt">X</span><span class="kw">,</span><span class="dt">Z</span>)<span class="kw">,</span> ancestor(<span class="dt">Z</span><span class="kw">,</span><span class="dt">Y</span>)<span class="kw">.</span></span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-9"><a href="#cb90-9" aria-hidden="true" tabindex="-1"></a><span class="co">% Query: ?- ancestor(alice, carol).</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-87" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-87">Why It Matters</h4>
<p>Logic brings transparency, interpretability, and rigor to AI. While deep learning excels at pattern recognition, logic ensures decisions are consistent and explainable—critical for safety, fairness, and accountability.</p>
</section>
<section id="try-it-yourself-89" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-89">Try It Yourself</h4>
<ol type="1">
<li>Write three facts about family relationships and a rule to infer grandparents.</li>
<li>Show how forward chaining can derive new knowledge from initial facts.</li>
<li>Explain how logic could complement deep learning in natural language question answering.</li>
</ol>
</section>
</section>
</section>
<section id="chapter-20.-stochastic-process-and-markov-chains" class="level2">
<h2 class="anchored" data-anchor-id="chapter-20.-stochastic-process-and-markov-chains">Chapter 20. Stochastic Process and Markov chains</h2>
<section id="random-processes-and-sequences" class="level3">
<h3 class="anchored" data-anchor-id="random-processes-and-sequences">191. Random Processes and Sequences</h3>
<p>A random process is a collection of random variables indexed by time or space, describing how uncertainty evolves. Sequences like coin tosses, signals, or sensor readings can be modeled as realizations of such processes, forming the basis for stochastic modeling in AI.</p>
<section id="picture-in-your-head-90" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-90">Picture in Your Head</h4>
<p>Think of flipping a coin repeatedly. Each toss is uncertain, but together they form a sequence with a well-defined structure. Over time, patterns emerge—like the proportion of heads approaching 0.5.</p>
</section>
<section id="deep-dive-90" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-90">Deep Dive</h4>
<ul>
<li><p>Random sequences: ordered collections of random variables <span class="math inline">\(\{X_t\}_{t=1}^\infty\)</span>.</p></li>
<li><p>Random processes: map from index set (time, space) to outcomes.</p>
<ul>
<li>Discrete-time vs continuous-time.</li>
<li>Discrete-state vs continuous-state.</li>
</ul></li>
<li><p>Key properties:</p>
<ul>
<li>Mean function: <span class="math inline">\(m(t) = E[X_t]\)</span>.</li>
<li>Autocorrelation: <span class="math inline">\(R(s,t) = E[X_s X_t]\)</span>.</li>
<li>Stationarity: statistical properties invariant over time.</li>
</ul></li>
<li><p>Examples:</p>
<ul>
<li>IID sequence: independent identically distributed.</li>
<li>Random walk: sum of IID noise terms.</li>
<li>Gaussian process: every finite subset has multivariate normal distribution.</li>
</ul></li>
<li><p>Applications in AI:</p>
<ul>
<li>Time-series prediction.</li>
<li>Bayesian optimization (Gaussian processes).</li>
<li>Modeling sensor noise in robotics.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 39%">
<col style="width: 42%">
</colgroup>
<thead>
<tr class="header">
<th>Process Type</th>
<th>Definition</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>IID sequence</td>
<td>Independent, identical distribution</td>
<td>Shuffling training data</td>
</tr>
<tr class="even">
<td>Random walk</td>
<td>Incremental sum of noise</td>
<td>Stock price models</td>
</tr>
<tr class="odd">
<td>Gaussian process</td>
<td>Distribution over functions</td>
<td>Bayesian regression</td>
</tr>
<tr class="even">
<td>Poisson process</td>
<td>Random events over time</td>
<td>Queueing systems, rare event modeling</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-73" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-73">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate random walk</span></span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a>steps <span class="op">=</span> np.random.choice([<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>], size<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a>random_walk <span class="op">=</span> np.cumsum(steps)</span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a>plt.plot(random_walk)</span>
<span id="cb91-10"><a href="#cb91-10" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Random Walk"</span>)</span>
<span id="cb91-11"><a href="#cb91-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-88" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-88">Why It Matters</h4>
<p>Random processes provide the mathematical foundation for uncertainty over time. In AI, they power predictive models, reinforcement learning, Bayesian inference, and uncertainty quantification. Without them, modeling dynamic, noisy environments would be impossible.</p>
</section>
<section id="try-it-yourself-90" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-90">Try It Yourself</h4>
<ol type="1">
<li>Simulate 100 coin tosses and compute the empirical frequency of heads.</li>
<li>Generate a Gaussian process with mean 0 and RBF kernel, and sample 3 functions.</li>
<li>Explain how a random walk could model user behavior in recommendation systems.</li>
</ol>
</section>
</section>
<section id="stationarity-and-ergodicity" class="level3">
<h3 class="anchored" data-anchor-id="stationarity-and-ergodicity">192. Stationarity and Ergodicity</h3>
<p>Stationarity describes when the statistical properties of a random process do not change over time. Ergodicity ensures that long-run averages from a single sequence equal expectations over the entire process. Together, they provide the foundations for making reliable inferences from time series.</p>
<section id="picture-in-your-head-91" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-91">Picture in Your Head</h4>
<p>Imagine watching waves at the beach. If the overall pattern of wave height doesn’t change day to day, the process is stationary. If one long afternoon of observation gives you the same average as many afternoons combined, the process is ergodic.</p>
</section>
<section id="deep-dive-91" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-91">Deep Dive</h4>
<ul>
<li><p>Stationarity:</p>
<ul>
<li><em>Strict-sense</em>: all joint distributions are time-invariant.</li>
<li><em>Weak-sense</em>: mean and autocovariance depend only on lag, not absolute time.</li>
<li>Examples: white noise (stationary), stock prices (non-stationary).</li>
</ul></li>
<li><p>Ergodicity:</p>
<ul>
<li>Ensures time averages ≈ ensemble averages.</li>
<li>Needed when we only have one sequence (common in practice).</li>
</ul></li>
<li><p>Testing stationarity:</p>
<ul>
<li>Visual inspection (mean, variance drift).</li>
<li>Unit root tests (ADF, KPSS).</li>
</ul></li>
<li><p>Applications in AI:</p>
<ul>
<li>Reliable training on time-series data.</li>
<li>Reinforcement learning policies assume ergodicity of environment states.</li>
<li>Signal processing in robotics and speech.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 43%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Definition</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Strict stationarity</td>
<td>Full distribution time-invariant</td>
<td>White noise process</td>
</tr>
<tr class="even">
<td>Weak stationarity</td>
<td>Mean, variance stable; covariance by lag</td>
<td>ARMA models in forecasting</td>
</tr>
<tr class="odd">
<td>Ergodicity</td>
<td>Time average = expectation</td>
<td>Long-run reward estimation in RL</td>
</tr>
</tbody>
</table>
<p>Tiny Code Sample (Python, checking weak stationarity)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.tsa.stattools <span class="im">import</span> adfuller</span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate AR(1) process: X_t = 0.7 X_{t-1} + noise</span></span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.zeros(n)</span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, n):</span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a>    x[t] <span class="op">=</span> <span class="fl">0.7</span> <span class="op">*</span> x[t<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> np.random.randn()</span>
<span id="cb92-11"><a href="#cb92-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-12"><a href="#cb92-12" aria-hidden="true" tabindex="-1"></a>plt.plot(x)</span>
<span id="cb92-13"><a href="#cb92-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"AR(1) Process"</span>)</span>
<span id="cb92-14"><a href="#cb92-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb92-15"><a href="#cb92-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-16"><a href="#cb92-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Augmented Dickey-Fuller test for stationarity</span></span>
<span id="cb92-17"><a href="#cb92-17" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> adfuller(x)</span>
<span id="cb92-18"><a href="#cb92-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"ADF p-value:"</span>, result[<span class="dv">1</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-89" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-89">Why It Matters</h4>
<p>AI systems often rely on single observed sequences (like user logs or sensor readings). Stationarity and ergodicity justify treating those samples as representative of the whole process, enabling robust forecasting, learning, and decision-making.</p>
</section>
<section id="try-it-yourself-91" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-91">Try It Yourself</h4>
<ol type="1">
<li>Simulate a random walk and test if it is stationary.</li>
<li>Compare the sample mean of one long trajectory to averages across many simulations.</li>
<li>Explain why non-stationarity (e.g., concept drift) is a major challenge for deployed AI models.</li>
</ol>
</section>
</section>
<section id="discrete-time-markov-chains" class="level3">
<h3 class="anchored" data-anchor-id="discrete-time-markov-chains">193. Discrete-Time Markov Chains</h3>
<p>A discrete-time Markov chain (DTMC) is a stochastic process where the next state depends only on the current state, not the past history. This memoryless property makes Markov chains a cornerstone of probabilistic modeling in AI.</p>
<section id="picture-in-your-head-92" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-92">Picture in Your Head</h4>
<p>Think of a board game where each move depends only on the square you’re currently on and the dice roll—not on how you got there. That’s how a Markov chain works: the present fully determines the future.</p>
</section>
<section id="deep-dive-92" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-92">Deep Dive</h4>
<ul>
<li><p>Definition:</p>
<ul>
<li><p>Sequence of random variables <span class="math inline">\(\{X_t\}\)</span>.</p></li>
<li><p>Markov property:</p>
<p><span class="math display">\[
P(X_{t+1} \mid X_t, X_{t-1}, \dots, X_0) = P(X_{t+1} \mid X_t).
\]</span></p></li>
</ul></li>
<li><p>Transition matrix <span class="math inline">\(P\)</span>:</p>
<ul>
<li><span class="math inline">\(P_{ij} = P(X_{t+1}=j \mid X_t=i)\)</span>.</li>
<li>Rows sum to 1.</li>
</ul></li>
<li><p>Key properties:</p>
<ul>
<li>Irreducibility: all states reachable.</li>
<li>Periodicity: cycles of fixed length.</li>
<li>Stationary distribution: <span class="math inline">\(\pi = \pi P\)</span>.</li>
<li>Convergence: under mild conditions, DTMC converges to stationary distribution.</li>
</ul></li>
<li><p>Applications in AI:</p>
<ul>
<li>Web search (PageRank).</li>
<li>Hidden Markov Models (HMMs) in NLP.</li>
<li>Reinforcement learning state transitions.</li>
<li>Stochastic simulations.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 40%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Term</th>
<th>Meaning</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Transition matrix</td>
<td>Probability of moving between states</td>
<td>PageRank random surfer</td>
</tr>
<tr class="even">
<td>Stationary distribution</td>
<td>Long-run probabilities</td>
<td>Importance ranking in networks</td>
</tr>
<tr class="odd">
<td>Irreducible chain</td>
<td>Every state reachable</td>
<td>Exploration in RL environments</td>
</tr>
<tr class="even">
<td>Periodicity</td>
<td>Fixed cycles of states</td>
<td>Oscillatory processes</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-74" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-74">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Transition matrix for 3 states</span></span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> np.array([[<span class="fl">0.1</span>, <span class="fl">0.6</span>, <span class="fl">0.3</span>],</span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a>              [<span class="fl">0.4</span>, <span class="fl">0.4</span>, <span class="fl">0.2</span>],</span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a>              [<span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span>]])</span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate Markov chain</span></span>
<span id="cb93-9"><a href="#cb93-9" aria-hidden="true" tabindex="-1"></a>n_steps <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb93-10"><a href="#cb93-10" aria-hidden="true" tabindex="-1"></a>state <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb93-11"><a href="#cb93-11" aria-hidden="true" tabindex="-1"></a>trajectory <span class="op">=</span> [state]</span>
<span id="cb93-12"><a href="#cb93-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_steps):</span>
<span id="cb93-13"><a href="#cb93-13" aria-hidden="true" tabindex="-1"></a>    state <span class="op">=</span> np.random.choice([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>], p<span class="op">=</span>P[state])</span>
<span id="cb93-14"><a href="#cb93-14" aria-hidden="true" tabindex="-1"></a>    trajectory.append(state)</span>
<span id="cb93-15"><a href="#cb93-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-16"><a href="#cb93-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Trajectory:"</span>, trajectory)</span>
<span id="cb93-17"><a href="#cb93-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-18"><a href="#cb93-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Approximate stationary distribution</span></span>
<span id="cb93-19"><a href="#cb93-19" aria-hidden="true" tabindex="-1"></a>dist <span class="op">=</span> np.array([<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>]) <span class="op">@</span> np.linalg.matrix_power(P, <span class="dv">50</span>)</span>
<span id="cb93-20"><a href="#cb93-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Stationary distribution:"</span>, dist)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-90" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-90">Why It Matters</h4>
<p>DTMCs strike a balance between simplicity and expressive power. They model dynamic systems where history matters only through the current state—perfect for many AI domains like sequence prediction, decision processes, and probabilistic planning.</p>
</section>
<section id="try-it-yourself-92" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-92">Try It Yourself</h4>
<ol type="1">
<li>Construct a 2-state weather model (sunny, rainy). Simulate 20 days.</li>
<li>Compute the stationary distribution of your model. What does it mean?</li>
<li>Explain why the Markov property simplifies reinforcement learning algorithms.</li>
</ol>
</section>
</section>
<section id="continuous-time-markov-processes" class="level3">
<h3 class="anchored" data-anchor-id="continuous-time-markov-processes">194. Continuous-Time Markov Processes</h3>
<p>Continuous-Time Markov Processes (CTMPs) extend the Markov property to continuous time. Instead of stepping forward in discrete ticks, the system evolves with random waiting times between transitions, often modeled with exponential distributions.</p>
<section id="picture-in-your-head-93" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-93">Picture in Your Head</h4>
<p>Imagine customers arriving at a bank. The arrivals don’t happen exactly every 5 minutes, but randomly—sometimes quickly, sometimes after a long gap. The “clock” is continuous, and the process is still memoryless: the future depends only on the current state, not how long you’ve been waiting.</p>
</section>
<section id="deep-dive-93" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-93">Deep Dive</h4>
<ul>
<li><p>Definition:</p>
<ul>
<li><p>A stochastic process <span class="math inline">\(\{X(t)\}_{t \geq 0}\)</span> with state space <span class="math inline">\(S\)</span>.</p></li>
<li><p>Markov property:</p>
<p><span class="math display">\[
P(X(t+\Delta t)=j \mid X(t)=i, \text{history}) = P(X(t+\Delta t)=j \mid X(t)=i).
\]</span></p></li>
</ul></li>
<li><p>Transition rates (generator matrix <span class="math inline">\(Q\)</span>):</p>
<ul>
<li><span class="math inline">\(Q_{ij} \geq 0\)</span> for <span class="math inline">\(i \neq j\)</span>.</li>
<li><span class="math inline">\(Q_{ii} = -\sum_{j \neq i} Q_{ij}\)</span>.</li>
<li>Probability of leaving state <span class="math inline">\(i\)</span> in small interval <span class="math inline">\(\Delta t\)</span>: <span class="math inline">\(-Q_{ii}\Delta t\)</span>.</li>
</ul></li>
<li><p>Waiting times:</p>
<ul>
<li>Time spent in a state is exponentially distributed.</li>
</ul></li>
<li><p>Stationary distribution:</p>
<ul>
<li>Solve <span class="math inline">\(\pi Q = 0\)</span>, with <span class="math inline">\(\sum_i \pi_i = 1\)</span>.</li>
</ul></li>
<li><p>Applications in AI:</p>
<ul>
<li>Queueing models in computer systems.</li>
<li>Continuous-time reinforcement learning.</li>
<li>Reliability modeling for robotics and networks.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 35%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Formula / Definition</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Generator matrix <span class="math inline">\(Q\)</span></td>
<td>Rates of transition between states</td>
<td>System reliability analysis</td>
</tr>
<tr class="even">
<td>Exponential waiting</td>
<td><span class="math inline">\(P(T&gt;t)=e^{-\lambda t}\)</span></td>
<td>Customer arrivals in queueing models</td>
</tr>
<tr class="odd">
<td>Stationary distribution</td>
<td><span class="math inline">\(\pi Q = 0\)</span></td>
<td>Long-run uptime vs downtime of systems</td>
</tr>
</tbody>
</table>
<p>Tiny Code Sample (Python, simulating CTMC)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Generator matrix Q for 2-state system</span></span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> np.array([[<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.5</span>],</span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>              [<span class="fl">0.2</span>, <span class="op">-</span><span class="fl">0.2</span>]])</span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a>n_steps <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a>state <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a>times <span class="op">=</span> [<span class="dv">0</span>]</span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a>trajectory <span class="op">=</span> [state]</span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_steps):</span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a>    rate <span class="op">=</span> <span class="op">-</span>Q[state,state]</span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a>    wait <span class="op">=</span> np.random.exponential(<span class="dv">1</span><span class="op">/</span>rate)  <span class="co"># exponential waiting time</span></span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a>    next_state <span class="op">=</span> np.random.choice([<span class="dv">0</span>,<span class="dv">1</span>], p<span class="op">=</span>[<span class="fl">0.0</span> <span class="cf">if</span> i<span class="op">==</span>state <span class="cf">else</span> Q[state,i]<span class="op">/</span>rate <span class="cf">for</span> i <span class="kw">in</span> [<span class="dv">0</span>,<span class="dv">1</span>]])</span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a>    times.append(times[<span class="op">-</span><span class="dv">1</span>]<span class="op">+</span>wait)</span>
<span id="cb94-17"><a href="#cb94-17" aria-hidden="true" tabindex="-1"></a>    trajectory.append(next_state)</span>
<span id="cb94-18"><a href="#cb94-18" aria-hidden="true" tabindex="-1"></a>    state <span class="op">=</span> next_state</span>
<span id="cb94-19"><a href="#cb94-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-20"><a href="#cb94-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Times:"</span>, times)</span>
<span id="cb94-21"><a href="#cb94-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Trajectory:"</span>, trajectory)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-91" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-91">Why It Matters</h4>
<p>Many AI systems operate in real time where events occur irregularly—like network failures, user interactions, or biological processes. Continuous-time Markov processes capture these dynamics, bridging probability theory and practical system modeling.</p>
</section>
<section id="try-it-yourself-93" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-93">Try It Yourself</h4>
<ol type="1">
<li>Model a machine that alternates between <em>working</em> and <em>failed</em> with exponential waiting times.</li>
<li>Compute the stationary distribution for the machine’s uptime.</li>
<li>Explain why CTMPs are better suited than DTMCs for modeling network traffic.</li>
</ol>
</section>
</section>
<section id="transition-matrices-and-probabilities" class="level3">
<h3 class="anchored" data-anchor-id="transition-matrices-and-probabilities">195. Transition Matrices and Probabilities</h3>
<p>Transition matrices describe how probabilities shift between states in a Markov process. Each row encodes the probability distribution of moving from one state to all others. They provide a compact and powerful way to analyze dynamics and long-term behavior.</p>
<section id="picture-in-your-head-94" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-94">Picture in Your Head</h4>
<p>Think of a subway map where each station is a state. The transition matrix is like the schedule: from each station, it lists the probabilities of ending up at the others after one ride.</p>
</section>
<section id="deep-dive-94" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-94">Deep Dive</h4>
<ul>
<li><p>Transition matrix (discrete-time Markov chain):</p>
<ul>
<li><span class="math inline">\(P_{ij} = P(X_{t+1}=j \mid X_t=i)\)</span>.</li>
<li>Rows sum to 1.</li>
</ul></li>
<li><p>n-step transitions:</p>
<ul>
<li><span class="math inline">\(P^n\)</span> gives probability of moving between states in n steps.</li>
</ul></li>
<li><p>Stationary distribution:</p>
<ul>
<li>Vector <span class="math inline">\(\pi\)</span> with <span class="math inline">\(\pi P = \pi\)</span>.</li>
</ul></li>
<li><p>Continuous-time case (generator matrix Q):</p>
<ul>
<li><p>Transition probabilities obtained via matrix exponential:</p>
<p><span class="math display">\[
P(t) = e^{Qt}.
\]</span></p></li>
</ul></li>
<li><p>Applications in AI:</p>
<ul>
<li>PageRank and ranking algorithms.</li>
<li>Hidden Markov Models for NLP and speech.</li>
<li>Modeling policies in reinforcement learning.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 31%">
<col style="width: 17%">
<col style="width: 51%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Formula</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>One-step probability</td>
<td><span class="math inline">\(P_{ij}\)</span></td>
<td>Next word prediction in HMM</td>
</tr>
<tr class="even">
<td>n-step probability</td>
<td><span class="math inline">\(P^n_{ij}\)</span></td>
<td>Multi-step planning in RL</td>
</tr>
<tr class="odd">
<td>Stationary distribution</td>
<td><span class="math inline">\(\pi P = \pi\)</span></td>
<td>Long-run importance in PageRank</td>
</tr>
<tr class="even">
<td>Continuous-time</td>
<td><span class="math inline">\(P(t)=e^{Qt}\)</span></td>
<td>Reliability modeling, queueing systems</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-75" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-75">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Transition matrix for 3-state chain</span></span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> np.array([[<span class="fl">0.7</span>, <span class="fl">0.2</span>, <span class="fl">0.1</span>],</span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>              [<span class="fl">0.1</span>, <span class="fl">0.6</span>, <span class="fl">0.3</span>],</span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>              [<span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span>]])</span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Two-step transition probabilities</span></span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a>P2 <span class="op">=</span> np.linalg.matrix_power(P, <span class="dv">2</span>)</span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-11"><a href="#cb95-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Stationary distribution (approximate via power method)</span></span>
<span id="cb95-12"><a href="#cb95-12" aria-hidden="true" tabindex="-1"></a>pi <span class="op">=</span> np.array([<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>]) <span class="op">@</span> np.linalg.matrix_power(P, <span class="dv">50</span>)</span>
<span id="cb95-13"><a href="#cb95-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-14"><a href="#cb95-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"P^2:</span><span class="ch">\n</span><span class="st">"</span>, P2)</span>
<span id="cb95-15"><a href="#cb95-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Stationary distribution:"</span>, pi)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-92" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-92">Why It Matters</h4>
<p>Transition matrices turn probabilistic dynamics into linear algebra, enabling efficient computation of future states, long-run distributions, and stability analysis. This bridges stochastic processes with numerical methods, making them core to AI reasoning under uncertainty.</p>
</section>
<section id="try-it-yourself-94" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-94">Try It Yourself</h4>
<ol type="1">
<li>Construct a 2-state transition matrix for weather (sunny, rainy). Compute probabilities after 3 days.</li>
<li>Find the stationary distribution of a 3-state Markov chain by solving <span class="math inline">\(\pi P = \pi\)</span>.</li>
<li>Explain why transition matrices are key to reinforcement learning policy evaluation.</li>
</ol>
</section>
</section>
<section id="markov-property-and-memorylessness" class="level3">
<h3 class="anchored" data-anchor-id="markov-property-and-memorylessness">196. Markov Property and Memorylessness</h3>
<p>The Markov property states that the future of a process depends only on its present state, not its past history. This “memorylessness” simplifies modeling dynamic systems, allowing them to be described with transition probabilities instead of full histories.</p>
<section id="picture-in-your-head-95" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-95">Picture in Your Head</h4>
<p>Imagine standing at a crossroads. To decide where you’ll go next, you only need to know where you are now—not the exact path you took to get there.</p>
</section>
<section id="deep-dive-95" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-95">Deep Dive</h4>
<ul>
<li><p>Formal definition: A stochastic process <span class="math inline">\(\{X_t\}\)</span> has the Markov property if</p>
<p><span class="math display">\[
P(X_{t+1} \mid X_t, X_{t-1}, \ldots, X_0) = P(X_{t+1} \mid X_t).
\]</span></p></li>
<li><p>Memorylessness:</p>
<ul>
<li>In discrete-time Markov chains, the next state depends only on the current state.</li>
<li>In continuous-time Markov processes, the waiting time in each state is exponentially distributed, which is also memoryless.</li>
</ul></li>
<li><p>Consequences:</p>
<ul>
<li>Simplifies analysis of stochastic systems.</li>
<li>Enables recursive computation of probabilities.</li>
<li>Forms basis for dynamic programming.</li>
</ul></li>
<li><p>Limitations:</p>
<ul>
<li>Not all processes are Markovian (e.g., stock markets with long-term dependencies).</li>
<li>Extensions: higher-order Markov models, hidden Markov models.</li>
</ul></li>
<li><p>Applications in AI:</p>
<ul>
<li>Reinforcement learning environments.</li>
<li>Hidden Markov Models in NLP and speech recognition.</li>
<li>State-space models for robotics and planning.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 39%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Definition</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Markov property</td>
<td>Future depends only on present</td>
<td>Reinforcement learning policies</td>
</tr>
<tr class="even">
<td>Memorylessness</td>
<td>No dependency on elapsed time/history</td>
<td>Exponential waiting times in CTMCs</td>
</tr>
<tr class="odd">
<td>Extension</td>
<td>Higher-order or hidden Markov models</td>
<td>Part-of-speech tagging, sequence labeling</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-76" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-76">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple 2-state Markov chain: Sunny (0), Rainy (1)</span></span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> np.array([[<span class="fl">0.8</span>, <span class="fl">0.2</span>],</span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a>              [<span class="fl">0.5</span>, <span class="fl">0.5</span>]])</span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a>state <span class="op">=</span> <span class="dv">0</span>  <span class="co"># start Sunny</span></span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a>trajectory <span class="op">=</span> [state]</span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb96-10"><a href="#cb96-10" aria-hidden="true" tabindex="-1"></a>    state <span class="op">=</span> np.random.choice([<span class="dv">0</span>,<span class="dv">1</span>], p<span class="op">=</span>P[state])</span>
<span id="cb96-11"><a href="#cb96-11" aria-hidden="true" tabindex="-1"></a>    trajectory.append(state)</span>
<span id="cb96-12"><a href="#cb96-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-13"><a href="#cb96-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Weather trajectory:"</span>, trajectory)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-93" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-93">Why It Matters</h4>
<p>The Markov property reduces complexity by removing dependence on the full past, making dynamic systems tractable for analysis and learning. Without it, reinforcement learning and probabilistic planning would be computationally intractable.</p>
</section>
<section id="try-it-yourself-95" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-95">Try It Yourself</h4>
<ol type="1">
<li>Write down a simple 3-state Markov chain and verify the Markov property holds.</li>
<li>Explain how the exponential distribution’s memorylessness supports continuous-time Markov processes.</li>
<li>Discuss a real-world process that violates the Markov property—what’s missing?</li>
</ol>
</section>
</section>
<section id="martingales-and-applications" class="level3">
<h3 class="anchored" data-anchor-id="martingales-and-applications">197. Martingales and Applications</h3>
<p>A martingale is a stochastic process where the conditional expectation of the next value equals the current value, given all past information. In other words, martingales are “fair game” processes with no predictable trend up or down.</p>
<section id="picture-in-your-head-96" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-96">Picture in Your Head</h4>
<p>Think of repeatedly betting on a fair coin toss. Your expected fortune after the next toss is exactly your current fortune, regardless of how many wins or losses you’ve had before.</p>
</section>
<section id="deep-dive-96" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-96">Deep Dive</h4>
<ul>
<li><p>Formal definition: A process <span class="math inline">\(\{X_t\}\)</span> is a martingale with respect to a filtration <span class="math inline">\(\mathcal{F}_t\)</span> if:</p>
<ol type="1">
<li><span class="math inline">\(E[|X_t|] &lt; \infty\)</span>.</li>
<li><span class="math inline">\(E[X_{t+1} \mid \mathcal{F}_t] = X_t\)</span>.</li>
</ol></li>
<li><p>Submartingale: expectation increases (<span class="math inline">\(E[X_{t+1}\mid \mathcal{F}_t] \geq X_t\)</span>).</p></li>
<li><p>Supermartingale: expectation decreases.</p></li>
<li><p>Key properties:</p>
<ul>
<li>Martingale convergence theorem: under conditions, martingales converge almost surely.</li>
<li>Optional stopping theorem: stopping a martingale at a fair time preserves expectation.</li>
</ul></li>
<li><p>Applications in AI:</p>
<ul>
<li>Analysis of randomized algorithms.</li>
<li>Reinforcement learning (value estimates as martingales).</li>
<li>Finance models (asset prices under no-arbitrage).</li>
<li>Bandit problems and regret analysis.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 36%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Definition</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Martingale</td>
<td>Fair game, expected next = current</td>
<td>RL value updates under unbiased estimates</td>
</tr>
<tr class="even">
<td>Submartingale</td>
<td>Expected value grows</td>
<td>Regret bounds in online learning</td>
</tr>
<tr class="odd">
<td>Supermartingale</td>
<td>Expected value shrinks</td>
<td>Discounted reward models</td>
</tr>
<tr class="even">
<td>Optional stopping</td>
<td>Fairness persists under stopping</td>
<td>Termination in stochastic simulations</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-77" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-77">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>steps <span class="op">=</span> np.random.choice([<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>], size<span class="op">=</span>n)  <span class="co"># fair coin tosses</span></span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a>martingale <span class="op">=</span> np.cumsum(steps)</span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Martingale sequence:"</span>, martingale)</span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expectation ~ 0:"</span>, martingale.mean())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-94" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-94">Why It Matters</h4>
<p>Martingales provide the mathematical language for fairness, stability, and unpredictability in stochastic systems. They allow AI researchers to prove convergence guarantees, analyze uncertainty, and ensure robustness in algorithms.</p>
</section>
<section id="try-it-yourself-96" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-96">Try It Yourself</h4>
<ol type="1">
<li>Simulate a random walk and check if it is a martingale.</li>
<li>Give an example of a process that is a submartingale but not a martingale.</li>
<li>Explain why martingale analysis is important in proving reinforcement learning convergence.</li>
</ol>
</section>
</section>
<section id="hidden-markov-models" class="level3">
<h3 class="anchored" data-anchor-id="hidden-markov-models">198. Hidden Markov Models</h3>
<p>A Hidden Markov Model (HMM) is a probabilistic model where the system evolves through hidden states according to a Markov chain, but we only observe outputs generated probabilistically from those states. HMMs bridge unobservable dynamics and observable data.</p>
<section id="picture-in-your-head-97" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-97">Picture in Your Head</h4>
<p>Imagine trying to infer the weather based only on whether people carry umbrellas. The actual weather (hidden state) follows a Markov chain, while the umbrellas you see (observations) are noisy signals of it.</p>
</section>
<section id="deep-dive-97" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-97">Deep Dive</h4>
<ul>
<li><p>Model structure:</p>
<ul>
<li>Hidden states: <span class="math inline">\(S = \{s_1, s_2, \dots, s_N\}\)</span>.</li>
<li>Transition probabilities: <span class="math inline">\(A = [a_{ij}]\)</span>.</li>
<li>Emission probabilities: <span class="math inline">\(B = [b_j(o)]\)</span>, likelihood of observation given state.</li>
<li>Initial distribution: <span class="math inline">\(\pi\)</span>.</li>
</ul></li>
<li><p>Key algorithms:</p>
<ul>
<li>Forward algorithm: compute likelihood of observation sequence.</li>
<li>Viterbi algorithm: most likely hidden state sequence.</li>
<li>Baum-Welch (EM): learn parameters from data.</li>
</ul></li>
<li><p>Assumptions:</p>
<ul>
<li>Markov property: next state depends only on current state.</li>
<li>Observations independent given hidden states.</li>
</ul></li>
<li><p>Applications in AI:</p>
<ul>
<li>Speech recognition (phonemes as states, audio as observations).</li>
<li>NLP (part-of-speech tagging, named entity recognition).</li>
<li>Bioinformatics (gene sequence modeling).</li>
<li>Finance (regime-switching models).</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 41%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Description</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Hidden states</td>
<td>Latent variables evolving by Markov chain</td>
<td>Phonemes, POS tags, weather</td>
</tr>
<tr class="even">
<td>Emission probabilities</td>
<td>Distribution over observations</td>
<td>Acoustic signals, words, user actions</td>
</tr>
<tr class="odd">
<td>Forward algorithm</td>
<td>Sequence likelihood</td>
<td>Speech recognition scoring</td>
</tr>
<tr class="even">
<td>Viterbi algorithm</td>
<td>Most probable hidden sequence</td>
<td>Decoding phoneme or tag sequences</td>
</tr>
</tbody>
</table>
<p>Tiny Code Sample (Python, hmmlearn)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> hmmlearn <span class="im">import</span> hmm</span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define HMM with 2 hidden states</span></span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> hmm.MultinomialHMM(n_components<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a>model.startprob_ <span class="op">=</span> np.array([<span class="fl">0.6</span>, <span class="fl">0.4</span>])</span>
<span id="cb98-7"><a href="#cb98-7" aria-hidden="true" tabindex="-1"></a>model.transmat_ <span class="op">=</span> np.array([[<span class="fl">0.7</span>, <span class="fl">0.3</span>],</span>
<span id="cb98-8"><a href="#cb98-8" aria-hidden="true" tabindex="-1"></a>                            [<span class="fl">0.4</span>, <span class="fl">0.6</span>]])</span>
<span id="cb98-9"><a href="#cb98-9" aria-hidden="true" tabindex="-1"></a>model.emissionprob_ <span class="op">=</span> np.array([[<span class="fl">0.5</span>, <span class="fl">0.5</span>],</span>
<span id="cb98-10"><a href="#cb98-10" aria-hidden="true" tabindex="-1"></a>                                [<span class="fl">0.1</span>, <span class="fl">0.9</span>]])</span>
<span id="cb98-11"><a href="#cb98-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-12"><a href="#cb98-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Observations: 0,1</span></span>
<span id="cb98-13"><a href="#cb98-13" aria-hidden="true" tabindex="-1"></a>obs <span class="op">=</span> np.array([[<span class="dv">0</span>],[<span class="dv">1</span>],[<span class="dv">0</span>],[<span class="dv">1</span>]])</span>
<span id="cb98-14"><a href="#cb98-14" aria-hidden="true" tabindex="-1"></a>logprob, states <span class="op">=</span> model.decode(obs, algorithm<span class="op">=</span><span class="st">"viterbi"</span>)</span>
<span id="cb98-15"><a href="#cb98-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-16"><a href="#cb98-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Most likely states:"</span>, states)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-95" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-95">Why It Matters</h4>
<p>HMMs are a foundational model for reasoning under uncertainty with sequential data. They remain essential in speech, language, and biological sequence analysis, and their principles inspire more advanced deep sequence models like RNNs and Transformers.</p>
</section>
<section id="try-it-yourself-97" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-97">Try It Yourself</h4>
<ol type="1">
<li>Define a 2-state HMM for “Rainy” vs “Sunny” with umbrella observations. Simulate a sequence.</li>
<li>Use the Viterbi algorithm to decode the most likely weather given observations.</li>
<li>Compare HMMs to modern sequence models—what advantages remain for HMMs?</li>
</ol>
</section>
</section>
<section id="stochastic-differential-equations" class="level3">
<h3 class="anchored" data-anchor-id="stochastic-differential-equations">199. Stochastic Differential Equations</h3>
<p>Stochastic Differential Equations (SDEs) extend ordinary differential equations by adding random noise terms, typically modeled with Brownian motion. They capture dynamics where systems evolve continuously but with uncertainty at every step.</p>
<section id="picture-in-your-head-98" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-98">Picture in Your Head</h4>
<p>Imagine watching pollen floating in water. Its overall drift follows physical laws, but random collisions with water molecules push it unpredictably. An SDE models both the smooth drift and the jittery randomness together.</p>
</section>
<section id="deep-dive-98" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-98">Deep Dive</h4>
<ul>
<li><p>General form:</p>
<p><span class="math display">\[
dX_t = \mu(X_t, t)dt + \sigma(X_t, t)dW_t
\]</span></p>
<ul>
<li>Drift term <span class="math inline">\(\mu\)</span>: deterministic trend.</li>
<li>Diffusion term <span class="math inline">\(\sigma\)</span>: random fluctuations.</li>
<li><span class="math inline">\(W_t\)</span>: Wiener process (Brownian motion).</li>
</ul></li>
<li><p>Solutions:</p>
<ul>
<li>Interpreted via Itô or Stratonovich calculus.</li>
<li>Numerical: Euler–Maruyama, Milstein methods.</li>
</ul></li>
<li><p>Examples:</p>
<ul>
<li>Geometric Brownian motion: <span class="math inline">\(dS_t = \mu S_t dt + \sigma S_t dW_t\)</span>.</li>
<li>Ornstein–Uhlenbeck process: mean-reverting dynamics.</li>
</ul></li>
<li><p>Applications in AI:</p>
<ul>
<li>Stochastic gradient Langevin dynamics (SGLD) for Bayesian learning.</li>
<li>Diffusion models in generative AI.</li>
<li>Continuous-time reinforcement learning.</li>
<li>Modeling uncertainty in robotics and finance.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 39%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Process Type</th>
<th>Equation Form</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Geometric Brownian Motion</td>
<td><span class="math inline">\(dS_t = \mu S_t dt + \sigma S_t dW_t\)</span></td>
<td>Asset pricing, probabilistic forecasting</td>
</tr>
<tr class="even">
<td>Ornstein–Uhlenbeck</td>
<td><span class="math inline">\(dX_t = \theta(\mu - X_t)dt + \sigma dW_t\)</span></td>
<td>Exploration in RL, noise in control</td>
</tr>
<tr class="odd">
<td>Langevin dynamics</td>
<td>Gradient + noise dynamics</td>
<td>Bayesian deep learning, diffusion models</td>
</tr>
</tbody>
</table>
<p>Tiny Code Sample (Python, Euler–Maruyama Simulation)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a>T, N <span class="op">=</span> <span class="fl">1.0</span>, <span class="dv">1000</span></span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> T<span class="op">/</span>N</span>
<span id="cb99-7"><a href="#cb99-7" aria-hidden="true" tabindex="-1"></a>mu, sigma <span class="op">=</span> <span class="fl">1.0</span>, <span class="fl">0.3</span></span>
<span id="cb99-8"><a href="#cb99-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-9"><a href="#cb99-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate geometric Brownian motion</span></span>
<span id="cb99-10"><a href="#cb99-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.zeros(N)</span>
<span id="cb99-11"><a href="#cb99-11" aria-hidden="true" tabindex="-1"></a>X[<span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb99-12"><a href="#cb99-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, N):</span>
<span id="cb99-13"><a href="#cb99-13" aria-hidden="true" tabindex="-1"></a>    dW <span class="op">=</span> np.sqrt(dt) <span class="op">*</span> np.random.randn()</span>
<span id="cb99-14"><a href="#cb99-14" aria-hidden="true" tabindex="-1"></a>    X[i] <span class="op">=</span> X[i<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> mu<span class="op">*</span>X[i<span class="op">-</span><span class="dv">1</span>]<span class="op">*</span>dt <span class="op">+</span> sigma<span class="op">*</span>X[i<span class="op">-</span><span class="dv">1</span>]<span class="op">*</span>dW</span>
<span id="cb99-15"><a href="#cb99-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-16"><a href="#cb99-16" aria-hidden="true" tabindex="-1"></a>plt.plot(np.linspace(<span class="dv">0</span>, T, N), X)</span>
<span id="cb99-17"><a href="#cb99-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Geometric Brownian Motion"</span>)</span>
<span id="cb99-18"><a href="#cb99-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-96" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-96">Why It Matters</h4>
<p>SDEs let AI systems model continuous uncertainty and randomness in dynamic environments. They are the mathematical foundation of diffusion-based generative models and stochastic optimization techniques that dominate modern machine learning.</p>
</section>
<section id="try-it-yourself-98" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-98">Try It Yourself</h4>
<ol type="1">
<li>Simulate an Ornstein–Uhlenbeck process and observe its mean-reverting behavior.</li>
<li>Explain how SDEs relate to diffusion models for image generation.</li>
<li>Use SGLD to train a simple regression model with Bayesian uncertainty.</li>
</ol>
</section>
</section>
<section id="monte-carlo-methods-1" class="level3">
<h3 class="anchored" data-anchor-id="monte-carlo-methods-1">200. Monte Carlo Methods</h3>
<p>Monte Carlo methods use randomness to approximate solutions to mathematical and computational problems. By simulating many random samples, they estimate expectations, probabilities, and integrals that are otherwise intractable.</p>
<section id="picture-in-your-head-99" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-99">Picture in Your Head</h4>
<p>Imagine trying to measure the area of an irregularly shaped pond. Instead of calculating exactly, you throw random pebbles into a square containing the pond. The fraction that lands inside gives an estimate of its area.</p>
</section>
<section id="deep-dive-99" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-99">Deep Dive</h4>
<ul>
<li><p>Core idea: approximate <span class="math inline">\(\mathbb{E}[f(X)]\)</span> by averaging over random draws of <span class="math inline">\(X\)</span>.</p>
<p><span class="math display">\[
\mathbb{E}[f(X)] \approx \frac{1}{N}\sum_{i=1}^N f(x_i), \quad x_i \sim p(x)
\]</span></p></li>
<li><p>Variance reduction:</p>
<ul>
<li>Importance sampling, control variates, stratified sampling.</li>
</ul></li>
<li><p>Monte Carlo integration:</p>
<ul>
<li>Estimate integrals over high-dimensional spaces.</li>
</ul></li>
<li><p>Markov Chain Monte Carlo (MCMC):</p>
<ul>
<li>Use dependent samples from a Markov chain to approximate distributions (Metropolis-Hastings, Gibbs sampling).</li>
</ul></li>
<li><p>Applications in AI:</p>
<ul>
<li>Bayesian inference (posterior estimation).</li>
<li>Reinforcement learning (policy evaluation with rollouts).</li>
<li>Probabilistic programming.</li>
<li>Simulation for planning under uncertainty.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 44%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>Technique</th>
<th>Description</th>
<th>AI Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Basic Monte Carlo</td>
<td>Average over random samples</td>
<td>Estimating expected reward in RL</td>
</tr>
<tr class="even">
<td>Importance sampling</td>
<td>Reweight samples from different distribution</td>
<td>Off-policy evaluation</td>
</tr>
<tr class="odd">
<td>MCMC</td>
<td>Generate dependent samples via Markov chain</td>
<td>Bayesian neural networks</td>
</tr>
<tr class="even">
<td>Variational Monte Carlo</td>
<td>Combine sampling with optimization</td>
<td>Approximate posterior inference</td>
</tr>
</tbody>
</table>
<p>Tiny Code Sample (Python, Monte Carlo for π)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">100000</span></span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a>points <span class="op">=</span> np.random.rand(N,<span class="dv">2</span>)</span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a>inside_circle <span class="op">=</span> np.<span class="bu">sum</span>(points[:,<span class="dv">0</span>]<span class="dv">2</span> <span class="op">+</span> points[:,<span class="dv">1</span>]<span class="dv">2</span> <span class="op">&lt;=</span> <span class="dv">1</span>)</span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a>pi_estimate <span class="op">=</span> <span class="dv">4</span> <span class="op">*</span> inside_circle <span class="op">/</span> N</span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-8"><a href="#cb100-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Monte Carlo estimate of π:"</span>, pi_estimate)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-97" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-97">Why It Matters</h4>
<p>Monte Carlo methods make the intractable tractable. They allow AI systems to approximate probabilities, expectations, and integrals in high dimensions, powering Bayesian inference, probabilistic models, and modern generative approaches.</p>
</section>
<section id="try-it-yourself-99" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-99">Try It Yourself</h4>
<ol type="1">
<li>Use Monte Carlo to estimate the integral of <span class="math inline">\(f(x)=e^{-x^2}\)</span> over <span class="math inline">\([0,1]\)</span>.</li>
<li>Implement importance sampling for a skewed distribution.</li>
<li>Explain how MCMC can approximate the posterior of a Bayesian linear regression model.</li>
</ol>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../books/en-US/volume_1.html" class="pagination-link" aria-label="Volume 1. First principles of Artificial Intelligence">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Volume 1. First principles of Artificial Intelligence</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../books/en-US/volume_3.html" class="pagination-link" aria-label="Volume 3. Data and Representation">
        <span class="nav-page-text"><span class="chapter-title">Volume 3. Data and Representation</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>