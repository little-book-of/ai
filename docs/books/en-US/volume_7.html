<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.23">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Volume 7. Machine Learning Theory and Practice – The Little Book of Artificial Intelligence</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../books/en-US/volume_8.html" rel="next">
<link href="../../books/en-US/volume_6.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-1fe81d0376b2c50856e68e651e390326.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-27c261d06b905028a18691de25d09dde.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../books/en-US/volume_7.html"><span class="chapter-title">Volume 7. Machine Learning Theory and Practice</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../../index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">The Little Book of Artificial Intelligence</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Contents</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-US/volume_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Volume 1. First principles of Artificial Intelligence</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-US/volume_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Volume 2. Mathematicial Foundations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-US/volume_3.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Volume 3. Data and Representation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-US/volume_4.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Volume 4. Search and Planning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-US/volume_5.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Volume 5. Logic and Knowledge</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-US/volume_6.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Volume 6. Probabilistic Modeling and Inference</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-US/volume_7.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Volume 7. Machine Learning Theory and Practice</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-US/volume_8.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Volume 8. Supervised Learning Systems</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-US/volume_9.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Volume 9. Unsupervised, self-supervised and representation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-US/volume_10.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Volume 10. Deep Learning Core</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-US/volume_11.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Volume 11. Large Language Models</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#chapter-61.-hyphothesis-spaces-bias-and-capacity" id="toc-chapter-61.-hyphothesis-spaces-bias-and-capacity" class="nav-link active" data-scroll-target="#chapter-61.-hyphothesis-spaces-bias-and-capacity">Chapter 61. Hyphothesis spaces, bias and capacity</a>
  <ul class="collapse">
  <li><a href="#hypotheses-as-functions-and-mappings" id="toc-hypotheses-as-functions-and-mappings" class="nav-link" data-scroll-target="#hypotheses-as-functions-and-mappings">601. Hypotheses as Functions and Mappings</a></li>
  <li><a href="#the-space-of-all-possible-hypotheses" id="toc-the-space-of-all-possible-hypotheses" class="nav-link" data-scroll-target="#the-space-of-all-possible-hypotheses">602. The Space of All Possible Hypotheses</a></li>
  <li><a href="#inductive-bias-choosing-among-hypotheses" id="toc-inductive-bias-choosing-among-hypotheses" class="nav-link" data-scroll-target="#inductive-bias-choosing-among-hypotheses">603. Inductive Bias: Choosing Among Hypotheses</a></li>
  <li><a href="#capacity-and-expressivity-of-models" id="toc-capacity-and-expressivity-of-models" class="nav-link" data-scroll-target="#capacity-and-expressivity-of-models">604. Capacity and Expressivity of Models</a></li>
  <li><a href="#the-biasvariance-tradeoff" id="toc-the-biasvariance-tradeoff" class="nav-link" data-scroll-target="#the-biasvariance-tradeoff">605. The Bias–Variance Tradeoff</a></li>
  <li><a href="#overfitting-vs.-underfitting" id="toc-overfitting-vs.-underfitting" class="nav-link" data-scroll-target="#overfitting-vs.-underfitting">606. Overfitting vs.&nbsp;Underfitting</a></li>
  <li><a href="#structural-risk-minimization" id="toc-structural-risk-minimization" class="nav-link" data-scroll-target="#structural-risk-minimization">607. Structural Risk Minimization</a></li>
  <li><a href="#occams-razor-in-learning-theory" id="toc-occams-razor-in-learning-theory" class="nav-link" data-scroll-target="#occams-razor-in-learning-theory">608. Occam’s Razor in Learning Theory</a></li>
  <li><a href="#complexity-vs.-interpretability" id="toc-complexity-vs.-interpretability" class="nav-link" data-scroll-target="#complexity-vs.-interpretability">609. Complexity vs.&nbsp;Interpretability</a></li>
  <li><a href="#case-studies-of-bias-and-capacity-in-practice" id="toc-case-studies-of-bias-and-capacity-in-practice" class="nav-link" data-scroll-target="#case-studies-of-bias-and-capacity-in-practice">610. Case Studies of Bias and Capacity in Practice</a></li>
  </ul></li>
  <li><a href="#chapter-62.-generalization-vc-rademacher-pac" id="toc-chapter-62.-generalization-vc-rademacher-pac" class="nav-link" data-scroll-target="#chapter-62.-generalization-vc-rademacher-pac">Chapter 62. Generalization, VC, Rademacher, PAC</a>
  <ul class="collapse">
  <li><a href="#generalization-as-out-of-sample-performance" id="toc-generalization-as-out-of-sample-performance" class="nav-link" data-scroll-target="#generalization-as-out-of-sample-performance">611. Generalization as Out-of-Sample Performance</a></li>
  <li><a href="#the-law-of-large-numbers-and-convergence" id="toc-the-law-of-large-numbers-and-convergence" class="nav-link" data-scroll-target="#the-law-of-large-numbers-and-convergence">612. The Law of Large Numbers and Convergence</a></li>
  <li><a href="#vc-dimension-definition-and-intuition" id="toc-vc-dimension-definition-and-intuition" class="nav-link" data-scroll-target="#vc-dimension-definition-and-intuition">613. VC Dimension: Definition and Intuition</a></li>
  <li><a href="#growth-functions-and-shattering" id="toc-growth-functions-and-shattering" class="nav-link" data-scroll-target="#growth-functions-and-shattering">614. Growth Functions and Shattering</a></li>
  <li><a href="#rademacher-complexity-and-data-dependent-bounds" id="toc-rademacher-complexity-and-data-dependent-bounds" class="nav-link" data-scroll-target="#rademacher-complexity-and-data-dependent-bounds">615. Rademacher Complexity and Data-Dependent Bounds</a></li>
  <li><a href="#pac-learning-framework" id="toc-pac-learning-framework" class="nav-link" data-scroll-target="#pac-learning-framework">616. PAC Learning Framework</a></li>
  <li><a href="#probably-approximately-correct-guarantees" id="toc-probably-approximately-correct-guarantees" class="nav-link" data-scroll-target="#probably-approximately-correct-guarantees">617. Probably Approximately Correct Guarantees</a></li>
  <li><a href="#uniform-convergence-and-concentration-inequalities" id="toc-uniform-convergence-and-concentration-inequalities" class="nav-link" data-scroll-target="#uniform-convergence-and-concentration-inequalities">618. Uniform Convergence and Concentration Inequalities</a></li>
  <li><a href="#limitations-of-pac-theory" id="toc-limitations-of-pac-theory" class="nav-link" data-scroll-target="#limitations-of-pac-theory">619. Limitations of PAC Theory</a></li>
  <li><a href="#implications-for-modern-machine-learning" id="toc-implications-for-modern-machine-learning" class="nav-link" data-scroll-target="#implications-for-modern-machine-learning">620. Implications for Modern Machine Learning</a></li>
  </ul></li>
  <li><a href="#chapter-63.-losses-regularization-and-optimization" id="toc-chapter-63.-losses-regularization-and-optimization" class="nav-link" data-scroll-target="#chapter-63.-losses-regularization-and-optimization">Chapter 63. Losses, Regularization, and Optimization</a>
  <ul class="collapse">
  <li><a href="#loss-functions-as-objectives" id="toc-loss-functions-as-objectives" class="nav-link" data-scroll-target="#loss-functions-as-objectives">621. Loss Functions as Objectives</a></li>
  <li><a href="#convex-vs.-non-convex-losses" id="toc-convex-vs.-non-convex-losses" class="nav-link" data-scroll-target="#convex-vs.-non-convex-losses">622. Convex vs.&nbsp;Non-Convex Losses</a></li>
  <li><a href="#l1-and-l2-regularization" id="toc-l1-and-l2-regularization" class="nav-link" data-scroll-target="#l1-and-l2-regularization">623. L1 and L2 Regularization</a></li>
  <li><a href="#norm-based-and-geometric-regularization" id="toc-norm-based-and-geometric-regularization" class="nav-link" data-scroll-target="#norm-based-and-geometric-regularization">624. Norm-Based and Geometric Regularization</a></li>
  <li><a href="#sparsity-inducing-penalties" id="toc-sparsity-inducing-penalties" class="nav-link" data-scroll-target="#sparsity-inducing-penalties">625. Sparsity-Inducing Penalties</a></li>
  <li><a href="#early-stopping-as-implicit-regularization" id="toc-early-stopping-as-implicit-regularization" class="nav-link" data-scroll-target="#early-stopping-as-implicit-regularization">626. Early Stopping as Implicit Regularization</a></li>
  <li><a href="#optimization-landscapes-and-saddle-points" id="toc-optimization-landscapes-and-saddle-points" class="nav-link" data-scroll-target="#optimization-landscapes-and-saddle-points">627. Optimization Landscapes and Saddle Points</a></li>
  <li><a href="#stochastic-vs.-batch-optimization" id="toc-stochastic-vs.-batch-optimization" class="nav-link" data-scroll-target="#stochastic-vs.-batch-optimization">628. Stochastic vs.&nbsp;Batch Optimization</a></li>
  <li><a href="#robust-and-adversarial-losses" id="toc-robust-and-adversarial-losses" class="nav-link" data-scroll-target="#robust-and-adversarial-losses">629. Robust and Adversarial Losses</a></li>
  <li><a href="#tradeoffs-regularization-strength-vs.-flexibility" id="toc-tradeoffs-regularization-strength-vs.-flexibility" class="nav-link" data-scroll-target="#tradeoffs-regularization-strength-vs.-flexibility">630. Tradeoffs: Regularization Strength vs.&nbsp;Flexibility</a></li>
  </ul></li>
  <li><a href="#chapter-64.-model-selection-cross-validation-bootstrapping" id="toc-chapter-64.-model-selection-cross-validation-bootstrapping" class="nav-link" data-scroll-target="#chapter-64.-model-selection-cross-validation-bootstrapping">Chapter 64. Model selection, cross validation, bootstrapping</a>
  <ul class="collapse">
  <li><a href="#the-problem-of-choosing-among-models" id="toc-the-problem-of-choosing-among-models" class="nav-link" data-scroll-target="#the-problem-of-choosing-among-models">631. The Problem of Choosing Among Models</a></li>
  <li><a href="#training-vs.-validation-vs.-test-splits" id="toc-training-vs.-validation-vs.-test-splits" class="nav-link" data-scroll-target="#training-vs.-validation-vs.-test-splits">632. Training vs.&nbsp;Validation vs.&nbsp;Test Splits</a></li>
  <li><a href="#k-fold-cross-validation" id="toc-k-fold-cross-validation" class="nav-link" data-scroll-target="#k-fold-cross-validation">633. k-Fold Cross-Validation</a></li>
  <li><a href="#leave-one-out-and-variants" id="toc-leave-one-out-and-variants" class="nav-link" data-scroll-target="#leave-one-out-and-variants">634. Leave-One-Out and Variants</a></li>
  <li><a href="#bootstrap-resampling-for-model-assessment" id="toc-bootstrap-resampling-for-model-assessment" class="nav-link" data-scroll-target="#bootstrap-resampling-for-model-assessment">635. Bootstrap Resampling for Model Assessment</a></li>
  <li><a href="#information-criteria-aic-bic-mdl" id="toc-information-criteria-aic-bic-mdl" class="nav-link" data-scroll-target="#information-criteria-aic-bic-mdl">636. Information Criteria: AIC, BIC, MDL</a></li>
  <li><a href="#nested-cross-validation-for-hyperparameter-tuning" id="toc-nested-cross-validation-for-hyperparameter-tuning" class="nav-link" data-scroll-target="#nested-cross-validation-for-hyperparameter-tuning">637. Nested Cross-Validation for Hyperparameter Tuning</a></li>
  <li><a href="#multiple-comparisons-and-statistical-significance" id="toc-multiple-comparisons-and-statistical-significance" class="nav-link" data-scroll-target="#multiple-comparisons-and-statistical-significance">638. Multiple Comparisons and Statistical Significance</a></li>
  <li><a href="#model-selection-under-data-scarcity" id="toc-model-selection-under-data-scarcity" class="nav-link" data-scroll-target="#model-selection-under-data-scarcity">639. Model Selection under Data Scarcity</a></li>
  <li><a href="#best-practices-in-evaluation-protocols" id="toc-best-practices-in-evaluation-protocols" class="nav-link" data-scroll-target="#best-practices-in-evaluation-protocols">640. Best Practices in Evaluation Protocols</a></li>
  </ul></li>
  <li><a href="#chapter-65.-linear-and-generalized-linear-models" id="toc-chapter-65.-linear-and-generalized-linear-models" class="nav-link" data-scroll-target="#chapter-65.-linear-and-generalized-linear-models">Chapter 65. Linear and Generalized Linear Models</a>
  <ul class="collapse">
  <li><a href="#linear-regression-basics" id="toc-linear-regression-basics" class="nav-link" data-scroll-target="#linear-regression-basics">641. Linear Regression Basics</a></li>
  <li><a href="#maximum-likelihood-and-least-squares" id="toc-maximum-likelihood-and-least-squares" class="nav-link" data-scroll-target="#maximum-likelihood-and-least-squares">642. Maximum Likelihood and Least Squares</a></li>
  <li><a href="#logistic-regression-for-classification" id="toc-logistic-regression-for-classification" class="nav-link" data-scroll-target="#logistic-regression-for-classification">643. Logistic Regression for Classification</a></li>
  <li><a href="#generalized-linear-model-framework" id="toc-generalized-linear-model-framework" class="nav-link" data-scroll-target="#generalized-linear-model-framework">644. Generalized Linear Model Framework</a></li>
  <li><a href="#link-functions-and-canonical-forms" id="toc-link-functions-and-canonical-forms" class="nav-link" data-scroll-target="#link-functions-and-canonical-forms">645. Link Functions and Canonical Forms</a></li>
  <li><a href="#poisson-and-exponential-regression-models" id="toc-poisson-and-exponential-regression-models" class="nav-link" data-scroll-target="#poisson-and-exponential-regression-models">646. Poisson and Exponential Regression Models</a></li>
  <li><a href="#multinomial-and-ordinal-regression" id="toc-multinomial-and-ordinal-regression" class="nav-link" data-scroll-target="#multinomial-and-ordinal-regression">647. Multinomial and Ordinal Regression</a></li>
  <li><a href="#regularized-linear-models-ridge-lasso-elastic-net" id="toc-regularized-linear-models-ridge-lasso-elastic-net" class="nav-link" data-scroll-target="#regularized-linear-models-ridge-lasso-elastic-net">648. Regularized Linear Models (Ridge, Lasso, Elastic Net)</a></li>
  <li><a href="#interpretability-and-coefficients" id="toc-interpretability-and-coefficients" class="nav-link" data-scroll-target="#interpretability-and-coefficients">649. Interpretability and Coefficients</a></li>
  <li><a href="#applications-across-domains" id="toc-applications-across-domains" class="nav-link" data-scroll-target="#applications-across-domains">650. Applications Across Domains</a></li>
  </ul></li>
  <li><a href="#chapter-66.-kernel-methods-and-svms" id="toc-chapter-66.-kernel-methods-and-svms" class="nav-link" data-scroll-target="#chapter-66.-kernel-methods-and-svms">Chapter 66. Kernel methods and SVMs</a>
  <ul class="collapse">
  <li><a href="#the-kernel-trick-from-linear-to-nonlinear" id="toc-the-kernel-trick-from-linear-to-nonlinear" class="nav-link" data-scroll-target="#the-kernel-trick-from-linear-to-nonlinear">651. The Kernel Trick: From Linear to Nonlinear</a></li>
  <li><a href="#common-kernels-polynomial-rbf-string" id="toc-common-kernels-polynomial-rbf-string" class="nav-link" data-scroll-target="#common-kernels-polynomial-rbf-string">652. Common Kernels (Polynomial, RBF, String)</a></li>
  <li><a href="#support-vector-machines-hard-margin" id="toc-support-vector-machines-hard-margin" class="nav-link" data-scroll-target="#support-vector-machines-hard-margin">653. Support Vector Machines: Hard Margin</a></li>
  <li><a href="#soft-margin-and-slack-variables" id="toc-soft-margin-and-slack-variables" class="nav-link" data-scroll-target="#soft-margin-and-slack-variables">654. Soft Margin and Slack Variables</a></li>
  <li><a href="#dual-formulation-and-optimization" id="toc-dual-formulation-and-optimization" class="nav-link" data-scroll-target="#dual-formulation-and-optimization">655. Dual Formulation and Optimization</a></li>
  <li><a href="#kernel-ridge-regression" id="toc-kernel-ridge-regression" class="nav-link" data-scroll-target="#kernel-ridge-regression">656. Kernel Ridge Regression</a></li>
  <li><a href="#svms-for-regression-svr" id="toc-svms-for-regression-svr" class="nav-link" data-scroll-target="#svms-for-regression-svr">657. SVMs for Regression (SVR)</a></li>
  <li><a href="#large-scale-kernel-learning-and-approximations" id="toc-large-scale-kernel-learning-and-approximations" class="nav-link" data-scroll-target="#large-scale-kernel-learning-and-approximations">658. Large-Scale Kernel Learning and Approximations</a></li>
  <li><a href="#interpretability-and-limitations-of-kernels" id="toc-interpretability-and-limitations-of-kernels" class="nav-link" data-scroll-target="#interpretability-and-limitations-of-kernels">659. Interpretability and Limitations of Kernels</a></li>
  <li><a href="#beyond-svms-kernelized-deep-architectures" id="toc-beyond-svms-kernelized-deep-architectures" class="nav-link" data-scroll-target="#beyond-svms-kernelized-deep-architectures">660. Beyond SVMs: Kernelized Deep Architectures</a></li>
  </ul></li>
  <li><a href="#chapter-67.-trees-random-forests-gradient-boosting" id="toc-chapter-67.-trees-random-forests-gradient-boosting" class="nav-link" data-scroll-target="#chapter-67.-trees-random-forests-gradient-boosting">Chapter 67. Trees, random forests, gradient boosting</a>
  <ul class="collapse">
  <li><a href="#decision-trees-splits-impurity-and-pruning" id="toc-decision-trees-splits-impurity-and-pruning" class="nav-link" data-scroll-target="#decision-trees-splits-impurity-and-pruning">661. Decision Trees: Splits, Impurity, and Pruning</a></li>
  <li><a href="#cart-vs.-id3-vs.-c4.5-algorithms" id="toc-cart-vs.-id3-vs.-c4.5-algorithms" class="nav-link" data-scroll-target="#cart-vs.-id3-vs.-c4.5-algorithms">662. CART vs.&nbsp;ID3 vs.&nbsp;C4.5 Algorithms</a></li>
  <li><a href="#bagging-and-the-random-forest-idea" id="toc-bagging-and-the-random-forest-idea" class="nav-link" data-scroll-target="#bagging-and-the-random-forest-idea">663. Bagging and the Random Forest Idea</a></li>
  <li><a href="#feature-importance-and-interpretability" id="toc-feature-importance-and-interpretability" class="nav-link" data-scroll-target="#feature-importance-and-interpretability">664. Feature Importance and Interpretability</a></li>
  <li><a href="#gradient-boosted-trees-gbdt-framework" id="toc-gradient-boosted-trees-gbdt-framework" class="nav-link" data-scroll-target="#gradient-boosted-trees-gbdt-framework">665. Gradient Boosted Trees (GBDT) Framework</a></li>
  <li><a href="#boosting-algorithms-adaboost-xgboost-lightgbm" id="toc-boosting-algorithms-adaboost-xgboost-lightgbm" class="nav-link" data-scroll-target="#boosting-algorithms-adaboost-xgboost-lightgbm">666. Boosting Algorithms: AdaBoost, XGBoost, LightGBM</a></li>
  <li><a href="#regularization-in-tree-ensembles" id="toc-regularization-in-tree-ensembles" class="nav-link" data-scroll-target="#regularization-in-tree-ensembles">667. Regularization in Tree Ensembles</a></li>
  <li><a href="#handling-imbalanced-data-with-trees" id="toc-handling-imbalanced-data-with-trees" class="nav-link" data-scroll-target="#handling-imbalanced-data-with-trees">668. Handling Imbalanced Data with Trees</a></li>
  <li><a href="#scalability-and-parallelization" id="toc-scalability-and-parallelization" class="nav-link" data-scroll-target="#scalability-and-parallelization">669. Scalability and Parallelization</a></li>
  <li><a href="#real-world-applications-of-tree-ensembles" id="toc-real-world-applications-of-tree-ensembles" class="nav-link" data-scroll-target="#real-world-applications-of-tree-ensembles">670. Real-World Applications of Tree Ensembles</a></li>
  </ul></li>
  <li><a href="#chapter-68.-feature-selection-and-dimensionality-reduction" id="toc-chapter-68.-feature-selection-and-dimensionality-reduction" class="nav-link" data-scroll-target="#chapter-68.-feature-selection-and-dimensionality-reduction">Chapter 68. Feature selection and dimensionality reduction</a>
  <ul class="collapse">
  <li><a href="#the-curse-of-dimensionality" id="toc-the-curse-of-dimensionality" class="nav-link" data-scroll-target="#the-curse-of-dimensionality">671. The Curse of Dimensionality</a></li>
  <li><a href="#filter-methods-correlation-mutual-information" id="toc-filter-methods-correlation-mutual-information" class="nav-link" data-scroll-target="#filter-methods-correlation-mutual-information">672. Filter Methods (Correlation, Mutual Information)</a></li>
  <li><a href="#wrapper-methods-and-search-strategies" id="toc-wrapper-methods-and-search-strategies" class="nav-link" data-scroll-target="#wrapper-methods-and-search-strategies">673. Wrapper Methods and Search Strategies</a></li>
  <li><a href="#embedded-methods-lasso-tree-based" id="toc-embedded-methods-lasso-tree-based" class="nav-link" data-scroll-target="#embedded-methods-lasso-tree-based">674. Embedded Methods (Lasso, Tree-Based)</a></li>
  <li><a href="#principal-component-analysis-pca" id="toc-principal-component-analysis-pca" class="nav-link" data-scroll-target="#principal-component-analysis-pca">675. Principal Component Analysis (PCA)</a></li>
  <li><a href="#linear-discriminant-analysis-lda" id="toc-linear-discriminant-analysis-lda" class="nav-link" data-scroll-target="#linear-discriminant-analysis-lda">676. Linear Discriminant Analysis (LDA)</a></li>
  <li><a href="#nonlinear-methods-t-sne-umap" id="toc-nonlinear-methods-t-sne-umap" class="nav-link" data-scroll-target="#nonlinear-methods-t-sne-umap">677. Nonlinear Methods: t-SNE, UMAP</a></li>
  <li><a href="#autoencoders-for-dimension-reduction" id="toc-autoencoders-for-dimension-reduction" class="nav-link" data-scroll-target="#autoencoders-for-dimension-reduction">678. Autoencoders for Dimension Reduction</a></li>
  <li><a href="#feature-selection-vs.-feature-extraction" id="toc-feature-selection-vs.-feature-extraction" class="nav-link" data-scroll-target="#feature-selection-vs.-feature-extraction">679. Feature Selection vs.&nbsp;Feature Extraction</a></li>
  <li><a href="#practical-guidelines-and-tradeoffs" id="toc-practical-guidelines-and-tradeoffs" class="nav-link" data-scroll-target="#practical-guidelines-and-tradeoffs">680. Practical Guidelines and Tradeoffs</a></li>
  </ul></li>
  <li><a href="#chapter-69.-imbalanced-data-and-cost-sensitive-learning" id="toc-chapter-69.-imbalanced-data-and-cost-sensitive-learning" class="nav-link" data-scroll-target="#chapter-69.-imbalanced-data-and-cost-sensitive-learning">Chapter 69. Imbalanced data and cost-sensitive learning</a>
  <ul class="collapse">
  <li><a href="#the-problem-of-skewed-class-distributions" id="toc-the-problem-of-skewed-class-distributions" class="nav-link" data-scroll-target="#the-problem-of-skewed-class-distributions">681. The Problem of Skewed Class Distributions</a></li>
  <li><a href="#sampling-methods-undersampling-and-oversampling" id="toc-sampling-methods-undersampling-and-oversampling" class="nav-link" data-scroll-target="#sampling-methods-undersampling-and-oversampling">682. Sampling Methods: Undersampling and Oversampling</a></li>
  <li><a href="#smote-and-synthetic-oversampling-variants" id="toc-smote-and-synthetic-oversampling-variants" class="nav-link" data-scroll-target="#smote-and-synthetic-oversampling-variants">683. SMOTE and Synthetic Oversampling Variants</a></li>
  <li><a href="#cost-sensitive-loss-functions" id="toc-cost-sensitive-loss-functions" class="nav-link" data-scroll-target="#cost-sensitive-loss-functions">684. Cost-Sensitive Loss Functions</a></li>
  <li><a href="#threshold-adjustment-and-roc-curves" id="toc-threshold-adjustment-and-roc-curves" class="nav-link" data-scroll-target="#threshold-adjustment-and-roc-curves">685. Threshold Adjustment and ROC Curves</a></li>
  <li><a href="#evaluation-metrics-for-imbalanced-data-f1-auc-pr" id="toc-evaluation-metrics-for-imbalanced-data-f1-auc-pr" class="nav-link" data-scroll-target="#evaluation-metrics-for-imbalanced-data-f1-auc-pr">686. Evaluation Metrics for Imbalanced Data (F1, AUC, PR)</a></li>
  <li><a href="#one-class-and-rare-event-detection" id="toc-one-class-and-rare-event-detection" class="nav-link" data-scroll-target="#one-class-and-rare-event-detection">687. One-Class and Rare Event Detection</a></li>
  <li><a href="#ensemble-methods-for-imbalanced-learning" id="toc-ensemble-methods-for-imbalanced-learning" class="nav-link" data-scroll-target="#ensemble-methods-for-imbalanced-learning">688. Ensemble Methods for Imbalanced Learning</a></li>
  <li><a href="#real-world-case-studies-fraud-medical-fault-detection" id="toc-real-world-case-studies-fraud-medical-fault-detection" class="nav-link" data-scroll-target="#real-world-case-studies-fraud-medical-fault-detection">689. Real-World Case Studies (Fraud, Medical, Fault Detection)</a></li>
  <li><a href="#challenges-and-open-questions" id="toc-challenges-and-open-questions" class="nav-link" data-scroll-target="#challenges-and-open-questions">690. Challenges and Open Questions</a></li>
  </ul></li>
  <li><a href="#chapter-70.-evaluation-error-analysis-and-debugging" id="toc-chapter-70.-evaluation-error-analysis-and-debugging" class="nav-link" data-scroll-target="#chapter-70.-evaluation-error-analysis-and-debugging">Chapter 70. Evaluation, error analysis, and debugging</a>
  <ul class="collapse">
  <li><a href="#beyond-accuracy-precision-recall-f1-auc" id="toc-beyond-accuracy-precision-recall-f1-auc" class="nav-link" data-scroll-target="#beyond-accuracy-precision-recall-f1-auc">691. Beyond Accuracy: Precision, Recall, F1, AUC</a></li>
  <li><a href="#calibration-of-probabilistic-predictions" id="toc-calibration-of-probabilistic-predictions" class="nav-link" data-scroll-target="#calibration-of-probabilistic-predictions">692. Calibration of Probabilistic Predictions</a></li>
  <li><a href="#error-analysis-techniques" id="toc-error-analysis-techniques" class="nav-link" data-scroll-target="#error-analysis-techniques">693. Error Analysis Techniques</a></li>
  <li><a href="#bias-variance-and-error-decomposition" id="toc-bias-variance-and-error-decomposition" class="nav-link" data-scroll-target="#bias-variance-and-error-decomposition">694. Bias, Variance, and Error Decomposition</a></li>
  <li><a href="#debugging-data-issues" id="toc-debugging-data-issues" class="nav-link" data-scroll-target="#debugging-data-issues">695. Debugging Data Issues</a></li>
  <li><a href="#debugging-model-issues" id="toc-debugging-model-issues" class="nav-link" data-scroll-target="#debugging-model-issues">696. Debugging Model Issues</a></li>
  <li><a href="#explainability-tools-in-error-analysis" id="toc-explainability-tools-in-error-analysis" class="nav-link" data-scroll-target="#explainability-tools-in-error-analysis">697. Explainability Tools in Error Analysis</a></li>
  <li><a href="#human-in-the-loop-debugging" id="toc-human-in-the-loop-debugging" class="nav-link" data-scroll-target="#human-in-the-loop-debugging">698. Human-in-the-Loop Debugging</a></li>
  <li><a href="#evaluation-under-distribution-shift" id="toc-evaluation-under-distribution-shift" class="nav-link" data-scroll-target="#evaluation-under-distribution-shift">699. Evaluation under Distribution Shift</a></li>
  <li><a href="#best-practices-and-case-studies" id="toc-best-practices-and-case-studies" class="nav-link" data-scroll-target="#best-practices-and-case-studies">700. Best Practices and Case Studies</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Volume 7. Machine Learning Theory and Practice</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Little</span> model learns,</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">mistakes</span> pile like building blocks,</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="ex">oops</span> becomes wisdom.</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<section id="chapter-61.-hyphothesis-spaces-bias-and-capacity" class="level2">
<h2 class="anchored" data-anchor-id="chapter-61.-hyphothesis-spaces-bias-and-capacity">Chapter 61. Hyphothesis spaces, bias and capacity</h2>
<section id="hypotheses-as-functions-and-mappings" class="level3">
<h3 class="anchored" data-anchor-id="hypotheses-as-functions-and-mappings">601. Hypotheses as Functions and Mappings</h3>
<p>At its core, a hypothesis in machine learning is a function. It maps inputs (features) to outputs (labels, predictions). The collection of all functions a learner might consider forms the hypothesis space. This framing lets us treat learning as the process of selecting one function from a vast set of possible mappings.</p>
<section id="picture-in-your-head" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head">Picture in Your Head</h4>
<p>Imagine a giant library of books, each book representing one possible function that explains your data. When you train a model, you’re browsing that library, searching for the book whose story best matches your dataset. The hypothesis space is the library itself.</p>
</section>
<section id="deep-dive" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive">Deep Dive</h4>
<p>Functions in the hypothesis space can be simple or complex. A linear model restricts the space to straight-line boundaries in feature space, while a deep neural network opens up a near-infinite set of nonlinear possibilities. The richness of the space dictates how flexible the model can be. Too small a space, and no function fits the data well. Too large, and many functions fit, but you risk overfitting.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 33%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>Model Type</th>
<th>Hypothesis Form</th>
<th>Space Characteristics</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear Regression</td>
<td><span class="math inline">\(h(x) = w^Tx + b\)</span></td>
<td>Limited, interpretable, simple</td>
</tr>
<tr class="even">
<td>Decision Tree</td>
<td>Branching rules</td>
<td>Flexible, discrete, piecewise constant</td>
</tr>
<tr class="odd">
<td>Neural Network</td>
<td>Composed nonlinear functions</td>
<td>Extremely large, highly expressive</td>
</tr>
</tbody>
</table>
<p>The hypothesis-as-function perspective also connects learning to mathematics: choosing hypotheses is equivalent to restricting the search domain over mappings from inputs to outputs. This restriction (the inductive bias) is what makes generalization possible.</p>
</section>
<section id="tiny-code" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># toy dataset</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">1</span>], [<span class="dv">2</span>], [<span class="dv">3</span>], [<span class="dv">4</span>]])</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">8</span>])  <span class="co"># perfect linear mapping</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># hypothesis: linear function</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression()</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Hypothesis function: y ="</span>, model.coef_[<span class="dv">0</span>], <span class="st">"* x +"</span>, model.intercept_)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Prediction for x=5:"</span>, model.predict([[<span class="dv">5</span>]])[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters">Why it Matters</h4>
<p>Viewing hypotheses as functions grounds machine learning in a precise framework: every model is an approximation of the true input–output mapping. This helps clarify the tradeoffs between model complexity, generalization, and interpretability. It’s the foundation upon which all later theory—capacity, bias-variance, generalization bounds—is built.</p>
</section>
<section id="try-it-yourself" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself">Try It Yourself</h4>
<ol type="1">
<li>Construct a simple dataset where the true mapping is quadratic (e.g., <span class="math inline">\(y = x^2\)</span>). Train a linear model and a polynomial model. Which hypothesis space better matches the data?</li>
<li>In scikit-learn, try <code>LinearRegression</code> vs.&nbsp;<code>DecisionTreeRegressor</code> on the same dataset. Observe how the choice of hypothesis space changes the model’s behavior.</li>
<li>Think about real-world examples: if you want to predict house prices, what kind of hypothesis function might make sense? Linear? Tree-based? Neural? Why?</li>
</ol>
</section>
</section>
<section id="the-space-of-all-possible-hypotheses" class="level3">
<h3 class="anchored" data-anchor-id="the-space-of-all-possible-hypotheses">602. The Space of All Possible Hypotheses</h3>
<p>The hypothesis space is the complete set of functions a learning algorithm can explore. It defines the boundaries of what a model is capable of learning. If the true mapping lies outside this space, no amount of training can recover it. The richness of this space determines both the potential and the limitations of a model class.</p>
<section id="picture-in-your-head-1" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-1">Picture in Your Head</h4>
<p>Imagine a map of all possible roads from a city to its destination. Some maps only include highways (linear models), while others include winding alleys and shortcuts (nonlinear models). The hypothesis space is that map: it constrains which paths you’re even allowed to consider.</p>
</section>
<section id="deep-dive-1" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-1">Deep Dive</h4>
<p>The size and shape of the hypothesis space vary by model family:</p>
<ul>
<li>Finite spaces: A decision stump has a small, countable hypothesis space.</li>
<li>Infinite but structured spaces: Linear models in <span class="math inline">\(\mathbb{R}^n\)</span> form an infinite but geometrically constrained space.</li>
<li>Infinite, unstructured spaces: Neural networks with sufficient depth approximate nearly any function, creating a hypothesis space that is vast and highly expressive.</li>
</ul>
<p>Mathematically, if <span class="math inline">\(X\)</span> is the input domain and <span class="math inline">\(Y\)</span> the output domain, then the universal hypothesis space is <span class="math inline">\(Y^X\)</span>, all possible mappings from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span>. Practical learning algorithms constrain this universal space to a manageable subset, which defines the inductive bias of the learner.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 29%">
<col style="width: 23%">
<col style="width: 18%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th>Hypothesis Space</th>
<th>Example Model</th>
<th>Expressivity</th>
<th>Risk</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Small, finite</td>
<td>Decision stumps</td>
<td>Low</td>
<td>Underfitting</td>
</tr>
<tr class="even">
<td>Medium, structured</td>
<td>Linear models</td>
<td>Moderate</td>
<td>Limited flexibility</td>
</tr>
<tr class="odd">
<td>Large, unstructured</td>
<td>Deep networks</td>
<td>Very high</td>
<td>Overfitting</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-1" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-1">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># data: nonlinear relationship</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">20</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> X.ravel()<span class="dv">2</span> <span class="op">+</span> np.random.randn(<span class="dv">20</span>) <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># linear hypothesis space</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>lin <span class="op">=</span> LinearRegression().fit(X, y)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># quadratic hypothesis space</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>poly <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>X_poly <span class="op">=</span> poly.fit_transform(X)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>quad <span class="op">=</span> LinearRegression().fit(X_poly, y)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Linear space prediction at x=6:"</span>, lin.predict([[<span class="dv">6</span>]])[<span class="dv">0</span>])</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Quadratic space prediction at x=6:"</span>, quad.predict(poly.transform([[<span class="dv">6</span>]]))[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-1" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-1">Why it Matters</h4>
<p>Understanding hypothesis spaces reveals why some models fail despite good optimization: the true mapping simply doesn’t exist in the space they search. It also explains the tradeoff between simplicity and flexibility—constraining the space promotes generalization but risks missing patterns, while enlarging the space enables expressivity but risks memorization.</p>
</section>
<section id="try-it-yourself-1" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-1">Try It Yourself</h4>
<ol type="1">
<li>Generate a sine-wave dataset and train both a linear regression and a polynomial regression. Which hypothesis space better approximates the true function?</li>
<li>Compare the performance of a shallow decision tree versus a deep one on the same dataset. How does expanding the hypothesis space affect the fit?</li>
<li>Reflect on real applications: for classifying emails as spam, what hypothesis space is “big enough” without being too big?</li>
</ol>
</section>
</section>
<section id="inductive-bias-choosing-among-hypotheses" class="level3">
<h3 class="anchored" data-anchor-id="inductive-bias-choosing-among-hypotheses">603. Inductive Bias: Choosing Among Hypotheses</h3>
<p>Inductive bias is the set of assumptions a learning algorithm makes to prefer one hypothesis over another. Without such bias, a learner cannot generalize beyond the training data. Every model family encodes its own inductive bias—linear models assume straight-line relationships, decision trees assume hierarchical splits, and neural networks assume compositional feature hierarchies.</p>
<section id="picture-in-your-head-2" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-2">Picture in Your Head</h4>
<p>Think of inductive bias like wearing tinted glasses. Red-tinted glasses make everything look reddish; similarly, a linear regression model interprets the world through straight-line boundaries. The bias is not a flaw—it’s what makes learning possible from limited data.</p>
</section>
<section id="deep-dive-2" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-2">Deep Dive</h4>
<p>Since data alone cannot determine the “true” function (many functions can fit a finite dataset), bias acts as a tie-breaker.</p>
<ul>
<li>Restrictive bias (e.g., linear models) makes learning easier but may miss complex patterns.</li>
<li>Flexible bias (e.g., deep nets) can approximate more but require more data to constrain.</li>
<li>No bias (the universal hypothesis space) means no ability to generalize, as any unseen point could map to any label.</li>
</ul>
<p>Formally, if multiple hypotheses yield equal empirical risk, the inductive bias determines which is selected. This connects to Occam’s Razor: prefer simpler hypotheses that explain the data.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 38%">
<col style="width: 42%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Inductive Bias</th>
<th>Implication</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear regression</td>
<td>Outputs are linear in inputs</td>
<td>Works well if relationships are simple</td>
</tr>
<tr class="even">
<td>Decision tree</td>
<td>Recursive if-then rules</td>
<td>Captures interactions, may overfit</td>
</tr>
<tr class="odd">
<td>CNN</td>
<td>Locality and translation invariance</td>
<td>Ideal for images</td>
</tr>
<tr class="even">
<td>RNN</td>
<td>Sequential dependence</td>
<td>Fits language, time-series</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-2" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-2">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># nonlinear data</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">20</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.sin(X).ravel()</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># linear bias</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>lin <span class="op">=</span> LinearRegression().fit(X, y)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co"># tree bias</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>tree <span class="op">=</span> DecisionTreeRegressor(max_depth<span class="op">=</span><span class="dv">3</span>).fit(X, y)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Linear prediction at x=2.5:"</span>, lin.predict([[<span class="fl">2.5</span>]])[<span class="dv">0</span>])</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Tree prediction at x=2.5:"</span>, tree.predict([[<span class="fl">2.5</span>]])[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-2" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-2">Why it Matters</h4>
<p>Bias explains why no single algorithm works best across all tasks (the “No Free Lunch” theorem). Choosing the right inductive bias means aligning model assumptions with the problem’s underlying structure. This alignment is what turns data into meaningful generalization instead of memorization.</p>
</section>
<section id="try-it-yourself-2" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-2">Try It Yourself</h4>
<ol type="1">
<li>Train a linear model and a small decision tree on sinusoidal data. Compare the predictions. Which bias aligns better with the true function?</li>
<li>Explore convolutional neural networks vs.&nbsp;fully connected networks on images. How does the convolutional inductive bias exploit image structure?</li>
<li>Think of real-world problems: for predicting stock trends, what inductive bias might be useful? For predicting protein folding, which might fail?</li>
</ol>
</section>
</section>
<section id="capacity-and-expressivity-of-models" class="level3">
<h3 class="anchored" data-anchor-id="capacity-and-expressivity-of-models">604. Capacity and Expressivity of Models</h3>
<p>Capacity measures how complex a set of functions a model class can represent. Expressivity is the richness of those functions: how well they capture patterns of varying complexity. A model with low capacity may underfit, while a model with very high capacity risks memorizing data without generalizing.</p>
<section id="picture-in-your-head-3" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-3">Picture in Your Head</h4>
<p>Imagine jars of different sizes used to collect rainwater. A small jar (low-capacity model) quickly overflows and misses most of the rain. A giant barrel (high-capacity model) can capture every drop, but it might also collect debris. The right capacity balances coverage with clarity.</p>
</section>
<section id="deep-dive-3" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-3">Deep Dive</h4>
<p>Capacity is influenced by parameters, architecture, and constraints:</p>
<ul>
<li>Linear models: Low capacity, limited to hyperplanes.</li>
<li>Polynomial models: Higher capacity as degree increases.</li>
<li>Neural networks: Extremely high capacity with sufficient width/depth.</li>
</ul>
<p>Mathematically, capacity relates to measures like VC dimension or Rademacher complexity, which describe how many different patterns a hypothesis class can fit. Expressivity reflects qualitative ability: decision trees capture discrete interactions, while CNNs capture translation-invariant features.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 15%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>Model Class</th>
<th>Capacity</th>
<th>Expressivity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear regression</td>
<td>Low</td>
<td>Only linear boundaries</td>
</tr>
<tr class="even">
<td>Polynomial regression (degree n)</td>
<td>Moderate–High</td>
<td>Increasingly complex curves</td>
</tr>
<tr class="odd">
<td>Deep networks</td>
<td>Very High</td>
<td>Universal function approximators</td>
</tr>
<tr class="even">
<td>Random forest</td>
<td>High</td>
<td>Captures nonlinearity and interactions</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-3" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-3">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># generate data</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">30</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.sin(X).ravel() <span class="op">+</span> np.random.randn(<span class="dv">30</span>) <span class="op">*</span> <span class="fl">0.2</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># fit polynomial models with different capacities</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> degree <span class="kw">in</span> [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">9</span>]:</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    poly <span class="op">=</span> PolynomialFeatures(degree)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    X_poly <span class="op">=</span> poly.fit_transform(X)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> LinearRegression().fit(X_poly, y)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    plt.plot(X, model.predict(X_poly), label<span class="op">=</span><span class="ss">f"degree </span><span class="sc">{</span>degree<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>plt.scatter(X, y, color<span class="op">=</span><span class="st">"black"</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-3" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-3">Why it Matters</h4>
<p>Capacity and expressivity determine whether a model can capture the true signal in data. Too little, and the model fails to represent reality. Too much, and the model memorizes noise. Striking the right balance is the art of model design.</p>
</section>
<section id="try-it-yourself-3" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-3">Try It Yourself</h4>
<ol type="1">
<li>Generate sinusoidal data and fit polynomial models of degree 1, 3, and 15. Observe how capacity influences overfitting.</li>
<li>Compare a shallow vs.&nbsp;deep decision tree on the same dataset. Which has more expressive power?</li>
<li>Consider practical tasks: is predicting housing prices better served by a low-capacity linear model or a high-capacity boosted ensemble?</li>
</ol>
</section>
</section>
<section id="the-biasvariance-tradeoff" class="level3">
<h3 class="anchored" data-anchor-id="the-biasvariance-tradeoff">605. The Bias–Variance Tradeoff</h3>
<p>The bias–variance tradeoff explains why models make errors for two different reasons: bias (systematic error from overly simple assumptions) and variance (sensitivity to noise and fluctuations in training data). Balancing these forces is central to achieving good generalization.</p>
<section id="picture-in-your-head-4" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-4">Picture in Your Head</h4>
<p>Picture shooting arrows at a target.</p>
<ul>
<li>A high-bias archer always misses in the same direction. the shots cluster away from the bullseye.</li>
<li>A high-variance archer’s shots scatter widely. sometimes near the bullseye, sometimes far away.</li>
<li>The ideal archer has both low bias and low variance, consistently hitting close to the center.</li>
</ul>
</section>
<section id="deep-dive-4" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-4">Deep Dive</h4>
<p>Bias comes from restricting the hypothesis space too much. Variance arises when the model adapts too closely to training examples.</p>
<ul>
<li>High bias, low variance: Simple models like linear regression on nonlinear data.</li>
<li>Low bias, high variance: Complex models like deep trees on small datasets.</li>
<li>Low bias, low variance: The sweet spot, often achieved with enough data and regularization.</li>
</ul>
<p>Formally, expected error can be decomposed as:</p>
<p><span class="math display">\[
E[(y - \hat{y})^2] = \text{Bias}^2 + \text{Variance} + \text{Irreducible noise}.
\]</span></p>
<table class="caption-top table">
<colgroup>
<col style="width: 48%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Model Situation</th>
<th>Bias</th>
<th>Variance</th>
<th>Typical Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear model on quadratic data</td>
<td>High</td>
<td>Low</td>
<td>Underfit</td>
</tr>
<tr class="even">
<td>Deep decision tree</td>
<td>Low</td>
<td>High</td>
<td>Overfit</td>
</tr>
<tr class="odd">
<td>Regularized ensemble</td>
<td>Moderate</td>
<td>Moderate</td>
<td>Balanced</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-4" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-4">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># dataset</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">50</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.sin(X).ravel() <span class="op">+</span> np.random.randn(<span class="dv">50</span>) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co"># high bias model</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>lin <span class="op">=</span> LinearRegression().fit(X, y)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>lin_pred <span class="op">=</span> lin.predict(X)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="co"># high variance model</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>tree <span class="op">=</span> DecisionTreeRegressor(max_depth<span class="op">=</span><span class="dv">20</span>).fit(X, y)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>tree_pred <span class="op">=</span> tree.predict(X)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Linear model MSE:"</span>, mean_squared_error(y, lin_pred))</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Deep tree MSE:"</span>, mean_squared_error(y, tree_pred))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-4" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-4">Why it Matters</h4>
<p>Understanding the tradeoff prevents chasing the illusion of a perfect model. Every model faces some combination of bias and variance; the key is finding the balance that minimizes overall error for the problem at hand.</p>
</section>
<section id="try-it-yourself-4" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-4">Try It Yourself</h4>
<ol type="1">
<li>Train linear regression and deep decision trees on the same noisy nonlinear dataset. Compare bias and variance visually.</li>
<li>Experiment with tree depth: how does increasing depth reduce bias but raise variance?</li>
<li>In a real-world task (e.g., predicting stock prices), which error source—bias or variance—do you think dominates?</li>
</ol>
</section>
</section>
<section id="overfitting-vs.-underfitting" class="level3">
<h3 class="anchored" data-anchor-id="overfitting-vs.-underfitting">606. Overfitting vs.&nbsp;Underfitting</h3>
<p>Overfitting occurs when a model captures noise instead of signal, performing well on training data but poorly on unseen data. Underfitting happens when a model is too simple to capture the underlying structure, failing on both training and test data. These are two sides of the same problem: mismatch between model capacity and task complexity.</p>
<section id="picture-in-your-head-5" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-5">Picture in Your Head</h4>
<p>Imagine fitting a curve through a set of points:</p>
<ul>
<li>A straight line across a wavy pattern leaves large gaps (underfitting).</li>
<li>A wild squiggle passing through every point bends unnaturally (overfitting).</li>
<li>The right curve flows smoothly through the points, capturing the pattern but ignoring random noise.</li>
</ul>
</section>
<section id="deep-dive-5" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-5">Deep Dive</h4>
<ul>
<li>Underfitting arises from models with high bias: linear models on nonlinear data, shallow trees, or too much regularization.</li>
<li>Overfitting arises from models with high variance: very deep trees, unregularized neural networks, or too many parameters relative to the data size.</li>
<li>The cure lies in capacity control, regularization, and validation techniques to ensure the model generalizes.</li>
</ul>
<p>Mathematically, error can be visualized as:</p>
<ul>
<li>Training error decreases as capacity increases.</li>
<li>Test error follows a U-shape, dropping at first, then rising once the model starts fitting noise.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 21%">
<col style="width: 15%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Case</th>
<th>Training Error</th>
<th>Test Error</th>
<th>Symptom</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Underfit</td>
<td>High</td>
<td>High</td>
<td>Misses patterns</td>
</tr>
<tr class="even">
<td>Good fit</td>
<td>Low</td>
<td>Low</td>
<td>Captures patterns, ignores noise</td>
</tr>
<tr class="odd">
<td>Overfit</td>
<td>Very Low</td>
<td>High</td>
<td>Memorizes training noise</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-5" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-5">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># data</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">10</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.sin(<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> X).ravel() <span class="op">+</span> np.random.randn(<span class="dv">10</span>) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co"># underfit (degree=1), good fit (degree=3), overfit (degree=9)</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>degrees <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">9</span>]</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>plt.scatter(X, y, color<span class="op">=</span><span class="st">"black"</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>X_plot <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> d <span class="kw">in</span> degrees:</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    poly <span class="op">=</span> PolynomialFeatures(d)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    X_poly <span class="op">=</span> poly.fit_transform(X)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> LinearRegression().fit(X_poly, y)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    plt.plot(X_plot, model.predict(poly.fit_transform(X_plot)), label<span class="op">=</span><span class="ss">f"deg </span><span class="sc">{</span>d<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-5" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-5">Why it Matters</h4>
<p>Overfitting and underfitting frame the practical struggle in machine learning. A good model must be flexible enough to capture true patterns but constrained enough to ignore noise. Recognizing these failure modes is essential for building robust systems.</p>
</section>
<section id="try-it-yourself-5" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-5">Try It Yourself</h4>
<ol type="1">
<li>Fit polynomial regressions of increasing degree to noisy sinusoidal data. Watch the transition from underfitting to overfitting.</li>
<li>Adjust the regularization strength in ridge regression and observe how it shifts the model from underfit to overfit.</li>
<li>Reflect on real-world systems: when predicting medical diagnoses, which is riskier—overfitting or underfitting?</li>
</ol>
</section>
</section>
<section id="structural-risk-minimization" class="level3">
<h3 class="anchored" data-anchor-id="structural-risk-minimization">607. Structural Risk Minimization</h3>
<p>Structural Risk Minimization (SRM) is a principle from statistical learning theory that balances model complexity with empirical performance. Instead of only minimizing training error (empirical risk), SRM introduces a hierarchy of hypothesis spaces—simpler to more complex—and selects the one that minimizes a bound on expected risk.</p>
<section id="picture-in-your-head-6" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-6">Picture in Your Head</h4>
<p>Think of buying shoes for a child:</p>
<ul>
<li>Shoes that are too small (underfitting) cause discomfort.</li>
<li>Shoes that are too big (overfitting) make walking unstable.</li>
<li>The best choice balances room for growth with a snug fit. SRM acts like this balancing act, selecting the right “fit” between data and model class.</li>
</ul>
</section>
<section id="deep-dive-6" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-6">Deep Dive</h4>
<p>ERM (Empirical Risk Minimization) chooses the hypothesis <span class="math inline">\(h\)</span> minimizing:</p>
<p><span class="math display">\[
R_{emp}(h) = \frac{1}{n} \sum_{i=1}^n L(h(x_i), y_i).
\]</span></p>
<p>But low empirical risk may not guarantee low true risk. SRM instead minimizes an upper bound:</p>
<p><span class="math display">\[
R(h) \leq R_{emp}(h) + \Omega(H),
\]</span></p>
<p>where <span class="math inline">\(\Omega(H)\)</span> is a complexity penalty depending on the hypothesis space <span class="math inline">\(H\)</span> (e.g., VC dimension).</p>
<p>The learner considers nested hypothesis classes:</p>
<p><span class="math display">\[
H_1 \subset H_2 \subset H_3 \subset \dots
\]</span></p>
<p>and selects the class where the sum of empirical risk and complexity penalty is minimized.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 52%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th>Approach</th>
<th>Focus</th>
<th>Limitation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ERM</td>
<td>Minimizes training error</td>
<td>Risks overfitting</td>
</tr>
<tr class="even">
<td>SRM</td>
<td>Balances training error + complexity</td>
<td>More computational effort</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-6" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-6">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># dataset</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">20</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.sin(<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> X).ravel() <span class="op">+</span> np.random.randn(<span class="dv">20</span>) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co"># compare polynomial degrees with regularization (structural hierarchy)</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> degree <span class="kw">in</span> [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">9</span>]:</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> make_pipeline(PolynomialFeatures(degree), Ridge(alpha<span class="op">=</span><span class="fl">0.1</span>))</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    model.fit(X, y)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Degree </span><span class="sc">{</span>degree<span class="sc">}</span><span class="ss">, Train MSE = </span><span class="sc">{</span>mean_squared_error(y, y_pred)<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-6" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-6">Why it Matters</h4>
<p>SRM provides the theoretical foundation for regularization and model selection. It explains why simply minimizing training error is insufficient and why penalties, validation, and complexity control are essential for building generalizable models.</p>
</section>
<section id="try-it-yourself-6" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-6">Try It Yourself</h4>
<ol type="1">
<li>Generate noisy data and fit polynomials of increasing degree. Compare results with and without regularization.</li>
<li>Explore how increasing Ridge <code>alpha</code> shrinks coefficients, effectively enforcing SRM.</li>
<li>Relate SRM to real-world practice: how do early stopping and cross-validation reflect this principle?</li>
</ol>
</section>
</section>
<section id="occams-razor-in-learning-theory" class="level3">
<h3 class="anchored" data-anchor-id="occams-razor-in-learning-theory">608. Occam’s Razor in Learning Theory</h3>
<p>Occam’s Razor is the principle that, all else being equal, simpler explanations should be preferred over more complex ones. In machine learning, this translates to choosing the simplest hypothesis that adequately fits the data. Simplicity reduces the risk of overfitting and often leads to better generalization.</p>
<section id="picture-in-your-head-7" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-7">Picture in Your Head</h4>
<p>Imagine explaining why the lights went out:</p>
<ul>
<li>A simple explanation: “The bulb burned out.”</li>
<li>A complex explanation: “A squirrel chewed the wire, causing a short, which tripped the breaker, after a voltage surge from the grid.” Both might be true, but the simple explanation is more plausible unless evidence demands the complex one. Machine learning applies the same logic to hypothesis choice.</li>
</ul>
</section>
<section id="deep-dive-7" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-7">Deep Dive</h4>
<p>Theoretical learning bounds reflect Occam’s Razor: simpler hypothesis classes (smaller VC dimension, fewer parameters) require fewer samples to generalize well. Complex hypotheses may explain the training data perfectly but risk poor performance on unseen data.</p>
<p>Mathematically, for a hypothesis space <span class="math inline">\(H\)</span>, generalization error bounds scale with <span class="math inline">\(\log|H|\)</span> (if finite) or with its complexity measure (e.g., VC dimension). Smaller spaces yield tighter bounds.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Hypothesis</th>
<th>Complexity</th>
<th>Risk</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Straight line</td>
<td>Low</td>
<td>May underfit</td>
</tr>
<tr class="even">
<td>Quadratic curve</td>
<td>Moderate</td>
<td>Balanced</td>
</tr>
<tr class="odd">
<td>High-degree polynomial</td>
<td>High</td>
<td>Overfits easily</td>
</tr>
</tbody>
</table>
<p>Occam’s Razor does not mean “always choose the simplest model.” It means prefer simplicity unless a more complex model is demonstrably better at capturing essential structure.</p>
</section>
<section id="tiny-code-7" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-7">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co"># data: quadratic relationship</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">20</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> X.ravel()<span class="dv">2</span> <span class="op">+</span> np.random.randn(<span class="dv">20</span>) <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co"># linear vs quadratic vs 9th degree polynomial</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> {</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Linear"</span>: make_pipeline(PolynomialFeatures(<span class="dv">1</span>), LinearRegression()),</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Quadratic"</span>: make_pipeline(PolynomialFeatures(<span class="dv">2</span>), LinearRegression()),</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"9th degree"</span>: make_pipeline(PolynomialFeatures(<span class="dv">9</span>), LinearRegression())</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, model <span class="kw">in</span> models.items():</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    model.fit(X, y)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss"> model R^2 score: </span><span class="sc">{</span>model<span class="sc">.</span>score(X, y)<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-7" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-7">Why it Matters</h4>
<p>Occam’s Razor underpins practical choices like preferring linear regression before trying deep nets, or using regularization to penalize unnecessary complexity. It keeps learning grounded: the goal isn’t to fit data as tightly as possible, but to generalize well.</p>
</section>
<section id="try-it-yourself-7" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-7">Try It Yourself</h4>
<ol type="1">
<li>Fit linear, quadratic, and high-degree polynomial regressions to noisy quadratic data. Which strikes the best balance?</li>
<li>Experiment with regularization to see how it enforces Occam’s Razor in practice.</li>
<li>Reflect on domains: why do simple baselines (like linear models in tabular data) often perform surprisingly well?</li>
</ol>
</section>
</section>
<section id="complexity-vs.-interpretability" class="level3">
<h3 class="anchored" data-anchor-id="complexity-vs.-interpretability">609. Complexity vs.&nbsp;Interpretability</h3>
<p>As models grow more complex, their internal workings become harder to interpret. Linear models and shallow trees are easily explained, while deep neural networks and ensemble methods act like “black boxes.” Complexity increases predictive power but decreases transparency, creating a tension between performance and interpretability.</p>
<section id="picture-in-your-head-8" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-8">Picture in Your Head</h4>
<p>Imagine different types of maps:</p>
<ul>
<li>A simple sketch map shows major roads—easy to read but lacking detail.</li>
<li>A highly detailed 3D terrain map captures every contour but is overwhelming to interpret. Models behave the same way: simpler ones are easier to explain, while complex ones capture more detail at the cost of clarity.</li>
</ul>
</section>
<section id="deep-dive-8" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-8">Deep Dive</h4>
<ul>
<li>Interpretable models: Linear regression, logistic regression, decision stumps. They offer transparency, coefficient inspection, and human-readable rules.</li>
<li>Complex models: Random forests, gradient boosting, deep neural networks. They achieve higher accuracy but lack direct interpretability.</li>
<li>Bridging methods: Post-hoc techniques like SHAP, LIME, saliency maps help explain black-box predictions, but explanations are approximations, not the true decision process.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 30%">
<col style="width: 15%">
<col style="width: 20%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Complexity</th>
<th>Interpretability</th>
<th>Typical Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear regression</td>
<td>Low</td>
<td>High</td>
<td>Risk scoring, tabular data</td>
</tr>
<tr class="even">
<td>Decision trees (shallow)</td>
<td>Low–Moderate</td>
<td>High</td>
<td>Rules-based systems</td>
</tr>
<tr class="odd">
<td>Random forest</td>
<td>High</td>
<td>Low</td>
<td>Robust tabular prediction</td>
</tr>
<tr class="even">
<td>Deep neural network</td>
<td>Very High</td>
<td>Very Low</td>
<td>Vision, NLP, speech</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-8" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-8">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># toy dataset</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(<span class="dv">100</span>, <span class="dv">1</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">3</span> <span class="op">*</span> X.ravel() <span class="op">+</span> np.random.randn(<span class="dv">100</span>) <span class="op">*</span> <span class="fl">0.2</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co"># interpretable model</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>lin <span class="op">=</span> LinearRegression().fit(X, y)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Linear coef:"</span>, lin.coef_, <span class="st">"Intercept:"</span>, lin.intercept_)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co"># complex model</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>rf <span class="op">=</span> RandomForestRegressor().fit(X, y)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Random forest prediction at X=0.5:"</span>, rf.predict([[<span class="fl">0.5</span>]])[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-8" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-8">Why it Matters</h4>
<p>In critical applications—healthcare, finance, justice—interpretability is as important as accuracy. Stakeholders must understand why a model made a decision. Conversely, in applications like image classification, raw predictive performance may outweigh interpretability. The right balance depends on context.</p>
</section>
<section id="try-it-yourself-8" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-8">Try It Yourself</h4>
<ol type="1">
<li>Train a linear regression and a random forest on the same dataset. Inspect the coefficients vs.&nbsp;feature importances.</li>
<li>Apply SHAP or LIME to explain a black-box model. Compare the explanation with a simple interpretable model.</li>
<li>Consider domains: where would you sacrifice accuracy for interpretability (e.g., medical diagnosis)? Where is accuracy more critical than explanation (e.g., ad click prediction)?</li>
</ol>
</section>
</section>
<section id="case-studies-of-bias-and-capacity-in-practice" class="level3">
<h3 class="anchored" data-anchor-id="case-studies-of-bias-and-capacity-in-practice">610. Case Studies of Bias and Capacity in Practice</h3>
<p>Bias and capacity are not just theoretical—they appear in real-world machine learning applications across industries. Practical systems must navigate underfitting, overfitting, and the tradeoff between model simplicity and expressivity. Case studies illustrate how these principles play out in actual deployments.</p>
<section id="picture-in-your-head-9" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-9">Picture in Your Head</h4>
<p>Think of three cooks:</p>
<ul>
<li>One uses only salt and pepper (high bias, underfits the taste).</li>
<li>Another uses every spice in the kitchen (high variance, overfits the recipe).</li>
<li>The best cook selects just enough seasoning to match the dish (balanced model).</li>
</ul>
</section>
<section id="deep-dive-9" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-9">Deep Dive</h4>
<ul>
<li><p>Medical Diagnosis: Logistic regression is often used for its interpretability, despite higher-bias assumptions. Doctors prefer transparent models, even at the cost of slightly lower accuracy.</p></li>
<li><p>Finance (Fraud Detection): Fraud patterns are complex and evolve quickly. High-capacity ensembles (e.g., gradient boosting, deep nets) outperform simple models but require careful regularization to avoid memorizing noise.</p></li>
<li><p>Computer Vision: Linear classifiers severely underfit. CNNs, with high capacity and built-in inductive biases, excel by balancing expressivity with structural constraints (locality, shared weights).</p></li>
<li><p>Natural Language Processing: Bag-of-words models underfit by ignoring context. Transformers, with enormous capacity, generalize well if trained on massive corpora. Without enough data, though, they overfit.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 23%">
<col style="width: 64%">
</colgroup>
<thead>
<tr class="header">
<th>Domain</th>
<th>Preferred Model</th>
<th>Bias/Capacity Rationale</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Healthcare</td>
<td>Logistic regression</td>
<td>High bias but interpretable</td>
</tr>
<tr class="even">
<td>Finance</td>
<td>Gradient boosting</td>
<td>High capacity, handles evolving patterns</td>
</tr>
<tr class="odd">
<td>Vision</td>
<td>CNNs</td>
<td>Inductive bias, high capacity where data is abundant</td>
</tr>
<tr class="even">
<td>NLP</td>
<td>Transformers</td>
<td>Extremely high capacity, effective at scale</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-9" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-9">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingClassifier</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># synthetic fraud-like data</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">500</span>, n_features<span class="op">=</span><span class="dv">20</span>, weights<span class="op">=</span>[<span class="fl">0.9</span>, <span class="fl">0.1</span>])</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># high-bias model</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>logreg <span class="op">=</span> LogisticRegression(max_iter<span class="op">=</span><span class="dv">1000</span>).fit(X, y)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"LogReg accuracy:"</span>, logreg.score(X, y))</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co"># high-capacity model</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>gb <span class="op">=</span> GradientBoostingClassifier().fit(X, y)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"GB accuracy:"</span>, gb.score(X, y))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-9" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-9">Why it Matters</h4>
<p>Case studies show that there is no one-size-fits-all solution. In practice, the “best” model depends on domain constraints: interpretability, risk tolerance, and data availability. The theory of bias and capacity guides practitioners in selecting and tuning models for each scenario.</p>
</section>
<section id="try-it-yourself-9" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-9">Try It Yourself</h4>
<ol type="1">
<li>On a tabular dataset, compare logistic regression and gradient boosting. Observe bias vs.&nbsp;capacity tradeoffs.</li>
<li>Train a CNN and a logistic regression on an image dataset (e.g., MNIST). Compare accuracy and interpretability.</li>
<li>Reflect on your own domain: is transparency more critical than raw performance, or the other way around?</li>
</ol>
</section>
</section>
</section>
<section id="chapter-62.-generalization-vc-rademacher-pac" class="level2">
<h2 class="anchored" data-anchor-id="chapter-62.-generalization-vc-rademacher-pac">Chapter 62. Generalization, VC, Rademacher, PAC</h2>
<section id="generalization-as-out-of-sample-performance" class="level3">
<h3 class="anchored" data-anchor-id="generalization-as-out-of-sample-performance">611. Generalization as Out-of-Sample Performance</h3>
<p>Generalization is the ability of a model to perform well on unseen data, not just the training set. It captures the essence of learning: moving beyond memorization toward discovering patterns that hold in the broader population.</p>
<section id="picture-in-your-head-10" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-10">Picture in Your Head</h4>
<p>Imagine a student preparing for an exam.</p>
<ul>
<li>A student who memorizes past questions performs well only if the exact same questions appear (overfit).</li>
<li>A student who understands the concepts can solve new questions they’ve never seen (generalization).</li>
</ul>
</section>
<section id="deep-dive-10" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-10">Deep Dive</h4>
<p>Generalization error is the difference between performance on training data and performance on test data. It depends on:</p>
<ul>
<li>Hypothesis space size: Larger spaces risk overfitting.</li>
<li>Sample size: More data reduces variance and improves generalization.</li>
<li>Noise level: High noise in data sets a lower bound on achievable accuracy.</li>
<li>Regularization and validation: Techniques to constrain fitting and measure out-of-sample behavior.</li>
</ul>
<p>Mathematically, if <span class="math inline">\(R(h)\)</span> is the true risk and <span class="math inline">\(R_{emp}(h)\)</span> is empirical risk:</p>
<p><span class="math display">\[
\text{Generalization gap} = R(h) - R_{emp}(h).
\]</span></p>
<p>Good learning algorithms minimize this gap rather than just <span class="math inline">\(R_{emp}(h)\)</span>.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Factor</th>
<th>Effect on Generalization</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Larger training data</td>
<td>Narrows gap</td>
</tr>
<tr class="even">
<td>Simpler hypothesis space</td>
<td>Reduces overfitting</td>
</tr>
<tr class="odd">
<td>More noise in data</td>
<td>Increases irreducible error</td>
</tr>
<tr class="even">
<td>Proper validation</td>
<td>Detects poor generalization</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-10" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-10">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co"># synthetic dataset</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(<span class="dv">200</span>, <span class="dv">5</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (X[:, <span class="dv">0</span>] <span class="op">+</span> X[:, <span class="dv">1</span>] <span class="op">&gt;</span> <span class="dv">1</span>).astype(<span class="bu">int</span>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co"># train/test split</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="co"># overfit-prone model</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>tree <span class="op">=</span> DecisionTreeClassifier(max_depth<span class="op">=</span><span class="va">None</span>).fit(X_train, y_train)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Train accuracy:"</span>, accuracy_score(y_train, tree.predict(X_train)))</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test accuracy :"</span>, accuracy_score(y_test, tree.predict(X_test)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-10" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-10">Why it Matters</h4>
<p>Generalization is the ultimate goal: models are rarely deployed to predict on their training set. Overfitting undermines real-world usefulness, while underfitting prevents capturing meaningful structure. Understanding and measuring generalization ensures AI systems stay reliable outside the lab.</p>
</section>
<section id="try-it-yourself-10" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-10">Try It Yourself</h4>
<ol type="1">
<li>Train decision trees of varying depth and compare training vs.&nbsp;test accuracy. How does generalization change?</li>
<li>Use k-fold cross-validation to estimate generalization performance. Compare it with a simple train/test split.</li>
<li>Consider real-world tasks: would you trust a model that achieves 99% training accuracy but only 60% test accuracy?</li>
</ol>
</section>
</section>
<section id="the-law-of-large-numbers-and-convergence" class="level3">
<h3 class="anchored" data-anchor-id="the-law-of-large-numbers-and-convergence">612. The Law of Large Numbers and Convergence</h3>
<p>The Law of Large Numbers (LLN) states that as the number of samples increases, the sample average converges to the true expectation. In machine learning, this means that with enough data, empirical measures (like training error) approximate the true population quantities, enabling reliable generalization.</p>
<section id="picture-in-your-head-11" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-11">Picture in Your Head</h4>
<p>Imagine flipping a coin.</p>
<ul>
<li>With 5 flips, you might see 4 heads and 1 tail (80% heads).</li>
<li>With 1000 flips, the ratio approaches 50%. In the same way, as the dataset grows, the behavior observed in training converges to the underlying distribution.</li>
</ul>
</section>
<section id="deep-dive-11" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-11">Deep Dive</h4>
<p>There are two main versions:</p>
<ul>
<li>Weak Law of Large Numbers: Sample averages converge in probability to the true mean.</li>
<li>Strong Law of Large Numbers: Sample averages converge almost surely to the true mean.</li>
</ul>
<p>In ML terms:</p>
<ul>
<li>Small datasets → high variance, unstable estimates.</li>
<li>Large datasets → stable estimates, smaller generalization gap.</li>
</ul>
<p>If <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are i.i.d. random variables with expectation <span class="math inline">\(\mu\)</span>, then:</p>
<p><span class="math display">\[
\frac{1}{n}\sum_{i=1}^n X_i \xrightarrow{n \to \infty} \mu.
\]</span></p>
<table class="caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 29%">
<col style="width: 42%">
</colgroup>
<thead>
<tr class="header">
<th>Dataset Size</th>
<th>Variance of Estimate</th>
<th>Reliability of Generalization</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Small (n=10)</td>
<td>High</td>
<td>Poor generalization</td>
</tr>
<tr class="even">
<td>Medium (n=1000)</td>
<td>Lower</td>
<td>Better</td>
</tr>
<tr class="odd">
<td>Large (n=1,000,000)</td>
<td>Very low</td>
<td>Stable and robust</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-11" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-11">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>true_mean <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>coin <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, true_mean, size<span class="op">=</span><span class="dv">100000</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> [<span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">1000</span>, <span class="dv">10000</span>]:</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    sample_mean <span class="op">=</span> coin[:n].mean()</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"n=</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">, sample mean=</span><span class="sc">{</span>sample_mean<span class="sc">:.3f}</span><span class="ss">, true mean=</span><span class="sc">{</span>true_mean<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-11" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-11">Why it Matters</h4>
<p>LLN provides the foundation for why more data leads to better learning. It reassures us that with sufficient examples, empirical performance reflects true performance. This is the backbone of cross-validation, estimation, and statistical guarantees in ML.</p>
</section>
<section id="try-it-yourself-11" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-11">Try It Yourself</h4>
<ol type="1">
<li>Simulate coin flips with different sample sizes. Watch how the sample proportion converges to the true probability.</li>
<li>Train a classifier with increasing dataset sizes. How does test accuracy stabilize?</li>
<li>Reflect: in domains like medicine, where data is scarce, how does the lack of LLN effects limit model reliability?</li>
</ol>
</section>
</section>
<section id="vc-dimension-definition-and-intuition" class="level3">
<h3 class="anchored" data-anchor-id="vc-dimension-definition-and-intuition">613. VC Dimension: Definition and Intuition</h3>
<p>The Vapnik–Chervonenkis (VC) dimension measures the capacity of a hypothesis space. Formally, it is the maximum number of points that can be shattered (i.e., perfectly classified in all possible labelings) by hypotheses in the space. A higher VC dimension means greater expressive power but also greater risk of overfitting.</p>
<section id="picture-in-your-head-12" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-12">Picture in Your Head</h4>
<p>Imagine placing points on a sheet of paper and drawing shapes around them.</p>
<ul>
<li>A straight line in 2D can separate up to 3 points in all possible ways, but not 4.</li>
<li>A circle can shatter 4 points but not 5. The VC dimension captures this ability to “flex” around data.</li>
</ul>
</section>
<section id="deep-dive-12" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-12">Deep Dive</h4>
<ul>
<li><p>Shattering: A set of points is shattered by a hypothesis class if, for every possible assignment of labels to those points, there exists a hypothesis that classifies them correctly.</p></li>
<li><p>Examples:</p>
<ul>
<li>Threshold functions on a line: VC = 1.</li>
<li>Intervals on a line: VC = 2.</li>
<li>Linear classifiers in 2D: VC = 3.</li>
<li>Linear classifiers in d dimensions: VC = d+1.</li>
</ul></li>
</ul>
<p>The VC dimension links capacity with sample complexity:</p>
<p><span class="math display">\[
n \geq \frac{1}{\epsilon}\left( VC(H)\log\frac{1}{\epsilon} + \log\frac{1}{\delta} \right)
\]</span></p>
<p>samples are needed to learn within error <span class="math inline">\(\epsilon\)</span> and confidence <span class="math inline">\(1-\delta\)</span>.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 16%">
<col style="width: 60%">
</colgroup>
<thead>
<tr class="header">
<th>Hypothesis Class</th>
<th>VC Dimension</th>
<th>Implication</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Threshold on line</td>
<td>1</td>
<td>Can separate 1 point arbitrarily</td>
</tr>
<tr class="even">
<td>Intervals on line</td>
<td>2</td>
<td>Can separate any 2 points</td>
</tr>
<tr class="odd">
<td>Linear in 2D</td>
<td>3</td>
<td>Can shatter triangles, not 4 arbitrary points</td>
</tr>
<tr class="even">
<td>Linear in d-D</td>
<td>d+1</td>
<td>Capacity grows with dimension</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-12" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-12">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> itertools <span class="im">import</span> product</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co"># check if points in 2D can be shattered by linear SVM</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>points <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">0</span>],[<span class="dv">0</span>,<span class="dv">1</span>],[<span class="dv">1</span>,<span class="dv">0</span>]])</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>labelings <span class="op">=</span> <span class="bu">list</span>(product([<span class="dv">0</span>,<span class="dv">1</span>], repeat<span class="op">=</span><span class="bu">len</span>(points)))</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> can_shatter(points, labelings):</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> labels <span class="kw">in</span> labelings:</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        clf <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">"linear"</span>, C<span class="op">=</span><span class="fl">1e6</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        clf.fit(points, labels)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">all</span>(clf.predict(points) <span class="op">==</span> labels):</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">True</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"3 points in 2D shattered?"</span>, can_shatter(points, labelings))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-12" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-12">Why it Matters</h4>
<p>VC dimension provides a rigorous way to quantify model capacity and connect it to generalization. It explains why higher-dimensional models need more data and why simpler models generalize better with limited data.</p>
</section>
<section id="try-it-yourself-12" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-12">Try It Yourself</h4>
<ol type="1">
<li>Place 3 points in 2D and try to separate them with a line for every labeling.</li>
<li>Try the same with 4 points—notice when shattering becomes impossible.</li>
<li>Relate VC dimension to real-world models: why do deep networks (with huge VC) require massive datasets?</li>
</ol>
</section>
</section>
<section id="growth-functions-and-shattering" class="level3">
<h3 class="anchored" data-anchor-id="growth-functions-and-shattering">614. Growth Functions and Shattering</h3>
<p>The growth function measures how many distinct labelings a hypothesis class can realize on a set of <span class="math inline">\(n\)</span> points. It quantifies the richness of the hypothesis space more finely than just VC dimension. Shattering is the extreme case where all <span class="math inline">\(2^n\)</span> possible labelings are achievable.</p>
<section id="picture-in-your-head-13" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-13">Picture in Your Head</h4>
<p>Imagine arranging <span class="math inline">\(n\)</span> dots in a row and asking: how many different ways can my model class separate them into two groups? If the model can realize every possible separation, the set is shattered. As <span class="math inline">\(n\)</span> grows, eventually the model runs out of flexibility, and the growth function flattens.</p>
</section>
<section id="deep-dive-13" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-13">Deep Dive</h4>
<ul>
<li>Growth Function <span class="math inline">\(m_H(n)\)</span>: maximum number of distinct dichotomies (labelings) achievable by hypothesis class <span class="math inline">\(H\)</span> on any <span class="math inline">\(n\)</span> points.</li>
<li>If <span class="math inline">\(H\)</span> can shatter <span class="math inline">\(n\)</span> points, then <span class="math inline">\(m_H(n) = 2^n\)</span>.</li>
<li>Beyond the VC dimension, the growth function grows more slowly than <span class="math inline">\(2^n\)</span>.</li>
<li>Sauer’s Lemma formalizes this:</li>
</ul>
<p><span class="math display">\[
m_H(n) \leq \sum_{i=0}^{d} \binom{n}{i},
\]</span></p>
<p>where <span class="math inline">\(d = VC(H)\)</span>.</p>
<p>This inequality bounds generalization by showing that complexity does not grow unchecked once VC limits are reached.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 35%">
<col style="width: 17%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th>Hypothesis Class</th>
<th>VC Dimension</th>
<th>Growth Function Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Threshold on line</td>
<td>1</td>
<td>Linear growth</td>
</tr>
<tr class="even">
<td>Intervals on line</td>
<td>2</td>
<td>Quadratic growth</td>
</tr>
<tr class="odd">
<td>Linear classifier in d-D</td>
<td>d+1</td>
<td>Polynomial in n up to degree d+1</td>
</tr>
<tr class="even">
<td>Arbitrary functions</td>
<td>Infinite</td>
<td><span class="math inline">\(2^n\)</span> (all possible labelings)</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-13" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-13">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> math <span class="im">import</span> comb</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> growth_function(n, d):</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">sum</span>(comb(n, i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(d<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co"># example: linear classifiers in 2D have VC = 3</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> [<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">10</span>]:</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"n=</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">, upper bound m_H(n)=</span><span class="sc">{</span>growth_function(n, <span class="dv">3</span>)<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-13" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-13">Why it Matters</h4>
<p>The growth function refines our understanding of model complexity. It explains how hypothesis spaces explode in capacity at small scales but are capped by VC dimension. This provides the bridge between combinatorial properties of models and statistical learning guarantees.</p>
</section>
<section id="try-it-yourself-13" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-13">Try It Yourself</h4>
<ol type="1">
<li>Compute <span class="math inline">\(m_H(n)\)</span> for intervals on a line (VC=2). Compare it to <span class="math inline">\(2^n\)</span>.</li>
<li>Simulate separating points in 2D with linear classifiers—count how many labelings are possible.</li>
<li>Reflect: how does the slowdown of the growth function beyond VC dimension help prevent overfitting?</li>
</ol>
</section>
</section>
<section id="rademacher-complexity-and-data-dependent-bounds" class="level3">
<h3 class="anchored" data-anchor-id="rademacher-complexity-and-data-dependent-bounds">615. Rademacher Complexity and Data-Dependent Bounds</h3>
<p>Rademacher complexity measures the capacity of a hypothesis class by quantifying how well it can fit random noise. Unlike VC dimension, it is data-dependent: it evaluates the richness of hypotheses relative to a specific sample. This makes it a finer-grained tool for understanding generalization.</p>
<section id="picture-in-your-head-14" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-14">Picture in Your Head</h4>
<p>Imagine giving a model completely random labels for your dataset.</p>
<ul>
<li>If the model can still fit these random labels well, it has high Rademacher complexity.</li>
<li>If it struggles, its capacity relative to that dataset is lower. This test reveals how much a model can “memorize” noise.</li>
</ul>
</section>
<section id="deep-dive-14" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-14">Deep Dive</h4>
<p>Formally, given data <span class="math inline">\(S = \{x_1, \dots, x_n\}\)</span> and hypothesis class <span class="math inline">\(H\)</span>, the empirical Rademacher complexity is:</p>
<p><span class="math display">\[
\hat{\mathfrak{R}}_S(H) = \mathbb{E}_\sigma \left[ \sup_{h \in H} \frac{1}{n}\sum_{i=1}^n \sigma_i h(x_i) \right],
\]</span></p>
<p>where <span class="math inline">\(\sigma_i\)</span> are random variables taking values <span class="math inline">\(\pm 1\)</span> with equal probability (Rademacher variables).</p>
<ul>
<li>High Rademacher complexity → hypothesis class can fit many noise patterns.</li>
<li>Low Rademacher complexity → class is restricted, less prone to overfitting.</li>
</ul>
<p>It leads to generalization bounds of the form:</p>
<p><span class="math display">\[
R(h) \leq R_{emp}(h) + 2\hat{\mathfrak{R}}_S(H) + O\left(\sqrt{\frac{\log(1/\delta)}{n}}\right).
\]</span></p>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 21%">
<col style="width: 26%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th>Measure</th>
<th>Depends On</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>VC Dimension</td>
<td>Hypothesis class only</td>
<td>Clean combinatorial theory</td>
<td>Distribution-free, can be loose</td>
</tr>
<tr class="even">
<td>Rademacher Complexity</td>
<td>Data sample + class</td>
<td>Tighter, data-sensitive</td>
<td>Harder to compute</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-14" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-14">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co"># dataset</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.randn(<span class="dv">50</span>, <span class="dv">1</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.random.randn(<span class="dv">50</span>)  <span class="co"># random noise</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co"># hypothesis class: linear functions</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>lin <span class="op">=</span> LinearRegression().fit(X, y)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>score <span class="op">=</span> lin.score(X, y)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Linear model R^2 on random labels (memorization ability):"</span>, score)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-14" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-14">Why it Matters</h4>
<p>Rademacher complexity captures how much a model can overfit to random fluctuations in <em>this dataset</em>. It refines the idea of capacity beyond abstract dimensions, making it useful for practical generalization bounds.</p>
</section>
<section id="try-it-yourself-14" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-14">Try It Yourself</h4>
<ol type="1">
<li>Train linear regression and decision trees on random labels. Which achieves higher fit? Relate to Rademacher complexity.</li>
<li>Increase dataset size and repeat. Does the ability to fit noise decrease?</li>
<li>Reflect: why do large neural networks often still generalize well, despite being able to fit random labels?</li>
</ol>
</section>
</section>
<section id="pac-learning-framework" class="level3">
<h3 class="anchored" data-anchor-id="pac-learning-framework">616. PAC Learning Framework</h3>
<p>Probably Approximately Correct (PAC) learning is a formal framework for defining when a concept class is learnable. A hypothesis class is PAC-learnable if, with high probability, a learner can find a hypothesis that is approximately correct given a reasonable amount of data and computation.</p>
<section id="picture-in-your-head-15" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-15">Picture in Your Head</h4>
<p>Imagine teaching a child to recognize cats. You want a guarantee like this:</p>
<ul>
<li>After seeing enough examples, the child will probably (with high probability) recognize cats approximately correctly (with small error), even if not perfectly. This is the essence of PAC learning.</li>
</ul>
</section>
<section id="deep-dive-15" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-15">Deep Dive</h4>
<p>Formally, a hypothesis class <span class="math inline">\(H\)</span> is PAC-learnable if for all <span class="math inline">\(\epsilon, \delta &gt; 0\)</span>, there exists an algorithm that, given enough i.i.d. training examples, outputs a hypothesis <span class="math inline">\(h \in H\)</span> such that:</p>
<p><span class="math display">\[
P(R(h) \leq \epsilon) \geq 1 - \delta
\]</span></p>
<p>with sample complexity polynomial in <span class="math inline">\(\frac{1}{\epsilon}, \frac{1}{\delta}, n,\)</span> and <span class="math inline">\(|H|\)</span>.</p>
<ul>
<li><span class="math inline">\(\epsilon\)</span>: accuracy parameter (allowed error).</li>
<li><span class="math inline">\(\delta\)</span>: confidence parameter (failure probability).</li>
<li>Sample complexity: number of examples required to achieve <span class="math inline">\((\epsilon, \delta)\)</span>-guarantees.</li>
</ul>
<p>Key results:</p>
<ul>
<li>Finite hypothesis spaces are PAC-learnable.</li>
<li>VC dimension provides a characterization of PAC-learnability for infinite classes.</li>
<li>PAC learning connects generalization to sample complexity bounds.</li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Term</th>
<th>Meaning in PAC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>“Probably”</td>
<td>With probability ≥ <span class="math inline">\(1-\delta\)</span></td>
</tr>
<tr class="even">
<td>“Approximately”</td>
<td>Error ≤ <span class="math inline">\(\epsilon\)</span></td>
</tr>
<tr class="odd">
<td>“Correct”</td>
<td>Generalizes beyond training data</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-15" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-15">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co"># synthetic dataset</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.randn(<span class="dv">500</span>, <span class="dv">5</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (X[:, <span class="dv">0</span>] <span class="op">+</span> X[:, <span class="dv">1</span>] <span class="op">&gt;</span> <span class="dv">0</span>).astype(<span class="bu">int</span>)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co"># PAC-style experiment: test error bound</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> LogisticRegression().fit(X_train, y_train)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>train_acc <span class="op">=</span> clf.score(X_train, y_train)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>test_acc <span class="op">=</span> clf.score(X_test, y_test)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training accuracy:"</span>, train_acc)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test accuracy:"</span>, test_acc)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generalization gap:"</span>, train_acc <span class="op">-</span> test_acc)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-15" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-15">Why it Matters</h4>
<p>The PAC framework is foundational: it shows that learning is possible under uncertainty, but not free. It formalizes the tradeoff between error, confidence, and sample size, guiding both theory and practice.</p>
</section>
<section id="try-it-yourself-15" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-15">Try It Yourself</h4>
<ol type="1">
<li>Fix <span class="math inline">\(\epsilon = 0.1\)</span>, <span class="math inline">\(\delta = 0.05\)</span>. Estimate how many samples you’d need for a finite hypothesis space of size 1000.</li>
<li>Train models with different dataset sizes. How does increasing <span class="math inline">\(n\)</span> affect the generalization gap?</li>
<li>Reflect: in practical ML, when do we care more about lowering <span class="math inline">\(\epsilon\)</span> (accuracy) vs.&nbsp;lowering <span class="math inline">\(\delta\)</span> (confidence of guarantee)?</li>
</ol>
</section>
</section>
<section id="probably-approximately-correct-guarantees" class="level3">
<h3 class="anchored" data-anchor-id="probably-approximately-correct-guarantees">617. Probably Approximately Correct Guarantees</h3>
<p>PAC guarantees formalize what it means for a learning algorithm to succeed. They assure us that, with high probability, the learned hypothesis will be close to the true concept. This shifts learning from being a matter of luck to one of statistical reliability.</p>
<section id="picture-in-your-head-16" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-16">Picture in Your Head</h4>
<p>Think of weather forecasting.</p>
<ul>
<li>You don’t expect forecasts to be perfect every day.</li>
<li>But you do expect them to be “probably” (with high confidence) “approximately” (within small error) “correct.” PAC guarantees apply the same idea to machine learning.</li>
</ul>
</section>
<section id="deep-dive-16" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-16">Deep Dive</h4>
<p>A PAC guarantee has two levers:</p>
<ul>
<li>Accuracy (<span class="math inline">\(\epsilon\)</span>): how close the learned hypothesis must be to the true concept.</li>
<li>Confidence (<span class="math inline">\(1 - \delta\)</span>): how likely it is that the guarantee holds.</li>
</ul>
<p>For finite hypothesis spaces <span class="math inline">\(H\)</span>, the sample complexity bound is:</p>
<p><span class="math display">\[
m \geq \frac{1}{\epsilon} \left( \ln |H| + \ln \frac{1}{\delta} \right).
\]</span></p>
<p>This means:</p>
<ul>
<li>Larger hypothesis spaces need more data.</li>
<li>Higher accuracy (<span class="math inline">\(\epsilon \to 0\)</span>) requires more samples.</li>
<li>Higher confidence (<span class="math inline">\(\delta \to 0\)</span>) also requires more samples.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 45%">
<col style="width: 25%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Effect on Guarantee</th>
<th>Cost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Smaller <span class="math inline">\(\epsilon\)</span> (higher accuracy)</td>
<td>Stricter requirement</td>
<td>More samples</td>
</tr>
<tr class="even">
<td>Smaller <span class="math inline">\(\delta\)</span> (higher confidence)</td>
<td>Safer guarantee</td>
<td>More samples</td>
</tr>
<tr class="odd">
<td>Larger hypothesis space</td>
<td>More expressive</td>
<td>Higher sample complexity</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-16" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-16">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pac_sample_complexity(H_size, epsilon, delta):</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">int</span>((<span class="dv">1</span><span class="op">/</span>epsilon) <span class="op">*</span> (math.log(H_size) <span class="op">+</span> math.log(<span class="dv">1</span><span class="op">/</span>delta)))</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co"># example: hypothesis space of size 1000</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>H_size <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># 90% accuracy</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>delta <span class="op">=</span> <span class="fl">0.05</span>   <span class="co"># 95% confidence</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Sample complexity:"</span>, pac_sample_complexity(H_size, epsilon, delta))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-16" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-16">Why it Matters</h4>
<p>PAC guarantees are the backbone of learning theory: they make precise how data size, model complexity, and performance requirements trade off. They show that learning is feasible with finite data, but also bounded by statistical laws.</p>
</section>
<section id="try-it-yourself-16" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-16">Try It Yourself</h4>
<ol type="1">
<li>Compute sample complexity for hypothesis spaces of size 100, 1000, and 1,000,000 with <span class="math inline">\(\epsilon=0.1\)</span>, <span class="math inline">\(\delta=0.05\)</span>. Compare growth.</li>
<li>Adjust <span class="math inline">\(\epsilon\)</span> from 0.1 to 0.01. How does required sample size explode?</li>
<li>Reflect: in real-world AI systems (e.g., autonomous driving), do we prioritize smaller <span class="math inline">\(\epsilon\)</span> (accuracy) or smaller <span class="math inline">\(\delta\)</span> (confidence)?</li>
</ol>
</section>
</section>
<section id="uniform-convergence-and-concentration-inequalities" class="level3">
<h3 class="anchored" data-anchor-id="uniform-convergence-and-concentration-inequalities">618. Uniform Convergence and Concentration Inequalities</h3>
<p>Uniform convergence is the principle that, as the sample size grows, the empirical risk of all hypotheses in a class converges uniformly to their true risk. Concentration inequalities (like Hoeffding’s and Chernoff bounds) provide the mathematical tools to quantify how tightly empirical averages concentrate around expectations.</p>
<section id="picture-in-your-head-17" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-17">Picture in Your Head</h4>
<p>Think of repeatedly tasting spoonfuls of soup. With only one spoon, your impression may be misleading. But as you take more spoons, every possible flavor profile (salty, spicy, sour) stabilizes toward the true taste of the soup. Uniform convergence means that this stabilization happens for all hypotheses simultaneously, not just one.</p>
</section>
<section id="deep-dive-17" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-17">Deep Dive</h4>
<ul>
<li>Pointwise convergence: For a fixed hypothesis <span class="math inline">\(h\)</span>, empirical risk approaches true risk as <span class="math inline">\(n \to \infty\)</span>.</li>
<li>Uniform convergence: For an entire hypothesis class <span class="math inline">\(H\)</span>, the difference <span class="math inline">\(|R_{emp}(h) - R(h)|\)</span> becomes small for all <span class="math inline">\(h \in H\)</span>.</li>
</ul>
<p>Concentration inequalities formalize this:</p>
<ul>
<li>Hoeffding’s inequality: For i.i.d. bounded random variables,</li>
</ul>
<p><span class="math display">\[
P\left( \left|\frac{1}{n}\sum_{i=1}^n X_i - \mathbb{E}[X]\right| \geq \epsilon \right) \leq 2 e^{-2n\epsilon^2}.
\]</span></p>
<ul>
<li>These inequalities are the building blocks of PAC bounds, linking sample size to generalization reliability.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 51%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th>Inequality</th>
<th>Key Idea</th>
<th>Application in ML</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Hoeffding</td>
<td>Averages of bounded variables concentrate</td>
<td>Generalization error bounds</td>
</tr>
<tr class="even">
<td>Chernoff</td>
<td>Exponential bounds on tail probabilities</td>
<td>Error rates in large datasets</td>
</tr>
<tr class="odd">
<td>McDiarmid</td>
<td>Bounded differences in functions</td>
<td>Stability of algorithms</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-17" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-17">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co"># simulate Hoeffding's inequality</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, <span class="fl">0.5</span>, size<span class="op">=</span>n)  <span class="co"># fair coin flips</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>emp_mean <span class="op">=</span> X.mean()</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>true_mean <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> np.exp(<span class="op">-</span><span class="dv">2</span> <span class="op">*</span> n <span class="op">*</span> epsilon2)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Empirical mean:"</span>, emp_mean)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Hoeffding bound (prob deviation &gt; 0.05):"</span>, bound)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-17" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-17">Why it Matters</h4>
<p>Uniform convergence is the reason finite data can approximate population-level performance. Concentration inequalities quantify how much trust we can place in training results. They ensure that empirical validation provides meaningful guarantees for generalization.</p>
</section>
<section id="try-it-yourself-17" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-17">Try It Yourself</h4>
<ol type="1">
<li>Simulate coin flips with increasing sample sizes. Compare empirical means with the Hoeffding bound.</li>
<li>Train classifiers on small vs.&nbsp;large datasets. Observe how test accuracy variance shrinks with more samples.</li>
<li>Reflect: why is uniform convergence stronger than just pointwise convergence for learning theory?</li>
</ol>
</section>
</section>
<section id="limitations-of-pac-theory" class="level3">
<h3 class="anchored" data-anchor-id="limitations-of-pac-theory">619. Limitations of PAC Theory</h3>
<p>While PAC learning provides a rigorous foundation, it has practical limitations. Many modern machine learning methods (like deep neural networks) fall outside the neat assumptions of PAC theory. The framework is powerful for understanding fundamentals but often too coarse or restrictive for real-world practice.</p>
<section id="picture-in-your-head-18" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-18">Picture in Your Head</h4>
<p>Think of PAC theory as a ruler: it measures length precisely but only in straight lines. If you need to measure a winding path, the ruler helps a little but doesn’t capture the whole story.</p>
</section>
<section id="deep-dive-18" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-18">Deep Dive</h4>
<p>Key limitations include:</p>
<ul>
<li>Distribution-free assumption: PAC guarantees hold for <em>any</em> data distribution, but this makes bounds very loose. Real data often has structure that PAC theory ignores.</li>
<li>Computational efficiency: PAC learning only asks whether a hypothesis <em>exists</em>, not whether it can be found efficiently. Some PAC-learnable classes are computationally intractable.</li>
<li>Sample complexity bounds: The bounds can be extremely large and pessimistic compared to practice.</li>
<li>Over-parameterized models: Neural networks with VC dimensions in the millions should, by PAC reasoning, require impossibly large datasets, yet they generalize well with much less.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 35%">
<col style="width: 64%">
</colgroup>
<thead>
<tr class="header">
<th>Limitation</th>
<th>Why It Matters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Loose bounds</td>
<td>Theory predicts impractical sample sizes</td>
</tr>
<tr class="even">
<td>No efficiency guarantees</td>
<td>Doesn’t ensure algorithms are feasible</td>
</tr>
<tr class="odd">
<td>Ignores distributional structure</td>
<td>Misses practical strengths of learners</td>
</tr>
<tr class="even">
<td>Struggles with deep learning</td>
<td>Can’t explain generalization in over-parameterized regimes</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-18" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-18">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co"># PAC bound example: hypothesis space size = 1e6</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>H_size <span class="op">=</span> <span class="dv">1_000_000</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>delta <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>sample_complexity <span class="op">=</span> <span class="bu">int</span>((<span class="dv">1</span><span class="op">/</span>epsilon) <span class="op">*</span> (math.log(H_size) <span class="op">+</span> math.log(<span class="dv">1</span><span class="op">/</span>delta)))</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"PAC sample complexity:"</span>, sample_complexity)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This bound suggests needing hundreds of thousands of samples, even though in practice many models generalize well with far fewer.</p>
</section>
<section id="why-it-matters-18" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-18">Why it Matters</h4>
<p>Recognizing PAC theory’s limits prevents misuse. It is a guiding framework for what is theoretically possible, but not a precise predictor of practical performance. Modern learning theory extends beyond PAC, incorporating margins, stability, algorithmic randomness, and compression-based analyses.</p>
</section>
<section id="try-it-yourself-18" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-18">Try It Yourself</h4>
<ol type="1">
<li>Compute PAC sample complexity for hypothesis spaces of size <span class="math inline">\(10^3\)</span>, <span class="math inline">\(10^6\)</span>, and <span class="math inline">\(10^9\)</span>. Compare them with typical dataset sizes you use.</li>
<li>Train a small neural network on MNIST. Compare actual generalization to what PAC theory would predict.</li>
<li>Reflect: why do over-parameterized deep networks generalize far better than PAC theory would allow?</li>
</ol>
</section>
</section>
<section id="implications-for-modern-machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="implications-for-modern-machine-learning">620. Implications for Modern Machine Learning</h3>
<p>The theory of generalization, bias, variance, VC dimension, Rademacher complexity, and PAC learning provides the backbone of statistical learning. Yet modern machine learning—especially deep learning—pushes beyond these frameworks. Understanding how classical theory connects to practice reveals both enduring lessons and open questions.</p>
<section id="picture-in-your-head-19" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-19">Picture in Your Head</h4>
<p>Imagine building a bridge: the blueprints (theory) give structure and safety guarantees, but real-world engineers must adapt to terrain, weather, and new materials. Classical learning theory is the blueprint; modern ML practice is the engineering in the wild.</p>
</section>
<section id="deep-dive-19" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-19">Deep Dive</h4>
<p>Key implications:</p>
<ul>
<li>Sample complexity matters: Big data improves generalization, consistent with LLN and PAC principles.</li>
<li>Regularization is structural risk minimization in practice: L1/L2 penalties, dropout, and early stopping operationalize theory.</li>
<li>Over-parameterization paradox: Deep networks often generalize well despite having capacity to shatter training data—something PAC theory predicts should overfit. This motivates new theories (e.g., double descent, implicit bias of optimization).</li>
<li>Data-dependent analysis: Tools like Rademacher complexity and algorithmic stability better explain why large models generalize.</li>
<li>Uniform convergence is insufficient: Deep learning highlights that generalization may rely on dynamics of optimization and properties of data distributions beyond classical bounds.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 72%">
</colgroup>
<thead>
<tr class="header">
<th>Theoretical Idea</th>
<th>Modern Reflection</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bias–variance tradeoff</td>
<td>Still visible, but double descent shows added complexity</td>
</tr>
<tr class="even">
<td>SRM &amp; Occam’s Razor</td>
<td>Realized through regularization and model selection</td>
</tr>
<tr class="odd">
<td>VC dimension</td>
<td>Too coarse for deep nets, but still valuable historically</td>
</tr>
<tr class="even">
<td>PAC guarantees</td>
<td>Foundational, but overly pessimistic for practice</td>
</tr>
<tr class="odd">
<td>Rademacher complexity</td>
<td>More refined, aligns better with over-parameterized models</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-19" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-19">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># simple deep net trained on random labels</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>(X_train, y_train), _ <span class="op">=</span> tf.keras.datasets.mnist.load_data()</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> X_train.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>) <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>y_random <span class="op">=</span> tf.random.uniform(shape<span class="op">=</span>(<span class="bu">len</span>(y_train),), maxval<span class="op">=</span><span class="dv">10</span>, dtype<span class="op">=</span>tf.int32)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    layers.Dense(<span class="dv">256</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    layers.Dense(<span class="dv">256</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>, loss<span class="op">=</span><span class="st">'sparse_categorical_crossentropy'</span>, metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_random, epochs<span class="op">=</span><span class="dv">3</span>, batch_size<span class="op">=</span><span class="dv">128</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This experiment shows a deep network can fit random labels—demonstrating extreme capacity—yet the same architectures generalize well on real data.</p>
</section>
<section id="why-it-matters-19" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-19">Why it Matters</h4>
<p>Modern ML builds on classical theory but also challenges it. Recognizing both continuity and gaps helps practitioners understand why some models generalize in practice and guides researchers to extend theory.</p>
</section>
<section id="try-it-yourself-19" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-19">Try It Yourself</h4>
<ol type="1">
<li>Train a deep net on real MNIST and on random labels. Compare generalization.</li>
<li>Explore how double descent appears when training models of increasing size.</li>
<li>Reflect: which parts of classical learning theory remain essential in your work, and which feel outdated in the deep learning era?</li>
</ol>
</section>
</section>
</section>
<section id="chapter-63.-losses-regularization-and-optimization" class="level2">
<h2 class="anchored" data-anchor-id="chapter-63.-losses-regularization-and-optimization">Chapter 63. Losses, Regularization, and Optimization</h2>
<section id="loss-functions-as-objectives" class="level3">
<h3 class="anchored" data-anchor-id="loss-functions-as-objectives">621. Loss Functions as Objectives</h3>
<p>A loss function quantifies the difference between a model’s prediction and the true outcome. It is the guiding objective that learning algorithms minimize during training. Choosing the right loss function directly shapes what the model learns and how it behaves.</p>
<section id="picture-in-your-head-20" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-20">Picture in Your Head</h4>
<p>Imagine a compass guiding a traveler:</p>
<ul>
<li>Without a compass (no loss function), the traveler wanders aimlessly.</li>
<li>With a compass pointing north (a chosen loss), the traveler has a clear direction. Similarly, the loss function gives orientation to learning—defining what “better” means.</li>
</ul>
</section>
<section id="deep-dive-20" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-20">Deep Dive</h4>
<p>Loss functions serve as optimization objectives and encode modeling assumptions:</p>
<ul>
<li><p>Regression:</p>
<ul>
<li>Mean Squared Error (MSE): penalizes squared deviations, sensitive to outliers.</li>
<li>Mean Absolute Error (MAE): penalizes absolute deviations, robust to outliers.</li>
</ul></li>
<li><p>Classification:</p>
<ul>
<li>Cross-Entropy: measures divergence between predicted probabilities and true labels.</li>
<li>Hinge Loss: encourages correct margin separation (SVMs).</li>
</ul></li>
<li><p>Ranking / Structured Tasks:</p>
<ul>
<li>Pairwise ranking loss, sequence-to-sequence losses.</li>
</ul></li>
<li><p>Custom Losses: Domain-specific, e.g., asymmetric cost for false positives vs.&nbsp;false negatives.</p></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Task</th>
<th>Common Loss</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Regression</td>
<td>MSE</td>
<td>Smooth, sensitive to outliers</td>
</tr>
<tr class="even">
<td>Regression</td>
<td>MAE</td>
<td>More robust, less smooth</td>
</tr>
<tr class="odd">
<td>Classification</td>
<td>Cross-Entropy</td>
<td>Sharp probabilistic guidance</td>
</tr>
<tr class="even">
<td>Classification</td>
<td>Hinge</td>
<td>Margin-based separation</td>
</tr>
<tr class="odd">
<td>Imbalanced data</td>
<td>Weighted loss</td>
<td>Penalizes minority errors more</td>
</tr>
</tbody>
</table>
<p>Loss functions are not just technical details—they embed our values into the model. For example, in medicine, false negatives may be costlier than false positives, leading to asymmetric loss design.</p>
</section>
<section id="tiny-code-20" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-20">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, log_loss</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># regression example</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> np.array([<span class="fl">3.0</span>, <span class="op">-</span><span class="fl">0.5</span>, <span class="fl">2.0</span>])</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> np.array([<span class="fl">2.5</span>, <span class="fl">0.0</span>, <span class="fl">2.0</span>])</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"MSE:"</span>, mean_squared_error(y_true, y_pred))</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="co"># classification example</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>y_true_cls <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>y_prob <span class="op">=</span> [[<span class="fl">0.9</span>, <span class="fl">0.1</span>], [<span class="fl">0.4</span>, <span class="fl">0.6</span>], [<span class="fl">0.2</span>, <span class="fl">0.8</span>]]</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Cross-Entropy:"</span>, log_loss(y_true_cls, y_prob))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-20" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-20">Why it Matters</h4>
<p>The choice of loss function defines the learning problem itself. It determines how errors are measured, what tradeoffs the model makes, and what kind of generalization emerges. A mismatch between loss and real-world objectives can render even high-accuracy models useless.</p>
</section>
<section id="try-it-yourself-20" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-20">Try It Yourself</h4>
<ol type="1">
<li>Train a regression model with MSE vs.&nbsp;MAE on data with outliers. Compare robustness.</li>
<li>Train a classifier with cross-entropy vs.&nbsp;hinge loss. Observe differences in decision boundaries.</li>
<li>Reflect: in a fraud detection system, would you prefer penalizing false negatives more heavily? How would you encode that in a custom loss?</li>
</ol>
</section>
</section>
<section id="convex-vs.-non-convex-losses" class="level3">
<h3 class="anchored" data-anchor-id="convex-vs.-non-convex-losses">622. Convex vs.&nbsp;Non-Convex Losses</h3>
<p>Loss functions can be convex or non-convex, and this distinction strongly influences optimization. Convex losses have a single global minimum, making them easier to optimize reliably. Non-convex losses may have many local minima or saddle points, complicating training but allowing richer model classes like deep networks.</p>
<section id="picture-in-your-head-21" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-21">Picture in Your Head</h4>
<p>Imagine a landscape:</p>
<ul>
<li>A convex loss is like a smooth bowl—roll a ball anywhere, and it will settle at the same bottom.</li>
<li>A non-convex loss is like a mountain range with many valleys—where the ball ends up depends on where it starts.</li>
</ul>
</section>
<section id="deep-dive-21" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-21">Deep Dive</h4>
<ul>
<li><p>Convex losses:</p>
<ul>
<li>Examples: Mean Squared Error (MSE), Logistic Loss, Hinge Loss.</li>
<li>Advantages: guarantees of convergence, easier analysis.</li>
<li>Disadvantage: limited expressivity, tied to simpler models.</li>
</ul></li>
<li><p>Non-convex losses:</p>
<ul>
<li>Examples: Losses from deep neural networks with nonlinear activations.</li>
<li>Advantages: extremely expressive, can model complex patterns.</li>
<li>Disadvantage: optimization harder, risk of local minima, saddle points, flat regions.</li>
</ul></li>
</ul>
<p>Formally:</p>
<ul>
<li>Convex if for all <span class="math inline">\(\theta_1, \theta_2\)</span> and <span class="math inline">\(\lambda \in [0,1]\)</span>:</li>
</ul>
<p><span class="math display">\[
L(\lambda \theta_1 + (1-\lambda)\theta_2) \leq \lambda L(\theta_1) + (1-\lambda)L(\theta_2).
\]</span></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Loss Type</th>
<th>Convex?</th>
<th>Typical Usage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MSE</td>
<td>Yes</td>
<td>Regression, linear models</td>
</tr>
<tr class="even">
<td>Logistic Loss</td>
<td>Yes</td>
<td>Logistic regression</td>
</tr>
<tr class="odd">
<td>Hinge Loss</td>
<td>Yes</td>
<td>SVMs</td>
</tr>
<tr class="even">
<td>Neural Net Loss</td>
<td>No</td>
<td>Deep learning</td>
</tr>
<tr class="odd">
<td>GAN Losses</td>
<td>No</td>
<td>Generative models</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-21" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-21">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="co"># convex loss: quadratic</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>convex_loss <span class="op">=</span> x2</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="co"># non-convex loss: sinusoidal + quadratic</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>nonconvex_loss <span class="op">=</span> np.sin(<span class="dv">3</span><span class="op">*</span>x) <span class="op">+</span> x2</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>plt.plot(x, convex_loss, label<span class="op">=</span><span class="st">"Convex (Quadratic)"</span>)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>plt.plot(x, nonconvex_loss, label<span class="op">=</span><span class="st">"Non-Convex (Sine+Quadratic)"</span>)</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-21" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-21">Why it Matters</h4>
<p>Convexity is central to classical ML: it guarantees solvability and well-defined solutions. Non-convexity defines modern ML: despite theoretical difficulty, optimization heuristics like SGD often find good enough solutions in practice. The shift from convex to non-convex marks the transition from traditional ML to deep learning.</p>
</section>
<section id="try-it-yourself-21" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-21">Try It Yourself</h4>
<ol type="1">
<li>Plot convex (MSE) vs.&nbsp;non-convex (neural network training) losses. Observe the landscape differences.</li>
<li>Train a linear regression (convex) vs.&nbsp;a two-layer neural net (non-convex) on the same dataset. Compare optimization behavior.</li>
<li>Reflect: why does stochastic gradient descent often succeed in non-convex problems despite no guarantees?</li>
</ol>
</section>
</section>
<section id="l1-and-l2-regularization" class="level3">
<h3 class="anchored" data-anchor-id="l1-and-l2-regularization">623. L1 and L2 Regularization</h3>
<p>Regularization adds penalty terms to a loss function to discourage overly complex models. L1 (Lasso) and L2 (Ridge) regularization are the most common forms. L1 encourages sparsity by driving some weights to zero, while L2 shrinks weights smoothly toward zero without eliminating them.</p>
<section id="picture-in-your-head-22" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-22">Picture in Your Head</h4>
<p>Think of packing for a trip:</p>
<ul>
<li>With L1 regularization, you only bring the essentials—many items are left out entirely.</li>
<li>With L2 regularization, you still bring everything, but pack lighter versions of each item.</li>
</ul>
</section>
<section id="deep-dive-22" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-22">Deep Dive</h4>
<p>The general form of a regularized objective is:</p>
<p><span class="math display">\[
L(\theta) = \text{Loss}(\theta) + \lambda \cdot \Omega(\theta),
\]</span></p>
<p>where <span class="math inline">\(\Omega(\theta)\)</span> is the penalty.</p>
<ul>
<li>L1 Regularization:</li>
</ul>
<p><span class="math display">\[
\Omega(\theta) = \|\theta\|_1 = \sum_i |\theta_i|.
\]</span></p>
<p>Encourages sparsity, useful for feature selection.</p>
<ul>
<li>L2 Regularization:</li>
</ul>
<p><span class="math display">\[
\Omega(\theta) = \|\theta\|_2^2 = \sum_i \theta_i^2.
\]</span></p>
<p>Prevents large weights, improves stability, reduces variance.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 25%">
<col style="width: 23%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th>Regularization</th>
<th>Formula</th>
<th>Effect</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>L1 (Lasso)</td>
<td>(</td>
<td>_i</td>
<td>)</td>
<td>Sparse weights, feature selection</td>
</tr>
<tr class="even">
<td>L2 (Ridge)</td>
<td><span class="math inline">\(\sum \theta_i^2\)</span></td>
<td>Small, smooth weights, stability</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Elastic Net</td>
<td>(</td>
<td>_i</td>
<td>+ (1-)_i^2)</td>
<td>Combines both</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-22" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-22">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Lasso, Ridge</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co"># toy dataset</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.randn(<span class="dv">100</span>, <span class="dv">5</span>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> X[:, <span class="dv">0</span>] <span class="op">*</span> <span class="dv">3</span> <span class="op">+</span> np.random.randn(<span class="dv">100</span>) <span class="op">*</span> <span class="fl">0.5</span>  <span class="co"># only feature 0 matters</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="co"># L1 regularization</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>lasso <span class="op">=</span> Lasso(alpha<span class="op">=</span><span class="fl">0.1</span>).fit(X, y)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Lasso coefficients:"</span>, lasso.coef_)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a><span class="co"># L2 regularization</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>ridge <span class="op">=</span> Ridge(alpha<span class="op">=</span><span class="fl">0.1</span>).fit(X, y)</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Ridge coefficients:"</span>, ridge.coef_)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-22" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-22">Why it Matters</h4>
<p>Regularization controls model capacity, improves generalization, and stabilizes training. L1 is valuable when only a few features are relevant, while L2 is effective when all features contribute but should be prevented from growing too large. Many real systems use Elastic Net to balance both.</p>
</section>
<section id="try-it-yourself-22" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-22">Try It Yourself</h4>
<ol type="1">
<li>Train linear models with and without regularization. Compare coefficients.</li>
<li>Increase L1 penalty and observe how more weights shrink to zero.</li>
<li>Reflect: in domains with thousands of features (e.g., genomics), why might L1 regularization be more useful than L2?</li>
</ol>
</section>
</section>
<section id="norm-based-and-geometric-regularization" class="level3">
<h3 class="anchored" data-anchor-id="norm-based-and-geometric-regularization">624. Norm-Based and Geometric Regularization</h3>
<p>Norm-based regularization extends the idea of L1 and L2 by penalizing weight vectors according to different geometric norms. By shaping the geometry of the parameter space, these penalties constrain the types of solutions a model can adopt, thereby guiding learning behavior.</p>
<section id="picture-in-your-head-23" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-23">Picture in Your Head</h4>
<p>Imagine tying a balloon with a rubber band:</p>
<ul>
<li>A tight rubber band (strong regularization) forces the balloon to stay small.</li>
<li>A looser band (weaker regularization) allows more expansion. Different norms are like different band shapes—circles, diamonds, or more exotic forms—that restrict how far the balloon (weights) can stretch.</li>
</ul>
</section>
<section id="deep-dive-23" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-23">Deep Dive</h4>
<ul>
<li>General p-norm regularization:</li>
</ul>
<p><span class="math display">\[
\Omega(\theta) = \|\theta\|_p = \left( \sum_i |\theta_i|^p \right)^{1/p}.
\]</span></p>
<ul>
<li><p><span class="math inline">\(p=1\)</span>: promotes sparsity (L1).</p></li>
<li><p><span class="math inline">\(p=2\)</span>: smooth shrinkage (L2).</p></li>
<li><p><span class="math inline">\(p=\infty\)</span>: limits the largest individual weight.</p></li>
<li><p>Geometric interpretation:</p>
<ul>
<li>L1 penalty corresponds to a diamond-shaped constraint region.</li>
<li>L2 penalty corresponds to a circular (elliptical) region.</li>
<li>Different norms define different feasible sets where optimization seeks a solution.</li>
</ul></li>
<li><p>Beyond norms: Other geometric constraints include margin maximization (SVMs), orthogonality constraints (for decorrelated features), and spectral norms (controlling weight matrix magnitude in deep networks).</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 29%">
<col style="width: 49%">
</colgroup>
<thead>
<tr class="header">
<th>Regularization</th>
<th>Constraint Geometry</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>L1</td>
<td>Diamond</td>
<td>Sparse solutions</td>
</tr>
<tr class="even">
<td>L2</td>
<td>Circle</td>
<td>Smooth shrinkage</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(L_\infty\)</span></td>
<td>Box</td>
<td>Limits largest weight</td>
</tr>
<tr class="even">
<td>Spectral norm</td>
<td>Matrix operator norm</td>
<td>Controls layer Lipschitz constant</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-23" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-23">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co"># visualize L1 vs L2 constraint regions</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>theta1 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">200</span>)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>theta2 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">200</span>)</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>T1, T2 <span class="op">=</span> np.meshgrid(theta1, theta2)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>L1 <span class="op">=</span> np.<span class="bu">abs</span>(T1) <span class="op">+</span> np.<span class="bu">abs</span>(T2)</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>L2 <span class="op">=</span> np.sqrt(T12 <span class="op">+</span> T22)</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>plt.contour(T1, T2, L1, levels<span class="op">=</span>[<span class="dv">1</span>], colors<span class="op">=</span><span class="st">"red"</span>, label<span class="op">=</span><span class="st">"L1"</span>)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>plt.contour(T1, T2, L2, levels<span class="op">=</span>[<span class="dv">1</span>], colors<span class="op">=</span><span class="st">"blue"</span>, label<span class="op">=</span><span class="st">"L2"</span>)</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>plt.gca().set_aspect(<span class="st">"equal"</span>)</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-23" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-23">Why it Matters</h4>
<p>Norm-based regularization generalizes the concept of capacity control. By choosing the right geometry, we encode structural preferences into models: sparsity, smoothness, robustness, or stability. In deep learning, norm constraints are essential for controlling gradient explosion and ensuring robustness to adversarial perturbations.</p>
</section>
<section id="try-it-yourself-23" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-23">Try It Yourself</h4>
<ol type="1">
<li>Train models with <span class="math inline">\(L_1\)</span>, <span class="math inline">\(L_2\)</span>, and <span class="math inline">\(L_\infty\)</span> constraints on the same dataset. Compare outcomes.</li>
<li>Visualize feasible regions for different norms and see how they influence the optimizer’s path.</li>
<li>Reflect: why might spectral norm regularization be important for stabilizing deep neural networks?</li>
</ol>
</section>
</section>
<section id="sparsity-inducing-penalties" class="level3">
<h3 class="anchored" data-anchor-id="sparsity-inducing-penalties">625. Sparsity-Inducing Penalties</h3>
<p>Sparsity-inducing penalties encourage models to use only a small subset of available features or parameters, driving many coefficients exactly to zero. This simplifies models, improves interpretability, and reduces overfitting in high-dimensional settings.</p>
<section id="picture-in-your-head-24" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-24">Picture in Your Head</h4>
<p>Think of editing a rough draft:</p>
<ul>
<li>You cross out redundant words until only the most essential ones remain. Sparsity penalties act the same way—removing unnecessary weights so the model keeps only what matters.</li>
</ul>
</section>
<section id="deep-dive-24" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-24">Deep Dive</h4>
<ul>
<li>L1 penalty (Lasso): The most common sparsity tool; its diamond-shaped constraint region intersects axes, driving coefficients to zero.</li>
<li>Elastic Net: Combines L1 (sparsity) and L2 (stability).</li>
<li>Group Lasso: Encourages entire groups of features to be included or excluded together.</li>
<li>Nonconvex penalties: SCAD (Smoothly Clipped Absolute Deviation) and MCP (Minimax Concave Penalty) provide stronger sparsity with less bias on large coefficients.</li>
</ul>
<p>Applications:</p>
<ul>
<li>Feature selection in genomics, text mining, and finance.</li>
<li>Compression of deep neural networks by pruning weights.</li>
<li>Improved interpretability in domains where simpler models are preferred.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 19%">
<col style="width: 21%">
<col style="width: 24%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th>Penalty</th>
<th>Formula</th>
<th>Effect</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>L1 (Lasso)</td>
<td>(</td>
<td>_i</td>
<td>)</td>
<td>Sparse coefficients</td>
</tr>
<tr class="even">
<td>Elastic Net</td>
<td>(</td>
<td>_i</td>
<td>+ (1-)_i^2)</td>
<td>Balance sparsity &amp; smoothness</td>
</tr>
<tr class="odd">
<td>Group Lasso</td>
<td><span class="math inline">\(\sum_g \|\theta_g\|_2\)</span></td>
<td>Selects feature groups</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>SCAD / MCP</td>
<td>Nonconvex forms</td>
<td>Strong sparsity, low bias</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-24" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-24">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Lasso</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co"># synthetic high-dimensional dataset</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.randn(<span class="dv">50</span>, <span class="dv">10</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> X[:, <span class="dv">0</span>] <span class="op">*</span> <span class="dv">3</span> <span class="op">+</span> np.random.randn(<span class="dv">50</span>) <span class="op">*</span> <span class="fl">0.1</span>  <span class="co"># only feature 0 matters</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>lasso <span class="op">=</span> Lasso(alpha<span class="op">=</span><span class="fl">0.1</span>).fit(X, y)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Coefficients:"</span>, lasso.coef_)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-24" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-24">Why it Matters</h4>
<p>Sparsity-inducing penalties are critical when the number of features far exceeds the number of samples. They help models remain interpretable, efficient, and less prone to overfitting. In deep learning, sparsity underpins model pruning and efficient deployment on resource-limited hardware.</p>
</section>
<section id="try-it-yourself-24" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-24">Try It Yourself</h4>
<ol type="1">
<li>Train a Lasso model on a dataset with many irrelevant features. How many coefficients shrink to zero?</li>
<li>Compare Lasso and Ridge regression on the same dataset. Which is more interpretable?</li>
<li>Reflect: why would sparsity be especially valuable in domains like healthcare or finance, where explanations matter?</li>
</ol>
</section>
</section>
<section id="early-stopping-as-implicit-regularization" class="level3">
<h3 class="anchored" data-anchor-id="early-stopping-as-implicit-regularization">626. Early Stopping as Implicit Regularization</h3>
<p>Early stopping halts training before a model fully minimizes training loss, preventing it from overfitting to noise. It acts as an implicit regularizer, limiting effective model capacity without altering the loss function or adding explicit penalties.</p>
<section id="picture-in-your-head-25" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-25">Picture in Your Head</h4>
<p>Imagine baking bread:</p>
<ul>
<li>Take it out too early → undercooked (underfitting).</li>
<li>Leave it too long → burnt (overfitting).</li>
<li>The perfect loaf comes from stopping at the right time. Early stopping is that careful timing in model training.</li>
</ul>
</section>
<section id="deep-dive-25" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-25">Deep Dive</h4>
<ul>
<li>During training, training error decreases steadily, but validation error follows a U-shape: it decreases, then increases once the model starts memorizing noise.</li>
<li>Early stopping chooses the point where validation error is minimized.</li>
<li>It’s especially effective for neural networks, where long training can push models into high-variance regions of the loss surface.</li>
<li>Theoretical view: early stopping constrains the optimization trajectory, similar to adding an <span class="math inline">\(L_2\)</span> penalty.</li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Phase</th>
<th>Training Error</th>
<th>Validation Error</th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Too early</td>
<td>High</td>
<td>High</td>
<td>Underfit</td>
</tr>
<tr class="even">
<td>Just right</td>
<td>Low</td>
<td>Low</td>
<td>Good generalization</td>
</tr>
<tr class="odd">
<td>Too late</td>
<td>Very low</td>
<td>Rising</td>
<td>Overfit</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-25" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-25">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>(X_train, y_train), (X_val, y_val) <span class="op">=</span> tf.keras.datasets.mnist.load_data()</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>X_train, X_val <span class="op">=</span> X_train<span class="op">/</span><span class="fl">255.0</span>, X_val<span class="op">/</span><span class="fl">255.0</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>X_train, X_val <span class="op">=</span> X_train.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>), X_val.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    layers.Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"adam"</span>, loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>early_stop <span class="op">=</span> tf.keras.callbacks.EarlyStopping(patience<span class="op">=</span><span class="dv">3</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train, y_train, validation_data<span class="op">=</span>(X_val, y_val),</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>                    epochs<span class="op">=</span><span class="dv">50</span>, batch_size<span class="op">=</span><span class="dv">128</span>, callbacks<span class="op">=</span>[early_stop])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-25" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-25">Why it Matters</h4>
<p>Early stopping is one of the simplest and most powerful regularization techniques in practice. It requires no modification to the loss and adapts to data automatically. In large-scale ML systems, it saves computation while improving generalization.</p>
</section>
<section id="try-it-yourself-25" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-25">Try It Yourself</h4>
<ol type="1">
<li>Train a neural net with and without early stopping. Compare validation accuracy.</li>
<li>Adjust patience (how many epochs to wait after the best validation result). How does this affect outcomes?</li>
<li>Reflect: why might early stopping be more effective than explicit penalties in high-dimensional deep learning?</li>
</ol>
</section>
</section>
<section id="optimization-landscapes-and-saddle-points" class="level3">
<h3 class="anchored" data-anchor-id="optimization-landscapes-and-saddle-points">627. Optimization Landscapes and Saddle Points</h3>
<p>The optimization landscape is the shape of the loss function across parameter space. For simple convex problems, it looks like a smooth bowl with a single minimum. For non-convex problems—common in deep learning—it is rugged, with many valleys, plateaus, and saddle points. Saddle points, where gradients vanish but are not minima, present particular challenges.</p>
<section id="picture-in-your-head-26" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-26">Picture in Your Head</h4>
<p>Imagine hiking:</p>
<ul>
<li>A convex landscape is like a valley leading to one clear lowest point.</li>
<li>A non-convex landscape is like a mountain range full of valleys, cliffs, and flat ridges.</li>
<li>A saddle point is like a mountain pass: flat in one direction (no incentive to move) but descending in another.</li>
</ul>
</section>
<section id="deep-dive-26" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-26">Deep Dive</h4>
<ul>
<li>Local minima: Points lower than neighbors but not the absolute lowest.</li>
<li>Global minimum: The absolute best point in the landscape.</li>
<li>Saddle points: Stationary points where the gradient is zero but curvature is mixed (some directions go up, others down).</li>
</ul>
<p>In high dimensions, saddle points are much more common than bad local minima. Escaping them is a central challenge for gradient-based optimization.</p>
<ul>
<li><p>Techniques to handle saddle points:</p>
<ul>
<li>Stochasticity in SGD helps escape flat regions.</li>
<li>Momentum and adaptive optimizers push through shallow areas.</li>
<li>Second-order methods (Hessian-based) explicitly detect curvature.</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Feature</th>
<th>Convex Landscape</th>
<th>Non-Convex Landscape</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Global minima</td>
<td>Unique</td>
<td>Often many</td>
</tr>
<tr class="even">
<td>Local minima</td>
<td>None</td>
<td>Common but often benign</td>
</tr>
<tr class="odd">
<td>Saddle points</td>
<td>None</td>
<td>Abundant, problematic</td>
</tr>
<tr class="even">
<td>Optimization difficulty</td>
<td>Low</td>
<td>High</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-26" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-26">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co"># visualize a simple saddle surface: f(x,y) = x^2 - y^2</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">100</span>)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">100</span>)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> np.meshgrid(x, y)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> X2 <span class="op">-</span> Y2</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>plt.contour(X, Y, Z, levels<span class="op">=</span>np.linspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">21</span>))</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Saddle Point Landscape (x^2 - y^2)"</span>)</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"y"</span>)</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-26" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-26">Why it Matters</h4>
<p>Understanding landscapes explains why training deep networks is hard yet feasible. While global minima are numerous and often good, saddle points and flat regions slow optimization. Practical algorithms succeed not because they avoid non-convexity, but because they exploit dynamics that navigate rugged terrain effectively.</p>
</section>
<section id="try-it-yourself-26" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-26">Try It Yourself</h4>
<ol type="1">
<li>Plot surfaces like <span class="math inline">\(f(x,y) = x^2 - y^2\)</span> and <span class="math inline">\(f(x,y) = \sin(x) + \cos(y)\)</span>. Identify minima, maxima, and saddles.</li>
<li>Train a small neural network and monitor gradient norms. Notice when training slows—often due to saddle regions.</li>
<li>Reflect: why are saddle points more common than bad local minima in high-dimensional deep learning?</li>
</ol>
</section>
</section>
<section id="stochastic-vs.-batch-optimization" class="level3">
<h3 class="anchored" data-anchor-id="stochastic-vs.-batch-optimization">628. Stochastic vs.&nbsp;Batch Optimization</h3>
<p>Optimization in machine learning often relies on gradient descent, but how we compute gradients makes a big difference. Batch Gradient Descent uses the entire dataset for each update, while Stochastic Gradient Descent (SGD) uses a single sample (or a mini-batch). The tradeoff is between precision and efficiency.</p>
<section id="picture-in-your-head-27" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-27">Picture in Your Head</h4>
<p>Think of steering a ship:</p>
<ul>
<li>Batch descent is like carefully calculating the perfect direction before every move—accurate but slow.</li>
<li>SGD is like adjusting course constantly using noisy signals—less precise per step, but much faster.</li>
</ul>
</section>
<section id="deep-dive-27" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-27">Deep Dive</h4>
<ul>
<li><p>Batch Gradient Descent:</p>
<ul>
<li>Update rule:</li>
</ul>
<p><span class="math display">\[
\theta \leftarrow \theta - \eta \nabla_\theta L(\theta; \text{all data})
\]</span></p>
<ul>
<li>Pros: exact gradient, stable convergence.</li>
<li>Cons: expensive for large datasets.</li>
</ul></li>
<li><p>Stochastic Gradient Descent:</p>
<ul>
<li>Update rule with one sample:</li>
</ul>
<p><span class="math display">\[
\theta \leftarrow \theta - \eta \nabla_\theta L(\theta; x_i, y_i)
\]</span></p>
<ul>
<li>Pros: cheap updates, escapes saddle points/local minima.</li>
<li>Cons: noisy convergence, requires careful learning rate scheduling.</li>
</ul></li>
<li><p>Mini-Batch Gradient Descent:</p>
<ul>
<li>Middle ground: use small batches (e.g., 32–512 samples).</li>
<li>Balances stability and efficiency, widely used in deep learning.</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Method</th>
<th>Gradient Estimate</th>
<th>Speed</th>
<th>Stability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Batch</td>
<td>Exact</td>
<td>Slow</td>
<td>High</td>
</tr>
<tr class="even">
<td>Stochastic</td>
<td>Noisy</td>
<td>Fast</td>
<td>Low</td>
</tr>
<tr class="odd">
<td>Mini-batch</td>
<td>Approximate</td>
<td>Balanced</td>
<td>Balanced</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-27" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-27">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="co"># simple quadratic loss: f(w) = (w-3)^2</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grad(w, X<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">2</span><span class="op">*</span>(w<span class="op">-</span><span class="dv">3</span>)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="co"># batch gradient descent</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    w <span class="op">-=</span> eta <span class="op">*</span> grad(w)</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Batch GD result:"</span>, w)</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a><span class="co"># stochastic gradient descent (simulate noisy grad)</span></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>    noisy_grad <span class="op">=</span> grad(w) <span class="op">+</span> np.random.randn()<span class="op">*</span><span class="fl">0.5</span></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>    w <span class="op">-=</span> eta <span class="op">*</span> noisy_grad</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"SGD result:"</span>, w)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-27" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-27">Why it Matters</h4>
<p>Batch methods guarantee convergence but are infeasible at scale. Stochastic methods dominate modern ML because they handle massive datasets efficiently and naturally regularize by injecting noise. Mini-batch SGD with momentum or adaptive learning rates is the workhorse of deep learning.</p>
</section>
<section id="try-it-yourself-27" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-27">Try It Yourself</h4>
<ol type="1">
<li>Implement gradient descent with full batch, SGD, and mini-batch on the same dataset. Compare convergence curves.</li>
<li>Train a neural network with batch size = 1, 32, and full dataset. How do training speed and generalization differ?</li>
<li>Reflect: why does noisy SGD often generalize better than perfectly optimized batch descent?</li>
</ol>
</section>
</section>
<section id="robust-and-adversarial-losses" class="level3">
<h3 class="anchored" data-anchor-id="robust-and-adversarial-losses">629. Robust and Adversarial Losses</h3>
<p>Standard loss functions assume clean data, but real-world data often contains outliers, noise, or adversarial manipulations. Robust and adversarial losses are designed to maintain stability and performance under such conditions, reducing sensitivity to problematic samples or malicious attacks.</p>
<section id="picture-in-your-head-28" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-28">Picture in Your Head</h4>
<p>Imagine teaching handwriting recognition:</p>
<ul>
<li>If one student scribbles nonsense (an outlier), the teacher shouldn’t let that ruin the whole lesson.</li>
<li>If a trickster deliberately alters a “7” to look like a “1” (adversarial), the teacher must defend against being fooled. Robust and adversarial losses protect models in these scenarios.</li>
</ul>
</section>
<section id="deep-dive-28" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-28">Deep Dive</h4>
<ul>
<li><p>Robust Losses: Reduce the impact of outliers.</p>
<ul>
<li>Huber loss: Quadratic for small errors, linear for large errors.</li>
<li>Quantile loss: Useful for median regression, focuses on asymmetric penalties.</li>
<li>Tukey’s biweight loss: Heavily downweights outliers.</li>
</ul></li>
<li><p>Adversarial Losses: Designed to defend against adversarial examples.</p>
<ul>
<li>Adversarial training: Minimizes worst-case loss under perturbations:</li>
</ul>
<p><span class="math display">\[
\min_\theta \max_{\|\delta\| \leq \epsilon} L(f_\theta(x+\delta), y).
\]</span></p>
<ul>
<li>Encourages robustness to small but malicious input changes.</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Loss Type</th>
<th>Example</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Robust</td>
<td>Huber</td>
<td>Less sensitive to outliers</td>
</tr>
<tr class="even">
<td>Robust</td>
<td>Quantile</td>
<td>Asymmetric error handling</td>
</tr>
<tr class="odd">
<td>Adversarial</td>
<td>Adversarial training</td>
<td>Improves robustness to attacks</td>
</tr>
<tr class="even">
<td>Adversarial</td>
<td>TRADES, MART</td>
<td>Balance accuracy and robustness</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-28" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-28">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> HuberRegressor, LinearRegression</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="co"># dataset with outlier</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.arange(<span class="dv">10</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>X.ravel() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>y[<span class="op">-</span><span class="dv">1</span>] <span class="op">+=</span> <span class="dv">30</span>  <span class="co"># strong outlier</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="co"># standard regression</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> LinearRegression().fit(X, y)</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="co"># robust regression</span></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>huber <span class="op">=</span> HuberRegressor().fit(X, y)</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Linear Regression coef:"</span>, lr.coef_)</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Huber Regression coef:"</span>, huber.coef_)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-28" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-28">Why it Matters</h4>
<p>Robust losses protect against noisy, imperfect data, while adversarial losses are essential in security-sensitive domains like finance, healthcare, and autonomous driving. Together, they make ML systems more trustworthy in the messy real world.</p>
</section>
<section id="try-it-yourself-28" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-28">Try It Yourself</h4>
<ol type="1">
<li>Fit linear regression vs.&nbsp;Huber regression on data with outliers. Compare coefficient stability.</li>
<li>Implement simple adversarial training on an image classifier (FGSM attack). How does robustness change?</li>
<li>Reflect: in your domain, are outliers or adversarial manipulations the bigger threat?</li>
</ol>
</section>
</section>
<section id="tradeoffs-regularization-strength-vs.-flexibility" class="level3">
<h3 class="anchored" data-anchor-id="tradeoffs-regularization-strength-vs.-flexibility">630. Tradeoffs: Regularization Strength vs.&nbsp;Flexibility</h3>
<p>Regularization controls model complexity by penalizing large or unnecessary parameters. The strength of regularization determines the balance between simplicity (bias) and flexibility (variance). Too strong, and the model underfits; too weak, and it overfits. Finding the right strength is key to robust generalization.</p>
<section id="picture-in-your-head-29" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-29">Picture in Your Head</h4>
<p>Think of a leash on a dog:</p>
<ul>
<li>A short, tight leash (strong regularization) keeps the dog very constrained, but it can’t explore.</li>
<li>A loose leash (weak regularization) allows free roaming, but risks wandering into trouble.</li>
<li>The best leash length balances freedom with safety—just like tuning regularization.</li>
</ul>
</section>
<section id="deep-dive-29" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-29">Deep Dive</h4>
<ul>
<li><p>High regularization (large penalty λ):</p>
<ul>
<li>Weights shrink heavily, model becomes simpler.</li>
<li>Reduces variance but increases bias.</li>
</ul></li>
<li><p>Low regularization (small λ):</p>
<ul>
<li>Model fits data closely, possibly capturing noise.</li>
<li>Reduces bias but increases variance.</li>
</ul></li>
<li><p>Optimal regularization:</p>
<ul>
<li>Achieved through validation methods like cross-validation or information criteria (AIC/BIC).</li>
<li>Depends on dataset size, noise, and task.</li>
</ul></li>
</ul>
<p>Regularization applies broadly:</p>
<ul>
<li>Linear models (L1, L2, Elastic Net).</li>
<li>Neural networks (dropout, weight decay, early stopping).</li>
<li>Trees and ensembles (depth limits, learning rate, shrinkage).</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 32%">
<col style="width: 40%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>Regularization Strength</th>
<th>Model Behavior</th>
<th>Risk</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Very strong</td>
<td>Very simple, high bias</td>
<td>Underfitting</td>
</tr>
<tr class="even">
<td>Moderate</td>
<td>Balanced</td>
<td>Good generalization</td>
</tr>
<tr class="odd">
<td>Very weak</td>
<td>Very flexible, high variance</td>
<td>Overfitting</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-29" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-29">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="co"># toy dataset</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.randn(<span class="dv">100</span>, <span class="dv">5</span>)</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> X[:, <span class="dv">0</span>] <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> np.random.randn(<span class="dv">100</span>) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="co"># test different regularization strengths</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha <span class="kw">in</span> [<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>]:</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>    ridge <span class="op">=</span> Ridge(alpha<span class="op">=</span>alpha)</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> cross_val_score(ridge, X, y, cv<span class="op">=</span><span class="dv">5</span>).mean()</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Alpha=</span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ss">, CV score=</span><span class="sc">{</span>score<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-29" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-29">Why it Matters</h4>
<p>Regularization strength is not a one-size-fits-all setting—it must be tuned to the dataset and domain. Striking the right balance ensures models remain flexible enough to capture patterns without memorizing noise.</p>
</section>
<section id="try-it-yourself-29" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-29">Try It Yourself</h4>
<ol type="1">
<li>Train Ridge regression with different α values. Plot validation error vs.&nbsp;α. Identify the “sweet spot.”</li>
<li>Compare models with no regularization, light, and heavy regularization. Which generalizes best?</li>
<li>Reflect: in high-stakes domains (e.g., medicine), would you prefer slightly underfitted (simpler, safer) or slightly overfitted (riskier) models?</li>
</ol>
</section>
</section>
</section>
<section id="chapter-64.-model-selection-cross-validation-bootstrapping" class="level2">
<h2 class="anchored" data-anchor-id="chapter-64.-model-selection-cross-validation-bootstrapping">Chapter 64. Model selection, cross validation, bootstrapping</h2>
<section id="the-problem-of-choosing-among-models" class="level3">
<h3 class="anchored" data-anchor-id="the-problem-of-choosing-among-models">631. The Problem of Choosing Among Models</h3>
<p>Model selection is the process of deciding which hypothesis, algorithm, or configuration best balances fit to data with the ability to generalize. Even with the same dataset, different models (linear regression, decision trees, neural nets) may perform differently depending on complexity, assumptions, and inductive biases.</p>
<section id="picture-in-your-head-30" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-30">Picture in Your Head</h4>
<p>Imagine choosing a vehicle for a trip:</p>
<ul>
<li>A bicycle (simple model) is efficient but limited to short distances.</li>
<li>A sports car (complex model) is powerful but expensive and fragile.</li>
<li>A SUV (balanced model) handles many terrains well. Model selection is picking the “right vehicle” for the journey defined by your data and goals.</li>
</ul>
</section>
<section id="deep-dive-30" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-30">Deep Dive</h4>
<p>Model selection involves tradeoffs:</p>
<ul>
<li>Complexity vs.&nbsp;Generalization: Simpler models generalize better with limited data; complex models capture richer structure but risk overfitting.</li>
<li>Bias vs.&nbsp;Variance: Related to the above; models differ in their error decomposition.</li>
<li>Interpretability vs.&nbsp;Accuracy: Transparent models may be preferable in sensitive domains.</li>
<li>Resource Constraints: Some models are too costly in time, memory, or energy.</li>
</ul>
<p>Techniques for selection:</p>
<ul>
<li>Cross-validation (e.g., k-fold).</li>
<li>Information criteria (AIC, BIC, MDL).</li>
<li>Bayesian model evidence.</li>
<li>Holdout validation sets.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 35%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Selection Criterion</th>
<th>Strength</th>
<th>Weakness</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Cross-validation</td>
<td>Reliable, widely applicable</td>
<td>Expensive computationally</td>
</tr>
<tr class="even">
<td>AIC / BIC</td>
<td>Fast, penalizes complexity</td>
<td>Assumes parametric models</td>
</tr>
<tr class="odd">
<td>Bayesian evidence</td>
<td>Theoretically rigorous</td>
<td>Hard to compute</td>
</tr>
<tr class="even">
<td>Holdout set</td>
<td>Simple, scalable</td>
<td>High variance on small datasets</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-30" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-30">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="co"># toy dataset</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(<span class="dv">100</span>, <span class="dv">3</span>)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> X[:,<span class="dv">0</span>] <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> np.sin(X[:,<span class="dv">1</span>]) <span class="op">+</span> np.random.randn(<span class="dv">100</span>)<span class="op">*</span><span class="fl">0.1</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="co"># compare linear vs tree</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>lin <span class="op">=</span> LinearRegression()</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>tree <span class="op">=</span> DecisionTreeRegressor(max_depth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> model <span class="kw">in</span> [lin, tree]:</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> cross_val_score(model, X, y, cv<span class="op">=</span><span class="dv">5</span>).mean()</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(model.<span class="va">__class__</span>.<span class="va">__name__</span>, <span class="st">"CV score:"</span>, score)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-30" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-30">Why it Matters</h4>
<p>Choosing the wrong model wastes data, time, and resources, and may yield misleading predictions. Model selection frameworks give principled ways to evaluate and compare options, ensuring robust deployment.</p>
</section>
<section id="try-it-yourself-30" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-30">Try It Yourself</h4>
<ol type="1">
<li>Compare linear regression, decision trees, and random forests on the same dataset using cross-validation.</li>
<li>Use AIC or BIC to select between polynomial models of different degrees.</li>
<li>Reflect: in your domain, is interpretability or raw accuracy more critical for model selection?</li>
</ol>
</section>
</section>
<section id="training-vs.-validation-vs.-test-splits" class="level3">
<h3 class="anchored" data-anchor-id="training-vs.-validation-vs.-test-splits">632. Training vs.&nbsp;Validation vs.&nbsp;Test Splits</h3>
<p>To evaluate models fairly, data is divided into training, validation, and test sets. Each serves a distinct role: training teaches the model, validation guides hyperparameter tuning and model selection, and testing provides an unbiased estimate of final performance.</p>
<section id="picture-in-your-head-31" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-31">Picture in Your Head</h4>
<p>Think of preparing for a sports competition:</p>
<ul>
<li>Training set = practice sessions where you learn skills.</li>
<li>Validation set = scrimmage games where you test strategies and adjust.</li>
<li>Test set = the real tournament, where results count.</li>
</ul>
</section>
<section id="deep-dive-31" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-31">Deep Dive</h4>
<ul>
<li>Training set: Used to fit model parameters. Larger training sets usually improve generalization.</li>
<li>Validation set: Held out to tune hyperparameters (regularization, architecture, learning rate). Prevents information leakage from test data.</li>
<li>Test set: Used only once at the end. Provides an unbiased estimate of model performance in deployment.</li>
</ul>
<p>Variants:</p>
<ul>
<li>Holdout method: Split once into train/val/test.</li>
<li>k-Fold Cross-Validation: Rotates validation across folds, improves robustness.</li>
<li>Nested Cross-Validation: Outer loop for evaluation, inner loop for hyperparameter tuning.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 30%">
<col style="width: 54%">
</colgroup>
<thead>
<tr class="header">
<th>Split</th>
<th>Purpose</th>
<th>Caution</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Training</td>
<td>Fit model parameters</td>
<td>Too small = underfit</td>
</tr>
<tr class="even">
<td>Validation</td>
<td>Tune hyperparameters</td>
<td>Don’t peek repeatedly (risk leakage)</td>
</tr>
<tr class="odd">
<td>Test</td>
<td>Final evaluation</td>
<td>Use only once</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-31" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-31">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="co"># synthetic dataset</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.randn(<span class="dv">200</span>, <span class="dv">5</span>)</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (X[:,<span class="dv">0</span>] <span class="op">+</span> X[:,<span class="dv">1</span>] <span class="op">&gt;</span> <span class="dv">0</span>).astype(<span class="bu">int</span>)</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="co"># split: train 60%, val 20%, test 20%</span></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>X_train, X_temp, y_train, y_temp <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>X_val, X_test, y_val, y_test <span class="op">=</span> train_test_split(X_temp, y_temp, test_size<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression().fit(X_train, y_train)</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Validation score:"</span>, model.score(X_val, y_val))</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test score:"</span>, model.score(X_test, y_test))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-31" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-31">Why it Matters</h4>
<p>Without clear splits, models risk overfitting to evaluation data, producing inflated performance estimates. Proper partitioning ensures reproducibility, fairness, and trustworthy deployment.</p>
</section>
<section id="try-it-yourself-31" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-31">Try It Yourself</h4>
<ol type="1">
<li>Create train/val/test splits with different ratios (e.g., 80/10/10 vs.&nbsp;60/20/20). How does test accuracy vary?</li>
<li>Compare results when you mistakenly use the test set for hyperparameter tuning. Notice the over-optimism.</li>
<li>Reflect: in domains with very limited data (like medical imaging), how would you balance the need for training vs.&nbsp;validation vs.&nbsp;testing?</li>
</ol>
</section>
</section>
<section id="k-fold-cross-validation" class="level3">
<h3 class="anchored" data-anchor-id="k-fold-cross-validation">633. k-Fold Cross-Validation</h3>
<p>k-Fold Cross-Validation (CV) is a resampling method for model evaluation. It partitions the dataset into <em>k</em> equal-sized folds, trains the model on <em>k–1</em> folds, and validates it on the remaining fold. This process repeats <em>k</em> times, with each fold serving once as validation. The results are averaged to give a robust estimate of model performance.</p>
<section id="picture-in-your-head-32" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-32">Picture in Your Head</h4>
<p>Think of dividing a pie into 5 slices:</p>
<ul>
<li>You taste 4 slices and save 1 to test.</li>
<li>Rotate until every slice has been tested. By the end, you’ve judged the whole pie fairly, not just one piece.</li>
</ul>
</section>
<section id="deep-dive-32" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-32">Deep Dive</h4>
<ul>
<li><p>Process:</p>
<ol type="1">
<li><p>Split dataset into <em>k</em> folds.</p></li>
<li><p>For each fold <span class="math inline">\(i\)</span>:</p>
<ul>
<li>Train on <span class="math inline">\(k-1\)</span> folds.</li>
<li>Validate on fold <span class="math inline">\(i\)</span>.</li>
</ul></li>
<li><p>Average results across all folds.</p></li>
</ol></li>
<li><p>Choice of k:</p>
<ul>
<li><span class="math inline">\(k=5\)</span> or <span class="math inline">\(k=10\)</span> are common tradeoffs between bias and variance.</li>
<li><span class="math inline">\(k=n\)</span> gives Leave-One-Out CV (LOO-CV), which is unbiased but computationally expensive.</li>
</ul></li>
<li><p>Advantages: Efficient use of limited data, reduced variance of evaluation.</p></li>
<li><p>Disadvantages: Higher computational cost than a single holdout split.</p></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>k</th>
<th>Bias</th>
<th>Variance</th>
<th>Cost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Small (e.g., 2–5)</td>
<td>Higher</td>
<td>Lower</td>
<td>Faster</td>
</tr>
<tr class="even">
<td>Large (e.g., 10)</td>
<td>Lower</td>
<td>Higher</td>
<td>Slower</td>
</tr>
<tr class="odd">
<td>LOO (n)</td>
<td>Minimal</td>
<td>Very high</td>
<td>Very expensive</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-32" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-32">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="co"># synthetic dataset</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.randn(<span class="dv">200</span>, <span class="dv">5</span>)</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (X[:,<span class="dv">0</span>] <span class="op">+</span> X[:,<span class="dv">1</span>] <span class="op">&gt;</span> <span class="dv">0</span>).astype(<span class="bu">int</span>)</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> cross_val_score(model, X, y, cv<span class="op">=</span><span class="dv">5</span>)  <span class="co"># 5-fold CV</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"CV scores:"</span>, scores)</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Mean CV score:"</span>, scores.mean())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-32" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-32">Why it Matters</h4>
<p>k-Fold CV provides a more reliable estimate of model generalization, especially when datasets are small. It helps in model selection, hyperparameter tuning, and comparing algorithms fairly.</p>
</section>
<section id="try-it-yourself-32" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-32">Try It Yourself</h4>
<ol type="1">
<li>Compare 5-fold vs.&nbsp;10-fold CV on the same dataset. Which is more stable?</li>
<li>Implement Leave-One-Out CV for a small dataset. Compare variance of results with 5-fold CV.</li>
<li>Reflect: in a production pipeline, when would you prefer a fast single holdout vs.&nbsp;thorough k-fold CV?</li>
</ol>
</section>
</section>
<section id="leave-one-out-and-variants" class="level3">
<h3 class="anchored" data-anchor-id="leave-one-out-and-variants">634. Leave-One-Out and Variants</h3>
<p>Leave-One-Out Cross-Validation (LOO-CV) is an extreme case of k-fold CV where <span class="math inline">\(k = n\)</span>, the number of samples. Each iteration trains on all but one sample and tests on the single left-out point. Variants like Leave-p-Out (LpO) generalize this idea by leaving out multiple samples.</p>
<section id="picture-in-your-head-33" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-33">Picture in Your Head</h4>
<p>Imagine grading a class of 30 students:</p>
<ul>
<li>You let each student step out one by one, then teach the remaining 29.</li>
<li>After the lesson, you test the student who stepped out. By repeating this for all students, you see how well your teaching generalizes to everyone individually.</li>
</ul>
</section>
<section id="deep-dive-33" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-33">Deep Dive</h4>
<ul>
<li><p>Leave-One-Out CV (LOO-CV):</p>
<ul>
<li>Runs <span class="math inline">\(n\)</span> training iterations.</li>
<li>Very low bias: nearly all data used for training each time.</li>
<li>High variance: each test is on a single sample, which can be unstable.</li>
<li>Very expensive computationally for large datasets.</li>
</ul></li>
<li><p>Leave-p-Out CV (LpO):</p>
<ul>
<li>Leaves out <span class="math inline">\(p\)</span> samples each time.</li>
<li><span class="math inline">\(p=1\)</span> reduces to LOO.</li>
<li>Larger <span class="math inline">\(p\)</span> smooths variance but grows combinatorial in cost.</li>
</ul></li>
<li><p>Stratified CV:</p>
<ul>
<li>Ensures class proportions are preserved in each fold.</li>
<li>Critical for imbalanced classification problems.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 12%">
<col style="width: 15%">
<col style="width: 20%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Bias</th>
<th>Variance</th>
<th>Cost</th>
<th>Best For</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>LOO-CV</td>
<td>Low</td>
<td>High</td>
<td>Very High</td>
<td>Small datasets</td>
</tr>
<tr class="even">
<td>LpO (p&gt;1)</td>
<td>Moderate</td>
<td>Moderate</td>
<td>Combinatorial</td>
<td>Very small datasets</td>
</tr>
<tr class="odd">
<td>Stratified CV</td>
<td>Low</td>
<td>Controlled</td>
<td>Moderate</td>
<td>Classification tasks</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-33" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-33">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> LeaveOneOut, cross_val_score</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="co"># synthetic dataset</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.randn(<span class="dv">20</span>, <span class="dv">3</span>)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (X[:,<span class="dv">0</span>] <span class="op">+</span> X[:,<span class="dv">1</span>] <span class="op">&gt;</span> <span class="dv">0</span>).astype(<span class="bu">int</span>)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>loo <span class="op">=</span> LeaveOneOut()</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> cross_val_score(model, X, y, cv<span class="op">=</span>loo)</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"LOO-CV scores:"</span>, scores)</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Mean LOO-CV score:"</span>, scores.mean())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-33" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-33">Why it Matters</h4>
<p>LOO-CV maximizes training data usage and is nearly unbiased, but its instability and high cost limit practical use. Understanding when to prefer it (tiny datasets) versus k-fold CV (larger datasets) is crucial for efficient model evaluation.</p>
</section>
<section id="try-it-yourself-33" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-33">Try It Yourself</h4>
<ol type="1">
<li>Apply LOO-CV to a dataset with fewer than 50 samples. Compare to 5-fold CV.</li>
<li>Try Leave-2-Out CV on the same dataset. Does variance reduce?</li>
<li>Reflect: why does LOO-CV often give misleading results on noisy datasets despite using “more” training data?</li>
</ol>
</section>
</section>
<section id="bootstrap-resampling-for-model-assessment" class="level3">
<h3 class="anchored" data-anchor-id="bootstrap-resampling-for-model-assessment">635. Bootstrap Resampling for Model Assessment</h3>
<p>Bootstrap resampling is a method for estimating model performance and variability by repeatedly sampling (with replacement) from the dataset. Each bootstrap sample is used to train the model, and performance is evaluated on the data not included (the “out-of-bag” set).</p>
<section id="picture-in-your-head-34" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-34">Picture in Your Head</h4>
<p>Imagine you have a basket of marbles. Instead of drawing each marble once, you draw marbles with replacement—so some marbles appear multiple times, and others are left out. By repeating this process many times, you understand the variability of the basket’s composition.</p>
</section>
<section id="deep-dive-34" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-34">Deep Dive</h4>
<ul>
<li><p>Bootstrap procedure:</p>
<ol type="1">
<li>Draw a dataset of size <span class="math inline">\(n\)</span> from the original data of size <span class="math inline">\(n\)</span>, sampling with replacement.</li>
<li>Train the model on this bootstrap sample.</li>
<li>Evaluate it on the out-of-bag (OOB) samples.</li>
<li>Repeat many times (e.g., 1000 iterations).</li>
</ol></li>
<li><p>Properties:</p>
<ul>
<li>Roughly <span class="math inline">\(63.2\%\)</span> of unique samples appear in each bootstrap sample; the rest are OOB.</li>
<li>Provides estimates of accuracy, variance, and confidence intervals.</li>
<li>Particularly useful with small datasets, where holding out a test set wastes data.</li>
</ul></li>
<li><p>Extensions:</p>
<ul>
<li>.632 Bootstrap: Combines in-sample and out-of-bag estimates.</li>
<li>Bayesian Bootstrap: Uses weighted sampling with Dirichlet priors.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 54%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Strength</th>
<th>Weakness</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bootstrap</td>
<td>Good variance estimates</td>
<td>Computationally expensive</td>
</tr>
<tr class="even">
<td>OOB error</td>
<td>Efficient for ensembles (e.g., Random Forests)</td>
<td>Less accurate for small n</td>
</tr>
<tr class="odd">
<td>.632 Bootstrap</td>
<td>Reduces bias</td>
<td>More complex to compute</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-34" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-34">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.utils <span class="im">import</span> resample</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="co"># synthetic dataset</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(<span class="dv">30</span>, <span class="dv">1</span>)</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">3</span><span class="op">*</span>X.ravel() <span class="op">+</span> np.random.randn(<span class="dv">30</span>)<span class="op">*</span><span class="fl">0.1</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>n_bootstraps <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>errors <span class="op">=</span> []</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_bootstraps):</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>    X_boot, y_boot <span class="op">=</span> resample(X, y)</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> LinearRegression().fit(X_boot, y_boot)</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># out-of-bag samples</span></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> np.ones(<span class="bu">len</span>(X), dtype<span class="op">=</span><span class="bu">bool</span>)</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>    mask[np.unique(np.where(X[:,<span class="va">None</span>]<span class="op">==</span>X_boot)[<span class="dv">0</span>])] <span class="op">=</span> <span class="va">False</span></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> mask.<span class="bu">sum</span>() <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>        errors.append(mean_squared_error(y[mask], model.predict(X[mask])))</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Bootstrap error estimate:"</span>, np.mean(errors))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-34" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-34">Why it Matters</h4>
<p>Bootstrap provides a powerful, distribution-free way to estimate uncertainty in model evaluation. It complements cross-validation, offering deeper insights into variability and confidence intervals for metrics.</p>
</section>
<section id="try-it-yourself-34" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-34">Try It Yourself</h4>
<ol type="1">
<li>Run bootstrap resampling on a small dataset and compute 95% confidence intervals for accuracy.</li>
<li>Compare bootstrap error estimates with 5-fold CV results. Are they consistent?</li>
<li>Reflect: why might bootstrap be preferred in medical or financial datasets with very limited samples?</li>
</ol>
</section>
</section>
<section id="information-criteria-aic-bic-mdl" class="level3">
<h3 class="anchored" data-anchor-id="information-criteria-aic-bic-mdl">636. Information Criteria: AIC, BIC, MDL</h3>
<p>Information criteria provide model selection tools that balance goodness of fit with model complexity. They penalize models with too many parameters, discouraging overfitting. The most common are AIC (Akaike Information Criterion), BIC (Bayesian Information Criterion), and MDL (Minimum Description Length).</p>
<section id="picture-in-your-head-35" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-35">Picture in Your Head</h4>
<p>Think of writing a story:</p>
<ul>
<li>A very short version (underfit) leaves out important details.</li>
<li>A very long version (overfit) includes unnecessary fluff. Information criteria measure both how well the story fits reality and how concise it is, rewarding the “just right” version.</li>
</ul>
</section>
<section id="deep-dive-35" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-35">Deep Dive</h4>
<ul>
<li>Akaike Information Criterion (AIC):</li>
</ul>
<p><span class="math display">\[
AIC = 2k - 2\ln(L)
\]</span></p>
<ul>
<li><p><span class="math inline">\(k\)</span>: number of parameters.</p></li>
<li><p><span class="math inline">\(L\)</span>: maximum likelihood.</p></li>
<li><p>Favors predictive accuracy, lighter penalty on complexity.</p></li>
<li><p>Bayesian Information Criterion (BIC):</p></li>
</ul>
<p><span class="math display">\[
BIC = k \ln(n) - 2\ln(L)
\]</span></p>
<ul>
<li><p>Stronger penalty on parameters, especially with large <span class="math inline">\(n\)</span>.</p></li>
<li><p>Favors simpler models as data grows.</p></li>
<li><p>Minimum Description Length (MDL):</p>
<ul>
<li>Inspired by information theory.</li>
<li>Best model is the one that compresses the data most efficiently.</li>
<li>Equivalent to preferring models that minimize both complexity and residual error.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 34%">
<col style="width: 52%">
</colgroup>
<thead>
<tr class="header">
<th>Criterion</th>
<th>Penalty Strength</th>
<th>Best For</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>AIC</td>
<td>Moderate</td>
<td>Prediction accuracy</td>
</tr>
<tr class="even">
<td>BIC</td>
<td>Stronger (grows with n)</td>
<td>Parsimony, true model selection</td>
</tr>
<tr class="odd">
<td>MDL</td>
<td>Flexible</td>
<td>Information-theoretic model balance</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-35" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-35">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="co"># synthetic data</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(<span class="dv">50</span>, <span class="dv">1</span>)</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>X.ravel() <span class="op">+</span> np.random.randn(<span class="dv">50</span>)<span class="op">*</span><span class="fl">0.1</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression().fit(X, y)</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>n, k <span class="op">=</span> X.shape[<span class="dv">0</span>], X.shape[<span class="dv">1</span>]</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>residual <span class="op">=</span> mean_squared_error(y, model.predict(X)) <span class="op">*</span> n</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>logL <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> residual  <span class="co"># simplified proxy for log-likelihood</span></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>AIC <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>k <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>logL</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a>BIC <span class="op">=</span> k<span class="op">*</span>math.log(n) <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>logL</span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"AIC:"</span>, AIC)</span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"BIC:"</span>, BIC)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-35" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-35">Why it Matters</h4>
<p>Information criteria provide quick, principled methods to compare models without requiring cross-validation. They are especially useful for nested models and statistical settings where likelihoods are available.</p>
</section>
<section id="try-it-yourself-35" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-35">Try It Yourself</h4>
<ol type="1">
<li>Fit polynomial regressions of degree 1–5. Compute AIC and BIC for each. Which degree is chosen?</li>
<li>Compare AIC vs.&nbsp;BIC as dataset size increases. Notice how BIC increasingly favors simpler models.</li>
<li>Reflect: in applied work (e.g., econometrics, biology), would you prioritize predictive accuracy (AIC) or finding the “true” simpler model (BIC/MDL)?</li>
</ol>
</section>
</section>
<section id="nested-cross-validation-for-hyperparameter-tuning" class="level3">
<h3 class="anchored" data-anchor-id="nested-cross-validation-for-hyperparameter-tuning">637. Nested Cross-Validation for Hyperparameter Tuning</h3>
<p>Nested cross-validation (nested CV) is a robust evaluation method that separates model selection (hyperparameter tuning) from model assessment (estimating generalization). It avoids overly optimistic estimates that occur if the same data is used both for tuning and evaluation.</p>
<section id="picture-in-your-head-36" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-36">Picture in Your Head</h4>
<p>Think of a cooking contest:</p>
<ul>
<li>Inner loop = you adjust your recipe (hyperparameters) by taste-testing with friends (validation).</li>
<li>Outer loop = a panel of judges (test folds) scores your final dish. Nested CV ensures your score reflects true ability, not just how well you catered to your friends’ tastes.</li>
</ul>
</section>
<section id="deep-dive-36" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-36">Deep Dive</h4>
<ul>
<li><p>Outer loop (k1 folds): Splits data into training and test folds. Test folds are used only for evaluation.</p></li>
<li><p>Inner loop (k2 folds): Within each outer training fold, further splits data for hyperparameter tuning.</p></li>
<li><p>Process:</p>
<ol type="1">
<li><p>For each outer fold:</p>
<ul>
<li>Run inner CV to select the best hyperparameters.</li>
<li>Train with chosen hyperparameters on outer training fold.</li>
<li>Evaluate on outer test fold.</li>
</ul></li>
<li><p>Average performance across outer folds.</p></li>
</ol></li>
</ul>
<p>This ensures that test folds remain completely unseen until final evaluation.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Inner CV</td>
<td>Tune hyperparameters</td>
</tr>
<tr class="even">
<td>Outer CV</td>
<td>Evaluate tuned model fairly</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-36" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-36">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV, cross_val_score, KFold</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> load_iris(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a><span class="co"># inner loop: hyperparameter search</span></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {<span class="st">'C'</span>: [<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>], <span class="st">'kernel'</span>: [<span class="st">'linear'</span>, <span class="st">'rbf'</span>]}</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>inner_cv <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">3</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>outer_cv <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">5</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> GridSearchCV(SVC(), param_grid, cv<span class="op">=</span>inner_cv)</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> cross_val_score(clf, X, y, cv<span class="op">=</span>outer_cv)</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Nested CV accuracy:"</span>, scores.mean())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-36" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-36">Why it Matters</h4>
<p>Without nested CV, models risk data leakage: hyperparameters overfit to validation data, leading to inflated performance estimates. Nested CV provides the gold standard for fair model comparison, especially in research and small-data settings.</p>
</section>
<section id="try-it-yourself-36" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-36">Try It Yourself</h4>
<ol type="1">
<li>Run nested CV with different outer folds (e.g., 3, 5, 10). Does stability improve with more folds?</li>
<li>Compare performance reported by simple cross-validation vs.&nbsp;nested CV. Notice the optimism gap.</li>
<li>Reflect: in high-stakes domains (medicine, finance), why is avoiding optimistic bias in evaluation critical?</li>
</ol>
</section>
</section>
<section id="multiple-comparisons-and-statistical-significance" class="level3">
<h3 class="anchored" data-anchor-id="multiple-comparisons-and-statistical-significance">638. Multiple Comparisons and Statistical Significance</h3>
<p>When testing many models or hypotheses, some will appear better just by chance. Multiple comparison corrections adjust for this effect, ensuring that improvements are statistically meaningful rather than random noise.</p>
<section id="picture-in-your-head-37" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-37">Picture in Your Head</h4>
<p>Imagine tossing 20 coins: by luck, a few may land heads 80% of the time. Without correction, you might mistakenly think those coins are “special.” Model comparisons suffer the same risk when many are tested.</p>
</section>
<section id="deep-dive-37" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-37">Deep Dive</h4>
<ul>
<li><p>Problem: Testing many models inflates the chance of false positives.</p>
<ul>
<li>If significance threshold is <span class="math inline">\(\alpha = 0.05\)</span>, then out of 100 tests, ~5 may appear significant purely by chance.</li>
</ul></li>
<li><p>Corrections:</p>
<ul>
<li>Bonferroni correction: Adjusts threshold to <span class="math inline">\(\alpha/m\)</span> for <span class="math inline">\(m\)</span> tests. Conservative but simple.</li>
<li>Holm–Bonferroni: Sequentially rejects hypotheses, less conservative.</li>
<li>False Discovery Rate (FDR, Benjamini–Hochberg): Controls expected proportion of false discoveries, widely used in high-dimensional ML (e.g., genomics).</li>
</ul></li>
<li><p>In ML model selection:</p>
<ul>
<li>Comparing many hyperparameter settings risks overestimating performance.</li>
<li>Correcting ensures reported improvements are genuine.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 34%">
<col style="width: 41%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Control</th>
<th>Tradeoff</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bonferroni</td>
<td>Family-wise error rate</td>
<td>Very conservative</td>
</tr>
<tr class="even">
<td>Holm–Bonferroni</td>
<td>Family-wise error rate</td>
<td>More powerful</td>
</tr>
<tr class="odd">
<td>FDR (Benjamini–Hochberg)</td>
<td>Proportion of false positives</td>
<td>Balanced</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-37" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-37">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.multitest <span class="im">import</span> multipletests</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 10 p-values from multiple tests</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>pvals <span class="op">=</span> np.array([<span class="fl">0.01</span>, <span class="fl">0.04</span>, <span class="fl">0.20</span>, <span class="fl">0.03</span>, <span class="fl">0.07</span>, <span class="fl">0.001</span>, <span class="fl">0.15</span>, <span class="fl">0.05</span>, <span class="fl">0.02</span>, <span class="fl">0.10</span>])</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Bonferroni and FDR corrections</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>bonf <span class="op">=</span> multipletests(pvals, alpha<span class="op">=</span><span class="fl">0.05</span>, method<span class="op">=</span><span class="st">'bonferroni'</span>)</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>fdr <span class="op">=</span> multipletests(pvals, alpha<span class="op">=</span><span class="fl">0.05</span>, method<span class="op">=</span><span class="st">'fdr_bh'</span>)</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Bonferroni significant:"</span>, bonf[<span class="dv">0</span>])</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"FDR significant:"</span>, fdr[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-37" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-37">Why it Matters</h4>
<p>Without correction, researchers and practitioners may claim spurious improvements. Multiple comparisons control is essential for rigorous ML research, high-dimensional data (omics, text), and sensitive applications.</p>
</section>
<section id="try-it-yourself-37" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-37">Try It Yourself</h4>
<ol type="1">
<li>Run hyperparameter tuning with dozens of settings. How many appear better than baseline? Apply FDR correction.</li>
<li>Compare Bonferroni vs.&nbsp;FDR on simulated experiments. Which finds more “discoveries”?</li>
<li>Reflect: in competitive ML benchmarks, why is it dangerous to report only the single best run without correction?</li>
</ol>
</section>
</section>
<section id="model-selection-under-data-scarcity" class="level3">
<h3 class="anchored" data-anchor-id="model-selection-under-data-scarcity">639. Model Selection under Data Scarcity</h3>
<p>When datasets are small, splitting into large train/validation/test partitions wastes precious information. Special strategies are needed to evaluate models fairly while making the most of limited data.</p>
<section id="picture-in-your-head-38" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-38">Picture in Your Head</h4>
<p>Imagine having just a handful of puzzle pieces:</p>
<ul>
<li>If you keep too many aside for testing, you can’t see the full picture.</li>
<li>If you use them all for training, you can’t check if the puzzle makes sense. Data scarcity forces careful balancing.</li>
</ul>
</section>
<section id="deep-dive-38" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-38">Deep Dive</h4>
<p>Common approaches:</p>
<ul>
<li>Leave-One-Out CV (LOO-CV): Maximizes training use, but has high variance.</li>
<li>Repeated k-Fold CV: Averages multiple rounds of k-fold CV to stabilize results.</li>
<li>Bootstrap methods: Provide confidence intervals on performance.</li>
<li>Bayesian model selection: Leverages prior knowledge to supplement limited data.</li>
<li>Transfer learning &amp; pretraining: Use external data to reduce reliance on scarce labeled data.</li>
</ul>
<p>Challenges:</p>
<ul>
<li>Risk of overfitting due to repeated reuse of small samples.</li>
<li>Large model classes (e.g., deep nets) are especially fragile with tiny datasets.</li>
<li>Interpretability often matters more than raw accuracy in low-data regimes.</li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Method</th>
<th>Strength</th>
<th>Weakness</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>LOO-CV</td>
<td>Max training size</td>
<td>High variance</td>
</tr>
<tr class="even">
<td>Repeated k-Fold</td>
<td>More stable</td>
<td>Costly</td>
</tr>
<tr class="odd">
<td>Bootstrap</td>
<td>Variability estimate</td>
<td>Can still overfit</td>
</tr>
<tr class="even">
<td>Bayesian priors</td>
<td>Incorporates knowledge</td>
<td>Requires domain expertise</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-38" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-38">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score, LeaveOneOut</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="co"># toy small dataset</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.randn(<span class="dv">20</span>, <span class="dv">3</span>)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (X[:,<span class="dv">0</span>] <span class="op">+</span> X[:,<span class="dv">1</span>] <span class="op">&gt;</span> <span class="dv">0</span>).astype(<span class="bu">int</span>)</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>loo <span class="op">=</span> LeaveOneOut()</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> cross_val_score(model, X, y, cv<span class="op">=</span>loo)</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"LOO-CV mean accuracy:"</span>, scores.mean())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-38" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-38">Why it Matters</h4>
<p>Data scarcity is common in medicine, law, and finance, where collecting labeled examples is costly. Proper model selection ensures reliable conclusions without overclaiming from limited evidence.</p>
</section>
<section id="try-it-yourself-38" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-38">Try It Yourself</h4>
<ol type="1">
<li>Compare LOO-CV and 5-fold CV on the same tiny dataset. Which is more stable?</li>
<li>Use bootstrap resampling to estimate variance of accuracy on small data.</li>
<li>Reflect: in domains with few labeled samples, would you trust a complex neural net or a simple linear model? Why?</li>
</ol>
</section>
</section>
<section id="best-practices-in-evaluation-protocols" class="level3">
<h3 class="anchored" data-anchor-id="best-practices-in-evaluation-protocols">640. Best Practices in Evaluation Protocols</h3>
<p>Evaluation protocols define how models are compared, tuned, and validated. Poorly designed evaluation leads to misleading conclusions, while rigorous protocols ensure fair, reproducible, and trustworthy results.</p>
<section id="picture-in-your-head-39" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-39">Picture in Your Head</h4>
<p>Think of judging a science fair:</p>
<ul>
<li>If every judge uses different criteria, results are chaotic.</li>
<li>If all judges follow the same clear rules, rankings are fair. Evaluation protocols are the “rules of judging” for machine learning models.</li>
</ul>
</section>
<section id="deep-dive-39" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-39">Deep Dive</h4>
<p>Best practices include:</p>
<ol type="1">
<li><p>Clear separation of data roles</p>
<ul>
<li>Train, validation, and test sets must not overlap.</li>
<li>Avoid test set leakage during hyperparameter tuning.</li>
</ul></li>
<li><p>Cross-validation for stability</p>
<ul>
<li>Use k-fold or nested CV instead of single holdout, especially with small datasets.</li>
</ul></li>
<li><p>Multiple metrics</p>
<ul>
<li>Accuracy alone is insufficient; include precision, recall, F1, calibration, robustness.</li>
</ul></li>
<li><p>Reporting variance</p>
<ul>
<li>Report mean ± standard deviation or confidence intervals, not just a single score.</li>
</ul></li>
<li><p>Baselines and ablations</p>
<ul>
<li>Always compare against simple baselines and show effect of each component.</li>
</ul></li>
<li><p>Statistical testing</p>
<ul>
<li>Use significance tests or multiple comparison corrections when comparing many models.</li>
</ul></li>
<li><p>Reproducibility</p>
<ul>
<li>Fix random seeds, log hyperparameters, and share code/data splits.</li>
</ul></li>
</ol>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Principle</th>
<th>Why It Matters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>No leakage</td>
<td>Prevents inflated results</td>
</tr>
<tr class="even">
<td>Multiple metrics</td>
<td>Captures tradeoffs</td>
</tr>
<tr class="odd">
<td>Variance reporting</td>
<td>Avoids cherry-picking</td>
</tr>
<tr class="even">
<td>Baselines</td>
<td>Clarifies improvement source</td>
</tr>
<tr class="odd">
<td>Statistical tests</td>
<td>Ensures results are real</td>
</tr>
<tr class="even">
<td>Reproducibility</td>
<td>Enables trust and verification</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-39" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-39">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> make_scorer, f1_score</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="co"># synthetic dataset</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.randn(<span class="dv">200</span>, <span class="dv">5</span>)</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (X[:,<span class="dv">0</span>] <span class="op">+</span> X[:,<span class="dv">1</span>] <span class="op">&gt;</span> <span class="dv">0</span>).astype(<span class="bu">int</span>)</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate with multiple metrics</span></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>acc_scores <span class="op">=</span> cross_val_score(model, X, y, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">"accuracy"</span>)</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>f1_scores <span class="op">=</span> cross_val_score(model, X, y, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span>make_scorer(f1_score))</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Accuracy mean ± std:"</span>, acc_scores.mean(), acc_scores.std())</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"F1 mean ± std:"</span>, f1_scores.mean(), f1_scores.std())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-39" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-39">Why it Matters</h4>
<p>A model that looks good under sloppy evaluation may fail in deployment. Following best practices avoids false claims, ensures fair comparison, and builds confidence in results.</p>
</section>
<section id="try-it-yourself-39" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-39">Try It Yourself</h4>
<ol type="1">
<li>Evaluate models with accuracy only, then add F1 and AUC. How does the ranking change?</li>
<li>Run cross-validation with different random seeds. Do your reported results remain stable?</li>
<li>Reflect: in a high-stakes domain (e.g., healthcare), which best practice is most critical—leakage prevention, multiple metrics, or reproducibility?</li>
</ol>
</section>
</section>
</section>
<section id="chapter-65.-linear-and-generalized-linear-models" class="level2">
<h2 class="anchored" data-anchor-id="chapter-65.-linear-and-generalized-linear-models">Chapter 65. Linear and Generalized Linear Models</h2>
<section id="linear-regression-basics" class="level3">
<h3 class="anchored" data-anchor-id="linear-regression-basics">641. Linear Regression Basics</h3>
<p>Linear regression is the foundation of supervised learning for regression tasks. It models the relationship between input features and a continuous target by fitting a straight line (or hyperplane in higher dimensions) that minimizes prediction error.</p>
<section id="picture-in-your-head-40" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-40">Picture in Your Head</h4>
<p>Imagine plotting house prices against square footage. Each point is a house, and linear regression draws the “best-fit” line through the cloud of points. The slope tells you how much price changes per square foot, and the intercept gives the baseline value.</p>
</section>
<section id="deep-dive-40" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-40">Deep Dive</h4>
<ul>
<li>Model form:</li>
</ul>
<p><span class="math display">\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \epsilon
\]</span></p>
<ul>
<li><p><span class="math inline">\(y\)</span>: target variable</p></li>
<li><p><span class="math inline">\(x_i\)</span>: features</p></li>
<li><p><span class="math inline">\(\beta_i\)</span>: coefficients (weights)</p></li>
<li><p><span class="math inline">\(\epsilon\)</span>: error term</p></li>
<li><p>Objective: Minimize Residual Sum of Squares (RSS)</p></li>
</ul>
<p><span class="math display">\[
RSS(\beta) = \sum_{i=1}^n (y_i - \hat{y}_i)^2
\]</span></p>
<ul>
<li>Solution (closed form):</li>
</ul>
<p><span class="math display">\[
\hat{\beta} = (X^TX)^{-1}X^Ty
\]</span></p>
<p>where <span class="math inline">\(X\)</span> is the design matrix of features.</p>
<ul>
<li><p>Assumptions:</p>
<ol type="1">
<li>Linearity (relationship between features and target is linear).</li>
<li>Independence (errors are independent).</li>
<li>Homoscedasticity (constant error variance).</li>
<li>Normality (errors follow normal distribution).</li>
</ol></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Strength</th>
<th>Weakness</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Simple, interpretable</td>
<td>Assumes linearity</td>
</tr>
<tr class="even">
<td>Fast to compute</td>
<td>Sensitive to outliers</td>
</tr>
<tr class="odd">
<td>Analytical solution</td>
<td>Multicollinearity causes instability</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-40" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-40">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="co"># toy dataset</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">1</span>], [<span class="dv">2</span>], [<span class="dv">3</span>], [<span class="dv">4</span>], [<span class="dv">5</span>]])</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">8</span>, <span class="dv">10</span>])  <span class="co"># perfectly linear</span></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression().fit(X, y)</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Intercept:"</span>, model.intercept_)</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Coefficient:"</span>, model.coef_)</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Prediction for x=6:"</span>, model.predict([[<span class="dv">6</span>]])[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-40" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-40">Why it Matters</h4>
<p>Linear regression remains one of the most widely used tools in data science. Its interpretability and simplicity make it a benchmark for more complex models. Even in modern ML, understanding linear regression builds intuition for optimization, regularization, and feature effects.</p>
</section>
<section id="try-it-yourself-40" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-40">Try It Yourself</h4>
<ol type="1">
<li>Fit linear regression on noisy data. How well does the line approximate the trend?</li>
<li>Add an irrelevant feature. Does it change coefficients significantly?</li>
<li>Reflect: why is linear regression still preferred in economics and healthcare despite the rise of deep learning?</li>
</ol>
</section>
</section>
<section id="maximum-likelihood-and-least-squares" class="level3">
<h3 class="anchored" data-anchor-id="maximum-likelihood-and-least-squares">642. Maximum Likelihood and Least Squares</h3>
<p>Linear regression can be derived from two perspectives: Least Squares Estimation (LSE) and Maximum Likelihood Estimation (MLE). Surprisingly, they lead to the same solution under standard assumptions, linking geometry and probability in regression.</p>
<section id="picture-in-your-head-41" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-41">Picture in Your Head</h4>
<p>Think of fitting a line through points:</p>
<ul>
<li>Least Squares: minimize the sum of squared vertical distances from points to the line.</li>
<li>Maximum Likelihood: assume errors are Gaussian and find parameters that maximize the probability of observing the data.</li>
</ul>
<p>Both methods give you the same fitted line.</p>
</section>
<section id="deep-dive-41" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-41">Deep Dive</h4>
<ul>
<li><p>Least Squares Estimation (LSE)</p>
<ul>
<li>Objective: minimize residual sum of squares</li>
</ul>
<p><span class="math display">\[
\hat{\beta} = \arg \min_\beta \sum_{i=1}^n (y_i - x_i^T\beta)^2
\]</span></p>
<ul>
<li>Solution:</li>
</ul>
<p><span class="math display">\[
\hat{\beta} = (X^TX)^{-1}X^Ty
\]</span></p></li>
<li><p>Maximum Likelihood Estimation (MLE)</p>
<ul>
<li>Assume errors <span class="math inline">\(\epsilon_i \sim \mathcal{N}(0, \sigma^2)\)</span>.</li>
<li>Likelihood function:</li>
</ul>
<p><span class="math display">\[
L(\beta, \sigma^2) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(y_i - x_i^T\beta)^2}{2\sigma^2} \right)
\]</span></p>
<ul>
<li>Log-likelihood maximization yields the same <span class="math inline">\(\hat{\beta}\)</span> as least squares.</li>
</ul></li>
<li><p>Connection:</p>
<ul>
<li>LSE = purely geometric criterion.</li>
<li>MLE = statistical inference criterion.</li>
<li>They coincide under Gaussian error assumptions.</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Method</th>
<th>Viewpoint</th>
<th>Assumptions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>LSE</td>
<td>Geometry (distances)</td>
<td>None beyond squared error</td>
</tr>
<tr class="even">
<td>MLE</td>
<td>Probability (likelihood)</td>
<td>Gaussian errors</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-41" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-41">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="co"># synthetic linear data</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.randn(<span class="dv">100</span>, <span class="dv">1</span>)</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">3</span><span class="op">*</span>X[:,<span class="dv">0</span>] <span class="op">+</span> <span class="dv">2</span> <span class="op">+</span> np.random.randn(<span class="dv">100</span>)<span class="op">*</span><span class="fl">0.5</span></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression().fit(X, y)</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Estimated coefficients:"</span>, model.coef_)</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Estimated intercept:"</span>, model.intercept_)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-41" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-41">Why it Matters</h4>
<p>Understanding the equivalence of least squares and maximum likelihood clarifies why linear regression is both geometrically intuitive and statistically grounded. It also highlights that different assumptions (e.g., non-Gaussian errors) can lead to different estimation methods.</p>
</section>
<section id="try-it-yourself-41" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-41">Try It Yourself</h4>
<ol type="1">
<li>Simulate data with Gaussian noise. Compare LSE and MLE results.</li>
<li>Simulate data with heavy-tailed noise (e.g., Laplace). Do LSE and MLE still coincide?</li>
<li>Reflect: in real-world regression, are you implicitly assuming Gaussian errors when using least squares?</li>
</ol>
</section>
</section>
<section id="logistic-regression-for-classification" class="level3">
<h3 class="anchored" data-anchor-id="logistic-regression-for-classification">643. Logistic Regression for Classification</h3>
<p>Logistic regression extends linear models to classification tasks by modeling the probability of class membership. Instead of predicting continuous values, it predicts the likelihood that an input belongs to a certain class, using the logistic (sigmoid) function.</p>
<section id="picture-in-your-head-42" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-42">Picture in Your Head</h4>
<p>Imagine a seesaw tilted by input features:</p>
<ul>
<li>On one side, the probability of “class 0.”</li>
<li>On the other, the probability of “class 1.” The logistic curve smoothly translates the seesaw’s tilt (linear score) into a probability between 0 and 1.</li>
</ul>
</section>
<section id="deep-dive-42" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-42">Deep Dive</h4>
<ul>
<li><p>Model form: For binary classification with features <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[
P(y=1 \mid x) = \sigma(x^T\beta) = \frac{1}{1 + e^{-x^T\beta}}
\]</span></p>
<p>where <span class="math inline">\(\sigma(\cdot)\)</span> is the sigmoid function.</p></li>
<li><p>Decision rule:</p>
<ul>
<li>Predict class 1 if <span class="math inline">\(P(y=1|x) &gt; 0.5\)</span>.</li>
<li>Threshold can be shifted depending on application (e.g., medical tests).</li>
</ul></li>
<li><p>Training:</p>
<ul>
<li>Parameters <span class="math inline">\(\beta\)</span> are estimated by Maximum Likelihood Estimation.</li>
<li>Loss function = Log Loss (Cross-Entropy):</li>
</ul>
<p><span class="math display">\[
L(\beta) = - \sum_{i=1}^n \left[ y_i \log \hat{p}_i + (1-y_i) \log (1-\hat{p}_i) \right]
\]</span></p></li>
<li><p>Extensions:</p>
<ul>
<li>Multinomial logistic regression for multi-class problems.</li>
<li>Regularized logistic regression with L1/L2 penalties for high-dimensional data.</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Feature</th>
<th>Linear Regression</th>
<th>Logistic Regression</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Output</td>
<td>Continuous value</td>
<td>Probability (0–1)</td>
</tr>
<tr class="even">
<td>Loss</td>
<td>Squared error</td>
<td>Cross-entropy</td>
</tr>
<tr class="odd">
<td>Task</td>
<td>Regression</td>
<td>Classification</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-42" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-42">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="co"># toy dataset</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">0</span>], [<span class="dv">1</span>], [<span class="dv">2</span>], [<span class="dv">3</span>]])</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>])  <span class="co"># binary classes</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression().fit(X, y)</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predicted probabilities:"</span>, model.predict_proba([[<span class="fl">1.5</span>]]))</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predicted class:"</span>, model.predict([[<span class="fl">1.5</span>]]))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-42" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-42">Why it Matters</h4>
<p>Logistic regression is one of the most widely used classification algorithms due to its interpretability, efficiency, and statistical foundation. It remains a baseline in machine learning, especially when explainability is required (e.g., healthcare, finance).</p>
</section>
<section id="try-it-yourself-42" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-42">Try It Yourself</h4>
<ol type="1">
<li>Train logistic regression on a binary dataset. Compare probability outputs vs.&nbsp;hard predictions.</li>
<li>Adjust classification threshold from 0.5 to 0.3. How do precision and recall change?</li>
<li>Reflect: why might logistic regression still be preferred over complex models in regulated industries?</li>
</ol>
</section>
</section>
<section id="generalized-linear-model-framework" class="level3">
<h3 class="anchored" data-anchor-id="generalized-linear-model-framework">644. Generalized Linear Model Framework</h3>
<p>Generalized Linear Models (GLMs) extend linear regression to handle different types of response variables (binary, counts, rates) by introducing a link function that connects the linear predictor to the expected value of the outcome. GLMs unify regression approaches under a single framework.</p>
<section id="picture-in-your-head-43" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-43">Picture in Your Head</h4>
<p>Think of a translator:</p>
<ul>
<li>The model computes a linear predictor (<span class="math inline">\(X\beta\)</span>).</li>
<li>The link function translates this into a valid outcome (e.g., probabilities, counts). Different translators (links) allow the same linear machinery to work across tasks.</li>
</ul>
</section>
<section id="deep-dive-43" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-43">Deep Dive</h4>
<p>A GLM has three components:</p>
<ol type="1">
<li><p>Random component: Specifies the distribution of the response variable (Gaussian, Binomial, Poisson, etc.).</p></li>
<li><p>Systematic component: A linear predictor, <span class="math inline">\(\eta = X\beta\)</span>.</p></li>
<li><p>Link function: Connects mean response <span class="math inline">\(\mu\)</span> to predictor:</p>
<p><span class="math display">\[
g(\mu) = \eta
\]</span></p></li>
</ol>
<p>Examples:</p>
<ul>
<li>Linear regression: Gaussian, identity link (<span class="math inline">\(\mu = \eta\)</span>).</li>
<li>Logistic regression: Binomial, logit link (<span class="math inline">\(\mu = \sigma(\eta)\)</span>).</li>
<li>Poisson regression: Count data, log link (<span class="math inline">\(\mu = e^\eta\)</span>).</li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Distribution</th>
<th>Link Function</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear regression</td>
<td>Gaussian</td>
<td>Identity</td>
</tr>
<tr class="even">
<td>Logistic regression</td>
<td>Binomial</td>
<td>Logit</td>
</tr>
<tr class="odd">
<td>Poisson regression</td>
<td>Poisson</td>
<td>Log</td>
</tr>
<tr class="even">
<td>Gamma regression</td>
<td>Gamma</td>
<td>Inverse</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, using statsmodels)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="co"># toy Poisson regression (count data)</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="dv">6</span>)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">11</span>])  <span class="co"># counts</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> sm.add_constant(X)  <span class="co"># add intercept</span></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> sm.GLM(y, X, family<span class="op">=</span>sm.families.Poisson()).fit()</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.summary())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-43" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-43">Why it Matters</h4>
<p>GLMs provide a unified framework that generalizes beyond continuous outcomes. They are widely used in healthcare, insurance, and social sciences, where outcomes may be binary (disease presence), counts (claims), or rates (events per time).</p>
</section>
<section id="try-it-yourself-43" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-43">Try It Yourself</h4>
<ol type="1">
<li>Fit logistic regression as a GLM with a logit link. Compare coefficients with scikit-learn’s LogisticRegression.</li>
<li>Model count data with Poisson regression. Does the log link improve fit over linear regression?</li>
<li>Reflect: why does a unified GLM framework simplify modeling across diverse domains?</li>
</ol>
</section>
</section>
<section id="link-functions-and-canonical-forms" class="level3">
<h3 class="anchored" data-anchor-id="link-functions-and-canonical-forms">645. Link Functions and Canonical Forms</h3>
<p>The link function in a Generalized Linear Model (GLM) transforms the expected value of the response variable into a scale where the linear predictor operates. Canonical link functions arise naturally from the exponential family of distributions and simplify estimation.</p>
<section id="picture-in-your-head-44" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-44">Picture in Your Head</h4>
<p>Imagine having different types of “lenses” for viewing data:</p>
<ul>
<li>With the identity lens, you see values directly.</li>
<li>With the logit lens, probabilities become linear.</li>
<li>With the log lens, counts grow additively instead of multiplicatively. Each lens makes the relationship easier to work with.</li>
</ul>
</section>
<section id="deep-dive-44" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-44">Deep Dive</h4>
<ul>
<li><p>General form:</p>
<p><span class="math display">\[
g(\mu) = \eta = X\beta
\]</span></p>
<p>where <span class="math inline">\(g(\cdot)\)</span> is the link function, <span class="math inline">\(\mu = E[y]\)</span>.</p></li>
<li><p>Canonical link function: the natural link derived from the exponential family distribution of the outcome.</p>
<ul>
<li>Makes estimation simpler (via sufficient statistics).</li>
<li>Provides desirable statistical properties (e.g., Fisher scoring efficiency).</li>
</ul></li>
</ul>
<p>Examples:</p>
<ul>
<li>Gaussian (normal) → Identity link (<span class="math inline">\(\mu = \eta\)</span>).</li>
<li>Binomial → Logit link (<span class="math inline">\(\mu = \frac{1}{1+e^{-\eta}}\)</span>).</li>
<li>Poisson → Log link (<span class="math inline">\(\mu = e^\eta\)</span>).</li>
<li>Gamma → Inverse link (<span class="math inline">\(\mu = 1/\eta\)</span>).</li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Distribution</th>
<th>Canonical Link</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Gaussian</td>
<td>Identity</td>
<td>Linear mean</td>
</tr>
<tr class="even">
<td>Binomial</td>
<td>Logit</td>
<td>Probability mapping</td>
</tr>
<tr class="odd">
<td>Poisson</td>
<td>Log</td>
<td>Counts grow multiplicatively</td>
</tr>
<tr class="even">
<td>Gamma</td>
<td>Inverse</td>
<td>Rates/scale modeling</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, statsmodels)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="co"># simulate binary outcome</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>])</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>])  <span class="co"># binary classes</span></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> sm.add_constant(X)</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>logit_model <span class="op">=</span> sm.GLM(y, X, family<span class="op">=</span>sm.families.Binomial(link<span class="op">=</span>sm.families.links.logit())).fit()</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(logit_model.summary())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-44" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-44">Why it Matters</h4>
<p>Link functions allow a single GLM framework to adapt across regression, classification, and count models. Choosing the canonical link often yields efficient, stable estimation, but alternative links may better match domain knowledge (e.g., probit for psychometrics).</p>
</section>
<section id="try-it-yourself-44" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-44">Try It Yourself</h4>
<ol type="1">
<li>Fit logistic regression with logit and probit links. Compare predictions.</li>
<li>Model count data using Poisson regression with log vs.&nbsp;identity link. Which fits better?</li>
<li>Reflect: in your field, do practitioners prefer canonical links for theory, or alternative links for interpretability?</li>
</ol>
</section>
</section>
<section id="poisson-and-exponential-regression-models" class="level3">
<h3 class="anchored" data-anchor-id="poisson-and-exponential-regression-models">646. Poisson and Exponential Regression Models</h3>
<p>Poisson and exponential regression models are special cases of GLMs designed for count data (Poisson) and time-to-event data (exponential). They connect linear predictors to non-negative outcomes via log or inverse links.</p>
<section id="picture-in-your-head-45" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-45">Picture in Your Head</h4>
<p>Think of counting buses at a station:</p>
<ul>
<li>Poisson regression models the expected number of buses arriving in an hour.</li>
<li>Exponential regression models the waiting time between buses.</li>
</ul>
</section>
<section id="deep-dive-45" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-45">Deep Dive</h4>
<ul>
<li><p>Poisson Regression</p>
<ul>
<li>Suitable for counts (<span class="math inline">\(y = 0, 1, 2, \dots\)</span>).</li>
<li>Model:</li>
</ul>
<p><span class="math display">\[
y \sim \text{Poisson}(\mu), \quad \log(\mu) = X\beta
\]</span></p>
<ul>
<li>Assumes mean = variance (equidispersion).</li>
<li>Extensions: quasi-Poisson, negative binomial for overdispersion.</li>
</ul></li>
<li><p>Exponential Regression</p>
<ul>
<li>Suitable for non-negative continuous data (e.g., survival time).</li>
<li>Model:</li>
</ul>
<p><span class="math display">\[
y \sim \text{Exponential}(\lambda), \quad \lambda = e^{X\beta}
\]</span></p>
<ul>
<li>Special case of survival models; hazard rate is constant.</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Outcome Type</th>
<th>Link</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Poisson</td>
<td>Counts</td>
<td>Log</td>
<td>Event counts, traffic, claims</td>
</tr>
<tr class="even">
<td>Exponential</td>
<td>Time-to-event</td>
<td>Log</td>
<td>Waiting times, reliability</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, statsmodels)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="co"># toy Poisson dataset</span></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="dv">6</span>)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">9</span>])  <span class="co"># count data</span></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> sm.add_constant(X)</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>poisson_model <span class="op">=</span> sm.GLM(y, X, family<span class="op">=</span>sm.families.Poisson()).fit()</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Poisson coefficients:"</span>, poisson_model.params)</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a><span class="co"># toy exponential regression can be modeled using survival analysis libraries</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-45" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-45">Why it Matters</h4>
<p>These models are widely used in epidemiology, reliability engineering, and insurance. They formalize how covariates influence event counts or waiting times and lay the foundation for survival analysis and hazard modeling.</p>
</section>
<section id="try-it-yourself-45" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-45">Try It Yourself</h4>
<ol type="1">
<li>Fit Poisson regression on count data (e.g., number of hospital visits per patient). Does variance ≈ mean?</li>
<li>Compare Poisson vs.&nbsp;negative binomial on overdispersed data.</li>
<li>Reflect: why is exponential regression often too restrictive for real-world survival times?</li>
</ol>
</section>
</section>
<section id="multinomial-and-ordinal-regression" class="level3">
<h3 class="anchored" data-anchor-id="multinomial-and-ordinal-regression">647. Multinomial and Ordinal Regression</h3>
<p>When the outcome variable has more than two categories, we extend logistic regression to multinomial regression (unordered categories) or ordinal regression (ordered categories). These models capture richer classification structures than binary logistic regression.</p>
<section id="picture-in-your-head-46" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-46">Picture in Your Head</h4>
<ul>
<li>Multinomial regression: Choosing a fruit at the market (apple, banana, orange). No inherent order.</li>
<li>Ordinal regression: Movie ratings (poor, fair, good, excellent). The labels have a clear ranking.</li>
</ul>
</section>
<section id="deep-dive-46" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-46">Deep Dive</h4>
<ul>
<li><p>Multinomial Logistic Regression</p>
<ul>
<li>Outcome <span class="math inline">\(y \in \{1,2,\dots,K\}\)</span>.</li>
<li>Probability of class <span class="math inline">\(k\)</span>:</li>
</ul>
<p><span class="math display">\[
P(y=k|x) = \frac{\exp(x^T\beta_k)}{\sum_{j=1}^K \exp(x^T\beta_j)}
\]</span></p>
<ul>
<li>Generalizes binary logistic regression via the softmax function.</li>
</ul></li>
<li><p>Ordinal Logistic Regression (Proportional Odds Model)</p>
<ul>
<li>Assumes an ordering among classes.</li>
<li>Cumulative logit model:</li>
</ul>
<p><span class="math display">\[
\log \frac{P(y \leq k)}{P(y &gt; k)} = \theta_k - x^T\beta
\]</span></p>
<ul>
<li>Separate thresholds <span class="math inline">\(\theta_k\)</span> for categories, but shared slope <span class="math inline">\(\beta\)</span>.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 28%">
<col style="width: 33%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Outcome Type</th>
<th>Assumption</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Multinomial</td>
<td>Nominal (unordered)</td>
<td>No ordering</td>
<td>Fruit choice</td>
</tr>
<tr class="even">
<td>Ordinal</td>
<td>Ordered</td>
<td>Monotonic relationship</td>
<td>Survey ratings</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, scikit-learn)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="co"># toy multinomial dataset</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">1</span>], [<span class="dv">2</span>], [<span class="dv">3</span>], [<span class="dv">4</span>], [<span class="dv">5</span>]])</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>])  <span class="co"># three classes</span></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(multi_class<span class="op">=</span><span class="st">"multinomial"</span>, solver<span class="op">=</span><span class="st">"lbfgs"</span>).fit(X, y)</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predicted probabilities for x=3:"</span>, model.predict_proba([[<span class="dv">3</span>]]))</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predicted class:"</span>, model.predict([[<span class="dv">3</span>]]))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-46" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-46">Why it Matters</h4>
<p>Many real-world problems involve multi-class or ordinal outcomes: medical diagnosis categories, customer satisfaction levels, credit ratings. Choosing between multinomial and ordinal regression ensures that models respect the data’s structure and provide meaningful predictions.</p>
</section>
<section id="try-it-yourself-46" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-46">Try It Yourself</h4>
<ol type="1">
<li>Train multinomial regression on the Iris dataset. Compare probabilities across classes.</li>
<li>Fit ordinal regression on a survey dataset with ordered responses. Does it capture monotonic effects?</li>
<li>Reflect: why would using multinomial regression on ordinal data lose valuable structure?</li>
</ol>
</section>
</section>
<section id="regularized-linear-models-ridge-lasso-elastic-net" class="level3">
<h3 class="anchored" data-anchor-id="regularized-linear-models-ridge-lasso-elastic-net">648. Regularized Linear Models (Ridge, Lasso, Elastic Net)</h3>
<p>Regularized linear models extend ordinary least squares by adding penalties on coefficients to control complexity and improve generalization. Ridge (L2), Lasso (L1), and Elastic Net (a mix of both) balance bias and variance while handling multicollinearity and high-dimensional data.</p>
<section id="picture-in-your-head-47" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-47">Picture in Your Head</h4>
<p>Think of pruning a tree:</p>
<ul>
<li>Ridge trims all branches evenly (shrinks all coefficients).</li>
<li>Lasso cuts off some branches entirely (drives coefficients to zero).</li>
<li>Elastic Net does both—shrinks most and removes a few completely.</li>
</ul>
</section>
<section id="deep-dive-47" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-47">Deep Dive</h4>
<ul>
<li>Ridge Regression (L2):</li>
</ul>
<p><span class="math display">\[
\hat{\beta} = \arg \min_\beta \left( \sum (y_i - x_i^T\beta)^2 + \lambda \sum \beta_j^2 \right)
\]</span></p>
<ul>
<li><p>Shrinks coefficients smoothly.</p></li>
<li><p>Handles multicollinearity well.</p></li>
<li><p>Lasso Regression (L1):</p></li>
</ul>
<p><span class="math display">\[
\hat{\beta} = \arg \min_\beta \left( \sum (y_i - x_i^T\beta)^2 + \lambda \sum |\beta_j| \right)
\]</span></p>
<ul>
<li><p>Produces sparse models (feature selection).</p></li>
<li><p>Elastic Net:</p></li>
</ul>
<p><span class="math display">\[
\hat{\beta} = \arg \min_\beta \left( \sum (y_i - x_i^T\beta)^2 + \lambda_1 \sum |\beta_j| + \lambda_2 \sum \beta_j^2 \right)
\]</span></p>
<ul>
<li>Balances sparsity and stability.</li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Penalty</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Ridge</td>
<td>L2</td>
<td>Shrinks coefficients, keeps all features</td>
</tr>
<tr class="even">
<td>Lasso</td>
<td>L1</td>
<td>Sparsity, automatic feature selection</td>
</tr>
<tr class="odd">
<td>Elastic Net</td>
<td>L1 + L2</td>
<td>Hybrid: stability + sparsity</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, scikit-learn)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge, Lasso, ElasticNet</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a><span class="co"># toy dataset</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.randn(<span class="dv">50</span>, <span class="dv">5</span>)</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> X[:,<span class="dv">0</span>]<span class="op">*</span><span class="dv">3</span> <span class="op">+</span> X[:,<span class="dv">1</span>]<span class="op">*-</span><span class="dv">2</span> <span class="op">+</span> np.random.randn(<span class="dv">50</span>)</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>ridge <span class="op">=</span> Ridge(alpha<span class="op">=</span><span class="fl">1.0</span>).fit(X, y)</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>lasso <span class="op">=</span> Lasso(alpha<span class="op">=</span><span class="fl">0.1</span>).fit(X, y)</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>enet <span class="op">=</span> ElasticNet(alpha<span class="op">=</span><span class="fl">0.1</span>, l1_ratio<span class="op">=</span><span class="fl">0.5</span>).fit(X, y)</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Ridge coefficients:"</span>, ridge.coef_)</span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Lasso coefficients:"</span>, lasso.coef_)</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Elastic Net coefficients:"</span>, enet.coef_)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-47" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-47">Why it Matters</h4>
<p>Regularization is essential when features are correlated or when data is high-dimensional. Ridge improves stability, Lasso enhances interpretability by selecting features, and Elastic Net strikes a balance, making them powerful tools in applied ML.</p>
</section>
<section id="try-it-yourself-47" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-47">Try It Yourself</h4>
<ol type="1">
<li>Compare Ridge vs.&nbsp;Lasso on data with irrelevant features. Which ignores them better?</li>
<li>Increase regularization strength (<span class="math inline">\(\lambda\)</span>) gradually. How do coefficients shrink?</li>
<li>Reflect: in domains with thousands of features (e.g., genomics), why might Elastic Net outperform Ridge or Lasso alone?</li>
</ol>
</section>
</section>
<section id="interpretability-and-coefficients" class="level3">
<h3 class="anchored" data-anchor-id="interpretability-and-coefficients">649. Interpretability and Coefficients</h3>
<p>Linear and generalized linear models are prized for their interpretability. Model coefficients directly quantify how features influence predictions, offering transparency that is often lost in more complex models.</p>
<section id="picture-in-your-head-48" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-48">Picture in Your Head</h4>
<p>Imagine adjusting knobs on a control panel:</p>
<ul>
<li>Each knob (coefficient) changes the output (prediction).</li>
<li>Positive knobs push the outcome upward, negative knobs push it downward.</li>
<li>The magnitude tells you how strongly each knob matters.</li>
</ul>
</section>
<section id="deep-dive-48" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-48">Deep Dive</h4>
<ul>
<li>Linear regression coefficients (<span class="math inline">\(\beta_j\)</span>): represent the expected change in the outcome for a one-unit increase in feature <span class="math inline">\(x_j\)</span>, holding others constant.</li>
<li>Logistic regression coefficients: represent the change in log-odds of the outcome per unit increase in <span class="math inline">\(x_j\)</span>. Exponentiating coefficients gives odds ratios.</li>
<li>Standardization: scaling features (mean 0, variance 1) makes coefficients comparable in magnitude.</li>
<li>Regularization effects: Lasso can zero out coefficients, highlighting the most relevant features; Ridge shrinks them but retains all.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 26%">
<col style="width: 73%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Coefficient Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear Regression</td>
<td>Change in outcome per unit change in feature</td>
</tr>
<tr class="even">
<td>Logistic Regression</td>
<td>Change in log-odds (odds ratio when exponentiated)</td>
</tr>
<tr class="odd">
<td>Poisson Regression</td>
<td>Change in log-counts (multiplicative effect on counts)</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, scikit-learn)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="co"># toy dataset</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">2</span>, <span class="dv">1</span>], [<span class="dv">3</span>, <span class="dv">4</span>], [<span class="dv">4</span>, <span class="dv">3</span>]])</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>])  <span class="co"># binary outcome</span></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression().fit(X, y)</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Coefficients:"</span>, model.coef_)</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Intercept:"</span>, model.intercept_)</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a><span class="co"># interpret as odds ratios</span></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>odds_ratios <span class="op">=</span> np.exp(model.coef_)</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Odds Ratios:"</span>, odds_ratios)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-48" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-48">Why it Matters</h4>
<p>Coefficient interpretation builds trust and provides insights beyond prediction. In regulated domains like medicine, finance, and law, stakeholders often demand explanations: “Which features drive this decision?” Linear models remain indispensable for this reason.</p>
</section>
<section id="try-it-yourself-48" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-48">Try It Yourself</h4>
<ol type="1">
<li>Train a logistic regression model and compute odds ratios. Which features increase vs.&nbsp;decrease the odds?</li>
<li>Standardize your data before fitting. Do coefficient magnitudes become more comparable?</li>
<li>Reflect: why is interpretability often valued over predictive power in high-stakes decision-making?</li>
</ol>
</section>
</section>
<section id="applications-across-domains" class="level3">
<h3 class="anchored" data-anchor-id="applications-across-domains">650. Applications Across Domains</h3>
<p>Linear and generalized linear models (GLMs) remain core tools across many fields. Their balance of simplicity, interpretability, and statistical rigor makes them the first choice in domains where transparency and reliability matter as much as predictive accuracy.</p>
<section id="picture-in-your-head-49" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-49">Picture in Your Head</h4>
<p>Think of GLMs as a Swiss army knife:</p>
<ul>
<li>Not the flashiest tool, but reliable and adaptable.</li>
<li>Economists, doctors, engineers, and social scientists all carry it in their toolkit.</li>
</ul>
</section>
<section id="deep-dive-49" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-49">Deep Dive</h4>
<ul>
<li><p>Economics &amp; Finance</p>
<ul>
<li>Linear regression: modeling returns, risk factors (CAPM, Fama–French).</li>
<li>Logistic regression: credit scoring, bankruptcy prediction.</li>
<li>Poisson/Negative binomial: modeling counts like number of trades.</li>
</ul></li>
<li><p>Healthcare &amp; Epidemiology</p>
<ul>
<li>Logistic regression: disease risk prediction, treatment effectiveness.</li>
<li>Poisson regression: modeling incidence rates of diseases.</li>
<li>Survival analysis extensions: exponential and Cox models.</li>
</ul></li>
<li><p>Social Sciences</p>
<ul>
<li>Ordinal regression: Likert scale survey responses.</li>
<li>Multinomial regression: voting choice modeling.</li>
<li>Linear regression: causal inference with covariates.</li>
</ul></li>
<li><p>Engineering &amp; Reliability</p>
<ul>
<li>Exponential regression: failure times of machines.</li>
<li>Poisson regression: number of breakdowns/events.</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Domain</th>
<th>Typical GLM Use</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Finance</td>
<td>Credit scoring, asset pricing</td>
</tr>
<tr class="even">
<td>Healthcare</td>
<td>Risk prediction, survival analysis</td>
</tr>
<tr class="odd">
<td>Social sciences</td>
<td>Surveys, voting behavior</td>
</tr>
<tr class="even">
<td>Engineering</td>
<td>Failure rates, reliability</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, scikit-learn)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="co"># toy credit scoring example</span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">50000</span>, <span class="dv">0</span>], [<span class="dv">60000</span>, <span class="dv">1</span>], [<span class="dv">40000</span>, <span class="dv">0</span>], [<span class="dv">30000</span>, <span class="dv">1</span>]])  <span class="co"># [income, late_payments]</span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>])  <span class="co"># default (1) or not (0)</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression().fit(X, y)</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Coefficients:"</span>, model.coef_)</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predicted default probability for income=55000, 1 late payment:"</span>,</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>      model.predict_proba([[<span class="dv">55000</span>, <span class="dv">1</span>]])[<span class="dv">0</span>,<span class="dv">1</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-49" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-49">Why it Matters</h4>
<p>Even as deep learning dominates headlines, GLMs remain indispensable where interpretability, efficiency, and trustworthiness are required. They often serve as baselines in ML pipelines and provide clarity that black-box models cannot.</p>
</section>
<section id="try-it-yourself-49" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-49">Try It Yourself</h4>
<ol type="1">
<li>Apply logistic regression to a medical dataset (e.g., predicting disease presence). Compare interpretability vs.&nbsp;neural networks.</li>
<li>Use Poisson regression for count data (e.g., customer purchases per month). Does the log link improve predictions?</li>
<li>Reflect: in your domain, would you trade interpretability for a few extra percentage points of accuracy?</li>
</ol>
</section>
</section>
</section>
<section id="chapter-66.-kernel-methods-and-svms" class="level2">
<h2 class="anchored" data-anchor-id="chapter-66.-kernel-methods-and-svms">Chapter 66. Kernel methods and SVMs</h2>
<section id="the-kernel-trick-from-linear-to-nonlinear" class="level3">
<h3 class="anchored" data-anchor-id="the-kernel-trick-from-linear-to-nonlinear">651. The Kernel Trick: From Linear to Nonlinear</h3>
<p>The kernel trick allows linear algorithms to learn nonlinear patterns by implicitly mapping data into a higher-dimensional feature space. Instead of explicitly computing transformations, kernels compute inner products in that space, keeping computations efficient.</p>
<section id="picture-in-your-head-50" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-50">Picture in Your Head</h4>
<p>Imagine drawing a line to separate two groups of points on paper:</p>
<ul>
<li>In 2D, the groups overlap.</li>
<li>If you lift the points into 3D, suddenly a flat plane separates them cleanly. The kernel trick lets you do this “lifting” without ever leaving 2D—like separating shadows by reasoning about the unseen 3D objects casting them.</li>
</ul>
</section>
<section id="deep-dive-50" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-50">Deep Dive</h4>
<ul>
<li><p>Feature mapping idea:</p>
<ul>
<li>Original input: <span class="math inline">\(x \in \mathbb{R}^d\)</span>.</li>
<li>Feature map: <span class="math inline">\(\phi(x) \in \mathbb{R}^D\)</span>, often with <span class="math inline">\(D \gg d\)</span>.</li>
<li>Kernel function:</li>
</ul>
<p><span class="math display">\[
K(x, x') = \langle \phi(x), \phi(x') \rangle
\]</span></p></li>
<li><p>Common kernels:</p>
<ul>
<li><p>Linear: <span class="math inline">\(K(x,x') = x^T x'\)</span>.</p></li>
<li><p>Polynomial: <span class="math inline">\(K(x,x') = (x^T x' + c)^d\)</span>.</p></li>
<li><p>RBF (Gaussian):</p>
<p><span class="math display">\[
K(x,x') = \exp\left(-\frac{\|x-x'\|^2}{2\sigma^2}\right)
\]</span></p></li>
</ul></li>
<li><p>Why it works: Many algorithms (like SVMs, PCA, regression) depend only on dot products. Replacing dot products with kernels makes them nonlinear without rewriting the algorithm.</p></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Kernel</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear</td>
<td>Standard inner product</td>
</tr>
<tr class="even">
<td>Polynomial</td>
<td>Captures feature interactions up to degree <span class="math inline">\(d\)</span></td>
</tr>
<tr class="odd">
<td>RBF (Gaussian)</td>
<td>Infinite-dimensional, captures local similarity</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, scikit-learn)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="co"># toy dataset</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">0</span>],[<span class="dv">1</span>,<span class="dv">1</span>],[<span class="dv">1</span>,<span class="dv">0</span>],[<span class="dv">0</span>,<span class="dv">1</span>]])</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>]</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a><span class="co"># linear vs RBF kernel</span></span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>svc_linear <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">"linear"</span>).fit(X,y)</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>svc_rbf <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">"rbf"</span>, gamma<span class="op">=</span><span class="dv">1</span>).fit(X,y)</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Linear kernel predictions:"</span>, svc_linear.predict(X))</span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"RBF kernel predictions:"</span>, svc_rbf.predict(X))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-50" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-50">Why it Matters</h4>
<p>The kernel trick powers many classical ML methods, most famously Support Vector Machines (SVMs). It extends linear methods into highly flexible nonlinear learners without the cost of explicit high-dimensional feature mapping.</p>
</section>
<section id="try-it-yourself-50" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-50">Try It Yourself</h4>
<ol type="1">
<li>Train SVMs with linear, polynomial, and RBF kernels. Compare decision boundaries.</li>
<li>Increase polynomial degree. How does overfitting risk change?</li>
<li>Reflect: why might kernels struggle on very large datasets compared to deep learning?</li>
</ol>
</section>
</section>
<section id="common-kernels-polynomial-rbf-string" class="level3">
<h3 class="anchored" data-anchor-id="common-kernels-polynomial-rbf-string">652. Common Kernels (Polynomial, RBF, String)</h3>
<p>Kernels define similarity measures between data points. Different kernels correspond to different implicit feature spaces, enabling models to capture varied patterns. Choosing the right kernel is critical for performance.</p>
<section id="picture-in-your-head-51" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-51">Picture in Your Head</h4>
<p>Think of comparing documents:</p>
<ul>
<li>If you just count shared words → linear kernel.</li>
<li>If you compare word sequences → string kernel.</li>
<li>If you judge similarity based on overall “closeness” in meaning → RBF kernel. Each kernel answers: <em>what does similarity mean in this domain?</em></li>
</ul>
</section>
<section id="deep-dive-51" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-51">Deep Dive</h4>
<ul>
<li><p>Linear Kernel</p>
<p><span class="math display">\[
K(x, x') = x^T x'
\]</span></p>
<ul>
<li>Equivalent to no feature mapping.</li>
<li>Best for linearly separable data.</li>
</ul></li>
<li><p>Polynomial Kernel</p>
<p><span class="math display">\[
K(x, x') = (x^T x' + c)^d
\]</span></p>
<ul>
<li>Captures feature interactions up to degree <span class="math inline">\(d\)</span>.</li>
<li>Larger <span class="math inline">\(d\)</span> → more complex boundaries, higher overfitting risk.</li>
</ul></li>
<li><p>RBF (Gaussian) Kernel</p>
<p><span class="math display">\[
K(x, x') = \exp\left(-\frac{\|x-x'\|^2}{2\sigma^2}\right)
\]</span></p>
<ul>
<li>Infinite-dimensional feature space.</li>
<li>Excellent for local, nonlinear patterns.</li>
</ul></li>
<li><p>Sigmoid Kernel</p>
<p><span class="math display">\[
K(x, x') = \tanh(\alpha x^T x' + c)
\]</span></p>
<ul>
<li>Related to neural network activations.</li>
</ul></li>
<li><p>String / Spectrum Kernels</p>
<ul>
<li>Compare subsequences of strings (n-grams).</li>
<li>Widely used in text, bioinformatics (DNA, proteins).</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 31%">
<col style="width: 53%">
</colgroup>
<thead>
<tr class="header">
<th>Kernel</th>
<th>Strength</th>
<th>Weakness</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear</td>
<td>Fast, interpretable</td>
<td>Limited to linear patterns</td>
</tr>
<tr class="even">
<td>Polynomial</td>
<td>Captures interactions</td>
<td>Sensitive to degree &amp; scaling</td>
</tr>
<tr class="odd">
<td>RBF</td>
<td>Very flexible</td>
<td>Prone to overfitting, tuning needed</td>
</tr>
<tr class="even">
<td>String</td>
<td>Domain-specific</td>
<td>Costly for long sequences</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, scikit-learn)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">0</span>],[<span class="dv">1</span>,<span class="dv">1</span>],[<span class="dv">2</span>,<span class="dv">2</span>],[<span class="dv">3</span>,<span class="dv">3</span>],[<span class="dv">0</span>,<span class="dv">1</span>],[<span class="dv">1</span>,<span class="dv">0</span>]])</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>]</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a><span class="co"># try different kernels</span></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> kernel <span class="kw">in</span> [<span class="st">"linear"</span>, <span class="st">"poly"</span>, <span class="st">"rbf"</span>, <span class="st">"sigmoid"</span>]:</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>    clf <span class="op">=</span> SVC(kernel<span class="op">=</span>kernel, degree<span class="op">=</span><span class="dv">3</span>, gamma<span class="op">=</span><span class="st">"scale"</span>).fit(X,y)</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(kernel, <span class="st">"accuracy:"</span>, clf.score(X,y))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-51" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-51">Why it Matters</h4>
<p>Kernel choice encodes prior knowledge about data structure. Polynomial captures interactions, RBF captures local smoothness, and string kernels capture sequence similarity. This flexibility made kernel methods the state of the art before deep learning.</p>
</section>
<section id="try-it-yourself-51" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-51">Try It Yourself</h4>
<ol type="1">
<li>Train SVMs with polynomial kernels of degrees 2, 3, 5. How do decision boundaries change?</li>
<li>Use RBF kernel on non-linearly separable data (e.g., circles dataset). Does it succeed where linear fails?</li>
<li>Reflect: in NLP or genomics, why might string kernels outperform generic RBF kernels?</li>
</ol>
</section>
</section>
<section id="support-vector-machines-hard-margin" class="level3">
<h3 class="anchored" data-anchor-id="support-vector-machines-hard-margin">653. Support Vector Machines: Hard Margin</h3>
<p>Support Vector Machines (SVMs) are powerful classifiers that separate classes with the maximum margin hyperplane. The hard margin SVM assumes data is perfectly linearly separable and finds the widest possible margin between classes.</p>
<section id="picture-in-your-head-52" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-52">Picture in Your Head</h4>
<p>Imagine placing a fence between two groups of cows in a field. The hard margin SVM builds the fence so that:</p>
<ul>
<li>It perfectly separates the groups.</li>
<li>It maximizes the distance to the nearest cow on either side. Those nearest cows are the support vectors—they “hold up” the fence.</li>
</ul>
</section>
<section id="deep-dive-52" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-52">Deep Dive</h4>
<ul>
<li><p>Decision function:</p>
<p><span class="math display">\[
f(x) = \text{sign}(w^T x + b)
\]</span></p></li>
<li><p>Optimization problem:</p>
<p><span class="math display">\[
\min_{w, b} \frac{1}{2}\|w\|^2
\]</span></p>
<p>subject to:</p>
<p><span class="math display">\[
y_i(w^T x_i + b) \geq 1 \quad \forall i
\]</span></p></li>
<li><p>The margin = <span class="math inline">\(2 / \|w\|\)</span>. Maximizing margin improves generalization.</p></li>
<li><p>Only points on the margin boundary (support vectors) influence the solution; others are irrelevant.</p></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Feature</th>
<th>Hard Margin SVM</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Assumption</td>
<td>Perfect separability</td>
</tr>
<tr class="even">
<td>Strength</td>
<td>Strong generalization if separable</td>
</tr>
<tr class="odd">
<td>Weakness</td>
<td>Not robust to noise or overlap</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, scikit-learn)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="co"># perfectly separable dataset</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">1</span>,<span class="dv">2</span>],[<span class="dv">2</span>,<span class="dv">3</span>],[<span class="dv">3</span>,<span class="dv">3</span>],[<span class="dv">6</span>,<span class="dv">6</span>],[<span class="dv">7</span>,<span class="dv">7</span>],[<span class="dv">8</span>,<span class="dv">8</span>]])</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>]</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">"linear"</span>, C<span class="op">=</span><span class="fl">1e6</span>)  <span class="co"># very large C ≈ hard margin</span></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>clf.fit(X, y)</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Support vectors:"</span>, clf.support_vectors_)</span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Coefficients:"</span>, clf.coef_)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-52" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-52">Why it Matters</h4>
<p>Hard margin SVM formalizes the principle of margin maximization, which underlies many modern ML methods. While impractical for noisy data, it sets the foundation for soft margin SVMs and kernelized extensions.</p>
</section>
<section id="try-it-yourself-52" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-52">Try It Yourself</h4>
<ol type="1">
<li>Train a hard margin SVM on a toy separable dataset. Which points become support vectors?</li>
<li>Add a small amount of noise. Does the classifier still work?</li>
<li>Reflect: why is maximizing the margin a good strategy for generalization?</li>
</ol>
</section>
</section>
<section id="soft-margin-and-slack-variables" class="level3">
<h3 class="anchored" data-anchor-id="soft-margin-and-slack-variables">654. Soft Margin and Slack Variables</h3>
<p>Real-world data is rarely perfectly separable. Soft margin SVMs relax the hard margin constraints by allowing some misclassifications, controlled by slack variables and a penalty parameter <span class="math inline">\(C\)</span>. This balances margin maximization with tolerance for noise.</p>
<section id="picture-in-your-head-53" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-53">Picture in Your Head</h4>
<p>Think of separating red and blue marbles with a ruler:</p>
<ul>
<li>If you demand zero mistakes (hard margin), the ruler may twist awkwardly.</li>
<li>If you allow a few marbles to be on the wrong side (soft margin), the ruler stays straighter and more generalizable.</li>
</ul>
</section>
<section id="deep-dive-53" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-53">Deep Dive</h4>
<ul>
<li><p>Optimization problem:</p>
<p><span class="math display">\[
\min_{w,b,\xi} \frac{1}{2}\|w\|^2 + C \sum_{i=1}^n \xi_i
\]</span></p>
<p>subject to:</p>
<p><span class="math display">\[
y_i (w^T x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
\]</span></p>
<ul>
<li><span class="math inline">\(\xi_i\)</span>: slack variable measuring violation of margin.</li>
<li><span class="math inline">\(C\)</span>: regularization parameter; high <span class="math inline">\(C\)</span> penalizes misclassifications heavily, low <span class="math inline">\(C\)</span> allows more flexibility.</li>
</ul></li>
<li><p>Tradeoff:</p>
<ul>
<li>Large <span class="math inline">\(C\)</span>: narrower margin, fewer errors (risk of overfitting).</li>
<li>Small <span class="math inline">\(C\)</span>: wider margin, more errors (better generalization).</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Parameter</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(C \to \infty\)</span></td>
<td>Hard margin behavior</td>
</tr>
<tr class="even">
<td>Large <span class="math inline">\(C\)</span></td>
<td>Prioritize minimizing errors</td>
</tr>
<tr class="odd">
<td>Small <span class="math inline">\(C\)</span></td>
<td>Prioritize maximizing margin</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, scikit-learn)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="co"># noisy dataset</span></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">1</span>,<span class="dv">2</span>],[<span class="dv">2</span>,<span class="dv">3</span>],[<span class="dv">3</span>,<span class="dv">3</span>],[<span class="dv">6</span>,<span class="dv">6</span>],[<span class="dv">7</span>,<span class="dv">7</span>],[<span class="dv">8</span>,<span class="dv">5</span>]])</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>]</span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>clf1 <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">"linear"</span>, C<span class="op">=</span><span class="dv">1000</span>).fit(X,y)  <span class="co"># nearly hard margin</span></span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>clf2 <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">"linear"</span>, C<span class="op">=</span><span class="fl">0.1</span>).fit(X,y)   <span class="co"># softer margin</span></span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Support vectors (C=1000):"</span>, clf1.support_vectors_)</span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Support vectors (C=0.1):"</span>, clf2.support_vectors_)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-53" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-53">Why it Matters</h4>
<p>Soft margin SVMs are practical for real-world, noisy data. They embody the bias–variance tradeoff: <span class="math inline">\(C\)</span> tunes model flexibility, allowing practitioners to adapt to the dataset’s structure.</p>
</section>
<section id="try-it-yourself-53" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-53">Try It Yourself</h4>
<ol type="1">
<li>Train SVMs with different <span class="math inline">\(C\)</span> values. Plot decision boundaries.</li>
<li>On noisy data, compare accuracy of large-<span class="math inline">\(C\)</span> vs.&nbsp;small-<span class="math inline">\(C\)</span> models.</li>
<li>Reflect: why might a small-<span class="math inline">\(C\)</span> SVM perform better on test data even if it makes more training errors?</li>
</ol>
</section>
</section>
<section id="dual-formulation-and-optimization" class="level3">
<h3 class="anchored" data-anchor-id="dual-formulation-and-optimization">655. Dual Formulation and Optimization</h3>
<p>Support Vector Machines can be expressed in two mathematically equivalent ways: the primal problem (optimize directly over weights <span class="math inline">\(w\)</span>) and the dual problem (optimize over Lagrange multipliers <span class="math inline">\(\alpha\)</span>). The dual formulation is especially powerful because it naturally incorporates kernels.</p>
<section id="picture-in-your-head-54" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-54">Picture in Your Head</h4>
<p>Think of two ways to solve a puzzle:</p>
<ul>
<li>Primal: arrange the pieces directly.</li>
<li>Dual: instead, keep track of the “forces” each piece exerts until the puzzle locks into place. The dual view shifts the problem into a space where similarities (kernels) are easier to compute.</li>
</ul>
</section>
<section id="deep-dive-54" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-54">Deep Dive</h4>
<ul>
<li>Primal soft-margin SVM:</li>
</ul>
<p><span class="math display">\[
\min_{w,b,\xi} \frac{1}{2}\|w\|^2 + C\sum_i \xi_i
\]</span></p>
<p>subject to margin constraints.</p>
<ul>
<li>Dual formulation:</li>
</ul>
<p><span class="math display">\[
\max_\alpha \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i,j} \alpha_i \alpha_j y_i y_j K(x_i, x_j)
\]</span></p>
<p>subject to:</p>
<p><span class="math display">\[
0 \leq \alpha_i \leq C, \quad \sum_i \alpha_i y_i = 0
\]</span></p>
<ul>
<li><p>Key insights:</p>
<ul>
<li>Solution depends only on inner products <span class="math inline">\(K(x_i, x_j)\)</span>.</li>
<li>Support vectors correspond to nonzero <span class="math inline">\(\alpha_i\)</span>.</li>
<li>Kernels plug in seamlessly by replacing dot products.</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Formulation</th>
<th>Advantage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Primal</td>
<td>Intuitive, works for linear SVMs</td>
</tr>
<tr class="even">
<td>Dual</td>
<td>Handles kernels, sparse solutions</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, CVXOPT solver for dual SVM)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: illustrative, scikit-learn hides the dual optimization</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> [[<span class="dv">0</span>,<span class="dv">0</span>],[<span class="dv">1</span>,<span class="dv">1</span>],[<span class="dv">1</span>,<span class="dv">0</span>],[<span class="dv">0</span>,<span class="dv">1</span>]]</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>]</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">"linear"</span>, C<span class="op">=</span><span class="dv">1</span>).fit(X,y)</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Support vectors:"</span>, clf.support_vectors_)</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Dual coefficients (alphas):"</span>, clf.dual_coef_)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-54" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-54">Why it Matters</h4>
<p>The dual perspective unlocks the kernel trick, enabling nonlinear SVMs without explicit feature expansion. It also explains why SVMs rely only on support vectors, making them efficient for sparse solutions.</p>
</section>
<section id="try-it-yourself-54" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-54">Try It Yourself</h4>
<ol type="1">
<li>Compare number of support vectors as <span class="math inline">\(C\)</span> changes. How do the <span class="math inline">\(\alpha_i\)</span> values behave?</li>
<li>Train linear vs.&nbsp;RBF SVMs and inspect dual coefficients.</li>
<li>Reflect: why is the dual formulation the natural place to introduce kernels?</li>
</ol>
</section>
</section>
<section id="kernel-ridge-regression" class="level3">
<h3 class="anchored" data-anchor-id="kernel-ridge-regression">656. Kernel Ridge Regression</h3>
<p>Kernel Ridge Regression (KRR) combines ridge regression with the kernel trick. Instead of fitting a linear model directly in input space, KRR fits a linear model in a high-dimensional feature space defined by a kernel, while using L2 regularization to prevent overfitting.</p>
<section id="picture-in-your-head-55" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-55">Picture in Your Head</h4>
<p>Imagine bending a flexible metal rod to fit scattered points:</p>
<ul>
<li>Ridge regression keeps the rod from over-bending.</li>
<li>The kernel trick allows you to bend it in curves, waves, or more complex shapes depending on the kernel chosen.</li>
</ul>
</section>
<section id="deep-dive-55" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-55">Deep Dive</h4>
<ul>
<li>Ridge regression:</li>
</ul>
<p><span class="math display">\[
\hat{\beta} = (X^TX + \lambda I)^{-1} X^Ty
\]</span></p>
<ul>
<li><p>Kernel ridge regression: works entirely in dual space.</p>
<ul>
<li>Predictor:</li>
</ul>
<p><span class="math display">\[
f(x) = \sum_{i=1}^n \alpha_i K(x, x_i)
\]</span></p>
<ul>
<li>Solution for coefficients:</li>
</ul>
<p><span class="math display">\[
\alpha = (K + \lambda I)^{-1} y
\]</span></p>
<p>where <span class="math inline">\(K\)</span> is the kernel (Gram) matrix.</p></li>
<li><p>Connection:</p>
<ul>
<li>If kernel = linear, KRR = ridge regression.</li>
<li>If kernel = RBF, KRR = nonlinear smoother.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 23%">
<col style="width: 57%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>Ridge Regression</th>
<th>Kernel Ridge Regression</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Model</td>
<td>Linear in features</td>
<td>Linear in feature space (nonlinear in input)</td>
</tr>
<tr class="even">
<td>Regularization</td>
<td>L2 penalty</td>
<td>L2 penalty</td>
</tr>
<tr class="odd">
<td>Flexibility</td>
<td>Limited</td>
<td>Highly flexible</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, scikit-learn)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.kernel_ridge <span class="im">import</span> KernelRidge</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="co"># toy dataset: nonlinear relationship</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">30</span>)[:, <span class="va">None</span>]</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.sin(X).ravel() <span class="op">+</span> np.random.randn(<span class="dv">30</span>)<span class="op">*</span><span class="fl">0.1</span></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> KernelRidge(kernel<span class="op">=</span><span class="st">"rbf"</span>, alpha<span class="op">=</span><span class="fl">1.0</span>, gamma<span class="op">=</span><span class="fl">0.5</span>).fit(X, y)</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Prediction at x=0.5:"</span>, model.predict([[<span class="fl">0.5</span>]])[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-55" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-55">Why it Matters</h4>
<p>KRR is a bridge between classical regression and kernel methods. It shows how regularization and kernels interact to yield flexible yet stable models. It is widely used in time series, geostatistics, and structured regression problems.</p>
</section>
<section id="try-it-yourself-55" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-55">Try It Yourself</h4>
<ol type="1">
<li>Fit KRR with linear, polynomial, and RBF kernels on the same dataset. Compare fits.</li>
<li>Increase regularization parameter <span class="math inline">\(\lambda\)</span>. How does smoothness change?</li>
<li>Reflect: why might KRR be preferable over SVM regression (SVR) in certain cases?</li>
</ol>
</section>
</section>
<section id="svms-for-regression-svr" class="level3">
<h3 class="anchored" data-anchor-id="svms-for-regression-svr">657. SVMs for Regression (SVR)</h3>
<p>Support Vector Regression (SVR) adapts the SVM framework for predicting continuous values. Instead of classifying points, SVR finds a function that approximates data within a tolerance margin <span class="math inline">\(\epsilon\)</span>, ignoring small errors while penalizing larger deviations.</p>
<section id="picture-in-your-head-56" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-56">Picture in Your Head</h4>
<p>Imagine drawing a tube around a curve:</p>
<ul>
<li>Points inside the tube are “close enough” → no penalty.</li>
<li>Points outside the tube are “errors” → penalized based on their distance from the tube. The tube’s width is set by <span class="math inline">\(\epsilon\)</span>.</li>
</ul>
</section>
<section id="deep-dive-56" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-56">Deep Dive</h4>
<ul>
<li><p>Optimization problem: Minimize</p>
<p><span class="math display">\[
\frac{1}{2}\|w\|^2 + C \sum (\xi_i + \xi_i^*)
\]</span></p>
<p>subject to:</p>
<p><span class="math display">\[
y_i - w^T x_i - b \leq \epsilon + \xi_i, \quad
w^T x_i + b - y_i \leq \epsilon + \xi_i^*, \quad
\xi_i, \xi_i^* \geq 0
\]</span></p></li>
<li><p>Parameters:</p>
<ul>
<li><span class="math inline">\(C\)</span>: penalty for errors beyond <span class="math inline">\(\epsilon\)</span>.</li>
<li><span class="math inline">\(\epsilon\)</span>: tube width (tolerance for errors).</li>
<li>Kernel: allows nonlinear regression (linear, polynomial, RBF).</li>
</ul></li>
<li><p>Tradeoffs:</p>
<ul>
<li>Small <span class="math inline">\(\epsilon\)</span>: sensitive fit, may overfit.</li>
<li>Large <span class="math inline">\(\epsilon\)</span>: smoother fit, ignores more detail.</li>
<li>Large <span class="math inline">\(C\)</span>: less tolerance for outliers.</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Parameter</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(C\)</span> large</td>
<td>Strict fit, less tolerance</td>
</tr>
<tr class="even">
<td><span class="math inline">\(C\)</span> small</td>
<td>Softer fit, more tolerance</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\epsilon\)</span> small</td>
<td>Narrow tube, sensitive</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\epsilon\)</span> large</td>
<td>Wide tube, smoother</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, scikit-learn)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVR</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a><span class="co"># nonlinear dataset</span></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">50</span>)[:, <span class="va">None</span>]</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.sin(X).ravel() <span class="op">+</span> np.random.randn(<span class="dv">50</span>)<span class="op">*</span><span class="fl">0.1</span></span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a><span class="co"># fit SVR with RBF kernel</span></span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>svr <span class="op">=</span> SVR(kernel<span class="op">=</span><span class="st">"rbf"</span>, C<span class="op">=</span><span class="dv">10</span>, epsilon<span class="op">=</span><span class="fl">0.1</span>).fit(X, y)</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>plt.scatter(X, y, color<span class="op">=</span><span class="st">"blue"</span>, label<span class="op">=</span><span class="st">"data"</span>)</span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a>plt.plot(X, svr.predict(X), color<span class="op">=</span><span class="st">"red"</span>, label<span class="op">=</span><span class="st">"SVR fit"</span>)</span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-56" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-56">Why it Matters</h4>
<p>SVR is powerful for tasks where exact predictions are less important than capturing trends within a tolerance. It is widely used in financial forecasting, energy demand prediction, and engineering control systems.</p>
</section>
<section id="try-it-yourself-56" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-56">Try It Yourself</h4>
<ol type="1">
<li>Train SVR with different <span class="math inline">\(\epsilon\)</span>. How does the fit change?</li>
<li>Compare SVR with linear regression on nonlinear data. Which generalizes better?</li>
<li>Reflect: why might SVR be chosen over KRR, even though both use kernels?</li>
</ol>
</section>
</section>
<section id="large-scale-kernel-learning-and-approximations" class="level3">
<h3 class="anchored" data-anchor-id="large-scale-kernel-learning-and-approximations">658. Large-Scale Kernel Learning and Approximations</h3>
<p>Kernel methods like SVMs and Kernel Ridge Regression are powerful but scale poorly: computing and storing the kernel matrix requires <span class="math inline">\(O(n^2)\)</span> memory and <span class="math inline">\(O(n^3)\)</span> time for inversion. For large datasets, we use approximations that make kernel learning feasible.</p>
<section id="picture-in-your-head-57" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-57">Picture in Your Head</h4>
<p>Think of trying to seat everyone in a giant stadium:</p>
<ul>
<li>If you calculate the distance between every single pair of people, it takes forever.</li>
<li>Instead, you group people into sections or approximate distances with shortcuts. Kernel approximations do exactly this for large datasets.</li>
</ul>
</section>
<section id="deep-dive-57" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-57">Deep Dive</h4>
<ul>
<li><p>Problem: Kernel matrix <span class="math inline">\(K \in \mathbb{R}^{n \times n}\)</span> grows quadratically with dataset size.</p></li>
<li><p>Solutions:</p>
<ul>
<li><p>Low-rank approximations:</p>
<ul>
<li>Nyström method: approximate kernel matrix using a subset of landmark points.</li>
<li>Randomized SVD for approximate eigendecomposition.</li>
</ul></li>
<li><p>Random feature maps:</p>
<ul>
<li>Random Fourier Features approximate shift-invariant kernels (e.g., RBF).</li>
<li>Reduce kernel methods to linear models in randomized feature space.</li>
</ul></li>
<li><p>Sparse methods:</p>
<ul>
<li>Budgeted online kernel learning keeps only a subset of support vectors.</li>
</ul></li>
<li><p>Distributed methods:</p>
<ul>
<li>Block-partitioning the kernel matrix for parallel training.</li>
</ul></li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 26%">
<col style="width: 44%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Idea</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Nyström</td>
<td>Landmark-based approximation</td>
<td><span class="math inline">\(O(mn)\)</span>, with <span class="math inline">\(m \ll n\)</span></td>
</tr>
<tr class="even">
<td>Random Fourier Features</td>
<td>Approximate kernels via random mapping</td>
<td>Linear in <span class="math inline">\(n\)</span></td>
</tr>
<tr class="odd">
<td>Sparse support vectors</td>
<td>Keep only important SVs</td>
<td>Depends on sparsity</td>
</tr>
<tr class="even">
<td>Distributed kernels</td>
<td>Partition computations</td>
<td>Scales with compute nodes</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, scikit-learn with Random Fourier Features)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.kernel_approximation <span class="im">import</span> RBFSampler</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> SGDClassifier</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a><span class="co"># toy dataset</span></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">500</span>, n_features<span class="op">=</span><span class="dv">20</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a><span class="co"># approximate RBF kernel with random Fourier features</span></span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>rbf_feature <span class="op">=</span> RBFSampler(gamma<span class="op">=</span><span class="dv">1</span>, n_components<span class="op">=</span><span class="dv">100</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>X_features <span class="op">=</span> rbf_feature.fit_transform(X)</span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a><span class="co"># train linear model in transformed space</span></span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> SGDClassifier().fit(X_features, y)</span>
<span id="cb59-15"><a href="#cb59-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training accuracy:"</span>, clf.score(X_features, y))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-57" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-57">Why it Matters</h4>
<p>Approximation techniques make kernel methods viable for millions of samples, extending their reach beyond academic settings. They allow practitioners to balance accuracy, memory, and compute resources.</p>
</section>
<section id="try-it-yourself-57" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-57">Try It Yourself</h4>
<ol type="1">
<li>Compare exact RBF SVM vs.&nbsp;Random Fourier Feature approximation on the same dataset. How close are results?</li>
<li>Experiment with different numbers of random features. What is the tradeoff between accuracy and speed?</li>
<li>Reflect: in the era of deep learning, why do kernel approximations still matter for medium-sized problems?</li>
</ol>
</section>
</section>
<section id="interpretability-and-limitations-of-kernels" class="level3">
<h3 class="anchored" data-anchor-id="interpretability-and-limitations-of-kernels">659. Interpretability and Limitations of Kernels</h3>
<p>Kernel methods are flexible and powerful, but their interpretability and scalability often lag behind simpler models. Understanding both their strengths and limitations helps decide when kernels are the right tool.</p>
<section id="picture-in-your-head-58" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-58">Picture in Your Head</h4>
<p>Imagine using a magnifying glass:</p>
<ul>
<li>It reveals fine patterns you couldn’t see before (kernel power).</li>
<li>But sometimes the view is distorted or too zoomed-in (kernel limitations).</li>
<li>And carrying a magnifying glass for every single object (scalability issue) quickly becomes impractical.</li>
</ul>
</section>
<section id="deep-dive-58" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-58">Deep Dive</h4>
<ul>
<li><p>Interpretability challenges</p>
<ul>
<li>Linear models: coefficients show direct feature effects.</li>
<li>Kernel models: decision boundaries depend on support vectors in transformed space.</li>
<li>Difficult to trace back to original features → “black-box” feeling compared to linear/logistic regression.</li>
</ul></li>
<li><p>Scalability issues</p>
<ul>
<li>Kernel matrix requires <span class="math inline">\(O(n^2)\)</span> memory.</li>
<li>Training cost grows as <span class="math inline">\(O(n^3)\)</span>.</li>
<li>Limits direct application to datasets beyond ~50k samples without approximation.</li>
</ul></li>
<li><p>Choice of kernel</p>
<ul>
<li>Kernel must encode meaningful similarity.</li>
<li>Poor kernel choice = poor performance, regardless of data size.</li>
<li>Requires domain knowledge or tuning (e.g., RBF width <span class="math inline">\(\sigma\)</span>).</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 55%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th>Strength</th>
<th>Limitation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Nonlinear power without explicit mapping</td>
<td>Poor interpretability</td>
</tr>
<tr class="even">
<td>Strong theoretical guarantees</td>
<td>High computational cost</td>
</tr>
<tr class="odd">
<td>Flexible across domains (text, bioinformatics, vision)</td>
<td>Sensitive to kernel choice &amp; hyperparameters</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, visualizing decision boundary)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="co"># toy nonlinear dataset</span></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(n_samples<span class="op">=</span><span class="dv">200</span>, noise<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">"rbf"</span>, gamma<span class="op">=</span><span class="dv">1</span>).fit(X, y)</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a><span class="co"># plot decision boundary</span></span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">200</span>), np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">200</span>))</span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)</span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a>plt.contourf(xx, yy, Z, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb60-15"><a href="#cb60-15" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:,<span class="dv">0</span>], X[:,<span class="dv">1</span>], c<span class="op">=</span>y, edgecolors<span class="op">=</span><span class="st">"k"</span>)</span>
<span id="cb60-16"><a href="#cb60-16" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-58" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-58">Why it Matters</h4>
<p>Kernel methods were state-of-the-art before deep learning. Today, their role is more niche: excellent for small- to medium-sized datasets with complex patterns, but less useful when interpretability or scalability are primary concerns.</p>
</section>
<section id="try-it-yourself-58" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-58">Try It Yourself</h4>
<ol type="1">
<li>Train an RBF SVM and inspect support vectors. How many does it rely on?</li>
<li>Compare interpretability of logistic regression vs.&nbsp;kernel SVM on the same dataset.</li>
<li>Reflect: in your domain, would you prioritize kernel flexibility or coefficient-level interpretability?</li>
</ol>
</section>
</section>
<section id="beyond-svms-kernelized-deep-architectures" class="level3">
<h3 class="anchored" data-anchor-id="beyond-svms-kernelized-deep-architectures">660. Beyond SVMs: Kernelized Deep Architectures</h3>
<p>Kernel methods inspired many deep learning ideas, and hybrid approaches now combine kernels with neural networks. These kernelized deep architectures aim to capture nonlinear relationships while leveraging scalability and representation learning from deep nets.</p>
<section id="picture-in-your-head-59" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-59">Picture in Your Head</h4>
<p>Imagine giving a neural network a special “similarity lens”:</p>
<ul>
<li>Kernels provide a powerful way to measure similarity.</li>
<li>Deep networks learn rich feature hierarchies.</li>
<li>Together, they act like a microscope that adjusts itself to reveal patterns across multiple levels.</li>
</ul>
</section>
<section id="deep-dive-59" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-59">Deep Dive</h4>
<ul>
<li><p>Neural Tangent Kernel (NTK)</p>
<ul>
<li>As neural networks get infinitely wide, their training dynamics converge to kernel regression with a specific kernel (the NTK).</li>
<li>Provides theoretical bridge between deep nets and kernel methods.</li>
</ul></li>
<li><p>Deep Kernel Learning (DKL)</p>
<ul>
<li>Combines deep neural networks (for feature learning) with Gaussian Processes (for uncertainty estimation).</li>
<li>Kernel is applied to learned embeddings, not raw data.</li>
</ul></li>
<li><p>Convolutional kernels</p>
<ul>
<li>Inspired by CNNs, kernels can incorporate local spatial structure.</li>
<li>Useful for images and structured data.</li>
</ul></li>
<li><p>Multiple Kernel Learning (MKL)</p>
<ul>
<li>Learns a weighted combination of kernels, sometimes with neural guidance.</li>
<li>Blends prior knowledge with data-driven flexibility.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 46%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th>Approach</th>
<th>Idea</th>
<th>Benefit</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>NTK</td>
<td>Infinite-width nets ≈ kernel regression</td>
<td>Theory for deep learning</td>
</tr>
<tr class="even">
<td>DKL</td>
<td>Neural embeddings + GP kernels</td>
<td>Uncertainty + representation learning</td>
</tr>
<tr class="odd">
<td>MKL</td>
<td>Combine multiple kernels</td>
<td>Flexibility across domains</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, Deep Kernel Learning via GPytorch)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Illustrative only (requires gpytorch)</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gpytorch</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a><span class="co"># simple neural feature extractor</span></span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FeatureExtractor(nn.Module):</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(nn.Linear(<span class="dv">10</span>, <span class="dv">50</span>), nn.ReLU(), nn.Linear(<span class="dv">50</span>, <span class="dv">2</span>))</span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x): <span class="cf">return</span> <span class="va">self</span>.net(x)</span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a><span class="co"># deep kernel = kernel applied on neural features</span></span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a>feature_extractor <span class="op">=</span> FeatureExtractor()</span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a>base_kernel <span class="op">=</span> gpytorch.kernels.RBFKernel()</span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a>deep_kernel <span class="op">=</span> gpytorch.kernels.ScaleKernel(</span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a>    gpytorch.kernels.RBFKernel(ard_num_dims<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-59" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-59">Why it Matters</h4>
<p>Kernel methods and deep learning are not rivals but complements. Kernelized architectures combine uncertainty estimation and interpretability from kernels with the scalability and feature learning of deep nets, making them valuable for modern AI.</p>
</section>
<section id="try-it-yourself-59" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-59">Try It Yourself</h4>
<ol type="1">
<li>Explore NTK literature: how do wide networks behave like kernel machines?</li>
<li>Try Deep Kernel Learning on small data where uncertainty is important (e.g., medical).</li>
<li>Reflect: in which scenarios would you prefer kernels wrapped around deep embeddings instead of raw deep networks?</li>
</ol>
</section>
</section>
</section>
<section id="chapter-67.-trees-random-forests-gradient-boosting" class="level2">
<h2 class="anchored" data-anchor-id="chapter-67.-trees-random-forests-gradient-boosting">Chapter 67. Trees, random forests, gradient boosting</h2>
<section id="decision-trees-splits-impurity-and-pruning" class="level3">
<h3 class="anchored" data-anchor-id="decision-trees-splits-impurity-and-pruning">661. Decision Trees: Splits, Impurity, and Pruning</h3>
<p>Decision trees are hierarchical models that split data into regions by asking a sequence of feature-based questions. At each node, the tree chooses the best split to maximize class purity (classification) or reduce variance (regression). Pruning ensures the tree does not grow overly complex.</p>
<section id="picture-in-your-head-60" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-60">Picture in Your Head</h4>
<p>Think of playing “20 Questions”:</p>
<ul>
<li>Each question (split) divides the possibilities in half.</li>
<li>By carefully choosing the best questions, you quickly narrow down to the correct answer.</li>
<li>But asking too many overly specific questions leads to memorization rather than generalization.</li>
</ul>
</section>
<section id="deep-dive-60" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-60">Deep Dive</h4>
<ul>
<li><p>Splitting criterion:</p>
<ul>
<li>Classification: maximize class purity using measures like Gini impurity or entropy.</li>
<li>Regression: minimize variance of target values within nodes.</li>
</ul></li>
<li><p>Impurity measures:</p>
<ul>
<li><p>Gini:</p>
<p><span class="math display">\[
Gini = 1 - \sum_{k} p_k^2
\]</span></p></li>
<li><p>Entropy:</p>
<p><span class="math display">\[
H = - \sum_{k} p_k \log p_k
\]</span></p></li>
</ul></li>
<li><p>Pruning:</p>
<ul>
<li>Prevents overfitting by limiting depth or removing branches.</li>
<li>Strategies: pre-pruning (early stopping, depth limit) or post-pruning (train fully, then cut weak branches).</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Classification</th>
<th>Regression</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Split choice</td>
<td>Max purity (Gini/Entropy)</td>
<td>Minimize variance</td>
</tr>
<tr class="even">
<td>Leaf prediction</td>
<td>Majority class</td>
<td>Mean target</td>
</tr>
<tr class="odd">
<td>Overfitting control</td>
<td>Pruning</td>
<td>Pruning</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, scikit-learn)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier, export_text</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a><span class="co"># toy dataset</span></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">0</span>],[<span class="dv">1</span>],[<span class="dv">2</span>],[<span class="dv">3</span>],[<span class="dv">4</span>],[<span class="dv">5</span>]])</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>])</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>tree <span class="op">=</span> DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">3</span>).fit(X, y)</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(export_text(tree, feature_names<span class="op">=</span>[<span class="st">"Feature"</span>]))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-60" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-60">Why it Matters</h4>
<p>Decision trees are interpretable, flexible, and form the foundation of powerful ensemble methods like Random Forests and Gradient Boosting. Understanding splits and pruning is essential to mastering modern tree-based models.</p>
</section>
<section id="try-it-yourself-60" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-60">Try It Yourself</h4>
<ol type="1">
<li>Train a decision tree with different impurity measures (Gini vs.&nbsp;Entropy). Do splits differ?</li>
<li>Compare deep unpruned vs.&nbsp;pruned trees. Which generalizes better?</li>
<li>Reflect: why might trees overfit badly on small datasets with many features?</li>
</ol>
</section>
</section>
<section id="cart-vs.-id3-vs.-c4.5-algorithms" class="level3">
<h3 class="anchored" data-anchor-id="cart-vs.-id3-vs.-c4.5-algorithms">662. CART vs.&nbsp;ID3 vs.&nbsp;C4.5 Algorithms</h3>
<p>Decision tree algorithms differ mainly in how they choose splits and handle categorical/continuous features. The most influential families are ID3, C4.5, and CART, each refining tree-building strategies over time.</p>
<section id="picture-in-your-head-61" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-61">Picture in Your Head</h4>
<p>Think of three chefs making soup:</p>
<ul>
<li>ID3 only checks flavor variety (entropy).</li>
<li>C4.5 adjusts for ingredient quantity (info gain ratio).</li>
<li>CART simplifies by tasting sweetness vs.&nbsp;bitterness (Gini), then pruning for balance.</li>
</ul>
</section>
<section id="deep-dive-61" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-61">Deep Dive</h4>
<ul>
<li><p>ID3 (Iterative Dichotomiser 3)</p>
<ul>
<li>Splits based on information gain (entropy reduction).</li>
<li>Handles categorical features well.</li>
<li>Struggles with continuous features and overfitting.</li>
</ul></li>
<li><p>C4.5 (successor to ID3 by Quinlan)</p>
<ul>
<li>Uses gain ratio (info gain normalized by split size) to avoid bias toward many-valued features.</li>
<li>Supports continuous attributes (threshold-based splits).</li>
<li>Handles missing values better.</li>
</ul></li>
<li><p>CART (Classification and Regression Trees, Breiman et al.)</p>
<ul>
<li>Uses Gini impurity (classification) or variance reduction (regression).</li>
<li>Produces strictly binary splits.</li>
<li>Employs post-pruning with cost-complexity pruning.</li>
<li>Most widely used today (basis for scikit-learn trees, Random Forests, XGBoost).</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 24%">
<col style="width: 12%">
<col style="width: 27%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th>Algorithm</th>
<th>Split Criterion</th>
<th>Splits</th>
<th>Handles Continuous</th>
<th>Pruning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ID3</td>
<td>Information Gain</td>
<td>Multiway</td>
<td>Poorly</td>
<td>None</td>
</tr>
<tr class="even">
<td>C4.5</td>
<td>Gain Ratio</td>
<td>Multiway</td>
<td>Yes</td>
<td>Post-pruning</td>
</tr>
<tr class="odd">
<td>CART</td>
<td>Gini / Variance</td>
<td>Binary</td>
<td>Yes</td>
<td>Cost-complexity</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, CART via scikit-learn)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier, export_text</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">1</span>,<span class="dv">0</span>],[<span class="dv">2</span>,<span class="dv">1</span>],[<span class="dv">3</span>,<span class="dv">0</span>],[<span class="dv">4</span>,<span class="dv">1</span>],[<span class="dv">5</span>,<span class="dv">0</span>]])</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>])</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>cart <span class="op">=</span> DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">"gini"</span>, max_depth<span class="op">=</span><span class="dv">3</span>).fit(X, y)</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(export_text(cart, feature_names<span class="op">=</span>[<span class="st">"Feature1"</span>,<span class="st">"Feature2"</span>]))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-61" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-61">Why it Matters</h4>
<p>These three algorithms shaped modern decision tree learning. CART’s binary, pruned approach dominates practice, while ID3 and C4.5 are key historically and conceptually in understanding entropy-based splitting.</p>
</section>
<section id="try-it-yourself-61" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-61">Try It Yourself</h4>
<ol type="1">
<li>Implement ID3 on a categorical dataset. How do splits compare to CART?</li>
<li>Train CART with Gini vs.&nbsp;Entropy. Do results differ significantly?</li>
<li>Reflect: why do modern libraries prefer CART’s binary splits over C4.5’s multiway ones?</li>
</ol>
</section>
</section>
<section id="bagging-and-the-random-forest-idea" class="level3">
<h3 class="anchored" data-anchor-id="bagging-and-the-random-forest-idea">663. Bagging and the Random Forest Idea</h3>
<p>Bagging (Bootstrap Aggregating) reduces variance by training multiple models on different bootstrap samples of the data and averaging their predictions. Random Forests extend bagging with decision trees by also randomizing feature selection, making the ensemble more robust.</p>
<section id="picture-in-your-head-62" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-62">Picture in Your Head</h4>
<p>Imagine asking a crowd of people to guess the weight of an ox:</p>
<ul>
<li>One guess might be off, but the average of many guesses is surprisingly accurate.</li>
<li>Bagging works the same way: many noisy learners, when averaged, yield a stable predictor.</li>
</ul>
</section>
<section id="deep-dive-62" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-62">Deep Dive</h4>
<ul>
<li><p>Bagging</p>
<ul>
<li>Generate <span class="math inline">\(B\)</span> bootstrap datasets by sampling with replacement.</li>
<li>Train a base model (often a decision tree) on each dataset.</li>
<li>Aggregate predictions (average for regression, majority vote for classification).</li>
<li>Reduces variance, especially for high-variance models like trees.</li>
</ul></li>
<li><p>Random Forests</p>
<ul>
<li>Adds feature randomness: at each tree split, only a random subset of features is considered.</li>
<li>Further decorrelates trees, reducing ensemble variance.</li>
<li>Out-of-bag (OOB) samples (not in bootstrap) can be used for unbiased error estimation.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 27%">
<col style="width: 33%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Data Randomness</th>
<th>Feature Randomness</th>
<th>Aggregation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bagging</td>
<td>Bootstrap resamples</td>
<td>None</td>
<td>Average / Vote</td>
</tr>
<tr class="even">
<td>Random Forest</td>
<td>Bootstrap resamples</td>
<td>Random subset per split</td>
<td>Average / Vote</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, scikit-learn)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> BaggingClassifier, RandomForestClassifier</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> load_iris(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>bagging <span class="op">=</span> BaggingClassifier(DecisionTreeClassifier(), n_estimators<span class="op">=</span><span class="dv">50</span>).fit(X, y)</span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>rf <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">50</span>).fit(X, y)</span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Bagging accuracy:"</span>, bagging.score(X, y))</span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Random Forest accuracy:"</span>, rf.score(X, y))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-62" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-62">Why it Matters</h4>
<p>Bagging and Random Forests are milestones in ensemble learning. They offer robustness, scalability, and strong baselines across tasks, often outperforming single complex models with minimal tuning.</p>
</section>
<section id="try-it-yourself-62" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-62">Try It Yourself</h4>
<ol type="1">
<li>Compare a single decision tree vs.&nbsp;bagging vs.&nbsp;random forest on the same dataset. Which generalizes better?</li>
<li>Experiment with different numbers of trees. Does accuracy plateau?</li>
<li>Reflect: why does adding feature randomness improve forests over plain bagging?</li>
</ol>
</section>
</section>
<section id="feature-importance-and-interpretability" class="level3">
<h3 class="anchored" data-anchor-id="feature-importance-and-interpretability">664. Feature Importance and Interpretability</h3>
<p>One of the advantages of tree-based methods is their built-in ability to measure feature importance—how much each feature contributes to prediction. Random Forests and Gradient Boosting make this especially useful for interpretability in complex models.</p>
<section id="picture-in-your-head-63" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-63">Picture in Your Head</h4>
<p>Imagine sorting ingredients by how often they appear in recipes:</p>
<ul>
<li>The most frequently used and decisive ones (like salt) are high-importance features.</li>
<li>Rarely used spices contribute little—similar to low-importance features in trees.</li>
</ul>
</section>
<section id="deep-dive-63" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-63">Deep Dive</h4>
<ul>
<li><p>Split-based importance (Gini importance / Mean Decrease in Impurity, MDI):</p>
<ul>
<li>Each split reduces node impurity.</li>
<li>Feature importance = sum of impurity decreases where the feature is used, averaged across trees.</li>
</ul></li>
<li><p>Permutation importance (Mean Decrease in Accuracy, MDA):</p>
<ul>
<li>Randomly shuffle a feature’s values.</li>
<li>Measure drop in accuracy. Larger drops = higher importance.</li>
</ul></li>
<li><p>SHAP values (Shapley Additive Explanations):</p>
<ul>
<li>From cooperative game theory.</li>
<li>Attribute contribution of each feature for each prediction.</li>
<li>Provides local (per-instance) and global (aggregate) importance.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 38%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Advantage</th>
<th>Limitation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Split-based</td>
<td>Fast, built-in</td>
<td>Biased toward high-cardinality features</td>
</tr>
<tr class="even">
<td>Permutation</td>
<td>Model-agnostic, robust</td>
<td>Costly for large datasets</td>
</tr>
<tr class="odd">
<td>SHAP</td>
<td>Local + global interpretability</td>
<td>Computationally expensive</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, scikit-learn)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> load_iris(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>rf <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>).fit(X, y)</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>importances <span class="op">=</span> rf.feature_importances_</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, imp <span class="kw">in</span> <span class="bu">enumerate</span>(importances):</span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Feature </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">: importance </span><span class="sc">{</span>imp<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-63" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-63">Why it Matters</h4>
<p>Feature importance turns tree ensembles from black boxes into interpretable tools, enabling trust and transparency. This is critical in healthcare, finance, and other high-stakes applications.</p>
</section>
<section id="try-it-yourself-63" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-63">Try It Yourself</h4>
<ol type="1">
<li>Train a Random Forest and plot feature importances. Do they align with domain intuition?</li>
<li>Compare split-based and permutation importance. Which is more stable?</li>
<li>Reflect: in regulated industries, why might SHAP values be preferred over raw feature importance scores?</li>
</ol>
</section>
</section>
<section id="gradient-boosted-trees-gbdt-framework" class="level3">
<h3 class="anchored" data-anchor-id="gradient-boosted-trees-gbdt-framework">665. Gradient Boosted Trees (GBDT) Framework</h3>
<p>Gradient Boosted Decision Trees (GBDT) build strong predictors by sequentially adding weak learners (small trees), each correcting the errors of the previous ones. Instead of averaging like bagging, boosting focuses on hard-to-predict cases through gradient-based optimization.</p>
<section id="picture-in-your-head-64" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-64">Picture in Your Head</h4>
<p>Think of teaching a student:</p>
<ul>
<li>Lesson 1 gives a rough idea.</li>
<li>Lesson 2 focuses on mistakes from Lesson 1.</li>
<li>Lesson 3 improves on Lesson 2’s weaknesses. Over time, the student (the boosted model) becomes highly skilled.</li>
</ul>
</section>
<section id="deep-dive-64" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-64">Deep Dive</h4>
<ul>
<li><p>Idea: Fit an additive model</p>
<p><span class="math display">\[
F_M(x) = \sum_{m=1}^M \gamma_m h_m(x)
\]</span></p>
<p>where <span class="math inline">\(h_m\)</span> are weak learners (small trees).</p></li>
<li><p>Training procedure:</p>
<ol type="1">
<li><p>Initialize with a constant prediction (e.g., mean for regression).</p></li>
<li><p>At step <span class="math inline">\(m\)</span>, compute negative gradients (residuals).</p></li>
<li><p>Fit a tree <span class="math inline">\(h_m\)</span> to residuals.</p></li>
<li><p>Update model:</p>
<p><span class="math display">\[
F_m(x) = F_{m-1}(x) + \gamma_m h_m(x)
\]</span></p></li>
</ol></li>
<li><p>Loss functions:</p>
<ul>
<li>Squared error (regression).</li>
<li>Logistic loss (classification).</li>
<li>Many others (Huber, quantile, etc.).</li>
</ul></li>
<li><p>Modern implementations:</p>
<ul>
<li>XGBoost, LightGBM, CatBoost: add optimizations for speed, scalability, and regularization.</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Ensemble Type</th>
<th>How It Combines Learners</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bagging</td>
<td>Parallel, average predictions</td>
</tr>
<tr class="even">
<td>Boosting</td>
<td>Sequential, correct mistakes</td>
</tr>
<tr class="odd">
<td>Random Forest</td>
<td>Bagging + feature randomness</td>
</tr>
<tr class="even">
<td>GBDT</td>
<td>Boosting + gradient optimization</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, scikit-learn)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingClassifier</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">500</span>, n_features<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>gbdt <span class="op">=</span> GradientBoostingClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>, learning_rate<span class="op">=</span><span class="fl">0.1</span>, max_depth<span class="op">=</span><span class="dv">3</span>).fit(X, y)</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training accuracy:"</span>, gbdt.score(X, y))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-64" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-64">Why it Matters</h4>
<p>GBDTs are among the most powerful ML methods for structured/tabular data. They dominate in Kaggle competitions and real-world applications where interpretability, speed, and accuracy are critical.</p>
</section>
<section id="try-it-yourself-64" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-64">Try It Yourself</h4>
<ol type="1">
<li>Train GBDT with different learning rates (0.1, 0.01). How does convergence change?</li>
<li>Compare GBDT vs.&nbsp;Random Forest on tabular data. Which performs better?</li>
<li>Reflect: why do GBDTs often outperform deep learning on small to medium structured datasets?</li>
</ol>
</section>
</section>
<section id="boosting-algorithms-adaboost-xgboost-lightgbm" class="level3">
<h3 class="anchored" data-anchor-id="boosting-algorithms-adaboost-xgboost-lightgbm">666. Boosting Algorithms: AdaBoost, XGBoost, LightGBM</h3>
<p>Boosting is a family of ensemble methods where weak learners (often shallow trees) are combined sequentially to create a strong model. Different boosting algorithms refine the framework for speed, accuracy, and robustness.</p>
<section id="picture-in-your-head-65" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-65">Picture in Your Head</h4>
<p>Imagine training an army:</p>
<ul>
<li>AdaBoost makes soldiers focus on the enemies they missed before.</li>
<li>XGBoost equips them with better gear and training efficiency.</li>
<li>LightGBM organizes them into fast, specialized squads for large-scale battles.</li>
</ul>
</section>
<section id="deep-dive-65" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-65">Deep Dive</h4>
<ul>
<li><p>AdaBoost (Adaptive Boosting)</p>
<ul>
<li>Reweights data points: misclassified samples get higher weights in the next iteration.</li>
<li>Final model = weighted sum of weak learners.</li>
<li>Works well for clean data, but sensitive to noise.</li>
</ul></li>
<li><p>XGBoost (Extreme Gradient Boosting)</p>
<ul>
<li><p>Optimized GBDT implementation with:</p>
<ul>
<li>Second-order gradient information.</li>
<li>Regularization (<span class="math inline">\(L1, L2\)</span>) for stability.</li>
<li>Efficient handling of sparse data.</li>
<li>Parallel and distributed training.</li>
</ul></li>
</ul></li>
<li><p>LightGBM</p>
<ul>
<li>Optimized for large-scale, high-dimensional data.</li>
<li>Uses Histogram-based learning (bucketizing continuous features).</li>
<li>Leaf-wise growth: grows the leaf with the largest loss reduction first.</li>
<li>Faster and more memory-efficient than XGBoost in many cases.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 33%">
<col style="width: 29%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>Algorithm</th>
<th>Key Innovation</th>
<th>Strength</th>
<th>Limitation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>AdaBoost</td>
<td>Reweighting samples</td>
<td>Simple, interpretable</td>
<td>Sensitive to noise</td>
</tr>
<tr class="even">
<td>XGBoost</td>
<td>Regularized, efficient boosting</td>
<td>Accuracy, scalability</td>
<td>Heavier resource use</td>
</tr>
<tr class="odd">
<td>LightGBM</td>
<td>Histogram + leaf-wise growth</td>
<td>Very fast, memory efficient</td>
<td>May overfit small datasets</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, scikit-learn / LightGBM)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> AdaBoostClassifier, GradientBoostingClassifier</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> lightgbm <span class="im">import</span> LGBMClassifier</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">500</span>, n_features<span class="op">=</span><span class="dv">20</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>ada <span class="op">=</span> AdaBoostClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>).fit(X, y)</span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>xgb <span class="op">=</span> GradientBoostingClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>).fit(X, y)  <span class="co"># scikit-learn proxy for XGBoost</span></span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>lgbm <span class="op">=</span> LGBMClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>).fit(X, y)</span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"AdaBoost acc:"</span>, ada.score(X, y))</span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"XGBoost-like acc:"</span>, xgb.score(X, y))</span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"LightGBM acc:"</span>, lgbm.score(X, y))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-65" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-65">Why it Matters</h4>
<p>Boosting algorithms dominate structured data ML competitions and real-world applications (finance, healthcare, search ranking). Choosing between AdaBoost, XGBoost, and LightGBM depends on data size, complexity, and interpretability needs.</p>
</section>
<section id="try-it-yourself-65" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-65">Try It Yourself</h4>
<ol type="1">
<li>Train AdaBoost on noisy data. Does performance degrade faster than XGBoost/LightGBM?</li>
<li>Benchmark training speed of XGBoost vs.&nbsp;LightGBM on a large dataset.</li>
<li>Reflect: why do boosting methods still win in Kaggle competitions despite deep learning’s popularity?</li>
</ol>
</section>
</section>
<section id="regularization-in-tree-ensembles" class="level3">
<h3 class="anchored" data-anchor-id="regularization-in-tree-ensembles">667. Regularization in Tree Ensembles</h3>
<p>Tree ensembles like Gradient Boosting and Random Forests can easily overfit if left unchecked. Regularization techniques control model complexity, improve generalization, and stabilize training.</p>
<section id="picture-in-your-head-66" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-66">Picture in Your Head</h4>
<p>Think of pruning a bonsai tree:</p>
<ul>
<li>Left alone, it grows wild and tangled (overfitting).</li>
<li>With careful trimming (regularization), it stays balanced, healthy, and elegant.</li>
</ul>
</section>
<section id="deep-dive-66" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-66">Deep Dive</h4>
<p>Common regularization methods in tree ensembles:</p>
<ul>
<li><p>Tree-level constraints</p>
<ul>
<li><code>max_depth</code>: limit tree depth.</li>
<li><code>min_samples_split</code> / <code>min_child_weight</code>: require enough samples before splitting.</li>
<li><code>min_samples_leaf</code>: ensure leaves are not too small.</li>
<li><code>max_leaf_nodes</code>: cap total number of leaves.</li>
</ul></li>
<li><p>Ensemble-level constraints</p>
<ul>
<li><p>Learning rate (<span class="math inline">\(\eta\)</span>): shrink contribution of each tree in boosting. Smaller values → slower but more robust learning.</p></li>
<li><p>Subsampling:</p>
<ul>
<li>Row sampling (<code>subsample</code>): use only a fraction of training rows per tree.</li>
<li>Column sampling (<code>colsample_bytree</code>): use only a subset of features per tree.</li>
</ul></li>
</ul></li>
<li><p>Weight regularization (used in XGBoost/LightGBM)</p>
<ul>
<li>L1 penalty (<span class="math inline">\(\alpha\)</span>): encourages sparsity in leaf weights.</li>
<li>L2 penalty (<span class="math inline">\(\lambda\)</span>): shrinks leaf weights smoothly.</li>
</ul></li>
<li><p>Early stopping</p>
<ul>
<li>Stop adding trees when validation loss stops improving.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 26%">
<col style="width: 27%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>Regularization Type</th>
<th>Example Parameter</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Tree-level</td>
<td>max_depth</td>
<td>Controls complexity per tree</td>
</tr>
<tr class="even">
<td>Ensemble-level</td>
<td>learning_rate</td>
<td>Controls additive strength</td>
</tr>
<tr class="odd">
<td>Weight penalty</td>
<td>L1/L2 on leaf scores</td>
<td>Reduces overfitting</td>
</tr>
<tr class="even">
<td>Data sampling</td>
<td>subsample, colsample</td>
<td>Adds randomness, reduces variance</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, XGBoost-style parameters)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> xgboost <span class="im">import</span> XGBClassifier</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">500</span>, n_features<span class="op">=</span><span class="dv">20</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>xgb <span class="op">=</span> XGBClassifier(</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.05</span>,</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>    subsample<span class="op">=</span><span class="fl">0.8</span>,</span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>    colsample_bytree<span class="op">=</span><span class="fl">0.8</span>,</span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a>    reg_alpha<span class="op">=</span><span class="fl">0.1</span>,   <span class="co"># L1 penalty</span></span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a>    reg_lambda<span class="op">=</span><span class="fl">1.0</span>   <span class="co"># L2 penalty</span></span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a>).fit(X, y)</span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-16"><a href="#cb68-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training accuracy:"</span>, xgb.score(X, y))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-66" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-66">Why it Matters</h4>
<p>Regularization makes tree ensembles more robust, especially in noisy, high-dimensional, or imbalanced datasets. Without it, models can memorize training data and fail on unseen cases.</p>
</section>
<section id="try-it-yourself-66" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-66">Try It Yourself</h4>
<ol type="1">
<li>Train a GBDT with no depth or leaf constraints. Does it overfit?</li>
<li>Compare shallow trees (depth=3) vs.&nbsp;deep trees (depth=10) under boosting. Which generalizes better?</li>
<li>Reflect: why is learning rate + early stopping considered the “master regularizer” in boosting?</li>
</ol>
</section>
</section>
<section id="handling-imbalanced-data-with-trees" class="level3">
<h3 class="anchored" data-anchor-id="handling-imbalanced-data-with-trees">668. Handling Imbalanced Data with Trees</h3>
<p>Decision trees and ensembles often face imbalanced datasets, where one class heavily outweighs the others (e.g., fraud detection, medical diagnosis). Without adjustments, models favor the majority class. Tree-based methods provide mechanisms to rebalance learning.</p>
<section id="picture-in-your-head-67" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-67">Picture in Your Head</h4>
<p>Imagine training a referee:</p>
<ul>
<li>If 99 players wear blue and 1 wears red, the referee might always call “blue” and be 99% accurate.</li>
<li>But the real challenge is recognizing the rare red player—just like detecting fraud or rare diseases.</li>
</ul>
</section>
<section id="deep-dive-67" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-67">Deep Dive</h4>
<p>Strategies for handling imbalance in tree models:</p>
<ul>
<li><p>Class weights / cost-sensitive learning</p>
<ul>
<li>Assign higher penalty to misclassifying minority class.</li>
<li>Most libraries (scikit-learn, XGBoost, LightGBM) support <code>class_weight</code> or <code>scale_pos_weight</code>.</li>
</ul></li>
<li><p>Sampling methods</p>
<ul>
<li>Oversampling: duplicate or synthesize minority samples (e.g., SMOTE).</li>
<li>Undersampling: remove majority samples.</li>
<li>Hybrid strategies combine both.</li>
</ul></li>
<li><p>Tree-specific adjustments</p>
<ul>
<li>Adjust splitting criteria to emphasize recall/precision for minority class.</li>
<li>Use metrics like G-mean, AUC-PR, or F1 instead of accuracy.</li>
</ul></li>
<li><p>Ensemble tricks</p>
<ul>
<li>Balanced Random Forest: bootstrap each tree with balanced class samples.</li>
<li>Gradient Boosting with custom loss emphasizing minority detection.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 47%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th>Strategy</th>
<th>How It Works</th>
<th>When Useful</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Class weights</td>
<td>Penalize minority errors more</td>
<td>Simple, fast</td>
</tr>
<tr class="even">
<td>Oversampling</td>
<td>Increase minority presence</td>
<td>Small datasets</td>
</tr>
<tr class="odd">
<td>Undersampling</td>
<td>Reduce majority dominance</td>
<td>Very large datasets</td>
</tr>
<tr class="even">
<td>Balanced ensembles</td>
<td>Force each tree to balance classes</td>
<td>Robust baselines</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, scikit-learn)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">1000</span>, n_features<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>                           weights<span class="op">=</span>[<span class="fl">0.95</span>, <span class="fl">0.05</span>], random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>rf <span class="op">=</span> RandomForestClassifier(class_weight<span class="op">=</span><span class="st">"balanced"</span>).fit(X, y)</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Minority class prediction sample:"</span>, rf.predict(X[:<span class="dv">10</span>]))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-67" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-67">Why it Matters</h4>
<p>In critical fields like fraud detection, cybersecurity, or medical screening, the cost of missing rare cases is enormous. Trees with imbalance-handling strategies allow models to focus on minority classes without sacrificing overall robustness.</p>
</section>
<section id="try-it-yourself-67" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-67">Try It Yourself</h4>
<ol type="1">
<li>Train a Random Forest on imbalanced data with and without <code>class_weight="balanced"</code>. Compare recall for the minority class.</li>
<li>Apply SMOTE before training a GBDT. Does performance improve on minority detection?</li>
<li>Reflect: why might optimizing for AUC-PR be more meaningful than accuracy in highly imbalanced settings?</li>
</ol>
</section>
</section>
<section id="scalability-and-parallelization" class="level3">
<h3 class="anchored" data-anchor-id="scalability-and-parallelization">669. Scalability and Parallelization</h3>
<p>Tree ensembles like Random Forests and Gradient Boosted Trees can be computationally expensive for large datasets. Scalability is achieved through parallelization, efficient data structures, and distributed training frameworks.</p>
<section id="picture-in-your-head-68" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-68">Picture in Your Head</h4>
<p>Think of building a forest:</p>
<ul>
<li>Planting trees one by one is slow.</li>
<li>With enough workers, you can plant many trees in parallel.</li>
<li>Smart organization (batching, splitting land) ensures everyone works efficiently.</li>
</ul>
</section>
<section id="deep-dive-68" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-68">Deep Dive</h4>
<ul>
<li><p>Random Forests</p>
<ul>
<li>Trees are independent → easy to parallelize.</li>
<li>Parallelization happens across trees.</li>
</ul></li>
<li><p>Gradient Boosted Trees (GBDT)</p>
<ul>
<li><p>Sequential by nature (each tree corrects the previous).</p></li>
<li><p>Parallelization possible within a tree:</p>
<ul>
<li>Histogram-based algorithms speed up split finding.</li>
<li>GPU acceleration for gradient/histogram computations.</li>
</ul></li>
<li><p>Modern libraries (XGBoost, LightGBM, CatBoost) implement distributed boosting.</p></li>
</ul></li>
<li><p>Distributed training strategies</p>
<ul>
<li>Data parallelism: split data across workers, each builds partial histograms, then aggregate.</li>
<li>Feature parallelism: split features across workers for split search.</li>
<li>Hybrid parallelism: combine both for very large datasets.</li>
</ul></li>
<li><p>Hardware acceleration</p>
<ul>
<li>GPUs: accelerate histogram building, matrix multiplications.</li>
<li>TPUs (less common): used for tree–deep hybrid methods.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 39%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Parallelism Type</th>
<th>Common in</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Random Forest</td>
<td>Tree-level</td>
<td>scikit-learn, Spark MLlib</td>
</tr>
<tr class="even">
<td>GBDT</td>
<td>Intra-tree (histograms)</td>
<td>XGBoost, LightGBM</td>
</tr>
<tr class="odd">
<td>Distributed</td>
<td>Data/feature partitioning</td>
<td>Spark, Dask, Ray</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, LightGBM with parallelization)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> lightgbm <span class="im">import</span> LGBMClassifier</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">100000</span>, n_features<span class="op">=</span><span class="dv">50</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LGBMClassifier(n_estimators<span class="op">=</span><span class="dv">200</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># use all CPU cores</span></span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training done with parallelization"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-68" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-68">Why it Matters</h4>
<p>Scalability allows tree ensembles to remain competitive even with deep learning on large datasets. Efficient parallelization has made libraries like LightGBM and XGBoost industry standards.</p>
</section>
<section id="try-it-yourself-68" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-68">Try It Yourself</h4>
<ol type="1">
<li>Train a Random Forest with <code>n_jobs=-1</code> (parallel CPU use). Compare runtime to single-threaded.</li>
<li>Benchmark LightGBM on CPU vs.&nbsp;GPU. How much faster is GPU training?</li>
<li>Reflect: why do GBDTs require more careful engineering for scalability than Random Forests?</li>
</ol>
</section>
</section>
<section id="real-world-applications-of-tree-ensembles" class="level3">
<h3 class="anchored" data-anchor-id="real-world-applications-of-tree-ensembles">670. Real-World Applications of Tree Ensembles</h3>
<p>Tree ensembles such as Random Forests and Gradient Boosted Trees dominate in structured/tabular data tasks. Their balance of accuracy, robustness, and interpretability makes them industry-standard across domains from finance to healthcare.</p>
<section id="picture-in-your-head-69" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-69">Picture in Your Head</h4>
<p>Think of a Swiss army knife for data problems:</p>
<ul>
<li>A blade for finance risk scoring,</li>
<li>A screwdriver for medical diagnosis,</li>
<li>A corkscrew for search ranking. Tree ensembles adapt flexibly to whatever task you hand them.</li>
</ul>
</section>
<section id="deep-dive-69" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-69">Deep Dive</h4>
<ul>
<li><p>Finance</p>
<ul>
<li>Credit scoring and default prediction.</li>
<li>Fraud detection in transactions.</li>
<li>Stock movement and risk modeling.</li>
</ul></li>
<li><p>Healthcare</p>
<ul>
<li>Disease diagnosis from lab results.</li>
<li>Patient risk stratification (predicting ICU admissions, mortality).</li>
<li>Genomic data interpretation.</li>
</ul></li>
<li><p>E-commerce &amp; Marketing</p>
<ul>
<li>Recommendation systems (ranking models).</li>
<li>Customer churn prediction.</li>
<li>Pricing optimization.</li>
</ul></li>
<li><p>Cybersecurity</p>
<ul>
<li>Intrusion detection and anomaly detection.</li>
<li>Malware classification.</li>
</ul></li>
<li><p>Search &amp; Information Retrieval</p>
<ul>
<li>Learning-to-rank systems (LambdaMART, XGBoost Rank).</li>
<li>Query relevance scoring.</li>
</ul></li>
<li><p>Industrial &amp; Engineering</p>
<ul>
<li>Predictive maintenance from sensor logs.</li>
<li>Quality control in manufacturing.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 38%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th>Domain</th>
<th>Typical Task</th>
<th>Why Trees Work Well</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Finance</td>
<td>Credit scoring, fraud detection</td>
<td>Handles imbalanced, structured data</td>
</tr>
<tr class="even">
<td>Healthcare</td>
<td>Diagnosis, prognosis</td>
<td>Interpretability, robustness</td>
</tr>
<tr class="odd">
<td>E-commerce</td>
<td>Ranking, churn prediction</td>
<td>Captures nonlinear feature interactions</td>
</tr>
<tr class="even">
<td>Security</td>
<td>Intrusion detection</td>
<td>Works with categorical + numerical logs</td>
</tr>
<tr class="odd">
<td>Industry</td>
<td>Predictive maintenance</td>
<td>Handles mixed noisy sensor data</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, XGBoost for fraud detection)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> xgboost <span class="im">import</span> XGBClassifier</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a><span class="co"># simulate imbalanced fraud dataset</span></span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">10000</span>, n_features<span class="op">=</span><span class="dv">30</span>,</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>                           weights<span class="op">=</span>[<span class="fl">0.95</span>, <span class="fl">0.05</span>], random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>xgb <span class="op">=</span> XGBClassifier(n_estimators<span class="op">=</span><span class="dv">300</span>, max_depth<span class="op">=</span><span class="dv">5</span>, scale_pos_weight<span class="op">=</span><span class="dv">19</span>).fit(X, y)</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training accuracy:"</span>, xgb.score(X, y))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-69" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-69">Why it Matters</h4>
<p>Tree ensembles are the go-to models for tabular data, often outperforming deep neural networks. Their success in Kaggle competitions and real-world deployments underscores their practicality.</p>
</section>
<section id="try-it-yourself-69" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-69">Try It Yourself</h4>
<ol type="1">
<li>Train a Gradient Boosted Tree on a customer churn dataset. Which features drive churn?</li>
<li>Apply Random Forest to a healthcare dataset. Do predictions remain interpretable?</li>
<li>Reflect: why do deep learning models often lag behind GBDTs on structured/tabular tasks?</li>
</ol>
</section>
</section>
</section>
<section id="chapter-68.-feature-selection-and-dimensionality-reduction" class="level2">
<h2 class="anchored" data-anchor-id="chapter-68.-feature-selection-and-dimensionality-reduction">Chapter 68. Feature selection and dimensionality reduction</h2>
<section id="the-curse-of-dimensionality" class="level3">
<h3 class="anchored" data-anchor-id="the-curse-of-dimensionality">671. The Curse of Dimensionality</h3>
<p>As the number of features (dimensions) grows, data becomes sparse, distances lose meaning, and models require exponentially more data to generalize well. This phenomenon is known as the curse of dimensionality.</p>
<section id="picture-in-your-head-70" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-70">Picture in Your Head</h4>
<p>Imagine inflating a balloon:</p>
<ul>
<li>In 1D, you only need a small segment.</li>
<li>In 2D, you need a circle.</li>
<li>In 3D, a sphere.</li>
<li>By the time you reach 100 dimensions, the “volume” is so vast that your data points are like lonely stars in space—far apart and unrepresentative.</li>
</ul>
</section>
<section id="deep-dive-70" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-70">Deep Dive</h4>
<ul>
<li><p>Distance concentration:</p>
<ul>
<li>In high dimensions, distances between nearest and farthest neighbors converge.</li>
<li>Example: Euclidean distances lose contrast → harder for algorithms like k-NN.</li>
</ul></li>
<li><p>Exponential data growth:</p>
<ul>
<li>To maintain density, required data grows exponentially with dimension <span class="math inline">\(d\)</span>.</li>
<li>A grid with 10 points per axis → <span class="math inline">\(10^d\)</span> points total.</li>
</ul></li>
<li><p>Impact on ML:</p>
<ul>
<li>Overfitting risk skyrockets with too many features relative to samples.</li>
<li>Feature selection and dimensionality reduction become essential.</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Effect</th>
<th>Low Dimension</th>
<th>High Dimension</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Density</td>
<td>Dense clusters possible</td>
<td>Points sparse</td>
</tr>
<tr class="even">
<td>Distance contrast</td>
<td>Clear nearest/farthest</td>
<td>All distances similar</td>
</tr>
<tr class="odd">
<td>Data needed</td>
<td>Manageable</td>
<td>Exponential growth</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, distance contrast)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> d <span class="kw">in</span> [<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>]:</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.random.rand(<span class="dv">1000</span>, d)</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>    dists <span class="op">=</span> np.linalg.norm(X[<span class="dv">0</span>] <span class="op">-</span> X, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Dim=</span><span class="sc">{</span>d<span class="sc">}</span><span class="ss">, min dist=</span><span class="sc">{</span>dists<span class="sc">.</span><span class="bu">min</span>()<span class="sc">:.3f}</span><span class="ss">, max dist=</span><span class="sc">{</span>dists<span class="sc">.</span><span class="bu">max</span>()<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-70" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-70">Why it Matters</h4>
<p>The curse of dimensionality explains why feature engineering, selection, and dimensionality reduction are central in machine learning. Without reducing irrelevant features, models struggle with noise and sparsity.</p>
</section>
<section id="try-it-yourself-70" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-70">Try It Yourself</h4>
<ol type="1">
<li>Run k-NN classification on datasets with increasing feature counts. How does accuracy change?</li>
<li>Apply PCA to high-dimensional data. Does performance improve?</li>
<li>Reflect: why do models like trees and boosting sometimes handle high dimensions better than distance-based methods?</li>
</ol>
</section>
</section>
<section id="filter-methods-correlation-mutual-information" class="level3">
<h3 class="anchored" data-anchor-id="filter-methods-correlation-mutual-information">672. Filter Methods (Correlation, Mutual Information)</h3>
<p>Filter methods for feature selection evaluate each feature’s relevance to the target independently of the model. They rely on statistical measures like correlation or mutual information to rank and select features.</p>
<section id="picture-in-your-head-71" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-71">Picture in Your Head</h4>
<p>Think of auditioning actors for a play:</p>
<ul>
<li>Each actor is evaluated individually on stage presence.</li>
<li>Only the strongest performers make it to the cast.</li>
<li>The director (model) later decides how they interact.</li>
</ul>
</section>
<section id="deep-dive-71" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-71">Deep Dive</h4>
<ul>
<li><p>Correlation-based selection</p>
<ul>
<li>Pearson correlation (linear relationships).</li>
<li>Spearman correlation (monotonic relationships).</li>
<li>Limitation: only captures simple linear/monotonic effects.</li>
</ul></li>
<li><p>Mutual Information (MI)</p>
<ul>
<li>Measures dependency between variables:</li>
</ul>
<p><span class="math display">\[
MI(X; Y) = \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}
\]</span></p>
<ul>
<li>Captures nonlinear associations.</li>
<li>Works for categorical, discrete, and continuous features.</li>
</ul></li>
<li><p>Statistical tests</p>
<ul>
<li>Chi-square test for categorical features.</li>
<li>ANOVA F-test for continuous features vs.&nbsp;categorical target.</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Method</th>
<th>Captures</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Pearson Correlation</td>
<td>Linear association</td>
<td>Continuous target</td>
</tr>
<tr class="even">
<td>Spearman</td>
<td>Monotonic</td>
<td>Ranked/ordinal target</td>
</tr>
<tr class="odd">
<td>Mutual Information</td>
<td>Nonlinear dependency</td>
<td>General-purpose</td>
</tr>
<tr class="even">
<td>Chi-square</td>
<td>Independence</td>
<td>Categorical features</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, scikit-learn)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> mutual_info_classif</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">500</span>, n_features<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>mi <span class="op">=</span> mutual_info_classif(X, y)</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, score <span class="kw">in</span> <span class="bu">enumerate</span>(mi):</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Feature </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">: MI score=</span><span class="sc">{</span>score<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-71" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-71">Why it Matters</h4>
<p>Filter methods are fast, scalable, and model-agnostic. They provide a strong first pass at reducing dimensionality before more complex selection methods.</p>
</section>
<section id="try-it-yourself-71" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-71">Try It Yourself</h4>
<ol type="1">
<li>Compare correlation vs.&nbsp;MI ranking of features in a dataset. Do they select the same features?</li>
<li>Use chi-square test for feature selection in a text classification task (bag-of-words).</li>
<li>Reflect: why might filter methods discard features that interact strongly only in combination?</li>
</ol>
</section>
</section>
<section id="wrapper-methods-and-search-strategies" class="level3">
<h3 class="anchored" data-anchor-id="wrapper-methods-and-search-strategies">673. Wrapper Methods and Search Strategies</h3>
<p>Wrapper methods evaluate feature subsets by training a model on them directly. Instead of ranking features individually, they search through combinations to find the best-performing subset.</p>
<section id="picture-in-your-head-72" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-72">Picture in Your Head</h4>
<p>Imagine building a sports team:</p>
<ul>
<li>Some players look strong individually (filter methods),</li>
<li>But only certain combinations of players form a winning team. Wrapper methods test different lineups until they find the best one.</li>
</ul>
</section>
<section id="deep-dive-72" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-72">Deep Dive</h4>
<ul>
<li><p>Forward Selection</p>
<ul>
<li>Start with no features.</li>
<li>Iteratively add the feature that improves performance the most.</li>
<li>Stop when no improvement or a limit is reached.</li>
</ul></li>
<li><p>Backward Elimination</p>
<ul>
<li>Start with all features.</li>
<li>Iteratively remove the least useful feature.</li>
</ul></li>
<li><p>Recursive Feature Elimination (RFE)</p>
<ul>
<li>Train model, rank features by importance, drop the weakest, repeat.</li>
<li>Works well with linear models and tree ensembles.</li>
</ul></li>
<li><p>Heuristic / Metaheuristic search</p>
<ul>
<li>Genetic algorithms, simulated annealing, reinforcement search for feature subsets.</li>
<li>Useful when feature space is very large.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 27%">
<col style="width: 32%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Process</th>
<th>Strength</th>
<th>Weakness</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Forward Selection</td>
<td>Start empty, add features</td>
<td>Efficient on small sets</td>
<td>Risk of local optima</td>
</tr>
<tr class="even">
<td>Backward Elimination</td>
<td>Start full, remove features</td>
<td>Detects redundancy</td>
<td>Costly for large sets</td>
</tr>
<tr class="odd">
<td>RFE</td>
<td>Iteratively drop weakest</td>
<td>Works well with model importance</td>
<td>Expensive</td>
</tr>
<tr class="even">
<td>Heuristics</td>
<td>Randomized search</td>
<td>Escapes local optima</td>
<td>Computationally heavy</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, Recursive Feature Elimination)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> RFE</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">500</span>, n_features<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(max_iter<span class="op">=</span><span class="dv">500</span>)</span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>rfe <span class="op">=</span> RFE(model, n_features_to_select<span class="op">=</span><span class="dv">5</span>).fit(X, y)</span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Selected features:"</span>, rfe.support_)</span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Ranking:"</span>, rfe.ranking_)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-72" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-72">Why it Matters</h4>
<p>Wrapper methods align feature selection with the actual model performance, often yielding better results than filter methods. However, they are computationally expensive and less scalable.</p>
</section>
<section id="try-it-yourself-72" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-72">Try It Yourself</h4>
<ol type="1">
<li>Run forward selection vs.&nbsp;RFE on the same dataset. Do they agree on key features?</li>
<li>Compare wrapper results when using logistic regression vs.&nbsp;random forest as the evaluator.</li>
<li>Reflect: why might wrapper methods overfit when the dataset is small?</li>
</ol>
</section>
</section>
<section id="embedded-methods-lasso-tree-based" class="level3">
<h3 class="anchored" data-anchor-id="embedded-methods-lasso-tree-based">674. Embedded Methods (Lasso, Tree-Based)</h3>
<p>Embedded methods perform feature selection during model training by incorporating selection directly into the learning algorithm. Unlike filter (pre-selection) or wrapper (post-selection) methods, embedded approaches are integrated and efficient.</p>
<section id="picture-in-your-head-73" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-73">Picture in Your Head</h4>
<p>Imagine building a bridge:</p>
<ul>
<li>Filter = choosing the strongest materials before construction.</li>
<li>Wrapper = testing different bridges after building them.</li>
<li>Embedded = the bridge strengthens or drops weak beams automatically as it’s built.</li>
</ul>
</section>
<section id="deep-dive-73" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-73">Deep Dive</h4>
<ul>
<li><p>Lasso (L1 Regularization)</p>
<ul>
<li>Adds penalty <span class="math inline">\(\lambda \sum |\beta_j|\)</span> to regression coefficients.</li>
<li>Drives some coefficients exactly to zero, performing feature selection.</li>
<li>Works well when only a few features matter (sparsity).</li>
</ul></li>
<li><p>Elastic Net</p>
<ul>
<li>Combines L1 (Lasso) and L2 (Ridge).</li>
<li>Useful when correlated features exist—Lasso alone may select one arbitrarily.</li>
</ul></li>
<li><p>Tree-Based Feature Importance</p>
<ul>
<li>Decision Trees, Random Forests, and GBDTs rank features by their split contributions.</li>
<li>Naturally embedded feature selection.</li>
</ul></li>
<li><p>Regularized Linear Models (Logistic Regression, SVM)</p>
<ul>
<li>L1 penalty → sparsity.</li>
<li>L2 penalty → shrinks coefficients but keeps all features.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 23%">
<col style="width: 20%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th>Embedded Method</th>
<th>Mechanism</th>
<th>Strength</th>
<th>Weakness</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Lasso</td>
<td>L1 regularization</td>
<td>Sparse, simple</td>
<td>Struggles with correlated features</td>
</tr>
<tr class="even">
<td>Elastic Net</td>
<td>L1 + L2</td>
<td>Handles correlation</td>
<td>Needs tuning</td>
</tr>
<tr class="odd">
<td>Trees</td>
<td>Split-based selection</td>
<td>Captures nonlinear</td>
<td>Can bias toward many-valued features</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, Lasso for feature selection)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Lasso</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_regression</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_regression(n_samples<span class="op">=</span><span class="dv">100</span>, n_features<span class="op">=</span><span class="dv">10</span>, n_informative<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>lasso <span class="op">=</span> Lasso(alpha<span class="op">=</span><span class="fl">0.1</span>).fit(X, y)</span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Selected features:"</span>, np.where(lasso.coef_ <span class="op">!=</span> <span class="dv">0</span>)[<span class="dv">0</span>])</span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Coefficients:"</span>, lasso.coef_)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-73" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-73">Why it Matters</h4>
<p>Embedded methods combine efficiency with accuracy by performing feature selection within model training. They are especially powerful in high-dimensional datasets like genomics, text, and finance.</p>
</section>
<section id="try-it-yourself-73" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-73">Try It Yourself</h4>
<ol type="1">
<li>Train Lasso with different regularization strengths. How does the number of selected features change?</li>
<li>Compare Elastic Net vs.&nbsp;Lasso when features are correlated. Which is more stable?</li>
<li>Reflect: why are tree-based embedded methods preferred for nonlinear, high-dimensional problems?</li>
</ol>
</section>
</section>
<section id="principal-component-analysis-pca" class="level3">
<h3 class="anchored" data-anchor-id="principal-component-analysis-pca">675. Principal Component Analysis (PCA)</h3>
<p>Principal Component Analysis (PCA) is a dimensionality reduction method that projects data into a lower-dimensional space while preserving as much variance as possible. It finds new axes (principal components) that capture the directions of maximum variability.</p>
<section id="picture-in-your-head-74" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-74">Picture in Your Head</h4>
<p>Imagine rotating a cloud of points:</p>
<ul>
<li>From one angle, it looks wide and spread out.</li>
<li>From another, it looks narrow. PCA finds the best rotation so that most of the information lies along the first few axes.</li>
</ul>
</section>
<section id="deep-dive-74" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-74">Deep Dive</h4>
<ul>
<li><p>Mathematics:</p>
<ul>
<li><p>Compute covariance matrix:</p>
<p><span class="math display">\[
\Sigma = \frac{1}{n} X^TX
\]</span></p></li>
<li><p>Solve eigenvalue decomposition:</p>
<p><span class="math display">\[
\Sigma v = \lambda v
\]</span></p></li>
<li><p>Eigenvectors = principal components.</p></li>
<li><p>Eigenvalues = variance explained.</p></li>
</ul></li>
<li><p>Steps:</p>
<ol type="1">
<li>Standardize data.</li>
<li>Compute covariance matrix.</li>
<li>Extract eigenvalues/eigenvectors.</li>
<li>Project data onto top <span class="math inline">\(k\)</span> components.</li>
</ol></li>
<li><p>Interpretation:</p>
<ul>
<li>PC1 = direction of maximum variance.</li>
<li>PC2 = orthogonal direction of next maximum variance.</li>
<li>Subsequent PCs capture diminishing variance.</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Term</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Principal Component</td>
<td>New axis (linear combination of features)</td>
</tr>
<tr class="even">
<td>Explained Variance</td>
<td>How much variability is captured</td>
</tr>
<tr class="odd">
<td>Scree Plot</td>
<td>Visualization of variance by component</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, scikit-learn)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>X, _ <span class="op">=</span> load_iris(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>).fit(X)</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Explained variance ratio:"</span>, pca.explained_variance_ratio_)</span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"First 2 components:</span><span class="ch">\n</span><span class="st">"</span>, pca.components_)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-74" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-74">Why it Matters</h4>
<p>PCA reduces noise, improves efficiency, and helps visualize high-dimensional data. It is widely used in preprocessing pipelines for clustering, visualization, and speeding up downstream models.</p>
</section>
<section id="try-it-yourself-74" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-74">Try It Yourself</h4>
<ol type="1">
<li>Perform PCA on a dataset and plot the first 2 principal components. Do clusters emerge?</li>
<li>Compare performance of a classifier before and after PCA.</li>
<li>Reflect: why might PCA discard features critical for interpretability, even if variance is low?</li>
</ol>
</section>
</section>
<section id="linear-discriminant-analysis-lda" class="level3">
<h3 class="anchored" data-anchor-id="linear-discriminant-analysis-lda">676. Linear Discriminant Analysis (LDA)</h3>
<p>Linear Discriminant Analysis (LDA) is both a dimensionality reduction technique and a classifier. Unlike PCA, which is unsupervised, LDA uses class labels to find projections that maximize between-class separation while minimizing within-class variance.</p>
<section id="picture-in-your-head-75" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-75">Picture in Your Head</h4>
<p>Imagine shining a flashlight on two clusters of objects:</p>
<ul>
<li>PCA points the light to capture the largest spread overall.</li>
<li>LDA points the light so the clusters look as far apart as possible on the wall.</li>
</ul>
</section>
<section id="deep-dive-75" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-75">Deep Dive</h4>
<ul>
<li><p>Objective: Find projection matrix <span class="math inline">\(W\)</span> that maximizes:</p>
<p><span class="math display">\[
J(W) = \frac{|W^T S_b W|}{|W^T S_w W|}
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(S_b\)</span>: between-class scatter matrix.</li>
<li><span class="math inline">\(S_w\)</span>: within-class scatter matrix.</li>
</ul></li>
<li><p>Steps:</p>
<ol type="1">
<li>Compute class means.</li>
<li>Compute <span class="math inline">\(S_b\)</span> and <span class="math inline">\(S_w\)</span>.</li>
<li>Solve generalized eigenvalue problem.</li>
<li>Project data onto top <span class="math inline">\(k\)</span> discriminant components.</li>
</ol></li>
<li><p>Interpretation:</p>
<ul>
<li>Number of discriminant components ≤ (#classes − 1).</li>
<li>For binary classification, projection is onto a single line.</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Method</th>
<th>Supervision</th>
<th>Goal</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>PCA</td>
<td>Unsupervised</td>
<td>Maximize variance</td>
</tr>
<tr class="even">
<td>LDA</td>
<td>Supervised</td>
<td>Maximize class separation</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, scikit-learn)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.discriminant_analysis <span class="im">import</span> LinearDiscriminantAnalysis</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> load_iris(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>lda <span class="op">=</span> LinearDiscriminantAnalysis(n_components<span class="op">=</span><span class="dv">2</span>).fit(X, y)</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>X_proj <span class="op">=</span> lda.transform(X)</span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Transformed shape:"</span>, X_proj.shape)</span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Explained variance ratio:"</span>, lda.explained_variance_ratio_)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-75" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-75">Why it Matters</h4>
<p>LDA is powerful when classes are linearly separable and dimensionality is high. It reduces noise and boosts interpretability in classification tasks, especially in bioinformatics, image recognition, and text categorization.</p>
</section>
<section id="try-it-yourself-75" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-75">Try It Yourself</h4>
<ol type="1">
<li>Compare PCA vs.&nbsp;LDA on the Iris dataset. Which separates species better?</li>
<li>Use LDA as a classifier. How does it compare to logistic regression?</li>
<li>Reflect: why is LDA limited when classes are not linearly separable?</li>
</ol>
</section>
</section>
<section id="nonlinear-methods-t-sne-umap" class="level3">
<h3 class="anchored" data-anchor-id="nonlinear-methods-t-sne-umap">677. Nonlinear Methods: t-SNE, UMAP</h3>
<p>When PCA and LDA fail to capture complex structures, nonlinear dimensionality reduction methods step in. Techniques like t-SNE and UMAP are especially effective for visualization, preserving local neighborhoods in high-dimensional data.</p>
<section id="picture-in-your-head-76" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-76">Picture in Your Head</h4>
<p>Imagine folding a paper map of a city:</p>
<ul>
<li>Straight folding (PCA) keeps distances globally but distorts local neighborhoods.</li>
<li>Smart folding (t-SNE, UMAP) ensures that nearby streets stay close on the folded map, even if global distances stretch.</li>
</ul>
</section>
<section id="deep-dive-76" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-76">Deep Dive</h4>
<ul>
<li><p>t-SNE (t-Distributed Stochastic Neighbor Embedding)</p>
<ul>
<li>Models pairwise similarities as probabilities in high and low dimensions.</li>
<li>Minimizes KL divergence between distributions.</li>
<li>Strengths: preserves local clusters, reveals hidden structures.</li>
<li>Weaknesses: poor at global structure, slow on large datasets.</li>
</ul></li>
<li><p>UMAP (Uniform Manifold Approximation and Projection)</p>
<ul>
<li>Based on manifold learning + topological data analysis.</li>
<li>Faster than t-SNE, scales to millions of points.</li>
<li>Preserves both local and some global structure better than t-SNE.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 33%">
<col style="width: 25%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Strength</th>
<th>Weakness</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>t-SNE</td>
<td>Excellent local clustering</td>
<td>Loses global structure, slow</td>
<td>Visualization of embeddings</td>
</tr>
<tr class="even">
<td>UMAP</td>
<td>Fast, local + some global preservation</td>
<td>Sensitive to hyperparams</td>
<td>Large-scale visualization, preprocessing</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, t-SNE &amp; UMAP)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_digits</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> umap</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> load_digits(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a><span class="co"># t-SNE</span></span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>X_tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>).fit_transform(X)</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a><span class="co"># UMAP</span></span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>X_umap <span class="op">=</span> umap.UMAP(n_components<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>).fit_transform(X)</span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"t-SNE shape:"</span>, X_tsne.shape)</span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"UMAP shape:"</span>, X_umap.shape)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-76" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-76">Why it Matters</h4>
<p>t-SNE and UMAP are go-to tools for visualizing high-dimensional embeddings (e.g., word vectors, image features). They help researchers discover structure in data that linear projections miss.</p>
</section>
<section id="try-it-yourself-76" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-76">Try It Yourself</h4>
<ol type="1">
<li>Apply t-SNE and UMAP to MNIST digit embeddings. Which clusters digits more clearly?</li>
<li>Increase dimensionality (2D → 3D). Does visualization improve?</li>
<li>Reflect: why are these methods excellent for visualization but risky for downstream predictive tasks?</li>
</ol>
</section>
</section>
<section id="autoencoders-for-dimension-reduction" class="level3">
<h3 class="anchored" data-anchor-id="autoencoders-for-dimension-reduction">678. Autoencoders for Dimension Reduction</h3>
<p>Autoencoders are neural networks trained to reconstruct their input. By compressing data into a low-dimensional latent space (the bottleneck) and then decoding it back, they learn efficient nonlinear representations useful for dimensionality reduction.</p>
<section id="picture-in-your-head-77" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-77">Picture in Your Head</h4>
<p>Think of squeezing a sponge:</p>
<ul>
<li>The water (information) gets compressed into a small shape.</li>
<li>When released, the sponge expands again. Autoencoders do the same: compress data → expand it back.</li>
</ul>
</section>
<section id="deep-dive-77" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-77">Deep Dive</h4>
<ul>
<li><p>Architecture:</p>
<ul>
<li>Encoder: maps input <span class="math inline">\(x\)</span> to latent representation <span class="math inline">\(z\)</span>.</li>
<li>Decoder: reconstructs input <span class="math inline">\(\hat{x}\)</span> from <span class="math inline">\(z\)</span>.</li>
<li>Bottleneck forces model to learn compressed features.</li>
</ul></li>
<li><p>Loss function:</p>
<p><span class="math display">\[
L(x, \hat{x}) = \|x - \hat{x}\|^2
\]</span></p>
<p>(Mean squared error for continuous data, cross-entropy for binary).</p></li>
<li><p>Variants:</p>
<ul>
<li>Denoising Autoencoder: reconstructs clean input from corrupted version.</li>
<li>Sparse Autoencoder: enforces sparsity on hidden units.</li>
<li>Variational Autoencoder (VAE): probabilistic latent space, good for generative tasks.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 45%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th>Type</th>
<th>Key Idea</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Vanilla AE</td>
<td>Compression via reconstruction</td>
<td>Dimensionality reduction</td>
</tr>
<tr class="even">
<td>Denoising AE</td>
<td>Robust to noise</td>
<td>Preprocessing</td>
</tr>
<tr class="odd">
<td>Sparse AE</td>
<td>Few active neurons</td>
<td>Feature learning</td>
</tr>
<tr class="even">
<td>VAE</td>
<td>Probabilistic latent space</td>
<td>Generative modeling</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, PyTorch Autoencoder)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Autoencoder(nn.Module):</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> nn.Sequential(nn.Linear(<span class="dv">100</span>, <span class="dv">32</span>), nn.ReLU(), nn.Linear(<span class="dv">32</span>, <span class="dv">8</span>))</span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> nn.Sequential(nn.Linear(<span class="dv">8</span>, <span class="dv">32</span>), nn.ReLU(), nn.Linear(<span class="dv">32</span>, <span class="dv">100</span>))</span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> <span class="va">self</span>.encoder(x)</span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.decoder(z)</span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-13"><a href="#cb79-13" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Autoencoder()</span>
<span id="cb79-14"><a href="#cb79-14" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb79-15"><a href="#cb79-15" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> model(x)</span>
<span id="cb79-16"><a href="#cb79-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input shape:"</span>, x.shape, <span class="st">"Output shape:"</span>, output.shape)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-77" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-77">Why it Matters</h4>
<p>Autoencoders generalize PCA to nonlinear settings, making them powerful for compressing high-dimensional data like images, text embeddings, and genomics. They also serve as building blocks for generative models.</p>
</section>
<section id="try-it-yourself-77" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-77">Try It Yourself</h4>
<ol type="1">
<li>Train an autoencoder on MNIST digits. Visualize the 2D latent space. Do digits cluster?</li>
<li>Add Gaussian noise to inputs and train a denoising autoencoder. Does it learn robust features?</li>
<li>Reflect: why might a VAE’s probabilistic latent space be more useful than a deterministic one?</li>
</ol>
</section>
</section>
<section id="feature-selection-vs.-feature-extraction" class="level3">
<h3 class="anchored" data-anchor-id="feature-selection-vs.-feature-extraction">679. Feature Selection vs.&nbsp;Feature Extraction</h3>
<p>Reducing dimensionality can be done in two ways:</p>
<ul>
<li>Feature Selection: keep a subset of the original features.</li>
<li>Feature Extraction: transform original features into a new space. Both aim to simplify models, reduce overfitting, and improve interpretability.</li>
</ul>
<section id="picture-in-your-head-78" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-78">Picture in Your Head</h4>
<p>Imagine packing for travel:</p>
<ul>
<li>Selection = choosing which clothes to take from your closet.</li>
<li>Extraction = compressing clothes into vacuum bags to save space. Both reduce load, but in different ways.</li>
</ul>
</section>
<section id="deep-dive-78" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-78">Deep Dive</h4>
<ul>
<li><p>Feature Selection</p>
<ul>
<li>Methods: filter (MI, correlation), wrapper (RFE), embedded (Lasso, trees).</li>
<li>Keeps original semantics of features.</li>
<li>Useful when interpretability matters (e.g., gene selection, finance).</li>
</ul></li>
<li><p>Feature Extraction</p>
<ul>
<li>Methods: PCA, LDA, autoencoders, t-SNE/UMAP.</li>
<li>Produces transformed features (linear or nonlinear combinations).</li>
<li>Improves performance but sacrifices interpretability.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 47%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>Feature Selection</th>
<th>Feature Extraction</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Output</td>
<td>Subset of original features</td>
<td>New transformed features</td>
</tr>
<tr class="even">
<td>Interpretability</td>
<td>High</td>
<td>Often low</td>
</tr>
<tr class="odd">
<td>Complexity</td>
<td>Simple to apply</td>
<td>Requires modeling step</td>
</tr>
<tr class="even">
<td>Example Methods</td>
<td>Lasso, RFE, Random Forest importance</td>
<td>PCA, Autoencoder, UMAP</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, selection vs.&nbsp;extraction)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> SelectKBest, f_classif</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">500</span>, n_features<span class="op">=</span><span class="dv">20</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Selection: keep top 5 features</span></span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>X_sel <span class="op">=</span> SelectKBest(f_classif, k<span class="op">=</span><span class="dv">5</span>).fit_transform(X, y)</span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Extraction: project to 5 principal components</span></span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a>X_pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">5</span>).fit_transform(X)</span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Selection shape:"</span>, X_sel.shape)</span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Extraction shape:"</span>, X_pca.shape)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-78" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-78">Why it Matters</h4>
<p>Choosing between selection and extraction depends on goals:</p>
<ul>
<li>If interpretability is critical → selection.</li>
<li>If performance and compression matter → extraction. Many workflows combine both.</li>
</ul>
</section>
<section id="try-it-yourself-78" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-78">Try It Yourself</h4>
<ol type="1">
<li>Apply selection (Lasso) and extraction (PCA) on the same dataset. Compare accuracy.</li>
<li>In a biomedical dataset, check if selected genes are interpretable to domain experts.</li>
<li>Reflect: when building explainable AI, why might feature selection be more appropriate than extraction?</li>
</ol>
</section>
</section>
<section id="practical-guidelines-and-tradeoffs" class="level3">
<h3 class="anchored" data-anchor-id="practical-guidelines-and-tradeoffs">680. Practical Guidelines and Tradeoffs</h3>
<p>Dimensionality reduction and feature handling involve balancing interpretability, performance, and computational cost. No single method fits all tasks—choosing wisely depends on the dataset and goals.</p>
<section id="picture-in-your-head-79" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-79">Picture in Your Head</h4>
<p>Think of navigating a city:</p>
<ul>
<li>Highways (extraction) get you there faster but hide the neighborhoods.</li>
<li>Side streets (selection) keep context but take longer. The best route depends on whether you care about speed or understanding.</li>
</ul>
</section>
<section id="deep-dive-79" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-79">Deep Dive</h4>
<p>Key considerations when reducing dimensions:</p>
<ul>
<li><p>Dataset size</p>
<ul>
<li>Small data → prefer feature selection to avoid overfitting.</li>
<li>Large data → feature extraction (PCA, autoencoders) scales better.</li>
</ul></li>
<li><p>Model type</p>
<ul>
<li>Linear models benefit from feature selection for interpretability.</li>
<li>Nonlinear models (trees, neural nets) tolerate more features but may still benefit from extraction.</li>
</ul></li>
<li><p>Interpretability vs.&nbsp;accuracy</p>
<ul>
<li>Feature selection preserves meaning.</li>
<li>Feature extraction often boosts accuracy but sacrifices clarity.</li>
</ul></li>
<li><p>Computation</p>
<ul>
<li>PCA, LDA are relatively cheap.</li>
<li>Nonlinear methods (t-SNE, UMAP, autoencoders) can be costly.</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Goal</th>
<th>Best Approach</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Interpretability</td>
<td>Selection</td>
<td>Lasso on genomic data</td>
</tr>
<tr class="even">
<td>Visualization</td>
<td>Extraction</td>
<td>t-SNE on embeddings</td>
</tr>
<tr class="odd">
<td>Compression</td>
<td>Extraction</td>
<td>Autoencoders on images</td>
</tr>
<tr class="even">
<td>Fast baseline</td>
<td>Filter-based selection</td>
<td>Correlation / MI ranking</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, comparing selection vs.&nbsp;extraction in a pipeline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> SelectKBest, f_classif</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">1000</span>, n_features<span class="op">=</span><span class="dv">50</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Selection pipeline</span></span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a>pipe_sel <span class="op">=</span> Pipeline([</span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"select"</span>, SelectKBest(f_classif, k<span class="op">=</span><span class="dv">10</span>)),</span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"clf"</span>, LogisticRegression(max_iter<span class="op">=</span><span class="dv">500</span>))</span>
<span id="cb81-13"><a href="#cb81-13" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb81-14"><a href="#cb81-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-15"><a href="#cb81-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Extraction pipeline</span></span>
<span id="cb81-16"><a href="#cb81-16" aria-hidden="true" tabindex="-1"></a>pipe_pca <span class="op">=</span> Pipeline([</span>
<span id="cb81-17"><a href="#cb81-17" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"pca"</span>, PCA(n_components<span class="op">=</span><span class="dv">10</span>)),</span>
<span id="cb81-18"><a href="#cb81-18" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"clf"</span>, LogisticRegression(max_iter<span class="op">=</span><span class="dv">500</span>))</span>
<span id="cb81-19"><a href="#cb81-19" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb81-20"><a href="#cb81-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-21"><a href="#cb81-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Selection acc:"</span>, pipe_sel.fit(X,y).score(X,y))</span>
<span id="cb81-22"><a href="#cb81-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Extraction acc:"</span>, pipe_pca.fit(X,y).score(X,y))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-79" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-79">Why it Matters</h4>
<p>Practical ML often hinges less on exotic algorithms and more on sensible preprocessing choices. Correctly balancing interpretability, accuracy, and scalability determines real-world success.</p>
</section>
<section id="try-it-yourself-79" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-79">Try It Yourself</h4>
<ol type="1">
<li>Build models with selection vs.&nbsp;extraction on the same dataset. Which generalizes better?</li>
<li>Test different dimensionality reduction techniques with cross-validation.</li>
<li>Reflect: in your domain, is explainability more important than squeezing out the last 1% of accuracy?</li>
</ol>
</section>
</section>
</section>
<section id="chapter-69.-imbalanced-data-and-cost-sensitive-learning" class="level2">
<h2 class="anchored" data-anchor-id="chapter-69.-imbalanced-data-and-cost-sensitive-learning">Chapter 69. Imbalanced data and cost-sensitive learning</h2>
<section id="the-problem-of-skewed-class-distributions" class="level3">
<h3 class="anchored" data-anchor-id="the-problem-of-skewed-class-distributions">681. The Problem of Skewed Class Distributions</h3>
<p>In many real-world datasets, one class heavily outweighs others. This class imbalance leads to models that appear accurate but fail to detect rare events. For example, predicting “no fraud” 99.5% of the time looks accurate, but misses almost all fraud cases.</p>
<section id="picture-in-your-head-80" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-80">Picture in Your Head</h4>
<p>Imagine looking for a needle in a haystack:</p>
<ul>
<li>A naive strategy of always guessing “hay” gives 99.9% accuracy.</li>
<li>But it never finds the needle. Class imbalance forces us to design models that care about the needles.</li>
</ul>
</section>
<section id="deep-dive-80" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-80">Deep Dive</h4>
<ul>
<li><p>Types of imbalance</p>
<ul>
<li>Binary imbalance: one positive class vs.&nbsp;many negatives (fraud detection).</li>
<li>Multiclass imbalance: some classes dominate (rare diseases in medical datasets).</li>
<li>Within-class imbalance: subclasses vary in density (rare fraud patterns).</li>
</ul></li>
<li><p>Impact on models</p>
<ul>
<li>Accuracy is misleading. dominated by majority class.</li>
<li>Classifiers biased toward majority → poor recall for minority.</li>
<li>Decision thresholds skew toward majority unless adjusted.</li>
</ul></li>
<li><p>Evaluation pitfalls</p>
<ul>
<li>Accuracy ≠ good metric.</li>
<li>Precision, Recall, F1, ROC-AUC, PR-AUC more informative.</li>
<li>PR-AUC is especially useful when positive class is very rare.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 21%">
<col style="width: 16%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Scenario</th>
<th>Majority Class</th>
<th>Minority Class</th>
<th>Risk</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Fraud detection</td>
<td>Legit transactions</td>
<td>Fraud</td>
<td>Fraud missed → huge financial loss</td>
</tr>
<tr class="even">
<td>Medical diagnosis</td>
<td>Healthy</td>
<td>Rare disease</td>
<td>Missed diagnosis → patient harm</td>
</tr>
<tr class="odd">
<td>Security logs</td>
<td>Normal activity</td>
<td>Intrusion</td>
<td>Attacks go undetected</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, simulate imbalance)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">1000</span>, n_features<span class="op">=</span><span class="dv">20</span>, weights<span class="op">=</span>[<span class="fl">0.95</span>, <span class="fl">0.05</span>], random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Class distribution:"</span>, Counter(y))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-80" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-80">Why it Matters</h4>
<p>Imbalanced data is the norm in critical applications. finance, healthcare, cybersecurity. Understanding its challenges is the foundation for effective resampling, cost-sensitive learning, and custom evaluation.</p>
</section>
<section id="try-it-yourself-80" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-80">Try It Yourself</h4>
<ol type="1">
<li>Train a logistic regression model on an imbalanced dataset. Check accuracy vs.&nbsp;recall for minority class.</li>
<li>Plot ROC and PR curves. Which gives a clearer picture of minority class performance?</li>
<li>Reflect: why is PR-AUC often more informative than ROC-AUC in extreme imbalance scenarios?</li>
</ol>
</section>
</section>
<section id="sampling-methods-undersampling-and-oversampling" class="level3">
<h3 class="anchored" data-anchor-id="sampling-methods-undersampling-and-oversampling">682. Sampling Methods: Undersampling and Oversampling</h3>
<p>Sampling methods balance class distributions by either reducing majority samples (undersampling) or increasing minority samples (oversampling). These approaches reshape the training data to give the minority class more influence during learning.</p>
<section id="picture-in-your-head-81" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-81">Picture in Your Head</h4>
<p>Imagine a classroom with 95 blue shirts and 5 red shirts:</p>
<ul>
<li>Undersampling: ask 5 blue shirts to stay and dismiss the rest → balanced but fewer total students.</li>
<li>Oversampling: duplicate or recruit more red shirts → balanced but risk of repetition.</li>
</ul>
</section>
<section id="deep-dive-81" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-81">Deep Dive</h4>
<ul>
<li><p>Undersampling</p>
<ul>
<li>Random undersampling: drop random majority samples.</li>
<li>Edited Nearest Neighbors (ENN), Tomek links: remove borderline or redundant majority points.</li>
<li>Pros: fast, reduces training size.</li>
<li>Cons: risks losing valuable information.</li>
</ul></li>
<li><p>Oversampling</p>
<ul>
<li>Random oversampling: duplicate minority samples.</li>
<li>SMOTE (Synthetic Minority Over-sampling Technique): interpolate new synthetic points between existing minority samples.</li>
<li>ADASYN: adaptive oversampling focusing on hard-to-learn regions.</li>
<li>Pros: enriches minority representation.</li>
<li>Cons: risk of overfitting (duplication) or noise (bad synthetic points).</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 15%">
<col style="width: 25%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Type</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Random undersampling</td>
<td>Undersampling</td>
<td>Simple, fast</td>
<td>May drop important data</td>
</tr>
<tr class="even">
<td>Tomek links / ENN</td>
<td>Undersampling</td>
<td>Cleaner boundaries</td>
<td>Computationally heavier</td>
</tr>
<tr class="odd">
<td>Random oversampling</td>
<td>Oversampling</td>
<td>Easy to apply</td>
<td>Overfitting risk</td>
</tr>
<tr class="even">
<td>SMOTE</td>
<td>Oversampling</td>
<td>Synthetic diversity</td>
<td>May create unrealistic points</td>
</tr>
<tr class="odd">
<td>ADASYN</td>
<td>Oversampling</td>
<td>Focuses on hard cases</td>
<td>Sensitive to noise</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, with imbalanced-learn)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.over_sampling <span class="im">import</span> SMOTE</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.under_sampling <span class="im">import</span> RandomUnderSampler</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">1000</span>, n_features<span class="op">=</span><span class="dv">10</span>, weights<span class="op">=</span>[<span class="fl">0.9</span>, <span class="fl">0.1</span>], random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Oversampling</span></span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>X_over, y_over <span class="op">=</span> SMOTE().fit_resample(X, y)</span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Undersampling</span></span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a>X_under, y_under <span class="op">=</span> RandomUnderSampler().fit_resample(X, y)</span>
<span id="cb83-12"><a href="#cb83-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-13"><a href="#cb83-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Original:"</span>, <span class="bu">sorted</span>({i:<span class="bu">sum</span>(y<span class="op">==</span>i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">set</span>(y)}.items()))</span>
<span id="cb83-14"><a href="#cb83-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Oversampled:"</span>, <span class="bu">sorted</span>({i:<span class="bu">sum</span>(y_over<span class="op">==</span>i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">set</span>(y_over)}.items()))</span>
<span id="cb83-15"><a href="#cb83-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Undersampled:"</span>, <span class="bu">sorted</span>({i:<span class="bu">sum</span>(y_under<span class="op">==</span>i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">set</span>(y_under)}.items()))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-81" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-81">Why it Matters</h4>
<p>Sampling is often the first line of defense against imbalance. While simple, it drastically affects classifier performance and is widely used in fraud detection, healthcare, and NLP pipelines.</p>
</section>
<section id="try-it-yourself-81" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-81">Try It Yourself</h4>
<ol type="1">
<li>Compare logistic regression performance with undersampled vs.&nbsp;oversampled data.</li>
<li>Try SMOTE vs.&nbsp;random oversampling. Which yields better generalization?</li>
<li>Reflect: why might undersampling be preferable in big data scenarios, but oversampling better in small-data domains?</li>
</ol>
</section>
</section>
<section id="smote-and-synthetic-oversampling-variants" class="level3">
<h3 class="anchored" data-anchor-id="smote-and-synthetic-oversampling-variants">683. SMOTE and Synthetic Oversampling Variants</h3>
<p>SMOTE (Synthetic Minority Over-sampling Technique) creates synthetic samples for the minority class instead of duplicating existing ones. It interpolates between real minority instances, producing new, plausible samples that help balance datasets.</p>
<section id="picture-in-your-head-82" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-82">Picture in Your Head</h4>
<p>Think of connecting dots:</p>
<ul>
<li>If you only copy the same dot (random oversampling), the picture doesn’t change.</li>
<li>SMOTE draws new dots along the lines between minority samples, filling in the space and giving a richer picture of the minority class.</li>
</ul>
</section>
<section id="deep-dive-82" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-82">Deep Dive</h4>
<ul>
<li><p>SMOTE algorithm:</p>
<ol type="1">
<li><p>For each minority instance, find its <em>k</em> nearest minority neighbors.</p></li>
<li><p>Randomly pick one neighbor.</p></li>
<li><p>Generate synthetic point:</p>
<p><span class="math display">\[
x_{new} = x_i + \delta \cdot (x_{neighbor} - x_i), \quad \delta \in [0,1]
\]</span></p></li>
</ol></li>
<li><p>Variants:</p>
<ul>
<li>Borderline-SMOTE: oversample only near decision boundaries.</li>
<li>SMOTEENN / SMOTETomek: combine SMOTE with cleaning undersampling (ENN or Tomek links).</li>
<li>ADASYN: adaptive oversampling; generate more synthetic points in harder-to-learn regions.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 28%">
<col style="width: 31%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Key Idea</th>
<th>Advantage</th>
<th>Limitation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>SMOTE</td>
<td>Interpolation</td>
<td>Reduces overfitting from duplication</td>
<td>May create unrealistic points</td>
</tr>
<tr class="even">
<td>Borderline-SMOTE</td>
<td>Focus near decision boundary</td>
<td>Improves minority recall</td>
<td>Ignores easy regions</td>
</tr>
<tr class="odd">
<td>SMOTEENN</td>
<td>SMOTE + Edited Nearest Neighbors</td>
<td>Cleans noisy points</td>
<td>Computationally heavier</td>
</tr>
<tr class="even">
<td>ADASYN</td>
<td>Focus on difficult samples</td>
<td>Emphasizes challenging regions</td>
<td>Sensitive to noise</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, imbalanced-learn)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.over_sampling <span class="im">import</span> SMOTE, BorderlineSMOTE, ADASYN</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">1000</span>, n_features<span class="op">=</span><span class="dv">10</span>, weights<span class="op">=</span>[<span class="fl">0.9</span>, <span class="fl">0.1</span>], random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Standard SMOTE</span></span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>X_smote, y_smote <span class="op">=</span> SMOTE().fit_resample(X, y)</span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Borderline-SMOTE</span></span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a>X_border, y_border <span class="op">=</span> BorderlineSMOTE().fit_resample(X, y)</span>
<span id="cb84-11"><a href="#cb84-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-12"><a href="#cb84-12" aria-hidden="true" tabindex="-1"></a><span class="co"># ADASYN</span></span>
<span id="cb84-13"><a href="#cb84-13" aria-hidden="true" tabindex="-1"></a>X_ada, y_ada <span class="op">=</span> ADASYN().fit_resample(X, y)</span>
<span id="cb84-14"><a href="#cb84-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-15"><a href="#cb84-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Before:"</span>, {<span class="dv">0</span>: <span class="bu">sum</span>(y<span class="op">==</span><span class="dv">0</span>), <span class="dv">1</span>: <span class="bu">sum</span>(y<span class="op">==</span><span class="dv">1</span>)})</span>
<span id="cb84-16"><a href="#cb84-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"After SMOTE:"</span>, {<span class="dv">0</span>: <span class="bu">sum</span>(y_smote<span class="op">==</span><span class="dv">0</span>), <span class="dv">1</span>: <span class="bu">sum</span>(y_smote<span class="op">==</span><span class="dv">1</span>)})</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-82" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-82">Why it Matters</h4>
<p>SMOTE and its variants are among the most widely used techniques for imbalanced learning, especially in domains like fraud detection, medical diagnosis, and cybersecurity. They create more realistic minority representation compared to simple duplication.</p>
</section>
<section id="try-it-yourself-82" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-82">Try It Yourself</h4>
<ol type="1">
<li>Train classifiers on datasets balanced with random oversampling vs.&nbsp;SMOTE. Which generalizes better?</li>
<li>Compare SMOTE vs.&nbsp;ADASYN on noisy data. Does ADASYN overfit?</li>
<li>Reflect: why might SMOTE-generated samples sometimes “invade” majority space and harm performance?</li>
</ol>
</section>
</section>
<section id="cost-sensitive-loss-functions" class="level3">
<h3 class="anchored" data-anchor-id="cost-sensitive-loss-functions">684. Cost-Sensitive Loss Functions</h3>
<p>Instead of reshaping the dataset, cost-sensitive learning changes the loss function so that misclassifying minority samples incurs a higher penalty. The model learns to take the imbalance into account directly during training.</p>
<section id="picture-in-your-head-83" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-83">Picture in Your Head</h4>
<p>Think of a security checkpoint:</p>
<ul>
<li>Missing a dangerous item (false negative) is far worse than flagging a safe item (false positive).</li>
<li>Cost-sensitive learning weights mistakes differently, just like stricter penalties for high-risk errors.</li>
</ul>
</section>
<section id="deep-dive-83" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-83">Deep Dive</h4>
<ul>
<li><p>Weighted loss</p>
<ul>
<li><p>Assign class weights inversely proportional to class frequency.</p></li>
<li><p>Example for binary classification:</p>
<p><span class="math display">\[
L = - \sum w_y \, y \log \hat{y}
\]</span></p>
<p>where <span class="math inline">\(w_y = \frac{N}{2 \cdot N_y}\)</span>.</p></li>
</ul></li>
<li><p>Algorithms supporting cost-sensitive learning</p>
<ul>
<li>Logistic regression, SVMs, decision trees (class_weight).</li>
<li>Gradient boosting frameworks (XGBoost <code>scale_pos_weight</code>, LightGBM <code>is_unbalance</code>).</li>
<li>Neural nets: custom weighted cross-entropy, focal loss.</li>
</ul></li>
<li><p>Focal loss (for extreme imbalance)</p>
<ul>
<li><p>Modifies cross-entropy:</p>
<p><span class="math display">\[
FL(p_t) = -(1 - p_t)^\gamma \log(p_t)
\]</span></p></li>
<li><p>Downweights easy examples, focuses on hard-to-classify minority cases.</p></li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 30%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Approach</th>
<th>How It Works</th>
<th>When Useful</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Weighted CE</td>
<td>Higher weight for minority</td>
<td>Mild imbalance</td>
</tr>
<tr class="even">
<td>Focal loss</td>
<td>Focus on hard cases</td>
<td>Extreme imbalance (e.g., object detection)</td>
</tr>
<tr class="odd">
<td>Algorithm params</td>
<td>Built-in cost settings</td>
<td>Convenient, fast</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, logistic regression with class weights)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">1000</span>, n_features<span class="op">=</span><span class="dv">20</span>, weights<span class="op">=</span>[<span class="fl">0.9</span>, <span class="fl">0.1</span>], random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Cost-sensitive logistic regression</span></span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(class_weight<span class="op">=</span><span class="st">"balanced"</span>, max_iter<span class="op">=</span><span class="dv">500</span>).fit(X, y)</span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training accuracy:"</span>, model.score(X, y))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-83" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-83">Why it Matters</h4>
<p>Cost-sensitive learning directly encodes real-world priorities: in fraud detection, cybersecurity, or healthcare, missing a rare positive is much costlier than flagging a false alarm.</p>
</section>
<section id="try-it-yourself-83" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-83">Try It Yourself</h4>
<ol type="1">
<li>Train the same model with and without class weights. Compare recall for the minority class.</li>
<li>Implement focal loss in a neural net. Does it improve detection of rare cases?</li>
<li>Reflect: why might cost-sensitive learning be preferable to oversampling in very large datasets?</li>
</ol>
</section>
</section>
<section id="threshold-adjustment-and-roc-curves" class="level3">
<h3 class="anchored" data-anchor-id="threshold-adjustment-and-roc-curves">685. Threshold Adjustment and ROC Curves</h3>
<p>Most classifiers output probabilities, then apply a threshold (often 0.5) to decide the class. In imbalanced data, this default threshold is rarely optimal. Adjusting thresholds allows better control over precision–recall tradeoffs.</p>
<section id="picture-in-your-head-84" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-84">Picture in Your Head</h4>
<p>Think of a smoke alarm:</p>
<ul>
<li>A low threshold makes it very sensitive (many false alarms).</li>
<li>A high threshold reduces false alarms but risks missing real fires. Choosing the right threshold balances safety and nuisance.</li>
</ul>
</section>
<section id="deep-dive-84" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-84">Deep Dive</h4>
<ul>
<li><p>Default issue: In imbalanced settings, a 0.5 threshold biases toward the majority class.</p></li>
<li><p>Threshold tuning:</p>
<ul>
<li>Adjust threshold to maximize F1, precision, recall, or cost-sensitive metric.</li>
<li>ROC (Receiver Operating Characteristic) curve: plots TPR vs.&nbsp;FPR at all thresholds.</li>
<li>Precision–Recall (PR) curve: more informative under high imbalance.</li>
</ul></li>
<li><p>Optimal threshold:</p>
<ul>
<li>From ROC curve → Youden’s J statistic: <span class="math inline">\(J = TPR - FPR\)</span>.</li>
<li>From PR curve → maximize F1 or another application-specific score.</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Metric</th>
<th>Threshold Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Precision ↑</td>
<td>Higher threshold</td>
</tr>
<tr class="even">
<td>Recall ↑</td>
<td>Lower threshold</td>
</tr>
<tr class="odd">
<td>F1 ↑</td>
<td>Balance between precision and recall</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, threshold tuning)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_recall_curve, f1_score</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">1000</span>, n_features<span class="op">=</span><span class="dv">20</span>, weights<span class="op">=</span>[<span class="fl">0.9</span>,<span class="fl">0.1</span>], random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression().fit(X, y)</span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> model.predict_proba(X)[:,<span class="dv">1</span>]</span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a>prec, rec, thresholds <span class="op">=</span> precision_recall_curve(y, probs)</span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a>f1_scores <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>prec<span class="op">*</span>rec<span class="op">/</span>(prec<span class="op">+</span>rec<span class="op">+</span><span class="fl">1e-8</span>)</span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a>best_thresh <span class="op">=</span> thresholds[np.argmax(f1_scores)]</span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best threshold:"</span>, best_thresh)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-84" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-84">Why it Matters</h4>
<p>Threshold adjustment is simple yet powerful: without resampling or retraining, it aligns the model to application needs (e.g., high recall in medical screening, high precision in fraud alerts).</p>
</section>
<section id="try-it-yourself-84" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-84">Try It Yourself</h4>
<ol type="1">
<li>Train a classifier on imbalanced data. Compare results at 0.5 vs.&nbsp;tuned threshold.</li>
<li>Plot ROC and PR curves. Which curve is more useful under imbalance?</li>
<li>Reflect: in a medical test, why might recall be prioritized over precision when setting thresholds?</li>
</ol>
</section>
</section>
<section id="evaluation-metrics-for-imbalanced-data-f1-auc-pr" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-metrics-for-imbalanced-data-f1-auc-pr">686. Evaluation Metrics for Imbalanced Data (F1, AUC, PR)</h3>
<p>Accuracy is misleading on imbalanced datasets. Alternative metrics—F1-score, ROC-AUC, and Precision–Recall AUC—better capture model performance by focusing on minority detection and tradeoffs between false positives and false negatives.</p>
<section id="picture-in-your-head-85" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-85">Picture in Your Head</h4>
<p>Imagine grading a doctor:</p>
<ul>
<li>If they declare everyone “healthy,” they’re 95% accurate in a dataset where 95% are healthy.</li>
<li>But this doctor misses all sick patients. We need metrics that reveal this failure, not hide it under “accuracy.”</li>
</ul>
</section>
<section id="deep-dive-85" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-85">Deep Dive</h4>
<ul>
<li><p>Confusion matrix basis:</p>
<ul>
<li>TP: correctly predicted minority.</li>
<li>FP: false alarms.</li>
<li>FN: missed positives.</li>
<li>TN: correctly predicted majority.</li>
</ul></li>
<li><p>F1-score</p>
<ul>
<li>Harmonic mean of precision and recall.</li>
</ul>
<p><span class="math display">\[
F1 = \frac{2 \cdot Precision \cdot Recall}{Precision + Recall}
\]</span></p>
<ul>
<li>Useful when both false positives and false negatives matter.</li>
</ul></li>
<li><p>ROC-AUC</p>
<ul>
<li>Plots TPR vs.&nbsp;FPR at all thresholds.</li>
<li>AUC = probability that model ranks a random positive higher than a random negative.</li>
<li>May be over-optimistic in extreme imbalance.</li>
</ul></li>
<li><p>PR-AUC</p>
<ul>
<li>Plots precision vs.&nbsp;recall.</li>
<li>Focuses directly on minority class performance.</li>
<li>More informative under heavy imbalance.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 8%">
<col style="width: 31%">
<col style="width: 32%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>Metric</th>
<th>Focus</th>
<th>Strength</th>
<th>Limitation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>F1</td>
<td>Balance of precision/recall</td>
<td>Good for balanced importance</td>
<td>Not threshold-free</td>
</tr>
<tr class="even">
<td>ROC-AUC</td>
<td>Ranking ability</td>
<td>Threshold-independent</td>
<td>Inflated under imbalance</td>
</tr>
<tr class="odd">
<td>PR-AUC</td>
<td>Minority performance</td>
<td>Robust under imbalance</td>
<td>Less intuitive</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-43" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-43">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> f1_score, roc_auc_score, average_precision_score</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">1000</span>, n_features<span class="op">=</span><span class="dv">20</span>, weights<span class="op">=</span>[<span class="fl">0.9</span>,<span class="fl">0.1</span>], random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression().fit(X, y)</span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> model.predict_proba(X)[:,<span class="dv">1</span>]</span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> model.predict(X)</span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"F1:"</span>, f1_score(y, preds))</span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"ROC-AUC:"</span>, roc_auc_score(y, probs))</span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"PR-AUC:"</span>, average_precision_score(y, probs))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-85" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-85">Why it Matters</h4>
<p>Choosing the right evaluation metric prevents misleading results and ensures models truly detect rare but critical cases (fraud, disease, security threats).</p>
</section>
<section id="try-it-yourself-85" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-85">Try It Yourself</h4>
<ol type="1">
<li>Compare ROC-AUC and PR-AUC on highly imbalanced data. Which metric reveals minority performance better?</li>
<li>Optimize a model for F1 vs.&nbsp;PR-AUC. How do predictions differ?</li>
<li>Reflect: why might ROC-AUC look good while PR-AUC reveals failure in extreme imbalance cases?</li>
</ol>
</section>
</section>
<section id="one-class-and-rare-event-detection" class="level3">
<h3 class="anchored" data-anchor-id="one-class-and-rare-event-detection">687. One-Class and Rare Event Detection</h3>
<p>When the minority class is extremely rare (e.g., &lt;1%), supervised learning struggles because there aren’t enough positive examples. One-class classification and rare event detection methods model the majority (normal) class and flag deviations as anomalies.</p>
<section id="picture-in-your-head-86" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-86">Picture in Your Head</h4>
<p>Think of airport security:</p>
<ul>
<li>Most passengers are harmless (majority class).</li>
<li>Instead of training on rare terrorists (minority class), security learns what “normal” looks like and flags anything unusual.</li>
</ul>
</section>
<section id="deep-dive-86" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-86">Deep Dive</h4>
<ul>
<li><p>One-Class SVM</p>
<ul>
<li>Learns a boundary around the majority class in feature space.</li>
<li>Points far from the boundary are flagged as anomalies.</li>
</ul></li>
<li><p>Isolation Forest</p>
<ul>
<li>Randomly splits features to isolate points.</li>
<li>Anomalies require fewer splits → higher anomaly score.</li>
</ul></li>
<li><p>Autoencoders (Anomaly Detection)</p>
<ul>
<li>Train to reconstruct normal data.</li>
<li>Anomalous inputs reconstruct poorly → high reconstruction error.</li>
</ul></li>
<li><p>Statistical models</p>
<ul>
<li>Gaussian mixture models, density estimation for majority class.</li>
<li>Outliers detected via low likelihood.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 25%">
<col style="width: 21%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Idea</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>One-Class SVM</td>
<td>Boundary around normal</td>
<td>Solid theory</td>
<td>Poor scaling</td>
</tr>
<tr class="even">
<td>Isolation Forest</td>
<td>Isolation via random splits</td>
<td>Fast, scalable</td>
<td>Less precise on complex anomalies</td>
</tr>
<tr class="odd">
<td>Autoencoder</td>
<td>Reconstruct normal</td>
<td>Captures nonlinearities</td>
<td>Needs large normal dataset</td>
</tr>
<tr class="even">
<td>GMM</td>
<td>Density estimation</td>
<td>Probabilistic</td>
<td>Sensitive to distributional assumptions</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, Isolation Forest)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> IsolationForest</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>X, _ <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">1000</span>, n_features<span class="op">=</span><span class="dv">20</span>, weights<span class="op">=</span>[<span class="fl">0.98</span>,<span class="fl">0.02</span>], random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>iso <span class="op">=</span> IsolationForest(contamination<span class="op">=</span><span class="fl">0.02</span>).fit(X)</span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> iso.decision_function(X)</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>anomalies <span class="op">=</span> iso.predict(X)  <span class="co"># -1 = anomaly, 1 = normal</span></span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Anomalies detected:"</span>, <span class="bu">sum</span>(anomalies <span class="op">==</span> <span class="op">-</span><span class="dv">1</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-86" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-86">Why it Matters</h4>
<p>In fraud detection, medical screening, or cybersecurity, the minority class can be so rare that direct supervised learning is infeasible. One-class methods provide practical solutions by focusing on normal vs.&nbsp;abnormal rather than majority vs.&nbsp;minority.</p>
</section>
<section id="try-it-yourself-86" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-86">Try It Yourself</h4>
<ol type="1">
<li>Train an Isolation Forest on imbalanced data. How many anomalies are flagged?</li>
<li>Compare One-Class SVM vs.&nbsp;Autoencoder anomaly detection on the same dataset.</li>
<li>Reflect: why might one-class models be better than SMOTE-style oversampling in ultra-rare cases?</li>
</ol>
</section>
</section>
<section id="ensemble-methods-for-imbalanced-learning" class="level3">
<h3 class="anchored" data-anchor-id="ensemble-methods-for-imbalanced-learning">688. Ensemble Methods for Imbalanced Learning</h3>
<p>Ensemble methods combine multiple models to better handle imbalanced data. By integrating resampling strategies, cost-sensitive learning, or anomaly detectors into ensembles, they improve minority detection while maintaining robustness.</p>
<section id="picture-in-your-head-87" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-87">Picture in Your Head</h4>
<p>Think of a jury:</p>
<ul>
<li>If most jurors are biased toward acquittal (majority class), the verdict may be unfair.</li>
<li>But if some jurors specialize in spotting suspicious behavior (minority-focused models), the combined decision is more balanced.</li>
</ul>
</section>
<section id="deep-dive-87" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-87">Deep Dive</h4>
<ul>
<li><p>Balanced Random Forest (BRF)</p>
<ul>
<li>Each tree is trained on a balanced bootstrap sample (undersampled majority + minority).</li>
<li>Improves minority recall while keeping variance low.</li>
</ul></li>
<li><p>EasyEnsemble</p>
<ul>
<li>Train multiple classifiers on different balanced subsets (via undersampling).</li>
<li>Combine predictions by averaging or majority vote.</li>
<li>Effective for extreme imbalance.</li>
</ul></li>
<li><p>RUSBoost (Random Undersampling + Boosting)</p>
<ul>
<li>Uses undersampling at each boosting iteration.</li>
<li>Reduces bias toward majority without overfitting.</li>
</ul></li>
<li><p>SMOTEBoost / ADASYNBoost</p>
<ul>
<li>Combine boosting with synthetic oversampling.</li>
<li>Focuses on hard minority examples with better diversity.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 30%">
<col style="width: 23%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Core Idea</th>
<th>Strength</th>
<th>Limitation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Balanced RF</td>
<td>Balanced bootstraps</td>
<td>Easy, interpretable</td>
<td>Risk of dropping useful majority data</td>
</tr>
<tr class="even">
<td>EasyEnsemble</td>
<td>Multiple undersampled ensembles</td>
<td>Handles extreme imbalance</td>
<td>Computationally heavy</td>
</tr>
<tr class="odd">
<td>RUSBoost</td>
<td>Undersampling + boosting</td>
<td>Improves recall</td>
<td>May lose info</td>
</tr>
<tr class="even">
<td>SMOTEBoost</td>
<td>Boosting + synthetic oversampling</td>
<td>Richer minority space</td>
<td>Sensitive to noise</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, EasyEnsembleClassifier)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.ensemble <span class="im">import</span> EasyEnsembleClassifier</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">2000</span>, n_features<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>                           weights<span class="op">=</span>[<span class="fl">0.95</span>, <span class="fl">0.05</span>], random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> EasyEnsembleClassifier(n_estimators<span class="op">=</span><span class="dv">10</span>).fit(X, y)</span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Balanced accuracy:"</span>, clf.score(X, y))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-87" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-87">Why it Matters</h4>
<p>Ensemble methods provide a powerful toolkit for handling imbalance. They integrate sampling and cost-awareness into robust models, making them state-of-the-art for fraud detection, medical prediction, and rare-event modeling.</p>
</section>
<section id="try-it-yourself-87" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-87">Try It Yourself</h4>
<ol type="1">
<li>Train Balanced Random Forest vs.&nbsp;standard Random Forest. Compare minority recall.</li>
<li>Experiment with EasyEnsemble. How does combining multiple subsets affect performance?</li>
<li>Reflect: why do ensemble methods often outperform standalone resampling approaches?</li>
</ol>
</section>
</section>
<section id="real-world-case-studies-fraud-medical-fault-detection" class="level3">
<h3 class="anchored" data-anchor-id="real-world-case-studies-fraud-medical-fault-detection">689. Real-World Case Studies (Fraud, Medical, Fault Detection)</h3>
<p>Imbalanced learning isn’t theoretical—it powers critical applications where rare events matter most. Case studies in fraud detection, healthcare, and industrial fault detection highlight how resampling, cost-sensitive learning, and ensembles are deployed in practice.</p>
<section id="picture-in-your-head-88" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-88">Picture in Your Head</h4>
<p>Think of three detectives:</p>
<ul>
<li>One hunts financial fraudsters hiding among millions of normal transactions.</li>
<li>Another diagnoses rare diseases among mostly healthy patients.</li>
<li>A third monitors machines, catching tiny glitches before catastrophic breakdowns. Each faces imbalance, but with domain-specific twists.</li>
</ul>
</section>
<section id="deep-dive-88" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-88">Deep Dive</h4>
<ul>
<li><p>Fraud Detection (Finance)</p>
<ul>
<li><p>Imbalance: &lt;1% fraudulent transactions.</p></li>
<li><p>Typical approaches:</p>
<ul>
<li>SMOTE + Random Forests.</li>
<li>Cost-sensitive boosting (XGBoost with <code>scale_pos_weight</code>).</li>
<li>Real-time anomaly detection for unusual spending patterns.</li>
</ul></li>
<li><p>Challenge: evolving fraud tactics → concept drift.</p></li>
</ul></li>
<li><p>Medical Diagnosis</p>
<ul>
<li><p>Imbalance: rare diseases, often &lt;5% prevalence.</p></li>
<li><p>Methods:</p>
<ul>
<li>Class-weighted logistic regression or neural nets.</li>
<li>One-class models when positive data is very limited.</li>
<li>Evaluation with PR-AUC to avoid inflated accuracy.</li>
</ul></li>
<li><p>Challenge: ethical stakes → prioritize recall (don’t miss positives).</p></li>
</ul></li>
<li><p>Fault Detection (Industry/IoT)</p>
<ul>
<li><p>Imbalance: faults occur in &lt;0.1% of machine logs.</p></li>
<li><p>Methods:</p>
<ul>
<li>Isolation Forests, Autoencoders for anomaly detection.</li>
<li>Ensemble of undersampled learners (EasyEnsemble).</li>
<li>Streaming learning to handle massive sensor data.</li>
</ul></li>
<li><p>Challenge: balancing false alarms vs.&nbsp;missed failures.</p></li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 16%">
<col style="width: 42%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Domain</th>
<th>Imbalance Level</th>
<th>Common Methods</th>
<th>Key Challenge</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Fraud detection</td>
<td>&lt;1% fraud</td>
<td>SMOTE, ensembles, cost-sensitive boosting</td>
<td>Fraudsters adapt fast</td>
</tr>
<tr class="even">
<td>Medical</td>
<td>&lt;5% rare disease</td>
<td>Weighted models, one-class, PR-AUC</td>
<td>Missing cases = high cost</td>
</tr>
<tr class="odd">
<td>Fault detection</td>
<td>&lt;0.1% faults</td>
<td>Isolation Forest, autoencoders</td>
<td>False alarms vs.&nbsp;safety</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, XGBoost for fraud-like imbalance)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> xgboost <span class="im">import</span> XGBClassifier</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">10000</span>, n_features<span class="op">=</span><span class="dv">20</span>, weights<span class="op">=</span>[<span class="fl">0.99</span>, <span class="fl">0.01</span>], random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> XGBClassifier(scale_pos_weight<span class="op">=</span><span class="dv">99</span>).fit(X, y)</span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training done. Minority recall focus applied."</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-88" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-88">Why it Matters</h4>
<p>Imbalanced learning isn’t just academic—it decides whether fraud is caught, diseases are diagnosed, and machines keep running safely. The cost of ignoring imbalance is measured in money, lives, and safety.</p>
</section>
<section id="try-it-yourself-88" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-88">Try It Yourself</h4>
<ol type="1">
<li>Simulate fraud-like data (1% positives) and train a Random Forest with and without class weights. Compare recall.</li>
<li>Use autoencoders for fault detection on synthetic sensor data. Which errors stand out?</li>
<li>Reflect: in which domain would false positives be more acceptable than false negatives, and why?</li>
</ol>
</section>
</section>
<section id="challenges-and-open-questions" class="level3">
<h3 class="anchored" data-anchor-id="challenges-and-open-questions">690. Challenges and Open Questions</h3>
<p>Despite decades of research, imbalanced learning still faces unresolved challenges. Rare-event modeling pushes the limits of data, algorithms, and evaluation. Open questions remain in scalability, robustness, and fairness.</p>
<section id="picture-in-your-head-89" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-89">Picture in Your Head</h4>
<p>Imagine shining a flashlight in a dark cave:</p>
<ul>
<li>You illuminate some rare gems (detected positives),</li>
<li>But shadows still hide others (missed anomalies). The challenge is to keep extending the light without being blinded by reflections (false positives).</li>
</ul>
</section>
<section id="deep-dive-89" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-89">Deep Dive</h4>
<ul>
<li><p>Key Challenges</p>
<ul>
<li>Extreme imbalance: when positives &lt;0.1%, oversampling and cost-sensitive methods may still fail.</li>
<li>Concept drift: in fraud or security, minority patterns change over time. Models must adapt.</li>
<li>Noisy labels: minority samples often mislabeled, further reducing effective data.</li>
<li>Evaluation metrics: PR-AUC works, but calibration and interpretability remain difficult.</li>
<li>Scalability: balancing methods must scale to billions of samples (e.g., credit card transactions).</li>
<li>Fairness: imbalance interacts with bias—rare groups may be further underrepresented.</li>
</ul></li>
<li><p>Open Questions</p>
<ul>
<li>How to generate realistic synthetic samples beyond SMOTE/ADASYN?</li>
<li>Can self-supervised learning pretraining help rare-event detection?</li>
<li>How to combine streaming learning with imbalance handling for real-time use?</li>
<li>Can we design metrics that better reflect real-world costs (beyond precision/recall)?</li>
<li>How to build models that stay robust under distribution shifts in minority data?</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 41%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th>Area</th>
<th>Current Limit</th>
<th>Research Direction</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sampling</td>
<td>Unrealistic synthetic points</td>
<td>Generative models (GANs, diffusion)</td>
</tr>
<tr class="even">
<td>Drift</td>
<td>Static models</td>
<td>Online &amp; adaptive learning</td>
</tr>
<tr class="odd">
<td>Metrics</td>
<td>PR-AUC not always intuitive</td>
<td>Cost-sensitive + human-aligned metrics</td>
</tr>
<tr class="even">
<td>Fairness</td>
<td>Minority within minority ignored</td>
<td>Fairness-aware imbalance methods</td>
</tr>
</tbody>
</table>
<p>Tiny Code Thought Experiment</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pseudocode for combining imbalance + drift handling</span></span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> stream_data:</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>    X_batch, y_batch <span class="op">=</span> get_new_data()</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a>    model.partial_fit(X_batch, y_batch, class_weight<span class="op">=</span><span class="st">"balanced"</span>)</span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a>    detect_drift()</span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> drift:</span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a>        resample_or_retrain()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-89" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-89">Why it Matters</h4>
<p>Imbalanced learning sits at the heart of mission-critical AI. Solving these challenges means safer healthcare, stronger fraud detection, and more reliable industrial systems.</p>
</section>
<section id="try-it-yourself-89" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-89">Try It Yourself</h4>
<ol type="1">
<li>Simulate a data stream with shifting minority distribution. Can your model adapt?</li>
<li>Explore GANs for minority oversampling. Do they produce realistic synthetic samples?</li>
<li>Reflect: in your application, is the bigger risk missing rare positives, or flooding with false alarms?</li>
</ol>
</section>
</section>
</section>
<section id="chapter-70.-evaluation-error-analysis-and-debugging" class="level2">
<h2 class="anchored" data-anchor-id="chapter-70.-evaluation-error-analysis-and-debugging">Chapter 70. Evaluation, error analysis, and debugging</h2>
<section id="beyond-accuracy-precision-recall-f1-auc" class="level3">
<h3 class="anchored" data-anchor-id="beyond-accuracy-precision-recall-f1-auc">691. Beyond Accuracy: Precision, Recall, F1, AUC</h3>
<p>Accuracy alone is misleading in imbalanced datasets. Alternative metrics like precision, recall, F1-score, ROC-AUC, and PR-AUC give a more complete picture of model performance, especially for rare events.</p>
<section id="picture-in-your-head-90" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-90">Picture in Your Head</h4>
<p>Imagine evaluating a lifeguard:</p>
<ul>
<li>If the pool is empty, they’ll be “100% accurate” by never saving anyone.</li>
<li>But their real job is to detect and act on the rare drowning events. That’s why metrics beyond accuracy are essential.</li>
</ul>
</section>
<section id="deep-dive-90" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-90">Deep Dive</h4>
<ul>
<li><p>Precision: Of predicted positives, how many are correct?</p>
<p><span class="math display">\[
Precision = \frac{TP}{TP + FP}
\]</span></p></li>
<li><p>Recall (Sensitivity, TPR): Of actual positives, how many were found?</p>
<p><span class="math display">\[
Recall = \frac{TP}{TP + FN}
\]</span></p></li>
<li><p>F1-score: Harmonic mean of precision and recall.</p>
<ul>
<li>Balances false positives and false negatives.</li>
</ul>
<p><span class="math display">\[
F1 = \frac{2 \cdot Precision \cdot Recall}{Precision + Recall}
\]</span></p></li>
<li><p>ROC-AUC: Probability model ranks a random positive higher than a random negative.</p>
<ul>
<li>Threshold-independent but can look good under extreme imbalance.</li>
</ul></li>
<li><p>PR-AUC: Area under Precision–Recall curve.</p>
<ul>
<li>Better reflects minority detection performance.</li>
</ul></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Metric</th>
<th>Focus</th>
<th>Best When</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Precision</td>
<td>Correctness of positives</td>
<td>Cost of false alarms is high</td>
</tr>
<tr class="even">
<td>Recall</td>
<td>Coverage of positives</td>
<td>Cost of misses is high</td>
</tr>
<tr class="odd">
<td>F1</td>
<td>Balance</td>
<td>Both errors matter</td>
</tr>
<tr class="even">
<td>ROC-AUC</td>
<td>Ranking ability</td>
<td>Moderate imbalance</td>
</tr>
<tr class="odd">
<td>PR-AUC</td>
<td>Rare class performance</td>
<td>Extreme imbalance</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-44" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-44">Tiny Code</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_score, recall_score, f1_score, roc_auc_score, average_precision_score</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">2000</span>, n_features<span class="op">=</span><span class="dv">20</span>, weights<span class="op">=</span>[<span class="fl">0.95</span>,<span class="fl">0.05</span>], random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression().fit(X, y)</span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> model.predict_proba(X)[:,<span class="dv">1</span>]</span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> model.predict(X)</span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Precision:"</span>, precision_score(y, preds))</span>
<span id="cb92-11"><a href="#cb92-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Recall:"</span>, recall_score(y, preds))</span>
<span id="cb92-12"><a href="#cb92-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"F1:"</span>, f1_score(y, preds))</span>
<span id="cb92-13"><a href="#cb92-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"ROC-AUC:"</span>, roc_auc_score(y, probs))</span>
<span id="cb92-14"><a href="#cb92-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"PR-AUC:"</span>, average_precision_score(y, probs))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-90" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-90">Why it Matters</h4>
<p>Choosing the right evaluation metric avoids false confidence. In fraud, healthcare, or security, missing rare events (recall) or generating too many false alarms (precision) have very different costs.</p>
</section>
<section id="try-it-yourself-90" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-90">Try It Yourself</h4>
<ol type="1">
<li>Train a classifier on imbalanced data. Compare accuracy vs.&nbsp;F1. Which is more informative?</li>
<li>Plot ROC and PR curves. Which shows minority class performance more clearly?</li>
<li>Reflect: in your domain, would you prioritize precision, recall, or a balance (F1)?</li>
</ol>
</section>
</section>
<section id="calibration-of-probabilistic-predictions" class="level3">
<h3 class="anchored" data-anchor-id="calibration-of-probabilistic-predictions">692. Calibration of Probabilistic Predictions</h3>
<p>A model’s predicted probabilities should match real-world frequencies—this property is called calibration. In imbalanced settings, models often produce poorly calibrated probabilities, leading to misleading confidence scores.</p>
<section id="picture-in-your-head-91" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-91">Picture in Your Head</h4>
<p>Imagine a weather app:</p>
<ul>
<li>If it says “30% chance of rain,” then it should rain on about 3 out of 10 such days.</li>
<li>If instead it rains almost every time, the forecast isn’t calibrated. Models work the same way: their probability outputs should reflect reality.</li>
</ul>
</section>
<section id="deep-dive-91" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-91">Deep Dive</h4>
<ul>
<li><p>Why calibration matters</p>
<ul>
<li>Imbalanced data skews predicted probabilities toward the majority class.</li>
<li>Poor calibration → bad decisions in cost-sensitive domains (medicine, finance).</li>
</ul></li>
<li><p>Calibration methods</p>
<ul>
<li>Platt Scaling: fit a logistic regression on the model’s outputs.</li>
<li>Isotonic Regression: non-parametric, flexible mapping from scores to probabilities.</li>
<li>Temperature Scaling: commonly used in deep learning; rescales logits.</li>
</ul></li>
<li><p>Calibration curves (Reliability diagrams)</p>
<ul>
<li>Plot predicted probability vs.&nbsp;observed frequency.</li>
<li>Perfect calibration = diagonal line.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 33%">
<col style="width: 41%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Strength</th>
<th>Weakness</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Platt scaling</td>
<td>Simple, effective for SVMs</td>
<td>May underfit complex cases</td>
</tr>
<tr class="even">
<td>Isotonic regression</td>
<td>Flexible, non-parametric</td>
<td>Needs more data</td>
</tr>
<tr class="odd">
<td>Temperature scaling</td>
<td>Easy for neural nets</td>
<td>Only rescales, doesn’t fix shape</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, calibration curve)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.calibration <span class="im">import</span> calibration_curve</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">2000</span>, n_features<span class="op">=</span><span class="dv">20</span>, weights<span class="op">=</span>[<span class="fl">0.9</span>,<span class="fl">0.1</span>], random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression().fit(X, y)</span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> model.predict_proba(X)[:,<span class="dv">1</span>]</span>
<span id="cb93-9"><a href="#cb93-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-10"><a href="#cb93-10" aria-hidden="true" tabindex="-1"></a>frac_pos, mean_pred <span class="op">=</span> calibration_curve(y, probs, n_bins<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb93-11"><a href="#cb93-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-12"><a href="#cb93-12" aria-hidden="true" tabindex="-1"></a>plt.plot(mean_pred, frac_pos, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb93-13"><a href="#cb93-13" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">0</span>,<span class="dv">1</span>],[<span class="dv">0</span>,<span class="dv">1</span>], linestyle<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb93-14"><a href="#cb93-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Predicted probability"</span>)</span>
<span id="cb93-15"><a href="#cb93-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Observed frequency"</span>)</span>
<span id="cb93-16"><a href="#cb93-16" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Calibration Curve"</span>)</span>
<span id="cb93-17"><a href="#cb93-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-91" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-91">Why it Matters</h4>
<p>Well-calibrated probabilities allow better decision-making under uncertainty. In fraud detection, knowing a transaction has a 5% vs.&nbsp;50% fraud probability determines whether it’s flagged, investigated, or ignored.</p>
</section>
<section id="try-it-yourself-91" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-91">Try It Yourself</h4>
<ol type="1">
<li>Train a model and check its calibration curve. Is it over- or under-confident?</li>
<li>Apply isotonic regression. Does the calibration curve improve?</li>
<li>Reflect: why might calibration be more important than raw accuracy in high-stakes decisions?</li>
</ol>
</section>
</section>
<section id="error-analysis-techniques" class="level3">
<h3 class="anchored" data-anchor-id="error-analysis-techniques">693. Error Analysis Techniques</h3>
<p>Error analysis is the systematic study of where and why a model fails. For imbalanced data, errors often concentrate in the minority class, so targeted analysis helps refine preprocessing, sampling, and model design.</p>
<section id="picture-in-your-head-92" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-92">Picture in Your Head</h4>
<p>Think of a teacher grading exams:</p>
<ul>
<li>Not just counting the total score, but looking at which questions students missed.</li>
<li>Patterns in mistakes reveal whether the problem is poor teaching, tricky questions, or careless slips. Error analysis for models works the same way.</li>
</ul>
</section>
<section id="deep-dive-92" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-92">Deep Dive</h4>
<ul>
<li><p>Confusion matrix inspection</p>
<ul>
<li>Examine FP (false alarms) vs.&nbsp;FN (missed positives).</li>
<li>In imbalanced cases, FNs are often more critical.</li>
</ul></li>
<li><p>Per-class performance</p>
<ul>
<li>Precision, recall, and F1 by class.</li>
<li>Identify if minority class is consistently underperforming.</li>
</ul></li>
<li><p>Feature-level analysis</p>
<ul>
<li>Which features correlate with misclassified samples?</li>
<li>Use SHAP/LIME to explain minority misclassifications.</li>
</ul></li>
<li><p>Slice-based error analysis</p>
<ul>
<li>Evaluate performance across subgroups (age, region, transaction type).</li>
<li>Helps uncover hidden biases.</li>
</ul></li>
<li><p>Error clustering</p>
<ul>
<li>Group misclassified samples using clustering or embedding spaces.</li>
<li>Detect systematic error patterns.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 29%">
<col style="width: 32%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Technique</th>
<th>Focus</th>
<th>Insight</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Confusion matrix</td>
<td>FN vs FP</td>
<td>Which mistakes dominate</td>
</tr>
<tr class="even">
<td>Class metrics</td>
<td>Minority vs majority</td>
<td>Skewed performance</td>
</tr>
<tr class="odd">
<td>Feature attribution</td>
<td>Misclassified samples</td>
<td>Why errors happen</td>
</tr>
<tr class="even">
<td>Slicing</td>
<td>Subgroups</td>
<td>Fairness and bias issues</td>
</tr>
<tr class="odd">
<td>Clustering</td>
<td>Similar errors</td>
<td>Systematic failure modes</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, confusion matrix + per-class report)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, classification_report</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">2000</span>, n_features<span class="op">=</span><span class="dv">20</span>, weights<span class="op">=</span>[<span class="fl">0.9</span>,<span class="fl">0.1</span>], random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression().fit(X, y)</span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> model.predict(X)</span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Confusion Matrix:</span><span class="ch">\n</span><span class="st">"</span>, confusion_matrix(y, preds))</span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Classification Report:</span><span class="ch">\n</span><span class="st">"</span>, classification_report(y, preds))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-92" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-92">Why it Matters</h4>
<p>Error analysis transforms “black box failure” into actionable improvements. By knowing where errors cluster, practitioners can decide whether to adjust thresholds, rebalance classes, engineer features, or gather new data.</p>
</section>
<section id="try-it-yourself-92" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-92">Try It Yourself</h4>
<ol type="1">
<li>Plot a confusion matrix for your imbalanced dataset. Are FNs concentrated in the minority class?</li>
<li>Use SHAP to analyze features in misclassified minority cases. Do certain signals get ignored?</li>
<li>Reflect: why is error analysis more important in imbalanced settings than just looking at overall accuracy?</li>
</ol>
</section>
</section>
<section id="bias-variance-and-error-decomposition" class="level3">
<h3 class="anchored" data-anchor-id="bias-variance-and-error-decomposition">694. Bias, Variance, and Error Decomposition</h3>
<p>Every model’s error can be broken into three parts: bias (systematic error), variance (sensitivity to data fluctuations), and irreducible noise. Understanding this decomposition helps explain underfitting, overfitting, and challenges with imbalanced data.</p>
<section id="picture-in-your-head-93" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-93">Picture in Your Head</h4>
<p>Think of archery practice:</p>
<ul>
<li>High bias: arrows cluster far from the bullseye (systematic miss).</li>
<li>High variance: arrows scatter widely (inconsistent aim).</li>
<li>Noise: wind gusts occasionally push arrows off course no matter how good the archer is.</li>
</ul>
</section>
<section id="deep-dive-93" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-93">Deep Dive</h4>
<ul>
<li><p>Expected squared error decomposition:</p>
<p><span class="math display">\[
E[(y - \hat{f}(x))^2] = \text{Bias}^2 + \text{Variance} + \text{Noise}
\]</span></p></li>
<li><p>Bias</p>
<ul>
<li>Error from overly simple assumptions (e.g., linear model on nonlinear data).</li>
<li>Leads to underfitting.</li>
</ul></li>
<li><p>Variance</p>
<ul>
<li>Error from sensitivity to training data fluctuations (e.g., deep trees).</li>
<li>Leads to overfitting.</li>
</ul></li>
<li><p>Noise</p>
<ul>
<li>Randomness inherent in the data (e.g., measurement errors).</li>
<li>Unavoidable.</li>
</ul></li>
<li><p>Imbalanced data effect</p>
<ul>
<li>Minority class errors often hidden under majority bias.</li>
<li>High variance models may overfit duplicated minority points (oversampling).</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 25%">
<col style="width: 54%">
</colgroup>
<thead>
<tr class="header">
<th>Error Source</th>
<th>Symptom</th>
<th>Fix</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>High bias</td>
<td>Underfitting</td>
<td>More complex model, better features</td>
</tr>
<tr class="even">
<td>High variance</td>
<td>Overfitting</td>
<td>Regularization, ensembles</td>
</tr>
<tr class="odd">
<td>Noise</td>
<td>Persistent error</td>
<td>Better data collection</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, bias vs.&nbsp;variance with simple vs.&nbsp;complex model)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a><span class="co"># True function</span></span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.sin(X).ravel() <span class="op">+</span> np.random.normal(scale<span class="op">=</span><span class="fl">0.1</span>, size<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-11"><a href="#cb95-11" aria-hidden="true" tabindex="-1"></a><span class="co"># High bias model</span></span>
<span id="cb95-12"><a href="#cb95-12" aria-hidden="true" tabindex="-1"></a>lin <span class="op">=</span> LinearRegression().fit(X, y)</span>
<span id="cb95-13"><a href="#cb95-13" aria-hidden="true" tabindex="-1"></a>y_lin <span class="op">=</span> lin.predict(X)</span>
<span id="cb95-14"><a href="#cb95-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-15"><a href="#cb95-15" aria-hidden="true" tabindex="-1"></a><span class="co"># High variance model</span></span>
<span id="cb95-16"><a href="#cb95-16" aria-hidden="true" tabindex="-1"></a>tree <span class="op">=</span> DecisionTreeRegressor(max_depth<span class="op">=</span><span class="dv">15</span>).fit(X, y)</span>
<span id="cb95-17"><a href="#cb95-17" aria-hidden="true" tabindex="-1"></a>y_tree <span class="op">=</span> tree.predict(X)</span>
<span id="cb95-18"><a href="#cb95-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-19"><a href="#cb95-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Linear Reg MSE (bias):"</span>, mean_squared_error(y, y_lin))</span>
<span id="cb95-20"><a href="#cb95-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Tree MSE (variance):"</span>, mean_squared_error(y, y_tree))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-93" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-93">Why it Matters</h4>
<p>Bias–variance analysis provides a lens for diagnosing errors. In imbalanced settings, it clarifies whether failure comes from ignoring the minority (bias) or overfitting synthetic signals (variance).</p>
</section>
<section id="try-it-yourself-93" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-93">Try It Yourself</h4>
<ol type="1">
<li>Compare a linear model vs.&nbsp;a deep tree on noisy nonlinear data. Which suffers more from bias vs.&nbsp;variance?</li>
<li>Use bootstrapping to measure variance of your model across resampled datasets.</li>
<li>Reflect: why does oversampling minority data sometimes reduce bias but increase variance?</li>
</ol>
</section>
</section>
<section id="debugging-data-issues" class="level3">
<h3 class="anchored" data-anchor-id="debugging-data-issues">695. Debugging Data Issues</h3>
<p>Many machine learning failures come not from the algorithm, but from bad data. In imbalanced datasets, even small errors—missing labels, skewed sampling, or noise—can disproportionately harm minority detection. Debugging data issues is a critical first step before model tuning.</p>
<section id="picture-in-your-head-94" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-94">Picture in Your Head</h4>
<p>Imagine building a house:</p>
<ul>
<li>If the foundation is cracked (bad data), no matter how good the architecture (model), the house will collapse.</li>
</ul>
</section>
<section id="deep-dive-94" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-94">Deep Dive</h4>
<p>Common data issues in imbalanced learning:</p>
<ul>
<li><p>Label errors</p>
<ul>
<li>Minority class labels often noisy due to human error.</li>
<li>Even a handful of mislabeled positives can cripple recall.</li>
</ul></li>
<li><p>Sampling bias</p>
<ul>
<li>Training data distribution differs from deployment (e.g., fraud types change over time).</li>
<li>Leads to concept drift.</li>
</ul></li>
<li><p>Data leakage</p>
<ul>
<li>Features accidentally encode target (e.g., timestamp or ID variables).</li>
<li>Model looks great offline but fails in production.</li>
</ul></li>
<li><p>Feature imbalance</p>
<ul>
<li>Some features informative only for majority, none for minority.</li>
<li>Causes minority underrepresentation in splits.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 36%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th>Issue</th>
<th>Symptom</th>
<th>Fix</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Label noise</td>
<td>Poor recall despite resampling</td>
<td>Relabel minority samples, active learning</td>
</tr>
<tr class="even">
<td>Sampling bias</td>
<td>Good offline, poor online</td>
<td>Domain adaptation, re-weighting</td>
</tr>
<tr class="odd">
<td>Data leakage</td>
<td>Unusually high validation accuracy</td>
<td>Audit features, stricter validation</td>
</tr>
<tr class="even">
<td>Feature imbalance</td>
<td>Minority ignored</td>
<td>Feature engineering for rare cases</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, detecting label imbalance)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">1000</span>, n_features<span class="op">=</span><span class="dv">10</span>, weights<span class="op">=</span>[<span class="fl">0.95</span>,<span class="fl">0.05</span>], random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Label distribution:"</span>, Counter(y))</span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate label noise: flip some minority labels</span></span>
<span id="cb96-10"><a href="#cb96-10" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(<span class="dv">42</span>)</span>
<span id="cb96-11"><a href="#cb96-11" aria-hidden="true" tabindex="-1"></a>flip_idx <span class="op">=</span> rng.choice(np.where(y<span class="op">==</span><span class="dv">1</span>)[<span class="dv">0</span>], size<span class="op">=</span><span class="dv">5</span>, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb96-12"><a href="#cb96-12" aria-hidden="true" tabindex="-1"></a>y[flip_idx] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb96-13"><a href="#cb96-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"After noise:"</span>, Counter(y))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-94" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-94">Why it Matters</h4>
<p>Fixing data issues often improves performance more than tweaking algorithms. For imbalanced problems, a single mislabeled minority instance may matter more than hundreds of majority samples.</p>
</section>
<section id="try-it-yourself-94" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-94">Try It Yourself</h4>
<ol type="1">
<li>Audit your dataset for mislabeled minority samples. How much do they affect recall?</li>
<li>Check feature distributions separately for majority vs.&nbsp;minority. Are they aligned?</li>
<li>Reflect: why might cleaning just the minority class labels yield disproportionate gains?</li>
</ol>
</section>
</section>
<section id="debugging-model-issues" class="level3">
<h3 class="anchored" data-anchor-id="debugging-model-issues">696. Debugging Model Issues</h3>
<p>Even with clean data, models may fail due to poor design, inappropriate algorithms, or misconfigured training. Debugging model issues means identifying whether errors come from underfitting, overfitting, miscalibration, or imbalance mismanagement.</p>
<section id="picture-in-your-head-95" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-95">Picture in Your Head</h4>
<p>Imagine tuning a musical instrument:</p>
<ul>
<li>If strings are too loose (underfitting), the notes sound flat.</li>
<li>If too tight (overfitting), the sound is sharp but breaks easily.</li>
<li>Debugging a model is like adjusting each string until harmony is achieved.</li>
</ul>
</section>
<section id="deep-dive-95" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-95">Deep Dive</h4>
<p>Common model issues in imbalanced settings:</p>
<ul>
<li><p>Underfitting</p>
<ul>
<li>Model too simple to capture minority signals.</li>
<li>Symptoms: low training and test performance, especially on minority class.</li>
<li>Fix: more expressive model, better features, non-linear methods.</li>
</ul></li>
<li><p>Overfitting</p>
<ul>
<li>Model memorizes noise, especially synthetic samples (e.g., SMOTE).</li>
<li>Symptoms: high training recall, low test recall.</li>
<li>Fix: stronger regularization, cross-validation, pruning.</li>
</ul></li>
<li><p>Threshold misconfiguration</p>
<ul>
<li>Default 0.5 threshold under-detects minority.</li>
<li>Fix: tune decision thresholds using PR curves.</li>
</ul></li>
<li><p>Probability miscalibration</p>
<ul>
<li>Outputs not trustworthy for decision-making.</li>
<li>Fix: calibration (Platt scaling, isotonic regression).</li>
</ul></li>
<li><p>Algorithm mismatch</p>
<ul>
<li>Using models insensitive to imbalance (e.g., vanilla logistic regression).</li>
<li>Fix: cost-sensitive algorithms, ensembles, anomaly detection.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 33%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Issue</th>
<th>Symptom</th>
<th>Fix</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Underfitting</td>
<td>Low recall &amp; precision</td>
<td>Complex model, feature engineering</td>
</tr>
<tr class="even">
<td>Overfitting</td>
<td>Good train, bad test</td>
<td>Regularization, less synthetic noise</td>
</tr>
<tr class="odd">
<td>Threshold</td>
<td>Poor PR tradeoff</td>
<td>Adjust threshold</td>
</tr>
<tr class="even">
<td>Calibration</td>
<td>Misleading probabilities</td>
<td>Platt/Isotonic scaling</td>
</tr>
<tr class="odd">
<td>Algorithm</td>
<td>Ignores imbalance</td>
<td>Cost-sensitive or ensemble methods</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, threshold debugging)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">2000</span>, n_features<span class="op">=</span><span class="dv">20</span>, weights<span class="op">=</span>[<span class="fl">0.95</span>,<span class="fl">0.05</span>], random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression().fit(X, y)</span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Default threshold</span></span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a>preds_default <span class="op">=</span> model.predict(X)</span>
<span id="cb97-10"><a href="#cb97-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-11"><a href="#cb97-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjusted threshold</span></span>
<span id="cb97-12"><a href="#cb97-12" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> model.predict_proba(X)[:,<span class="dv">1</span>]</span>
<span id="cb97-13"><a href="#cb97-13" aria-hidden="true" tabindex="-1"></a>preds_adjusted <span class="op">=</span> (probs <span class="op">&gt;</span> <span class="fl">0.2</span>).astype(<span class="bu">int</span>)</span>
<span id="cb97-14"><a href="#cb97-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-15"><a href="#cb97-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Default threshold:</span><span class="ch">\n</span><span class="st">"</span>, classification_report(y, preds_default))</span>
<span id="cb97-16"><a href="#cb97-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Adjusted threshold:</span><span class="ch">\n</span><span class="st">"</span>, classification_report(y, preds_adjusted))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-95" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-95">Why it Matters</h4>
<p>Debugging model issues ensures that imbalance-handling strategies actually work. Without it, you risk deploying a system that “looks accurate” but misses critical minority cases.</p>
</section>
<section id="try-it-yourself-95" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-95">Try It Yourself</h4>
<ol type="1">
<li>Train a model with SMOTE data. Check if overfitting occurs.</li>
<li>Tune decision thresholds. Does minority recall improve without oversampling?</li>
<li>Reflect: how can you tell whether poor recall is due to data imbalance vs.&nbsp;underfitting?</li>
</ol>
</section>
</section>
<section id="explainability-tools-in-error-analysis" class="level3">
<h3 class="anchored" data-anchor-id="explainability-tools-in-error-analysis">697. Explainability Tools in Error Analysis</h3>
<p>Explainability tools like SHAP, LIME, and feature importance help uncover <em>why</em> models misclassify cases, especially in the minority class. They turn black-box errors into insights about decision-making.</p>
<section id="picture-in-your-head-96" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-96">Picture in Your Head</h4>
<p>Imagine a doctor misdiagnoses a patient. Instead of just saying “wrong,” we ask:</p>
<ul>
<li>Which symptoms were considered?</li>
<li>Which ones were ignored? Explainability tools act like X-rays for the model’s reasoning process.</li>
</ul>
</section>
<section id="deep-dive-96" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-96">Deep Dive</h4>
<ul>
<li><p>Feature Importance</p>
<ul>
<li>Global view of which features influence predictions.</li>
<li>Tree-based ensembles (Random Forest, XGBoost) provide natural importances.</li>
<li>Risk: may be biased toward high-cardinality features.</li>
</ul></li>
<li><p>LIME (Local Interpretable Model-agnostic Explanations)</p>
<ul>
<li>Approximates model behavior around a single prediction using a simple interpretable model (e.g., linear regression).</li>
<li>Useful for explaining individual misclassifications.</li>
</ul></li>
<li><p>SHAP (SHapley Additive exPlanations)</p>
<ul>
<li>Based on cooperative game theory.</li>
<li>Assigns each feature a contribution value toward the prediction.</li>
<li>Provides both local and global interpretability.</li>
</ul></li>
<li><p>Partial Dependence &amp; ICE (Individual Conditional Expectation) Plots</p>
<ul>
<li>Show how varying a feature influences predictions.</li>
<li>Useful for checking if features affect minority predictions differently.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 15%">
<col style="width: 35%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Tool</th>
<th>Scope</th>
<th>Strength</th>
<th>Limitation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Feature importance</td>
<td>Global</td>
<td>Easy to compute</td>
<td>Can mislead</td>
</tr>
<tr class="even">
<td>LIME</td>
<td>Local</td>
<td>Simple, intuitive</td>
<td>Approximation, unstable</td>
</tr>
<tr class="odd">
<td>SHAP</td>
<td>Local + global</td>
<td>Theoretically sound, consistent</td>
<td>Computationally heavy</td>
</tr>
<tr class="even">
<td>PDP/ICE</td>
<td>Feature trends</td>
<td>Visual insights</td>
<td>Limited to a few features</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, SHAP with XGBoost)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> shap</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> xgboost <span class="im">import</span> XGBClassifier</span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">1000</span>, n_features<span class="op">=</span><span class="dv">10</span>, weights<span class="op">=</span>[<span class="fl">0.9</span>,<span class="fl">0.1</span>], random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> XGBClassifier().fit(X, y)</span>
<span id="cb98-7"><a href="#cb98-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-8"><a href="#cb98-8" aria-hidden="true" tabindex="-1"></a>explainer <span class="op">=</span> shap.TreeExplainer(model)</span>
<span id="cb98-9"><a href="#cb98-9" aria-hidden="true" tabindex="-1"></a>shap_values <span class="op">=</span> explainer.shap_values(X)</span>
<span id="cb98-10"><a href="#cb98-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-11"><a href="#cb98-11" aria-hidden="true" tabindex="-1"></a>shap.summary_plot(shap_values, X)  <span class="co"># visualize feature impact</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-96" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-96">Why it Matters</h4>
<p>In imbalanced learning, explainability reveals <em>why the model misses minority cases</em>. It builds trust, guides feature engineering, and helps domain experts validate model reasoning.</p>
</section>
<section id="try-it-yourself-96" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-96">Try It Yourself</h4>
<ol type="1">
<li>Use SHAP to analyze misclassified minority examples. Which features misled the model?</li>
<li>Compare global vs.&nbsp;local feature importance. Are minority errors explained differently?</li>
<li>Reflect: why might explainability be especially important in healthcare or fraud detection?</li>
</ol>
</section>
</section>
<section id="human-in-the-loop-debugging" class="level3">
<h3 class="anchored" data-anchor-id="human-in-the-loop-debugging">698. Human-in-the-Loop Debugging</h3>
<p>Human-in-the-loop (HITL) debugging integrates expert feedback into the model improvement cycle. Instead of treating ML as fully automated, humans review errors—especially on the minority class—and guide corrections through labeling, feature engineering, or threshold adjustment.</p>
<section id="picture-in-your-head-97" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-97">Picture in Your Head</h4>
<p>Think of a pilot with autopilot on:</p>
<ul>
<li>The system handles routine tasks (majority cases).</li>
<li>But when turbulence (rare events) hits, the human steps in. That partnership ensures safety.</li>
</ul>
</section>
<section id="deep-dive-97" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-97">Deep Dive</h4>
<ul>
<li><p>Error Review</p>
<ul>
<li>Experts inspect false negatives in rare-event detection (fraud cases, rare diseases).</li>
<li>Identify patterns unseen by the model.</li>
</ul></li>
<li><p>Active Learning</p>
<ul>
<li>Model selects uncertain samples for human labeling.</li>
<li>Efficient way to improve minority coverage.</li>
</ul></li>
<li><p>Interactive Thresholding</p>
<ul>
<li>Human feedback sets acceptable tradeoffs between false alarms and misses.</li>
</ul></li>
<li><p>Domain Knowledge Injection</p>
<ul>
<li>Rules or constraints added to models (e.g., “flag any transaction &gt; $10,000 from new accounts”).</li>
</ul></li>
<li><p>Iterative Loop</p>
<ol type="1">
<li>Train model.</li>
<li>Human reviews errors.</li>
<li>Correct labels, add rules, tune thresholds.</li>
<li>Retrain and repeat.</li>
</ol></li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>HITL Role</th>
<th>Contribution</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Labeler</td>
<td>Improves minority ground truth</td>
</tr>
<tr class="even">
<td>Analyst</td>
<td>Interprets false positives/negatives</td>
</tr>
<tr class="odd">
<td>Domain Expert</td>
<td>Injects contextual rules</td>
</tr>
<tr class="even">
<td>Operator</td>
<td>Sets thresholds based on risk tolerance</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, simulate active learning loop)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">500</span>, n_features<span class="op">=</span><span class="dv">10</span>, weights<span class="op">=</span>[<span class="fl">0.9</span>,<span class="fl">0.1</span>], random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression().fit(X[:<span class="dv">400</span>], y[:<span class="dv">400</span>])</span>
<span id="cb99-7"><a href="#cb99-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-8"><a href="#cb99-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Model uncertainty = probs near 0.5</span></span>
<span id="cb99-9"><a href="#cb99-9" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> model.predict_proba(X[<span class="dv">400</span>:])[:,<span class="dv">1</span>]</span>
<span id="cb99-10"><a href="#cb99-10" aria-hidden="true" tabindex="-1"></a>uncertain_idx <span class="op">=</span> np.argsort(np.<span class="bu">abs</span>(probs <span class="op">-</span> <span class="fl">0.5</span>))[:<span class="dv">10</span>]</span>
<span id="cb99-11"><a href="#cb99-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-12"><a href="#cb99-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Samples for human review:"</span>, uncertain_idx)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-97" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-97">Why it Matters</h4>
<p>HITL debugging makes imbalanced learning practical and trustworthy. Automated systems alone may miss rare but critical cases; human review ensures these gaps are caught and fed back for improvement.</p>
</section>
<section id="try-it-yourself-97" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-97">Try It Yourself</h4>
<ol type="1">
<li>Identify uncertain predictions in your model. Would human review help resolve them?</li>
<li>Simulate active learning with iterative labeling. Does minority recall improve faster?</li>
<li>Reflect: in which domains (finance, healthcare, security) is HITL essential rather than optional?</li>
</ol>
</section>
</section>
<section id="evaluation-under-distribution-shift" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-under-distribution-shift">699. Evaluation under Distribution Shift</h3>
<p>A model trained on one data distribution may fail when the test or deployment data shifts—a common problem in imbalanced settings, where the minority class changes faster than the majority. Evaluating under distribution shift ensures robustness beyond static datasets.</p>
<section id="picture-in-your-head-98" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-98">Picture in Your Head</h4>
<p>Imagine training a guard dog:</p>
<ul>
<li>It learns to bark at thieves wearing masks.</li>
<li>But if thieves stop wearing masks, the dog might stay silent. That’s a distribution shift—the world changes, and old rules stop working.</li>
</ul>
</section>
<section id="deep-dive-98" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-98">Deep Dive</h4>
<ul>
<li><p>Types of shifts</p>
<ul>
<li>Covariate shift: Input distribution <span class="math inline">\(P(X)\)</span> changes, but <span class="math inline">\(P(Y|X)\)</span> stays the same.</li>
<li>Prior probability shift: Class proportions change (e.g., fraud rate rises from 1% → 5%).</li>
<li>Concept drift: The relationship <span class="math inline">\(P(Y|X)\)</span> itself changes (new fraud tactics).</li>
</ul></li>
<li><p>Detection methods</p>
<ul>
<li>Statistical tests (e.g., KS-test, chi-square) to compare distributions.</li>
<li>Drift detectors (ADWIN, DDM) in streaming data.</li>
<li>Monitoring calibration over time.</li>
</ul></li>
<li><p>Evaluation strategies</p>
<ul>
<li>Train/validation split across time (temporal validation).</li>
<li>Stress testing with simulated shifts (downsampling, oversampling).</li>
<li>Domain adaptation evaluation (source vs.&nbsp;target domain).</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 40%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th>Shift Type</th>
<th>Example</th>
<th>Mitigation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Covariate</td>
<td>New customer demographics</td>
<td>Reweight training samples</td>
</tr>
<tr class="even">
<td>Prior prob.</td>
<td>More fraud cases in crisis</td>
<td>Update thresholds</td>
</tr>
<tr class="odd">
<td>Concept drift</td>
<td>New fraud techniques</td>
<td>Online/continual learning</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, KS-test for drift)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> ks_2samp</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate old vs. new feature distributions</span></span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a>old_data <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1000</span>)</span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a>new_data <span class="op">=</span> np.random.normal(<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">1000</span>)</span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-8"><a href="#cb100-8" aria-hidden="true" tabindex="-1"></a>stat, pval <span class="op">=</span> ks_2samp(old_data, new_data)</span>
<span id="cb100-9"><a href="#cb100-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"KS test stat:"</span>, stat, <span class="st">"p-value:"</span>, pval)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-98" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-98">Why it Matters</h4>
<p>Ignoring distribution shift leads to silent model decay—performance metrics look fine offline but collapse in deployment. In fraud, healthcare, or cybersecurity, this means missing rare but evolving threats.</p>
</section>
<section id="try-it-yourself-98" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-98">Try It Yourself</h4>
<ol type="1">
<li>Perform temporal validation on your dataset. Does performance degrade over time?</li>
<li>Simulate a prior probability shift (change minority ratio) and measure impact.</li>
<li>Reflect: how would you set up continuous monitoring for drift in your production system?</li>
</ol>
</section>
</section>
<section id="best-practices-and-case-studies" class="level3">
<h3 class="anchored" data-anchor-id="best-practices-and-case-studies">700. Best Practices and Case Studies</h3>
<p>Effective model evaluation in imbalanced learning requires a toolbox of best practices that combine metrics, threshold tuning, calibration, and monitoring. Real-world case studies highlight how practitioners adapt evaluation to domain-specific needs.</p>
<section id="picture-in-your-head-99" class="level4">
<h4 class="anchored" data-anchor-id="picture-in-your-head-99">Picture in Your Head</h4>
<p>Think of running a hospital emergency room:</p>
<ul>
<li>You don’t just track how many patients you treated (accuracy).</li>
<li>You monitor survival rates, triage speed, and error reports. Evaluation in ML is the same: multiple signals together give a true picture of success.</li>
</ul>
</section>
<section id="deep-dive-99" class="level4">
<h4 class="anchored" data-anchor-id="deep-dive-99">Deep Dive</h4>
<ul>
<li><p>Best Practices</p>
<ul>
<li>Always use confusion-matrix-derived metrics (precision, recall, F1, PR-AUC).</li>
<li>Tune thresholds for cost-sensitive tradeoffs.</li>
<li>Evaluate calibration curves to check probability reliability.</li>
<li>Use temporal validation for non-stationary domains.</li>
<li>Report per-class performance, not just overall scores.</li>
<li>Perform error analysis with explainability tools.</li>
<li>Set up continuous monitoring for drift in deployment.</li>
</ul></li>
<li><p>Case Studies</p>
<ul>
<li><p>Fraud detection (finance):</p>
<ul>
<li>PR-AUC as main metric.</li>
<li>Cost-sensitive boosting with human-in-the-loop alerts.</li>
</ul></li>
<li><p>Medical diagnosis (healthcare):</p>
<ul>
<li>Prioritize recall.</li>
<li>HITL review for high-uncertainty cases.</li>
<li>Calibration checked before deployment.</li>
</ul></li>
<li><p>Industrial fault detection (IoT):</p>
<ul>
<li>One-class anomaly detection.</li>
<li>Thresholds tuned to minimize false alarms while catching rare breakdowns.</li>
</ul></li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 31%">
<col style="width: 20%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th>Domain</th>
<th>Primary Metric</th>
<th>Special Practices</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Finance (fraud)</td>
<td>PR-AUC</td>
<td>Threshold tuning + HITL</td>
</tr>
<tr class="even">
<td>Healthcare (diagnosis)</td>
<td>Recall</td>
<td>Calibration + expert review</td>
</tr>
<tr class="odd">
<td>Industry (faults)</td>
<td>F1 / Precision</td>
<td>One-class methods + alarm filters</td>
</tr>
</tbody>
</table>
<p>Tiny Code Recipe (Python, evaluation pipeline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report, average_precision_score</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate_model(model, X, y):</span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> model.predict_proba(X)[:,<span class="dv">1</span>]</span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> (probs <span class="op">&gt;</span> <span class="fl">0.3</span>).astype(<span class="bu">int</span>)  <span class="co"># tuned threshold</span></span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y, preds))</span>
<span id="cb101-7"><a href="#cb101-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"PR-AUC:"</span>, average_precision_score(y, probs))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-99" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-99">Why it Matters</h4>
<p>Best practices make the difference between a model that <em>looks good offline</em> and one that saves money, lives, or safety in deployment. Evaluating with care is the cornerstone of trustworthy AI in imbalanced domains.</p>
</section>
<section id="try-it-yourself-99" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-99">Try It Yourself</h4>
<ol type="1">
<li>Pick an imbalanced dataset and set up an evaluation pipeline with PR-AUC, F1, and calibration.</li>
<li>Simulate drift and track metrics over time. Which metric degrades first?</li>
<li>Reflect: in your domain, which “best practice” is non-negotiable before deployment?</li>
</ol>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../books/en-US/volume_6.html" class="pagination-link" aria-label="Volume 6. Probabilistic Modeling and Inference">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Volume 6. Probabilistic Modeling and Inference</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../books/en-US/volume_8.html" class="pagination-link" aria-label="Volume 8. Supervised Learning Systems">
        <span class="nav-page-text"><span class="chapter-title">Volume 8. Supervised Learning Systems</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>